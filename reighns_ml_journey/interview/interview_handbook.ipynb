{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28eba63b-b34e-4306-8c21-cee9890eba26",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\F}{\\mathbb{F}}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\a}{\\mathbf{a}}\n",
    "\\newcommand{\\b}{\\mathbf{b}}\n",
    "\\newcommand{\\betaa}{\\mathbf{\\beta}}\n",
    "\\newcommand{\\c}{\\mathbf{c}}\n",
    "\\newcommand{\\u}{\\mathbf{u}}\n",
    "\\newcommand{\\v}{\\mathbf{v}}\n",
    "\\newcommand{\\w}{\\mathbf{w}}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\z}{\\mathbf{z}}\n",
    "\\newcommand{\\A}{\\mathbf{A}}\n",
    "\\newcommand{\\B}{\\mathbf{B}}\n",
    "\\newcommand{\\C}{\\mathbf{C}}\n",
    "\\newcommand{\\X}{\\mathbf{X}}\n",
    "\\newcommand{\\rank}{\\textbf{rank}}\n",
    "\\newcommand{\\0}{\\mathbf{0}}\n",
    "\\newcommand{\\1}{\\mathbf{1}}\n",
    "\\newcommand{\\U}{\\mathrm{U}}\n",
    "\\newcommand{\\V}{\\mathrm{V}}\n",
    "\\newcommand{\\W}{\\mathrm{W}}\n",
    "\\newcommand{\\L}{\\mathcal{L}}\n",
    "\\newcommand{\\P}{\\mathbb{P}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e6e1c7-f72a-414d-85d2-64620fe176b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Why is Softmax Layer Monotonic? \n",
    "\n",
    "- https://stackoverflow.com/questions/56096598/is-softmax-used-when-only-the-most-probable-class-will-be-used#:~:text=But%20softmax%20is%20a%20monotonic,layer%2C%20leaving%20the%20softmax%20out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11f976f-05e2-4f65-a92a-ae4053e6a4e9",
   "metadata": {},
   "source": [
    "## Data Imbalance\n",
    "\n",
    "- https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/\n",
    "\n",
    "### Robinhood Question [^Kevin Huo & Nick Singh: Ace The Data Science Interview, 2021. (pp. 118)]\n",
    "\n",
    "- First, check if we can collect more data for the minority class, this may not always be feasible, but worth a try to ask first.\n",
    "- Make sure the metrics defined for such problem is sensible, for example, using accuracy is not good and one should explain why this is the case by using the `ZeroR` example. One should also mention that AUROC, precision-recall metrics that are not so misleading.\n",
    "- Over/Under sampling: delete majority class randomly till convergence to desired distribution, or resample with replacement minority class,\n",
    "- Smote\n",
    "\n",
    "[^Kevin Huo & Nick Singh: Ace The Data Science Interview, 2021. (pp. 118)]: **Kevin Huo & Nick Singh: Ace The Data Science Interview, 2021. (pp. 118)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a3aa84-fd21-4e47-a668-6baab0701384",
   "metadata": {},
   "source": [
    "## Bias-Variance Tradeoff\n",
    "\n",
    "- Definition\n",
    "- Intuition and Examples\n",
    "- My code on understanding the math and stats behind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588ee662-56f0-4eaf-ac96-a354c6eb42fa",
   "metadata": {},
   "source": [
    "### Association with Overfit-Underfit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075c02e3-d4a6-45af-9946-a153e735f317",
   "metadata": {},
   "source": [
    "### How to tackle Bias-Variance Tradeoff\n",
    "\n",
    "In the Ace the data science interview book, author mentioned how to approach case specific question.\n",
    "\n",
    "Example\n",
    "\n",
    "- high bias -> increase model complexity\n",
    "- high var -> increase data etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a589ca8-e9ef-453f-ad59-f8ae43cfe96a",
   "metadata": {},
   "source": [
    "## Overfitting and Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a8d6e2-e6d1-49ac-a1ce-4564238fd0c6",
   "metadata": {},
   "source": [
    "### What is overfitting?\n",
    "\n",
    "- https://sebastianraschka.com/faq/docs/overfitting.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c510937c-ae1d-4836-8959-c0ba2cf3a596",
   "metadata": {},
   "source": [
    "### How to tackle Overfitting?\n",
    "\n",
    "#### Regularization\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceecfbd-2dd4-419a-b286-b33eeae31988",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4ae15f-4e22-4ae3-9b8c-9d53f28bc84a",
   "metadata": {},
   "source": [
    "### Briefly desribe Regularization in Logistic Regression\n",
    "\n",
    "- https://sebastianraschka.com/faq/docs/regularized-logistic-regression-performance.html\n",
    "\n",
    "The visualization in the example is insane coupled with clear and intuitive explanation! Initially, you see the global minimum of the convex function is in the center of the graph, which is unregularized, we want to make the model a bit less \"accurate\" so we can have it generalize, so you see the small circle there is the \"L2-term\", now you can still find minimum, but with constraint to be in the small L2 circle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6563170-a199-446d-97cc-1a338d3dd2da",
   "metadata": {},
   "source": [
    "### Why is the L1 constraint a diamond and L2 constraint a circle?\n",
    "\n",
    "Consider a naive vector $\\v = \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\in \\R^2$. Assume all vectors start from the origin.\n",
    "\n",
    "Define L2 norm as $\\Vert \\v \\Vert_2 = \\sqrt{x^2 + y^2}$ and then the **set of vectors (coordinates)** that satisfies this definition of L2 norm can be rewritten as:\n",
    "\n",
    "$$\n",
    "\\left\\{(x, y) ~|~ x^2+y^2 = r^2 \\right\\}\n",
    "$$\n",
    "\n",
    "where this definition means that if $\\Vert \\v \\Vert_2 = 1$, then all vectors $\\v$ having this norm must lie on the unit circle.\n",
    "\n",
    "\n",
    "See the image from ISLR, consider a simple linear equation with two independent variables $\\y = \\beta_1 \\x_1 + \\beta_2 \\x_2$, then the beta vector $\\betaa = \\begin{bmatrix} \\beta_1 \\\\ \\beta_2 \\end{bmatrix}$ not has to fulfil a constraint such that say for example L2 norm (ridge) of $\\betaa$ must be less than or equals to $r = 1$. Geomtrically, you can see the circle that the $\\betaa$ must fall within and note that it may or may not contain the \"true optimal $\\betaa$\".\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/interview/misc_and_untidied/reg.PNG\" style=\"margin-left:auto; margin-right:auto\"/>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>ISLR</b>\n",
    "</p>\n",
    "\n",
    "\n",
    "**See pp.242-244 from ISLR for more intuition**.\n",
    "\n",
    "- https://medium.com/uwaterloo-voice/a-deep-dive-into-regularization-eec8ab648bce\n",
    "- https://www.mathworks.com/matlabcentral/answers/347583-how-to-plot-the-l2-norm-circle\n",
    "- https://medium.com/@kiprono_65591/regularization-a-technique-used-to-prevent-over-fitting-886d5b361700\n",
    "- https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e716f21b-7af1-4bac-b923-6ba91c0292e7",
   "metadata": {},
   "source": [
    "## Preprocessing Techniques\n",
    "\n",
    "- should also check cs3244."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3c5cb1-6942-4560-87f3-4b929564b86e",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "- An article on which models may need scale: https://www.thekerneltrip.com/statistics/when-scale-my-data/\n",
    "- Sebastian god: https://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n",
    "- More math: https://stats.stackexchange.com/questions/29781/when-conducting-multiple-regression-when-should-you-center-your-predictor-varia\n",
    "- https://stats.stackexchange.com/questions/48360/is-standardization-needed-before-fitting-logistic-regression says If you use logistic regression with LASSO or ridge regression (as Weka Logistic class does) you should. As Hastie,Tibshirani and Friedman points out (page 82 of the pdf or at page 63 of the book): The ridge solutions are not equivariant under scaling of the inputs, and so one normally standardizes the inputs before solving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3661c27-b4ab-4633-8978-402255204086",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "- https://stats.stackexchange.com/questions/288095/what-algorithms-require-one-hot-encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772a12b4-47e8-4d2c-b485-4a5470551328",
   "metadata": {},
   "source": [
    "## Model Selection, Evaluation\n",
    "\n",
    "- Must read: Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning by Seb\n",
    "\n",
    "Some summary from the paper.\n",
    "\n",
    "- 1.1 Performance Estimation: Generalization Performance vs. Model Selection\n",
    "    - Why evaluate the predictive performance of a model?\n",
    "    - Relative performance is accurate if we assume bias affects all the models we trained.\n",
    "    - For example, if we trained 3 models, $M_1, M_2, M_3$, and their validation accuracy is ranked as $M_2: 75\\% > M_1: 70\\% > M_3: 65\\%$, then if we add pessismistic bias of $10\\%$, the three models still ranked in the same order relatively: $M_2: 65\\% > M_1: 60\\% > M_3: 55\\%$. \n",
    "    - But do not think a $65\\%$ validation accuracy translates to $65\\%$ accuracy on the unseen test set.\n",
    "    \n",
    "- 1.2 Assumptions and Terminology\n",
    "- 1.5 Holdout validation: v impt idea\n",
    "- 3.9: Talks about whether feature selection in cv loop or out, data leak or ok.\n",
    "- section 4: Hypothesis testing and algorithm comparison!\n",
    "\n",
    "### What is Model Selection?\n",
    "\n",
    "The process of finding the best-performing model from a set of models that were produced by different hyperparameter settings is called model selection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984c6e88-bc02-4394-8927-adb5f793f603",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "- https://realpython.com/gradient-descent-algorithm-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba2158b-030d-40ce-b556-9e0b33c66261",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7dfdd2-79b6-4ab5-96ad-2d660b371451",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8a291b-74db-4923-8987-515f5d1b3517",
   "metadata": {},
   "source": [
    "### Exploding and Vanishing Gradients\n",
    "\n",
    "- https://www.analyticsvidhya.com/blog/2021/06/the-challenge-of-vanishing-exploding-gradients-in-deep-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ae0c67-d3d0-47b3-80ce-f1ff9b0ef14d",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de174bb6-d049-4ae1-9108-97852f68b474",
   "metadata": {},
   "source": [
    "### What models produce well calibrated probabilities?\n",
    "\n",
    "- https://stats.stackexchange.com/questions/208867/why-does-logistic-regression-produce-well-calibrated-models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21925592-03b4-4e95-81c8-828f2f883504",
   "metadata": {},
   "source": [
    "### Why do we sometimes use \"multi-head\" to evaluate multi-label classification?\n",
    "\n",
    "- https://debuggercafe.com/multi-head-deep-learning-models-for-multi-label-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612ffbd9-cd6c-4e54-aad4-0bd06391edf0",
   "metadata": {},
   "source": [
    "### When should one use Sigmoid or Softmax in a Neural Network?\n",
    "\n",
    "> [Sigmoid vs Softmax](https://stats.stackexchange.com/questions/233658/softmax-vs-sigmoid-function-in-logistic-classifier) I've noticed people often get directed to this question when searching whether to use sigmoid vs softmax in neural networks. If you are one of those people building a neural network classifier, here is how to decide whether to apply sigmoid or softmax to the raw output values from your network:\n",
    "\n",
    "- If you have a multi-label classification problem = there is more than one \"right answer\" = the outputs are NOT mutually exclusive, then use a sigmoid function on each raw output independently. The sigmoid will allow you to have high probability for all of your classes, some of them, or none of them. Example: classifying diseases in a chest x-ray image. The image might contain pneumonia, emphysema, and/or cancer, or none of those findings.\n",
    "\n",
    "---\n",
    "\n",
    "- If you have a multi-class classification problem = there is only one \"right answer\" = the outputs are mutually exclusive, then use a softmax function. The softmax will enforce that the sum of the probabilities of your output classes are equal to one, so in order to increase the probability of a particular class, your model must correspondingly decrease the probability of at least one of the other classes. Example: classifying images from the MNIST data set of handwritten digits. A single picture of a digit has only one true identity - the picture cannot be a 7 and an 8 at the same time.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# micro-averaged AUC\n",
    "roc_auc_score(y_true.flatten(), y_pred.flatten())\n",
    "# macro-averaged AUC\n",
    "np.mean([roc_auc_score(y_true[:, i], y_pred[:, i]) for i in range(11)])\n",
    "```\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/multiclass.html#multilabel-classification\n",
    "- https://glassboxmedicine.com/2019/05/26/classification-sigmoid-vs-softmax/\n",
    "- https://scikit-learn.org/stable/modules/multiclass.html#multilabel-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e969a8b-7a31-4d3f-adcf-477c3ead84df",
   "metadata": {},
   "source": [
    "## Probability and Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52cec47-1384-472c-a641-7abdbea4705c",
   "metadata": {},
   "source": [
    "### Why do you multiply probabilities? \n",
    "\n",
    "Also appear in Chapter 2 summary.\n",
    "\n",
    "Consider rolling a dice twice, what is the probability that you roll a 5 and 6 respectively.\n",
    "\n",
    "We all know the answer is $\\dfrac{1}{6} \\times \\dfrac{1}{6} = \\dfrac{1}{36}$. But why?\n",
    "\n",
    "This can be first understood that our denominator is the total outcomes in our sample space $\\S$. This is $36$, why? By our counting principle on multiplication, we know that if we have $6$ choices in roll $1$ and $6$ choices in roll 2, then the cross-product is $6 \\times 6 = 36$ total choices. One can enumerate $\\{(1,1), (1,2), \\ldots, (6,6)\\}$ to see why.\n",
    "\n",
    "Now the numerator is also related to the counting principle of multiplication as well! In roll 1, rolling a 5 is 1 choice, rolling a 6 next is 1 choice, so total there is a only one combination choice $1 \\times 1$!\n",
    "\n",
    "Now if we reframe the problem to what is the probability that you roll a 1, 2 or 3 in the first roll and 2 or 3 in the second roll. Then of course our denominator don't change as $36$, but our numerator changes, since in roll 1 we have 3 choices, and roll 2 have 2 choices, by the multiplicative principle we have a total of $3 \\times 2 = 6$ choices, and so our probability is $\\dfrac{6}{36}$ now. You can verify that there are indeed $6$ choices manually.\n",
    "\n",
    "> **Now the most important part is we can use this if both events are independent! If not we need to be careful!**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4266afe5-82f5-4df5-9e54-84a3fc72fb6e",
   "metadata": {},
   "source": [
    "### What is Priori, Posterior and Conditional\n",
    "\n",
    "Common terms in ML world.\n",
    "\n",
    "Suppose there are three types of players in a tennis tournament: A, B, and C. Fifty percent of the contestants in the tournament are A players, 25% are B players, and 25% are C players. Your chance of beating the contestants depends on the class of the player, as follows:\n",
    "\n",
    "- 0.3 against an A player\n",
    "- 0.4 against a B player\n",
    "- 0.5 against a C player\n",
    "\n",
    "If you play a match in this tournament, what is the probability of your winning the\n",
    "match? Supposing that you have won a match, what is the probability that you played\n",
    "against an A player?\n",
    "\n",
    "Let $W$ be the event that you win and $A$ be the event that you played vs player $A$, then\n",
    "\n",
    "- Conditional: $\\P(W~|~A)$ = given you played player $A$, what is your probability of winning?\n",
    "- Priori: $\\P(A)$ = **without entering the game**, what is your probability of facing player $A$?\n",
    "- Posterior: $\\P(A~|~W)$ = **after entering the game and winning the match**, what is your probability that you have actually played with $A$?\n",
    "- Machine Learning: In many practical engineering problems, the question of interest is often the last one. That\n",
    "is, supposing that you have observed something, what is the most likely cause of that event?\n",
    "For example, supposing we have observed this particular dataset, what is the best Gaussian\n",
    "model that would fit the dataset? Questions like these require some analysis of conditional\n",
    "probability, prior probability, and posterior probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fab22fb-c213-44c6-ab30-ff1560f2adc2",
   "metadata": {},
   "source": [
    "## Generic Questions\n",
    "\n",
    "Usually generic and fundamental questions. These questions are \"less code oriented\", and usually requires a short and sweet answer, alongside with an example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f999bd31-9d8e-4036-bb08-0f8887eee913",
   "metadata": {},
   "source": [
    "### What are common pitfalls that lead to **Data Leakage** in a pipeline?\n",
    "\n",
    "- https://scikit-learn.org/stable/common_pitfalls.html\n",
    "- See also numerous question I asked on cross-validated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dac5461-a3cd-4eaa-ab17-ea9647eb1153",
   "metadata": {},
   "source": [
    "### What is the meaning of \"Train/Test\" Distribution in the context of Machine Learning?\n",
    "\n",
    "- https://towardsdatascience.com/understanding-entropy-the-golden-measurement-of-machine-learning-4ea97c663dc3#:~:text=By%20using%20entropy%20in%20machine,be%20desired%20in%20model%2Dbuilding.\n",
    "- https://www.kdnuggets.com/2019/01/when-your-training-testing-data-different-distributions.html\n",
    "\n",
    "\n",
    "Let us talk this in a less formal way without invoking formal definitions of \"probability distributions\".\n",
    "\n",
    "#### Train and Test Distribution\n",
    "\n",
    "Let us say we are imposed a Machine Learning problem to predict cats vs dogs. We are handed a training set, and there is also a hidden test set that we do not know of. Let us pretend we browse through each image in the training set $\\mathcal{D}$, and plot out the pixel distribution of each image, we observe that each image pixel distribution follows a gaussian curve with mean 0 and std 1 (i.e. each image in $\\mathcal{D}$ is generated from $\\mathcal{N}(0,1)$). Then we trained a good model using training set, but performed badly on the test set $\\mathcal{D}_{test}$, and after we also plot the pixel density of the test set, we discovered that this set follows a very different distributions, say follow $\\mathcal{N}(255, 255)$, and each image has 10 times the resolution of the training set. We can therefore conclude, that, both train and test set have different \"probability distributions\".\n",
    "\n",
    "---\n",
    "\n",
    "#### Distribution from your model\n",
    "\n",
    "Let us also continue our example from the cat vs dog, this time we assume both train, test and every other images for dogs and cats are generated from $\\mathcal{N}(0, 1)$. That is to say, we **know** our population distribution, and if we do know, then why do we need a Machine Learning model at all? We can just approximate all images with the known population distribution with the parameters. Even simpler, if we have a dataset that is beknownst to us with a perfect linear relationship that can be predicted perfectly by a simple linear regression model, (i.e. the population parameter is known), then why do we even need linear regression?\n",
    "\n",
    "However, this world is far from perfect, and we do not know the population parameters of the underlying distribution of the dataset given to us, hence we use models to predict it.\n",
    "\n",
    "Thus, we can analogously say that if a training set $\\mathcal{D}$ follows a distribution $p$, then our model follows a distribution $q$, where in the simplest case in linear regression, we know our $q$ follows a distribution with 2 parameters, $m$ the slope, $c$ the bias.\n",
    "\n",
    "With that in mind, our machine learning models can be phrased as using $q$ to approximate $q$. And this is often used in Cross-Entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89aae7b-40ed-4ee2-9f3e-53636196aaa4",
   "metadata": {},
   "source": [
    "### Parametric and Non-Parametric Models.\n",
    "\n",
    "- ISLR (p21-24)\n",
    "- https://stats.stackexchange.com/questions/268638/what-exactly-is-the-difference-between-a-parametric-and-non-parametric-model\n",
    "- https://sebastianraschka.com/faq/docs/parametric_vs_nonparametric.html\n",
    "\n",
    "> https://sebastianraschka.com/faq/docs/parametric_vs_nonparametric.html:\n",
    "> The term “non-parametric” might sound a bit confusing at first: non-parametric does not mean that they have NO parameters! On the contrary, non-parametric models (can) become more and more complex with an increasing amount of data.\n",
    "\n",
    "> So, in a parametric model, we have a finite number of parameters, and in nonparametric models, the number of parameters is (potentially) infinite. Or in other words, in nonparametric models, the complexity of the model grows with the number of training data; in parametric models, we have a fixed number of parameters (or a fixed structure if you will).\n",
    "\n",
    "> Linear models such as linear regression, logistic regression, and linear Support Vector Machines are typical examples of a parametric “learners;” here, we have a fixed size of parameters (the weight coefficient.) In contrast, K-nearest neighbor, decision trees, or RBF kernel SVMs are considered as non-parametric learning algorithms since the number of parameters grows with the size of the training set. – K-nearest neighbor and decision trees, that makes sense, but why is an RBF kernel SVM non-parametric whereas a linear SVM is parametric? In the RBF kernel SVM, we construct the kernel matrix by computing the pair-wise distances between the training points, which makes it non-parametric.\n",
    "\n",
    "> In the field of statistics, the term parametric is also associated with a specified probability distribution that you “assume” your data follows, and this distribution comes with the finite number of parameters (for example, the mean and standard deviation of a normal distribution); you don’t make/have these assumptions in non-parametric models. So, in intuitive terms, we can think of a non-parametric model as a “distribution” or (quasi) assumption-free model.\n",
    "\n",
    "> However, keep in mind that the definitions of “parametric” and “non-parametric” are “a bit ambiguous” at best; according to the “The Handbook of Nonparametric Statistics 1 (1962) on p. 2: “A precise and universally acceptable definition of the term ‘nonparametric’ is not presently available. The viewpoint adopted in this handbook is that a statistical procedure is of a nonparametric type if it has properties which are satisfied to a reasonable approximation when some assumptions that are at least of a moderately general nature hold.\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b240eb80-13a6-4be7-837d-fd984ece43a3",
   "metadata": {},
   "source": [
    "### Supervised and Unsupervised Learning.\n",
    "\n",
    "- ISLR (p26-28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31a8346-a9ce-4cec-9c14-24110cdf6d21",
   "metadata": {},
   "source": [
    "### Tradeoff between Flexibility and Interpretability of Machine Learning Models.\n",
    "\n",
    "- ISLR (p24-26)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5555edd7-4853-4672-b7e1-cc94460d160d",
   "metadata": {},
   "source": [
    "### Reducible and Irreducible Errors.\n",
    "\n",
    "- ISLR (p18-19)\n",
    "\n",
    "#### Definition\n",
    "\n",
    "Formal Definition\n",
    "\n",
    "#### Example (Intuition)\n",
    "\n",
    "- We can use features $\\X$ to predict $\\y$, for example, use **age of house, number of bedrooms, etc** to predict the **price of the house**.\n",
    "- The **irreducible errors** can be random events such as Elon Musk tweeting something good about a particular house, which our model cannot account for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d26e8a-7c8e-4ada-a156-8c320d598434",
   "metadata": {},
   "source": [
    "### Describe Ensembling in laymen terms.\n",
    "\n",
    "**François Chollet**(Creator of Keras) explained it as : Ensembling relies on the assumption that different good models trained independently are likely to be good for different reasons: each model looks at slightly different aspects of the data to make its predictions, getting part of the “truth” but not all of it. You may be familiar with the ancient parable of the blind men and the elephant: a group of blind men come across an elephant for the first time and try to understand what the elephant is by touching it. Each man touches a different part of the elephant’s body—just one part, such as the trunk or a leg. Then the men describe toeach other what an elephant is: “It’s like a snake,” “Like a pillar or a tree,” and so on.The blind men are essentially machine-learning models trying to understand the manifold of the training data, each from its own perspective, using its own assumptions(provided by the unique architecture of the model and the unique random weight initialization). Each of them gets part of the truth of the data, but not the whole truth. By pooling their perspectives together, you can get a far more accurate description of the data. The elephant is a combination of parts: not any single blind man gets it quiteright, but, interviewed together, they can tell a fairly accurate story.\n",
    "\n",
    "Let’s use classification as an example. The easiest way to pool the predictions of a setof classifiers (to ensemble the classifiers) is to average their predictions at inference time:Use four different models to compute initial predictions.\n",
    "\n",
    "```\n",
    "preds_a = model_a.predict(x_val)\n",
    "preds_b = model_b.predict(x_val)\n",
    "preds_c = model_c.predict(x_val)\n",
    "preds_d = model_d.predict(x_val)\n",
    "\n",
    "```\n",
    "\n",
    "This new prediction array should be more accurate than any of the initial ones.\n",
    "\n",
    "`final_preds = 0.25 * (preds_a + preds_b + preds_c + preds_d)`\n",
    "\n",
    "This will work only if the classifiers are more or less equally good. If one of them is significantly worse than the others, the final predictions may not be as good as the best classifier of the group.A smarter way to ensemble classifiers is to do a weighted average, where the weights are learned on the validation data typically, the better classifiers are given a higher weight, and the worse classifiers are given a lower weight. To search for a good set of ensembling weights, you can use random search or a simple optimization algorithm such as Nelder-Mead:\n",
    "\n",
    "```\n",
    "preds_a = model_a.predict(x_val)\n",
    "preds_b = model_b.predict(x_val)\n",
    "preds_c = model_c.predict(x_val)\n",
    "preds_d = model_d.predict(x_val)\n",
    "\n",
    "```\n",
    "\n",
    "These weights (0.5, 0.25,0.1, 0.15) are assumed to be learned empirically.`final_preds = 0.5 * preds_a + 0.25 * preds_b + 0.1 * preds_c + 0.15 * preds_d`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748afc29-7d6e-4820-ad88-9b2cb7b5b304",
   "metadata": {},
   "source": [
    "## Ensembling Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff6ccb7-612b-45aa-adcf-73a8286b3ba1",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce1c753-5b0b-4702-b12f-8fee72a650b0",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b38bfe8-b331-4b66-8be4-e99155bc864c",
   "metadata": {},
   "source": [
    "### Why does Cross-Validation reduce Variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf492f5-4568-4651-b494-2496e593b4c1",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "- https://www.ritchieng.com/applying-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8ed80b-f7ce-4176-87b6-0e88809751d0",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7e4632-453a-43f1-b6e1-ad1bac450d95",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Polynomial Regression\n",
    "\n",
    "- https://zerowithdot.com/polynomial-regression-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99716bf4-c97d-4aa7-9903-e9dec63238ab",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f412956f-a031-4ecd-b817-642d89d67daa",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452f3603-1c4a-4218-941d-eeb1e6e7045a",
   "metadata": {},
   "source": [
    "### Describe DT briefly\n",
    "\n",
    "- Suprvised Learning: will need corresponding label\n",
    "- Non-parametric\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b0e037-2b74-4b6b-b2f2-f15ec5a0a6e2",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fa5d42d-8af5-48bd-98fa-321514a9d804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Seed Number 1992\n"
     ]
    }
   ],
   "source": [
    "# !pip install torchinfo\n",
    "import timm\n",
    "import torch\n",
    "import torchvision\n",
    "from typing import Dict, Union, Callable, OrderedDict, Tuple\n",
    "import os, random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "\n",
    "def seed_all(seed: int = 1992) -> None:\n",
    "    \"\"\"Seed all random number generators.\"\"\"\n",
    "    print(f\"Using Seed Number {seed}\")\n",
    "\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)  # set PYTHONHASHSEED env var at fixed value\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)  # pytorch (both CPU and CUDA)\n",
    "    np.random.seed(seed)  # for numpy pseudo-random generator\n",
    "    # set fixed value for python built-in pseudo-random generator\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "\n",
    "\n",
    "def seed_worker(_worker_id) -> None:\n",
    "    \"\"\"Seed a worker with the given ID.\"\"\"\n",
    "    worker_seed = torch.initial_seed() % 2 ** 32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "\n",
    "seed_all(seed=1992)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60485634-09ce-4d97-b100-e99f706098e7",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5552fc-d22c-4e55-9544-66836b656881",
   "metadata": {},
   "source": [
    "#### What is the role of zero padding?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2533e0aa-5618-4beb-801e-04316dd3f5ea",
   "metadata": {},
   "source": [
    "### Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fbe715-0bb5-4b83-90ec-8e860e2923fb",
   "metadata": {},
   "source": [
    "#### Global Average Pooling\n",
    "\n",
    "- https://rwightman.github.io/pytorch-image-models/feature_extraction/#penultimate-layer-features-pre-classifier-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "16b48559-3ef2-44a7-95b5-f021485fc000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "\n",
    "model = timm.create_model('resnet18', pretrained=True, num_classes=1000)\n",
    "batch_size = 2\n",
    "image_shape = (3, 224, 224)\n",
    "input_image_tensor = torch.rand(size=(batch_size, *image_shape))\n",
    "\n",
    "\n",
    "# print(summary(model, (2, 3, 224, 224)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ffdded-88f1-48e6-bff1-c802ff2b12e1",
   "metadata": {},
   "source": [
    "- `model.reset_classifier(num_classes=0, global_pool=\"\")` means we **do not want global average pooling** and thus the shape at the last conv layer (penultimate layer) is $(2, 512, 7, 7)$\n",
    "- `model.reset_classifier(num_classes=0, global_pool=\"avg\")` means we **do want global average pooling** and thus the shape at the last conv layer (penultimate layer) is $(2, 512)$\n",
    "   whereby for each and every of the 512 feature maps $f_i$, we average $f_i$ across all pixels (i.e. if $f_i$ is 3 by 3 then average means add all $3 \\times 3 = 9$ pixels and average) and concat to become one $512$ vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3d551d91-b05b-43c4-ab66-ecfcc2fac85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([2, 1000])\n",
      "Unpooled shape: torch.Size([2, 512, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model('resnet18', pretrained=True, num_classes=1000)\n",
    "\n",
    "o = model(input_image_tensor)\n",
    "print(f'Original shape: {o.shape}')\n",
    "\n",
    "model.reset_classifier(num_classes=0, global_pool=\"\")\n",
    "\n",
    "o = model(input_image_tensor)\n",
    "print(f'Unpooled shape: {o.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d83238a5-01c0-41ca-bb62-0c90efaec3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([2, 1000])\n",
      "Pooled shape: torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model('resnet18', pretrained=True, num_classes=1000)\n",
    "\n",
    "o = model(input_image_tensor)\n",
    "print(f'Original shape: {o.shape}')\n",
    "\n",
    "model.reset_classifier(num_classes=0, global_pool=\"avg\")\n",
    "\n",
    "o = model(input_image_tensor)\n",
    "print(f'Pooled shape: {o.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9990a7be-d592-413e-a147-df02f221b1a6",
   "metadata": {
    "id": "cD-bT3WEn2Cx",
    "papermill": {
     "duration": 0.083045,
     "end_time": "2021-01-15T14:40:42.718396",
     "exception": false,
     "start_time": "2021-01-15T14:40:42.635351",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Image Normalization in Transfer Learning\n",
    "\n",
    "[Image Normalization](https://reighns92.github.io/reighns-ml-blog/reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/image_normalization/Image_Normalization_and_Standardization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f05e7a-f74d-4bcd-8653-2e07a502d531",
   "metadata": {},
   "source": [
    "### 1x1 Convolution\n",
    "\n",
    "- [A Gentle Introduction to 1×1 Convolutions to Manage Model Complexity](https://machinelearningmastery.com/introduction-to-1x1-convolutions-to-reduce-the-complexity-of-convolutional-neural-networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4ba4a0-7e02-48d5-9c4f-6fe5cb7a4325",
   "metadata": {},
   "source": [
    "#### Intuition\n",
    "\n",
    "Pooling layers are designed to downscale **feature maps** and systematically halve the **width and height** of feature maps in the network. Nevertheless, pooling layers do not change the number of filters in the model which is the **depth or number of channels**. The intuition of 1x1 convolution is that we can think of it as a **downsample** operation on the **feature maps** but this time shrinking the **depth/channels** instead of the **width/height**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd078ac-2d6b-4e90-8f74-ccf9aaa3732a",
   "metadata": {},
   "source": [
    "#### Downsample Feature Maps with 1x1 Convolution\n",
    "\n",
    "In Andrew Ng's example, let us define:\n",
    "\n",
    "- Batch size of 1;\n",
    "- A conv layer of with kernel size of 5, same padding of 2 and stride of 1, and an output depth of 32;\n",
    "- The input shape is a stack of feature maps of 192 depth/channels, each of 28 by 28;\n",
    "- It follows that the learnable params of the this conv layer is $32 \\times 5 \\times 5 \\times 192 + 32 = 153, 632$.  \n",
    "- It follows that the output shape is of $(32, 28, 28)$, which is a stack of feature maps of 32 depth, each of 28 by 28 size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c4c996b-54ff-4a9f-9977-3b4630b15484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Sequential                               --                        --\n",
      "├─Conv2d: 1-1                            [1, 32, 28, 28]           153,632\n",
      "==========================================================================================\n",
      "Total params: 153,632\n",
      "Trainable params: 153,632\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 120.45\n",
      "==========================================================================================\n",
      "Input size (MB): 0.60\n",
      "Forward/backward pass size (MB): 0.20\n",
      "Params size (MB): 0.61\n",
      "Estimated Total Size (MB): 1.42\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(\n",
    "        in_channels=192, out_channels=32, kernel_size=5, padding=2, stride=1\n",
    "    ),\n",
    "\n",
    ")\n",
    "batch_size = 1\n",
    "input_image = torch.rand(size=(192, 28, 28))\n",
    "\n",
    "print(summary(model, (batch_size, *input_image.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0931a24-d848-4b58-bb59-ec0b1343ddfd",
   "metadata": {},
   "source": [
    "Now if we design a **intermediate** layer that holds 1x1 convolution that first decreases the depth from 192 to 16 instead of 32, then connect back with another conv layer of kernel size 5 and depth 32, we can recover the same output shape but at a much leseer computational cost. As we can see below, the params reduced almost 10 folds.\n",
    "\n",
    "Commonly, we call this **intermediate** layer a **bottleneck layer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1d2331e-d0e5-400d-9de2-8e53b4f8bc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Sequential                               --                        --\n",
      "├─Conv2d: 1-1                            [1, 16, 28, 28]           3,088\n",
      "├─Conv2d: 1-2                            [1, 32, 28, 28]           12,832\n",
      "==========================================================================================\n",
      "Total params: 15,920\n",
      "Trainable params: 15,920\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 12.48\n",
      "==========================================================================================\n",
      "Input size (MB): 0.60\n",
      "Forward/backward pass size (MB): 0.30\n",
      "Params size (MB): 0.06\n",
      "Estimated Total Size (MB): 0.97\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(\n",
    "        in_channels=192, out_channels=16, kernel_size=1, padding=0, stride=1\n",
    "    ),\n",
    "    nn.Conv2d(\n",
    "        in_channels=16, out_channels=32, kernel_size=5, padding=2, stride=1\n",
    "    ),\n",
    ")\n",
    "batch_size = 1\n",
    "input_image = torch.rand(size=(192, 28, 28))\n",
    "\n",
    "print(summary(model, (batch_size, *input_image.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3c9bcf-3d30-49f8-96f3-a6f12bf3c538",
   "metadata": {},
   "source": [
    "#### Upsample Feature Maps with 1x1 Convolution\n",
    "\n",
    "Similarly, we can increase the **depth/channel** of the **feature maps** while maintaining the **width and height**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791551c4-ba3a-4831-8688-9dfb75e589a1",
   "metadata": {},
   "source": [
    "#### 1x1 Convolution is Equivalent to a FC-layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05a64eb-1b99-4634-9215-0c93eef88f5d",
   "metadata": {},
   "source": [
    "### Downsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d7c8fa-26fb-4d03-b631-7da4757ba484",
   "metadata": {},
   "source": [
    "#### Convolutional Stride (Downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "981560d9-b93c-4529-8816-8cc74d32efa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "input_image = torch.rand(size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a4405083-f78f-45ad-8eef-a50323791130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Sequential                               --                        --\n",
      "├─Conv2d: 1-1                            [1, 64, 224, 224]         9,408\n",
      "==========================================================================================\n",
      "Total params: 9,408\n",
      "Trainable params: 9,408\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 472.06\n",
      "==========================================================================================\n",
      "Input size (MB): 0.60\n",
      "Forward/backward pass size (MB): 25.69\n",
      "Params size (MB): 0.04\n",
      "Estimated Total Size (MB): 26.33\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(\n",
    "        in_channels=3, out_channels=64, kernel_size=7, padding=3, stride=1, bias=False\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "print(summary(model, (batch_size, *input_image.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "765fa448-7224-4af3-8112-1bd35e40d02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Sequential                               --                        --\n",
      "├─Conv2d: 1-1                            [1, 64, 112, 112]         9,408\n",
      "==========================================================================================\n",
      "Total params: 9,408\n",
      "Trainable params: 9,408\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 118.01\n",
      "==========================================================================================\n",
      "Input size (MB): 0.60\n",
      "Forward/backward pass size (MB): 6.42\n",
      "Params size (MB): 0.04\n",
      "Estimated Total Size (MB): 7.06\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(\n",
    "        in_channels=3, out_channels=64, kernel_size=7, padding=3, stride=2, bias=False\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "print(summary(model, (batch_size, *input_image.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef799ec2-b8e2-40e4-aaa8-37fb17d06598",
   "metadata": {},
   "source": [
    "This is considered a **downsampling** operation due to our stride changed from 2 to 1. The output shape of the feature maps is from (64, 224, 224) to (64, 112, 112), effectively halving the width and height of the feature maps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d65a80-64ee-424e-a083-e55b941a360b",
   "metadata": {},
   "source": [
    "#### Pooling (Downsampling)\n",
    "\n",
    "- https://www.quora.com/What-is-a-downsampling-layer-in-Convolutional-Neural-Network-CNN\n",
    "\n",
    "<div class=\"container\" style=\"display: inline-block;\">  \n",
    "  <figure>\n",
    "  <div style=\"float: left; padding: 10px;\">\n",
    "    <img src='https://storage.googleapis.com/reighns/reighns_ml_projects/docs/deep_learning/computer_vision/data/misc/Illustration-of-Max-Pooling-and-Average-Pooling-Figure-2-above-shows-an-example-of-max.png' width=\"350\" height=\"350\" align=\"center\"/>\n",
    "    <figcaption align=\"center\"><b>Pooling as a form of Downsampling. <a href=\"https://www.researchgate.net/figure/Illustration-of-Max-Pooling-and-Average-Pooling-Figure-2-above-shows-an-example-of-max_fig2_333593451\">Source</a></b></figcaption>\n",
    "  </div>\n",
    "  </figure>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6456c8a-5120-4ee2-afe5-6c766c785d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Sequential                               --                        --\n",
      "├─Conv2d: 1-1                            [1, 64, 224, 224]         9,408\n",
      "==========================================================================================\n",
      "Total params: 9,408\n",
      "Trainable params: 9,408\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 472.06\n",
      "==========================================================================================\n",
      "Input size (MB): 0.60\n",
      "Forward/backward pass size (MB): 25.69\n",
      "Params size (MB): 0.04\n",
      "Estimated Total Size (MB): 26.33\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(\n",
    "        in_channels=3, out_channels=64, kernel_size=7, padding=3, stride=1, bias=False\n",
    "    )\n",
    ")\n",
    "print(summary(model, (batch_size, *input_image.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "53969f86-c440-4b3a-95e3-f1ceeb558378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Sequential                               --                        --\n",
      "├─Conv2d: 1-1                            [1, 64, 224, 224]         9,408\n",
      "├─MaxPool2d: 1-2                         [1, 64, 112, 112]         --\n",
      "==========================================================================================\n",
      "Total params: 9,408\n",
      "Trainable params: 9,408\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 472.06\n",
      "==========================================================================================\n",
      "Input size (MB): 0.60\n",
      "Forward/backward pass size (MB): 25.69\n",
      "Params size (MB): 0.04\n",
      "Estimated Total Size (MB): 26.33\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(\n",
    "        in_channels=3, out_channels=64, kernel_size=7, padding=3, stride=1, bias=False\n",
    "    ),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    ")\n",
    "\n",
    "print(summary(model, (batch_size, *input_image.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29128660-7eca-4842-bda9-226d7e64df0d",
   "metadata": {},
   "source": [
    "Spatial pooling layers such as `MaxPool2d` with a size of 2 and stride 2 also reduces the feature map by a factor of 2 by 2. \n",
    "\n",
    "Notice that both downsampling methods have the same number of **trainable parameters** and also the exact **same output shape**, so what's the difference?\n",
    "\n",
    "> Maybe the additional `MaxPool2d` is an additional operation that is not needed? Let's see two examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18271e37-6142-42b5-8816-83cc50a5b3a2",
   "metadata": {},
   "source": [
    "#### Convolutional Stride vs Pooling for Downsampling\n",
    "\n",
    "The advantage of the convolution layer is that it can learn certain properties that you might not think of while you add pooling layer. Pooling is a fixed operation and convolution can be learned. On the other hand, pooling is a cheaper operation than convolution, both in terms of the amount of computation that you need to do and number of parameters that you need to store (no parameters for pooling layer).\n",
    "\n",
    "There are examples when one of them is better choice than the other.\n",
    "\n",
    "##### Example when the convolution with strides is better than pooling\n",
    "\n",
    "The first layer in the ResNet uses convolution with strides. This is a great example of when striding gives you an advantage. This layer by itself significantly reduces the amount of computation that has to be done by the network in the subsequent layers. It compresses multiple 3x3 convolution (3 to be exact) in to one 7x7 convolution, to make sure that it has exactly the same receptive field as 3 convolution layers (even though it is less powerful in terms of what it can learn). At the same time this layer applies stride=2 that downsamples the image. Because this first layer in ResNet does convolution and downsampling at the same time, the operation becomes significantly cheaper computationally. If you use stride=1 and pooling for downsampling, then you will end up with convolution that does 4 times more computation + extra computation for the next pooling layer. The same trick was used in SqueezeNet and some other neural network architectures.\n",
    "\n",
    "##### Example where pooling is better than convolution\n",
    "\n",
    "In the NIPS 2018, there was a new architecture presented called [FishNet][1]. One thing that they try is to fix the problems with the residual connections used in the ResNet. In the ResNet, in few places, they put 1x1 convolution in the skip connection when downsampling was applied to the image. This convolution layer makes gradient propagation harder. One of the major changes in their paper is that they get rid of the convolutions in the residual connections and replaced them with pooling and simple upscales/identities/concatenations. This solution fixes problem with gradient propagation in very deep networks. \n",
    "\n",
    "From the FishNet paper (Section 3.2)\n",
    "\n",
    "> The layers in the head are composed of concatenation, convolution with\n",
    "> identity mapping, and max-pooling. Therefore, the gradient propagation\n",
    "> problem from the previous backbone network in the tail are solved with\n",
    "> the FishNet by 1) excluding I-conv at the head; and 2) using\n",
    "> concatenation at the body and the head.\n",
    "\n",
    "\n",
    "  [1]: https://papers.nips.cc/paper/7356-fishnet-a-versatile-backbone-for-image-region-and-pixel-level-prediction.pdf\n",
    " \n",
    "- [Pooling vs. stride for downsampling](https://stats.stackexchange.com/questions/387482/pooling-vs-stride-for-downsampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8546acff-eaec-4631-8291-727077f6fa74",
   "metadata": {},
   "source": [
    "### Feature Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caee3af-8b92-4946-898e-c0c4d0cc2c30",
   "metadata": {},
   "source": [
    "#### Why are the last few Feature Maps more Useful in Feature extraction?\n",
    "\n",
    "- The backbone network (\"convolution and pooling\") is responsible for extracting a feature map from the image that contains higher level summarized information. Each head uses this feature map as input to predict its desired outcome. \n",
    "- The main intuition why feature maps of the last few layers (last layer usually) are important is one needs to recognize the earlier conv layer's feature maps find simple features like shapes, sizes, edges from an image, while the deep conv layers will be of more abstract features in an image. As a result, we really just want the abstract feature maps as they are **more class specific to the image** instead of the earlier layers which gives **generic shapes**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dddee8-7233-4ee1-aa7b-895b2c9a73b4",
   "metadata": {},
   "source": [
    "## Object Detection (YOLO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dfb84b-e081-42df-9434-a3a090e9c6df",
   "metadata": {},
   "source": [
    "## Object Detection (Bounding Boxes)\n",
    "\n",
    "- Different conversions\n",
    "    - https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\n",
    "    - https://github.com/awsaf49/bbox/blob/main/bbox/utils.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
