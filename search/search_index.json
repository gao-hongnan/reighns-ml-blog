{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome Welcome to Hongnan G.'s website. Here you can follow my learning journey. I write consistently about mathematics, computer science, and programming, with heavy focus on fundamental concepts. I also dabble in computer vision and participate in competitions from time to time.","title":"Home"},{"location":"#welcome","text":"Welcome to Hongnan G.'s website. Here you can follow my learning journey. I write consistently about mathematics, computer science, and programming, with heavy focus on fundamental concepts. I also dabble in computer vision and participate in competitions from time to time.","title":"Welcome"},{"location":"about/","text":"My name is Hongnan Gao, hailing from Singapore. I graduated from the National University of Singapore with a degree in Mathematics in 2019.","title":"About"},{"location":"reighns_ml_journey/sp_ppe/","text":"Gearing Up For Development Set Up Main Directory (IDE) Let us assume that we are residing in our root folder ~/gaohn and we want to create a new project called YOLOX , we can do as follows: creating main directory 1 2 3 ~/gaohn $ mkdir YOLOX ~/gaohn $ cd YOLOX ~/gaohn/YOLOX $ code . # (1) Open the project directory in Visual Studio Code. To change appropriately if using different IDE. If you are cloning a repository to your local folder YOLOX , you can further do: cloning repository 1 ~/gaohn/YOLOX $ git clone https://github.com/Megvii-BaseDetection/YOLOX.git . where . means cloning to the current directory. Set Up Virtual Environment Follow the steps below to set up a virtual environment for your development. Windows macOS M1 Linux venv 1 2 3 ~/gaohn/YOLOX $ python -m venv <name of virtual env> # (1) ~/gaohn/YOLOX $ . \\v env \\S cripts \\a ctivate # (2) ~/gaohn/YOLOX ( venv ) $ python -m pip install --upgrade pip setuptools wheel # (3) Create virtual environment named venv in the current directory. Activate virtual environment. Upgrade pip, setuptools and wheel. venv 1 2 3 4 ~/gaohn/YOLOX $ pip3 install virtualenv ~/gaohn/YOLOX $ virtualenv <name of virtual env> ~/gaohn/YOLOX $ source . \\v env \\b in \\a ctivate ~/gaohn/YOLOX ( venv ) $ python3 -m pip install --upgrade pip setuptools wheel venv 1 2 3 4 ~/gaohn/YOLOX $ sudo apt install python3.8 python3.8-venv python3-venv ~/gaohn/YOLOX $ python3 -m venv <name of virtual env> ~/gaohn/YOLOX $ source . \\v env \\b in \\a ctivate ~/gaohn/YOLOX ( venv ) $ python3 -m pip install --upgrade pip setuptools wheel You should see the following directory structure: main directory tree 1 2 \ud83d\udcc1 YOLOX/ \ud83d\udcc4 \u2514\u2500\u2500 venv/ Requirements and Setup Note For small projects, we can have requirements.txt and just run pip install -r requirements.txt . For larger projects, we can add a setup.py file. In short, requirements.txt specifies the dependencies of your project, and setup.py file informs you about the module or package-dependencies you are about to install has been packaged and distributed with Distutils, which is the standard for distributing Python Modules. You can skip setup.py if you are just using requirements.txt to install dependencies. Refer to madewithml 's requirements and setup section for more details. creating requirements 1 2 ~/gaohn/YOLOX ( venv ) $ touch requirements.txt setup.py ~/gaohn/YOLOX ( venv ) $ pip install -e . Something worth taking note is when you download PyTorch Library, there is a dependency link since we are downloading cuda directly, you may execute as such: ~/gaohn/YOLOX ( venv ) $ pip install -e . -f https://download.pytorch.org/whl/torch_stable.html If you further specify packages for development , testing and documentation in setup.py , you can choose which version to install: installing packages 1 2 3 ~/gaohn/YOLOX ( venv ) $ python -m pip install -e \".[dev]\" # installs required + dev packages ~/gaohn/YOLOX ( venv ) $ python -m pip install -e \".[test]\" # installs required + test packages ~/gaohn/YOLOX ( venv ) $ python -m pip install -e \".[docs_packages]\" # installs required documentation packages TODO To add in new examples here. You should see the following directory structure: main directory tree 1 2 3 4 \ud83d\udcc1 YOLOX/ \ud83d\udcc4 \u251c\u2500\u2500 venv/ \ud83d\udcc4 \u251c\u2500\u2500 requirements.txt \ud83d\udcc4 \u2514\u2500\u2500 setup.py For this version of YOLOX, use the commands below to do installation: Windows macOS M1 install 1 2 3 4 5 6 7 8 9 10 11 12 pip install -U pip && pip install -r requirements.txt pip install -v -e . # or python setup.py develop pip install cython # can install pycocotools==2.0.2 manually pip install git+https://github.com/philferriere/cocoapi.git#subdirectory = PythonAPI pip install pycocotools == 2 .0.2 # not sure why my com not playing nice with cuda 11.6 but only work with this for now # even cu102 will not work pip install torch == 1 .10.0+cu113 torchvision == 0 .11.1+cu113 -f https://download.pytorch.org/whl/torch_stable.html pip install setuptools == 59 .5.0 # for https://stackoverflow.com/questions/70520120/attributeerror-module-setuptools-distutils-has-no-attribute-version pip install tensorboard pip install wandb install 1 ... Get Raw Data The incoming raw data is a zip file named sp_ppe_all_combination_images.zip and contains 1,600 images alongside a csv file named all_annotations.csv which contains the bounding box coordinates and its corresponding class labels. Our goal is to download these data to our local machine (main directory). main directory tree 1 2 3 4 5 6 7 8 9 10 \ud83d\udcc1 YOLOX/ \ud83d\udcc4 \u251c\u2500\u2500 venv/ \ud83d\udcc1 \u251c\u2500\u2500 datasets/ \u2514\u2500\u2500 \ud83d\udcc1 \u2514\u2500\u2500 sp_ppe_data/ \u2514\u2500\u2500 \ud83d\udcc1 \u2514\u2500\u2500 raw/ \u251c\u2500\u2500 \ud83d\udcc4 \u251c\u2500\u2500 sp_ppe_all_images/ \u2514\u2500\u2500 \ud83d\udcc4 \u2514\u2500\u2500 sp_ppe_all_annotations/ \ud83d\udcc4 | \u2500\u2500 ... \ud83d\udcc4 \u251c\u2500\u2500 requirements.txt \ud83d\udcc4 \u2514\u2500\u2500 setup.py where sp_ppe_all_images contains 1,600 images and sp_ppe_all_annotations contains 1,600 corresponding annotations in .xml format. We will touch on that later. First, we create a folder named datasets in the root directory of the project alongside its sub-folder sp_ppe_data/raw and sp_ppe_all_images and sp_ppe_all_annotations . creating datasets folder 1 2 3 ~/gaohn/YOLOX ( venv ) $ mkdir -p datasets/sp_ppe_data ~/gaohn/YOLOX ( venv ) $ mkdir -p datasets/sp_ppe_data/raw/sp_ppe_all_images ~/gaohn/YOLOX ( venv ) $ mkdir -p datasets/sp_ppe_data/raw/sp_ppe_all_annotations Take note that in this YOLOX repo, the datasets folder is already there when you clone the repo, but having the command -p allows you to create the subfolder without raising an error. To download the data into datasets/sp_ppe_data , we can issue the below commands: Windows macOS M1 download raw data 1 2 ~/gaohn/YOLOX ( venv ) $ wget -P <destination folder> <url> ~/gaohn/YOLOX ( venv ) $ tar xf <zip file> -C <destination folder> download raw data 1 2 ~/gaohn/YOLOX ( venv ) $ wget -P <destination folder> <url> ~/gaohn/YOLOX ( venv ) $ unzip <zip file> -d <destination folder> where the <url> for the raw data is https://storage.googleapis.com/reighns/datasets/sp_ppe_all_combination_images.zip . More concretely, we have: download raw data windows 1 2 3 ~/gaohn/YOLOX ( venv ) $ wget -P datasets/sp_ppe_data/raw/sp_ppe_all_images https://storage.googleapis.com/reighns/datasets/sp_ppe_all_combination_images.zip # https://storage.googleapis.com/peekingduck/data/sp_ppe_all_combination_images.zip ~/gaohn/YOLOX ( venv ) $ tar xf datasets/sp_ppe_data/raw/sp_ppe_all_images/sp_ppe_all_combination_images.zip -C datasets/sp_ppe_data/sp_ppe_all_images ~/gaohn/YOLOX ( venv ) $ rm datasets/sp_ppe_data/raw/sp_ppe_all_images/sp_ppe_all_combination_images.zip Now we want to move our all_annotations.csv file into datasets/sp_ppe_data/raw/sp_ppe_all_annotations . We can do this by issuing the following command: Windows macOS Linux move files 1 ~/gaohn/YOLOX ( venv ) $ move < source file> <destination folder> move files 1 ~/gaohn/YOLOX ( venv ) $ mv < source file> <destination folder> move files filesnv 1 ~/gaohn/YOLOX ( venv ) $ mv < source file> <destination folder> So in our case, it is simply: move files windows 1 ~/gaohn/YOLOX ( venv ) $ move datasets/sp_ppe_data/raw/sp_ppe_all_images/all_annotations.csv datasets/sp_ppe_data/raw/sp_ppe_all_annotations The above steps can be a good opportunity to introduce a very basic shell script. Convert Raw Data into Required Formats Even though we have the raw images and annotations, we need to convert them into the required format for most out of the box models. This is because when we load the data into our deep learning model, it will expect the data to be in a certain format. The format is largely determined by how we write the dataset class in PyTorch (or TensorFlow). Pascal VOC Assume that we already have downloaded pascal voc format data from roboflow in this format: main directory tree 1 2 3 4 5 6 7 8 9 10 11 12 13 \ud83d\udcc1 YOLOX/ \ud83d\udcc4 \u251c\u2500\u2500 venv/ \ud83d\udcc1 \u251c\u2500\u2500 datasets/ \u2514\u2500\u2500 \ud83d\udcc1 \u2514\u2500\u2500 sp_ppe_data/ \u251c\u2500\u2500 \ud83d\udcc1 \u2514\u2500\u2500 raw/ \u2502 \u251c\u2500\u2500 \ud83d\udcc4 \u251c\u2500\u2500 sp_ppe_all_images/ \u2502 \u2514\u2500\u2500 \ud83d\udcc4 \u2514\u2500\u2500 sp_ppe_all_annotations/ \u2514\u2500\u2500 \ud83d\udcc1 \u2514\u2500\u2500 pascal_voc/ \u251c\u2500\u2500 \ud83d\udcc4 \u251c\u2500\u2500 images/ \u2514\u2500\u2500 \ud83d\udcc4 \u2514\u2500\u2500 annotations/ \ud83d\udcc4 | \u2500\u2500 ... \ud83d\udcc4 \u251c\u2500\u2500 requirements.txt \ud83d\udcc4 \u2514\u2500\u2500 setup.py download pascal voc 1 2 3 ~/gaohn/YOLOX ( venv ) $ mkdir -p datasets/sp_ppe_data ~/gaohn/YOLOX ( venv ) $ mkdir -p datasets/sp_ppe_data/pascal_voc/images ~/gaohn/YOLOX ( venv ) $ mkdir -p datasets/sp_ppe_data/pascal_voc/annotations Then move all the images to the images folder and all the annotations to the annotations folder. Note that the annotations are already in .xml pascal voc format. For YOLOX however, we need to strictly adhere to their VOC format, as follows YOLOX Pascal VOC 1 2 3 4 5 6 7 8 9 10 11 12 13 \ud83d\udcc1 YOLOX/ \ud83d\udcc4 \u251c\u2500\u2500 datasets/ \ud83d\udcc4 \u2502 \u251c\u2500\u2500 VOCdevkit/ \ud83d\udcc4 \u2502 \u2502 \u251c\u2500\u2500 VOC2012/ \ud83d\udcc4 \u2502 \u2502 \u2502 \u251c\u2500\u2500 sp_ppe_voc_helmet_mask_vest/ \ud83d\udcc4 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 Annotations/ \ud83d\udcc4 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 ImageSets/ \ud83d\udcc4 | | | | | \u251c\u2500\u2500 Main/ \ud83d\udcc4 | | | | | | \u251c\u2500\u2500 train.txt \ud83d\udcc4 | | | | | | \u251c\u2500\u2500 val.txt \ud83d\udcc4 | | | | | | \u251c\u2500\u2500 test.txt \ud83d\udcc4 | | | | | | \u251c\u2500\u2500 trainval.txt \ud83d\udcc4 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 JPEGImages/ Download Weights Download weights to weights folder YOLOX/weights ~/gaohn/YOLOX ( venv ) $ mkdir -p weights ~/gaohn/YOLOX ( venv ) $ wget.exe -P weights https://github.com/Megvii-BaseDetection/storage/releases/download/0.0.1/yolox_s.pth Train CLI Tricky part is to convert the images to COCO or VOC format, if you convert to COCO format and then ask Roboflow to convert to voc format, you need to use the script below to do so: from pathlib import Path train_p = Path ( \"./datasets/VOCdevkit/VOC2012/sp_ppe_voc_all_combinations/train\" ) valid_p = Path ( \"./datasets/VOCdevkit/VOC2012/sp_ppe_voc_all_combinations/valid\" ) test_p = Path ( \"./datasets/VOCdevkit/VOC2012/sp_ppe_voc_all_combinations/test\" ) with open ( \"train.txt\" , \"w\" ) as f : for p in train_p . glob ( \"*.jpg\" ): f . write ( f \" { p . stem } \\n \" ) with open ( \"valid.txt\" , \"w\" ) as f : for p in valid_p . glob ( \"*.jpg\" ): f . write ( f \" { p . stem } \\n \" ) with open ( \"test.txt\" , \"w\" ) as f : for p in test_p . glob ( \"*.jpg\" ): f . write ( f \" { p . stem } \\n \" ) Then run the training python tools/train.py -f . \\e xps \\c ustom \\s p_ppe \\s p_ppe_voc_all_combinations.py -d 1 -b 16 --fp16 -o -c . \\w eights \\y olox_s.pth and infer whole folder: python tools/demo.py image -f . \\e xps \\c ustom \\s p_ppe \\s p_ppe_voc_all_combinations.py -c . \\Y OLOX_outputs \\s p_ppe_voc_all_combinations \\b est_ckpt.pth --path . \\d atasets \\V OCdevkit \\V OC2012 \\s p_ppe_voc_all_combinations \\t est \\ --conf 0 .25 --nms 0 .45 --tsize 640 --save_result --device gpu Important rmb change both voc and coco class file because later demo py uses coco class lame. Label Tools CVAT macOS M1 Follow the installation guide : Download Docker for Mac M1 (without Rosetta), as mentioned in issue opened on GitHub, we need to select use docker compose v2 in the settings of docker as it is unselected by default, causing an error when using docker compose . Download and install CVAT: $ git clone https://github.com/opencv/cvat $ cd cvat $ CVAT_VERSION = dev docker-compose pull $ CVAT_VERSION = dev docker-compose up -d Note that the commands above are unique to macOS M1 as the CVAT installation guide does not fully support M1 yet, so have to go through some loops and hoops. See issue here for reference. You can register a user but by default it will not have rights even to view list of tasks. Thus you should create a superuser. A superuser can use an admin panel to assign correct groups to other users. Please use the command below: docker exec -it cvat_server bash -ic 'python3 ~/manage.py createsuperuser' username: django/django2 password: ZGRu3pUsCw66HGa Somehow you need to re-run steps 2-3 when you restart you computer? See my short video clip for annotation steps. Windows Install WSL2 Newer windows can directly call the following command in adminstrator Powershell: $ wsl --install If not, the safe choice is to follow the guide here , starting from step 4. Install Docker Download and install docker here . Install Git Make sure there is Git for Windows. Run CVAT Clone CVAT to local machine. $ git clone https://github.com/opencv/cvat $ cd cvat Run docker containers. $ docker-compose up -d # run below since now latest version has issues $ docker-compose -f docker-compose.yml -f docker-compose.dev.yml build However, there is an issue opened just 2 weeks ago: See https://github.com/opencv/cvat/issues/4888 and https://github.com/opencv/cvat/issues/4816 Create username and password: # enter docker image first $ docker exec -it cvat_server /bin/bash # then run $ python3 ~/manage.py createsuperuser username: django password: ZGRu3pUsCw66HGa Open the installed Google Chrome browser and go to localhost:8080 . Type your login/password for the superuser on the login page and press the Login button. Now you should be able to create a new annotation task. Please read the CVAT manual for more details. End-to-End ML Workflow Clarify the Problem and Constrains This one can ask Lucas more when the time comes, we will assume that this is a real life problem that requires a solution. We can then ask the below potential questions. What is the problem? How do end users use the product and benefit from it? Benefit Structure? Ethical issues (e.g. discrimination, privacy, etc.) Deployment: latency vs accuracy etc; Where we deploy the model on? ...... etc We can then form a problem statement along with a summary, where we will constantly refer to it as we move along. Establish Metrics Once we are clear on the problem statement, we can define our metrics: Model Performance Metrics The typical metrics such as accuracy, precision, recall, F1 score, etc. More sophisticated usage includes AUC, ROC, etc. Constrainment includes \\(\\max \\textbf{Mean Average Precision}@\\textbf{Mean Recall} 0.8\\) which means you constrain mean recall to be at least 0.8 while maximizing mean average precision. Model Performance is not everything to stakeholder, sometimes may need to consider their KPIs. Business Performance Metrics Unsure, need some product sense, but I guess our project is not too clear yet. Data, Data, Data We can further split this into 3 parts: Data Collection Data Cleaning Data Preparation Info Arguably, the most important part of the whole process is the data, as we move from a model-centric approach to a data-centric approach (re: Andrew NG), we really need to ensure the quality of the data. Data Collection Place holder. Data Cleaning Place holder. Data Preparation TODO: To be refined, here is some collection of my initial findings. To test on coco128 when developing next time. There are no bounding box annotations, therefore we need to label/annotate the images: Created a branch node-convert-to-coco to convert the node's output data to coco format. This branch will make inference on the images and give us the bounding box annotations. This reduces overhead for my quick prototyping, but is not the best if one cares about the quality of data. This is because although inferencing on human is easy, it is not trivial when the human wears a helmet , the bounding box is not tight above. The output using csv_writer will be converted to COCO. See my tmp.py for more details. Once converted, tested it on google colab to see if it works. It did since macOS cannot run cuda and this repo only allows cuda for training, we can loosen this restriction. See my google drive. See YOLOXX to see how to annotate using COCO format, this was developed on macOS initially. TODO Some considerations: Do we need a node to convert our inference results to COCO/VOC format? We can have a generic node for this purpose; like we enforce a certain format in the form of csv/df, then convert them to either. COCO Dataset COCO Detection Eval Pascal VOC Dataset TODOS: Currently convert coco to voc online, need write a script maybe since I wrote one for coco temporarily. For YOLOX, Pascal VOC is not well maintained, so for now the folder structure is rigidly defined below: \ud83d\udcc1 YOLOX/ \ud83d\udcc4 \u251c\u2500\u2500 datasets/ \ud83d\udcc4 \u2502 \u251c\u2500\u2500 VOCdevkit/ \ud83d\udcc4 \u2502 \u2502 \u251c\u2500\u2500 VOC2012/ \ud83d\udcc4 \u2502 \u2502 \u2502 \u251c\u2500\u2500 sp_ppe_voc_helmet_mask_vest/ \ud83d\udcc4 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 Annotations/ \ud83d\udcc4 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 ImageSets/ \ud83d\udcc4 | | | | | \u251c\u2500\u2500 Main/ \ud83d\udcc4 | | | | | | \u251c\u2500\u2500 train.txt \ud83d\udcc4 | | | | | | \u251c\u2500\u2500 val.txt \ud83d\udcc4 | | | | | | \u251c\u2500\u2500 test.txt \ud83d\udcc4 | | | | | | \u251c\u2500\u2500 trainval.txt \ud83d\udcc4 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 JPEGImages/ Without changing source code, the VOCdevkit and VOC{year} folder names are fixed and not negotiable. I attempted to do away with the VOC{year} folder, but it really requires changing a few places, and I am aware that even PyTorch's own voc.py dataset uses this same format. So I keep it as such for now but it is possible to loosen this restriction. Annotations folder contains all the xml files, which is each image's annotations info in Pascal VOC format. JPEGImages folder contains all the images. ImageSets folder contains the train/val/test split, which is a txt file with each line being the image name without the extension. For example, if the image is 000001.jpg , then the line in the txt file is 000001 . Note we do not need to use trainval.txt since we can just use train.txt and val.txt to train and validate. Note we should find a way to use KFolds here, might be cumbersome to train every fold manually? Model Training Training your own YoloX Object Detection Model on Colab - YoloX Object Detection Model Deployment This quite useful as a starter. Coding your own dataset or dataloader class MMDetection Dataset Class https://github.com/pytorch/vision/blob/main/torchvision/datasets/voc.py Config (Exp) File The main config file of YOLOX resides in the exps/ folder. YOLOX/exps/default/yolox_s.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class Exp ( MyExp ): def __init__ ( self ): super ( Exp , self ) . __init__ () self . num_classes = NUM_CLASSES # 20 print ( \"num_classes\" , self . num_classes ) self . depth = 0.33 self . width = 0.50 self . warmup_epochs = 1 # ---------- transform config ------------ # self . mosaic_prob = 1.0 self . mixup_prob = 1.0 self . hsv_prob = 1.0 self . flip_prob = 0.5 self . degrees = 10.0 self . translate = 0.1 self . scale = ( 0.1 , 2 ) self . mosaic_scale = ( 0.8 , 1.6 ) self . shear = 2.0 self . perspective = 0.0 self . enable_mixup = True Steps Make the data into pascal voc format and place it under YOLOX/datasets/VOCdevkit folder. Note you need to create VOCdevkit folder first. Follow exactly the format I have inside my folder. When making the data, I hardcoded a bit, make sure eventually is same format as the one here for now https://www.youtube.com/watch?v=be_D3V9Pxlg&t=2371s with some tweaks. Download weights to weights folder YOLOX/weights mkdir weights wget.exe -P . \\w eights \\ https://github.com/Megvii-BaseDetection/storage/releases/download/0.0.1/yolox_s.pth 4. Create exp file, meaning the configuration file for the experiment. mkdir exp/custom/sp_ppe cd exp/custom/sp_ppe touch sp_ppe_voc.py cd ../../../ see code inside, this file is very important as it overwrites the base class. 4.1. Note it is very important to change coco class and voc class file. Run the training python tools/train.py -f . \\e xps \\c ustom \\s p_ppe \\s p_ppe_voc_human_only.py -d 1 -b 4 --fp16 -o -c . \\w eights \\y olox_s.pth if there is Weights & Biases: python tools/train.py -f . \\e xps \\c ustom \\s p_ppe \\s p_ppe_voc_human_only.py -d 1 -b 4 --fp16 -o -c . \\w eights \\y olox_s.pth \\ --logger wandb \\ wandb-project yolox \\ wandb-log_checkpoints True \\ wandb-num_eval_images 3 \\ where num_eval_images means at every evaluation step, the dashboard also shows 3 images from the validation set along with the predicted bounding box. Run test MODEL_PATH = 'C:\\Users\\reighns\\reighns_ml\\ml_projects\\YOLOX\\YOLOX_outputs\\sp_ppe_voc_helmet_mask_vest\\latest_ckpt.pth' TEST_IMAGE_PATH python tools/demo.py image -f . \\e xps \\c ustom \\s p_ppe \\s p_ppe_voc_helmet_mask_vest.py -c . \\Y OLOX_outputs \\s p_ppe_voc_helmet_mask_vest \\l ast_epoch_ckpt.pth --path . \\d atasets \\V OCdevkit \\V OC2012 \\s p_ppe_voc_helmet_mask_vest \\t est \\h elmet--147-_jpg.rf.ed1d0dab4fc4b4f0c4213e6569a7ef02.jpg --conf 0 .25 --nms 0 .45 --tsize 640 --save_result --device gpu Weights & Biases See https://docs.wandb.ai/quickstart for more details. But basically use pip install wandb wandb login however for now comment out a chunk of code in logger since it has self.cats which only works for coco. Model Capability We first check if model can memorize, then generalize, the former is important to check for bugs, and to ensure our model has basic capability to learn, but no way does it guarantee that it can generalize. Regardless, we still want to ensure this as if it cannot even memorize in-sample data points, it for sure cannot generalize. References https://docs.wandb.ai/guides/track/log/media to log bounding boxes https://www.youtube.com/watch?v=be_D3V9Pxlg&t=2371s https://github.com/awsaf49/bbox/blob/main/bbox/utils.py https://www.kaggle.com/code/ayuraj/train-yolov5-cross-validation-ensemble-w-b/notebook https://github.com/roboflow-ai/YOLOX https://www.kaggle.com/code/awsaf49/great-barrier-reef-yolov5-train/notebook#%F0%9F%93%81-Create-Folds Transfer learning for YOLOX: https://github.com/Megvii-BaseDetection/YOLOX/issues/1105 + https://github.com/Megvii-BaseDetection/YOLOX/pull/1156 Converting darknet or yolov5 datasets to COCO format for YOLOX: YOLO2COCO from Daniel https://stackoverflow.com/questions/67733876/create-pascol-voc-xml-from-csv https://pyimagesearch.com/2022/05/02/mean-average-precision-map-using-the-coco-evaluator/ https://wandb.ai/manan-goel/yolox-nano/reports/Tracking-your-YOLOX-Runs-with-Weights-Biases---VmlldzoxNzc0NjA0 https://docs.wandb.ai/guides/integrations/other/yolox https://www.kaggle.com/code/remekkinas/yolox-training-pipeline-cots-dataset-lb-0-507/notebook#6.-RUN-INFERENCE https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch https://blog.roboflow.com/how-to-train-yolox-on-a-custom-dataset/ https://www.kaggle.com/code/litaldavar/hard-head-detection-with-yolov5/notebook https://www.kaggle.com/code/billiemage/object-detection","title":"SP PPE"},{"location":"reighns_ml_journey/sp_ppe/#gearing-up-for-development","text":"","title":"Gearing Up For Development"},{"location":"reighns_ml_journey/sp_ppe/#set-up-main-directory-ide","text":"Let us assume that we are residing in our root folder ~/gaohn and we want to create a new project called YOLOX , we can do as follows: creating main directory 1 2 3 ~/gaohn $ mkdir YOLOX ~/gaohn $ cd YOLOX ~/gaohn/YOLOX $ code . # (1) Open the project directory in Visual Studio Code. To change appropriately if using different IDE. If you are cloning a repository to your local folder YOLOX , you can further do: cloning repository 1 ~/gaohn/YOLOX $ git clone https://github.com/Megvii-BaseDetection/YOLOX.git . where . means cloning to the current directory.","title":"Set Up Main Directory (IDE)"},{"location":"reighns_ml_journey/sp_ppe/#set-up-virtual-environment","text":"Follow the steps below to set up a virtual environment for your development. Windows macOS M1 Linux venv 1 2 3 ~/gaohn/YOLOX $ python -m venv <name of virtual env> # (1) ~/gaohn/YOLOX $ . \\v env \\S cripts \\a ctivate # (2) ~/gaohn/YOLOX ( venv ) $ python -m pip install --upgrade pip setuptools wheel # (3) Create virtual environment named venv in the current directory. Activate virtual environment. Upgrade pip, setuptools and wheel. venv 1 2 3 4 ~/gaohn/YOLOX $ pip3 install virtualenv ~/gaohn/YOLOX $ virtualenv <name of virtual env> ~/gaohn/YOLOX $ source . \\v env \\b in \\a ctivate ~/gaohn/YOLOX ( venv ) $ python3 -m pip install --upgrade pip setuptools wheel venv 1 2 3 4 ~/gaohn/YOLOX $ sudo apt install python3.8 python3.8-venv python3-venv ~/gaohn/YOLOX $ python3 -m venv <name of virtual env> ~/gaohn/YOLOX $ source . \\v env \\b in \\a ctivate ~/gaohn/YOLOX ( venv ) $ python3 -m pip install --upgrade pip setuptools wheel You should see the following directory structure: main directory tree 1 2 \ud83d\udcc1 YOLOX/ \ud83d\udcc4 \u2514\u2500\u2500 venv/","title":"Set Up Virtual Environment"},{"location":"reighns_ml_journey/sp_ppe/#requirements-and-setup","text":"Note For small projects, we can have requirements.txt and just run pip install -r requirements.txt . For larger projects, we can add a setup.py file. In short, requirements.txt specifies the dependencies of your project, and setup.py file informs you about the module or package-dependencies you are about to install has been packaged and distributed with Distutils, which is the standard for distributing Python Modules. You can skip setup.py if you are just using requirements.txt to install dependencies. Refer to madewithml 's requirements and setup section for more details. creating requirements 1 2 ~/gaohn/YOLOX ( venv ) $ touch requirements.txt setup.py ~/gaohn/YOLOX ( venv ) $ pip install -e . Something worth taking note is when you download PyTorch Library, there is a dependency link since we are downloading cuda directly, you may execute as such: ~/gaohn/YOLOX ( venv ) $ pip install -e . -f https://download.pytorch.org/whl/torch_stable.html If you further specify packages for development , testing and documentation in setup.py , you can choose which version to install: installing packages 1 2 3 ~/gaohn/YOLOX ( venv ) $ python -m pip install -e \".[dev]\" # installs required + dev packages ~/gaohn/YOLOX ( venv ) $ python -m pip install -e \".[test]\" # installs required + test packages ~/gaohn/YOLOX ( venv ) $ python -m pip install -e \".[docs_packages]\" # installs required documentation packages TODO To add in new examples here. You should see the following directory structure: main directory tree 1 2 3 4 \ud83d\udcc1 YOLOX/ \ud83d\udcc4 \u251c\u2500\u2500 venv/ \ud83d\udcc4 \u251c\u2500\u2500 requirements.txt \ud83d\udcc4 \u2514\u2500\u2500 setup.py For this version of YOLOX, use the commands below to do installation: Windows macOS M1 install 1 2 3 4 5 6 7 8 9 10 11 12 pip install -U pip && pip install -r requirements.txt pip install -v -e . # or python setup.py develop pip install cython # can install pycocotools==2.0.2 manually pip install git+https://github.com/philferriere/cocoapi.git#subdirectory = PythonAPI pip install pycocotools == 2 .0.2 # not sure why my com not playing nice with cuda 11.6 but only work with this for now # even cu102 will not work pip install torch == 1 .10.0+cu113 torchvision == 0 .11.1+cu113 -f https://download.pytorch.org/whl/torch_stable.html pip install setuptools == 59 .5.0 # for https://stackoverflow.com/questions/70520120/attributeerror-module-setuptools-distutils-has-no-attribute-version pip install tensorboard pip install wandb install 1 ...","title":"Requirements and Setup"},{"location":"reighns_ml_journey/sp_ppe/#get-raw-data","text":"The incoming raw data is a zip file named sp_ppe_all_combination_images.zip and contains 1,600 images alongside a csv file named all_annotations.csv which contains the bounding box coordinates and its corresponding class labels. Our goal is to download these data to our local machine (main directory). main directory tree 1 2 3 4 5 6 7 8 9 10 \ud83d\udcc1 YOLOX/ \ud83d\udcc4 \u251c\u2500\u2500 venv/ \ud83d\udcc1 \u251c\u2500\u2500 datasets/ \u2514\u2500\u2500 \ud83d\udcc1 \u2514\u2500\u2500 sp_ppe_data/ \u2514\u2500\u2500 \ud83d\udcc1 \u2514\u2500\u2500 raw/ \u251c\u2500\u2500 \ud83d\udcc4 \u251c\u2500\u2500 sp_ppe_all_images/ \u2514\u2500\u2500 \ud83d\udcc4 \u2514\u2500\u2500 sp_ppe_all_annotations/ \ud83d\udcc4 | \u2500\u2500 ... \ud83d\udcc4 \u251c\u2500\u2500 requirements.txt \ud83d\udcc4 \u2514\u2500\u2500 setup.py where sp_ppe_all_images contains 1,600 images and sp_ppe_all_annotations contains 1,600 corresponding annotations in .xml format. We will touch on that later. First, we create a folder named datasets in the root directory of the project alongside its sub-folder sp_ppe_data/raw and sp_ppe_all_images and sp_ppe_all_annotations . creating datasets folder 1 2 3 ~/gaohn/YOLOX ( venv ) $ mkdir -p datasets/sp_ppe_data ~/gaohn/YOLOX ( venv ) $ mkdir -p datasets/sp_ppe_data/raw/sp_ppe_all_images ~/gaohn/YOLOX ( venv ) $ mkdir -p datasets/sp_ppe_data/raw/sp_ppe_all_annotations Take note that in this YOLOX repo, the datasets folder is already there when you clone the repo, but having the command -p allows you to create the subfolder without raising an error. To download the data into datasets/sp_ppe_data , we can issue the below commands: Windows macOS M1 download raw data 1 2 ~/gaohn/YOLOX ( venv ) $ wget -P <destination folder> <url> ~/gaohn/YOLOX ( venv ) $ tar xf <zip file> -C <destination folder> download raw data 1 2 ~/gaohn/YOLOX ( venv ) $ wget -P <destination folder> <url> ~/gaohn/YOLOX ( venv ) $ unzip <zip file> -d <destination folder> where the <url> for the raw data is https://storage.googleapis.com/reighns/datasets/sp_ppe_all_combination_images.zip . More concretely, we have: download raw data windows 1 2 3 ~/gaohn/YOLOX ( venv ) $ wget -P datasets/sp_ppe_data/raw/sp_ppe_all_images https://storage.googleapis.com/reighns/datasets/sp_ppe_all_combination_images.zip # https://storage.googleapis.com/peekingduck/data/sp_ppe_all_combination_images.zip ~/gaohn/YOLOX ( venv ) $ tar xf datasets/sp_ppe_data/raw/sp_ppe_all_images/sp_ppe_all_combination_images.zip -C datasets/sp_ppe_data/sp_ppe_all_images ~/gaohn/YOLOX ( venv ) $ rm datasets/sp_ppe_data/raw/sp_ppe_all_images/sp_ppe_all_combination_images.zip Now we want to move our all_annotations.csv file into datasets/sp_ppe_data/raw/sp_ppe_all_annotations . We can do this by issuing the following command: Windows macOS Linux move files 1 ~/gaohn/YOLOX ( venv ) $ move < source file> <destination folder> move files 1 ~/gaohn/YOLOX ( venv ) $ mv < source file> <destination folder> move files filesnv 1 ~/gaohn/YOLOX ( venv ) $ mv < source file> <destination folder> So in our case, it is simply: move files windows 1 ~/gaohn/YOLOX ( venv ) $ move datasets/sp_ppe_data/raw/sp_ppe_all_images/all_annotations.csv datasets/sp_ppe_data/raw/sp_ppe_all_annotations The above steps can be a good opportunity to introduce a very basic shell script.","title":"Get Raw Data"},{"location":"reighns_ml_journey/sp_ppe/#convert-raw-data-into-required-formats","text":"Even though we have the raw images and annotations, we need to convert them into the required format for most out of the box models. This is because when we load the data into our deep learning model, it will expect the data to be in a certain format. The format is largely determined by how we write the dataset class in PyTorch (or TensorFlow).","title":"Convert Raw Data into Required Formats"},{"location":"reighns_ml_journey/sp_ppe/#pascal-voc","text":"Assume that we already have downloaded pascal voc format data from roboflow in this format: main directory tree 1 2 3 4 5 6 7 8 9 10 11 12 13 \ud83d\udcc1 YOLOX/ \ud83d\udcc4 \u251c\u2500\u2500 venv/ \ud83d\udcc1 \u251c\u2500\u2500 datasets/ \u2514\u2500\u2500 \ud83d\udcc1 \u2514\u2500\u2500 sp_ppe_data/ \u251c\u2500\u2500 \ud83d\udcc1 \u2514\u2500\u2500 raw/ \u2502 \u251c\u2500\u2500 \ud83d\udcc4 \u251c\u2500\u2500 sp_ppe_all_images/ \u2502 \u2514\u2500\u2500 \ud83d\udcc4 \u2514\u2500\u2500 sp_ppe_all_annotations/ \u2514\u2500\u2500 \ud83d\udcc1 \u2514\u2500\u2500 pascal_voc/ \u251c\u2500\u2500 \ud83d\udcc4 \u251c\u2500\u2500 images/ \u2514\u2500\u2500 \ud83d\udcc4 \u2514\u2500\u2500 annotations/ \ud83d\udcc4 | \u2500\u2500 ... \ud83d\udcc4 \u251c\u2500\u2500 requirements.txt \ud83d\udcc4 \u2514\u2500\u2500 setup.py download pascal voc 1 2 3 ~/gaohn/YOLOX ( venv ) $ mkdir -p datasets/sp_ppe_data ~/gaohn/YOLOX ( venv ) $ mkdir -p datasets/sp_ppe_data/pascal_voc/images ~/gaohn/YOLOX ( venv ) $ mkdir -p datasets/sp_ppe_data/pascal_voc/annotations Then move all the images to the images folder and all the annotations to the annotations folder. Note that the annotations are already in .xml pascal voc format. For YOLOX however, we need to strictly adhere to their VOC format, as follows YOLOX Pascal VOC 1 2 3 4 5 6 7 8 9 10 11 12 13 \ud83d\udcc1 YOLOX/ \ud83d\udcc4 \u251c\u2500\u2500 datasets/ \ud83d\udcc4 \u2502 \u251c\u2500\u2500 VOCdevkit/ \ud83d\udcc4 \u2502 \u2502 \u251c\u2500\u2500 VOC2012/ \ud83d\udcc4 \u2502 \u2502 \u2502 \u251c\u2500\u2500 sp_ppe_voc_helmet_mask_vest/ \ud83d\udcc4 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 Annotations/ \ud83d\udcc4 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 ImageSets/ \ud83d\udcc4 | | | | | \u251c\u2500\u2500 Main/ \ud83d\udcc4 | | | | | | \u251c\u2500\u2500 train.txt \ud83d\udcc4 | | | | | | \u251c\u2500\u2500 val.txt \ud83d\udcc4 | | | | | | \u251c\u2500\u2500 test.txt \ud83d\udcc4 | | | | | | \u251c\u2500\u2500 trainval.txt \ud83d\udcc4 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 JPEGImages/","title":"Pascal VOC"},{"location":"reighns_ml_journey/sp_ppe/#download-weights","text":"Download weights to weights folder YOLOX/weights ~/gaohn/YOLOX ( venv ) $ mkdir -p weights ~/gaohn/YOLOX ( venv ) $ wget.exe -P weights https://github.com/Megvii-BaseDetection/storage/releases/download/0.0.1/yolox_s.pth","title":"Download Weights"},{"location":"reighns_ml_journey/sp_ppe/#train-cli","text":"Tricky part is to convert the images to COCO or VOC format, if you convert to COCO format and then ask Roboflow to convert to voc format, you need to use the script below to do so: from pathlib import Path train_p = Path ( \"./datasets/VOCdevkit/VOC2012/sp_ppe_voc_all_combinations/train\" ) valid_p = Path ( \"./datasets/VOCdevkit/VOC2012/sp_ppe_voc_all_combinations/valid\" ) test_p = Path ( \"./datasets/VOCdevkit/VOC2012/sp_ppe_voc_all_combinations/test\" ) with open ( \"train.txt\" , \"w\" ) as f : for p in train_p . glob ( \"*.jpg\" ): f . write ( f \" { p . stem } \\n \" ) with open ( \"valid.txt\" , \"w\" ) as f : for p in valid_p . glob ( \"*.jpg\" ): f . write ( f \" { p . stem } \\n \" ) with open ( \"test.txt\" , \"w\" ) as f : for p in test_p . glob ( \"*.jpg\" ): f . write ( f \" { p . stem } \\n \" ) Then run the training python tools/train.py -f . \\e xps \\c ustom \\s p_ppe \\s p_ppe_voc_all_combinations.py -d 1 -b 16 --fp16 -o -c . \\w eights \\y olox_s.pth and infer whole folder: python tools/demo.py image -f . \\e xps \\c ustom \\s p_ppe \\s p_ppe_voc_all_combinations.py -c . \\Y OLOX_outputs \\s p_ppe_voc_all_combinations \\b est_ckpt.pth --path . \\d atasets \\V OCdevkit \\V OC2012 \\s p_ppe_voc_all_combinations \\t est \\ --conf 0 .25 --nms 0 .45 --tsize 640 --save_result --device gpu Important rmb change both voc and coco class file because later demo py uses coco class lame.","title":"Train CLI"},{"location":"reighns_ml_journey/sp_ppe/#label-tools","text":"","title":"Label Tools"},{"location":"reighns_ml_journey/sp_ppe/#cvat","text":"","title":"CVAT"},{"location":"reighns_ml_journey/sp_ppe/#macos-m1","text":"Follow the installation guide : Download Docker for Mac M1 (without Rosetta), as mentioned in issue opened on GitHub, we need to select use docker compose v2 in the settings of docker as it is unselected by default, causing an error when using docker compose . Download and install CVAT: $ git clone https://github.com/opencv/cvat $ cd cvat $ CVAT_VERSION = dev docker-compose pull $ CVAT_VERSION = dev docker-compose up -d Note that the commands above are unique to macOS M1 as the CVAT installation guide does not fully support M1 yet, so have to go through some loops and hoops. See issue here for reference. You can register a user but by default it will not have rights even to view list of tasks. Thus you should create a superuser. A superuser can use an admin panel to assign correct groups to other users. Please use the command below: docker exec -it cvat_server bash -ic 'python3 ~/manage.py createsuperuser' username: django/django2 password: ZGRu3pUsCw66HGa Somehow you need to re-run steps 2-3 when you restart you computer? See my short video clip for annotation steps.","title":"macOS M1"},{"location":"reighns_ml_journey/sp_ppe/#windows","text":"Install WSL2 Newer windows can directly call the following command in adminstrator Powershell: $ wsl --install If not, the safe choice is to follow the guide here , starting from step 4. Install Docker Download and install docker here . Install Git Make sure there is Git for Windows. Run CVAT Clone CVAT to local machine. $ git clone https://github.com/opencv/cvat $ cd cvat Run docker containers. $ docker-compose up -d # run below since now latest version has issues $ docker-compose -f docker-compose.yml -f docker-compose.dev.yml build However, there is an issue opened just 2 weeks ago: See https://github.com/opencv/cvat/issues/4888 and https://github.com/opencv/cvat/issues/4816 Create username and password: # enter docker image first $ docker exec -it cvat_server /bin/bash # then run $ python3 ~/manage.py createsuperuser username: django password: ZGRu3pUsCw66HGa Open the installed Google Chrome browser and go to localhost:8080 . Type your login/password for the superuser on the login page and press the Login button. Now you should be able to create a new annotation task. Please read the CVAT manual for more details.","title":"Windows"},{"location":"reighns_ml_journey/sp_ppe/#end-to-end-ml-workflow","text":"","title":"End-to-End ML Workflow"},{"location":"reighns_ml_journey/sp_ppe/#clarify-the-problem-and-constrains","text":"This one can ask Lucas more when the time comes, we will assume that this is a real life problem that requires a solution. We can then ask the below potential questions. What is the problem? How do end users use the product and benefit from it? Benefit Structure? Ethical issues (e.g. discrimination, privacy, etc.) Deployment: latency vs accuracy etc; Where we deploy the model on? ...... etc We can then form a problem statement along with a summary, where we will constantly refer to it as we move along.","title":"Clarify the Problem and Constrains"},{"location":"reighns_ml_journey/sp_ppe/#establish-metrics","text":"Once we are clear on the problem statement, we can define our metrics: Model Performance Metrics The typical metrics such as accuracy, precision, recall, F1 score, etc. More sophisticated usage includes AUC, ROC, etc. Constrainment includes \\(\\max \\textbf{Mean Average Precision}@\\textbf{Mean Recall} 0.8\\) which means you constrain mean recall to be at least 0.8 while maximizing mean average precision. Model Performance is not everything to stakeholder, sometimes may need to consider their KPIs. Business Performance Metrics Unsure, need some product sense, but I guess our project is not too clear yet.","title":"Establish Metrics"},{"location":"reighns_ml_journey/sp_ppe/#data-data-data","text":"We can further split this into 3 parts: Data Collection Data Cleaning Data Preparation Info Arguably, the most important part of the whole process is the data, as we move from a model-centric approach to a data-centric approach (re: Andrew NG), we really need to ensure the quality of the data.","title":"Data, Data, Data"},{"location":"reighns_ml_journey/sp_ppe/#data-collection","text":"Place holder.","title":"Data Collection"},{"location":"reighns_ml_journey/sp_ppe/#data-cleaning","text":"Place holder.","title":"Data Cleaning"},{"location":"reighns_ml_journey/sp_ppe/#data-preparation","text":"TODO: To be refined, here is some collection of my initial findings. To test on coco128 when developing next time. There are no bounding box annotations, therefore we need to label/annotate the images: Created a branch node-convert-to-coco to convert the node's output data to coco format. This branch will make inference on the images and give us the bounding box annotations. This reduces overhead for my quick prototyping, but is not the best if one cares about the quality of data. This is because although inferencing on human is easy, it is not trivial when the human wears a helmet , the bounding box is not tight above. The output using csv_writer will be converted to COCO. See my tmp.py for more details. Once converted, tested it on google colab to see if it works. It did since macOS cannot run cuda and this repo only allows cuda for training, we can loosen this restriction. See my google drive. See YOLOXX to see how to annotate using COCO format, this was developed on macOS initially. TODO Some considerations: Do we need a node to convert our inference results to COCO/VOC format? We can have a generic node for this purpose; like we enforce a certain format in the form of csv/df, then convert them to either.","title":"Data Preparation"},{"location":"reighns_ml_journey/sp_ppe/#coco-dataset","text":"COCO Detection Eval","title":"COCO Dataset"},{"location":"reighns_ml_journey/sp_ppe/#pascal-voc-dataset","text":"TODOS: Currently convert coco to voc online, need write a script maybe since I wrote one for coco temporarily. For YOLOX, Pascal VOC is not well maintained, so for now the folder structure is rigidly defined below: \ud83d\udcc1 YOLOX/ \ud83d\udcc4 \u251c\u2500\u2500 datasets/ \ud83d\udcc4 \u2502 \u251c\u2500\u2500 VOCdevkit/ \ud83d\udcc4 \u2502 \u2502 \u251c\u2500\u2500 VOC2012/ \ud83d\udcc4 \u2502 \u2502 \u2502 \u251c\u2500\u2500 sp_ppe_voc_helmet_mask_vest/ \ud83d\udcc4 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 Annotations/ \ud83d\udcc4 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 ImageSets/ \ud83d\udcc4 | | | | | \u251c\u2500\u2500 Main/ \ud83d\udcc4 | | | | | | \u251c\u2500\u2500 train.txt \ud83d\udcc4 | | | | | | \u251c\u2500\u2500 val.txt \ud83d\udcc4 | | | | | | \u251c\u2500\u2500 test.txt \ud83d\udcc4 | | | | | | \u251c\u2500\u2500 trainval.txt \ud83d\udcc4 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 JPEGImages/ Without changing source code, the VOCdevkit and VOC{year} folder names are fixed and not negotiable. I attempted to do away with the VOC{year} folder, but it really requires changing a few places, and I am aware that even PyTorch's own voc.py dataset uses this same format. So I keep it as such for now but it is possible to loosen this restriction. Annotations folder contains all the xml files, which is each image's annotations info in Pascal VOC format. JPEGImages folder contains all the images. ImageSets folder contains the train/val/test split, which is a txt file with each line being the image name without the extension. For example, if the image is 000001.jpg , then the line in the txt file is 000001 . Note we do not need to use trainval.txt since we can just use train.txt and val.txt to train and validate. Note we should find a way to use KFolds here, might be cumbersome to train every fold manually?","title":"Pascal VOC Dataset"},{"location":"reighns_ml_journey/sp_ppe/#model-training","text":"Training your own YoloX Object Detection Model on Colab - YoloX Object Detection Model Deployment This quite useful as a starter. Coding your own dataset or dataloader class MMDetection Dataset Class https://github.com/pytorch/vision/blob/main/torchvision/datasets/voc.py","title":"Model Training"},{"location":"reighns_ml_journey/sp_ppe/#config-exp-file","text":"The main config file of YOLOX resides in the exps/ folder. YOLOX/exps/default/yolox_s.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class Exp ( MyExp ): def __init__ ( self ): super ( Exp , self ) . __init__ () self . num_classes = NUM_CLASSES # 20 print ( \"num_classes\" , self . num_classes ) self . depth = 0.33 self . width = 0.50 self . warmup_epochs = 1 # ---------- transform config ------------ # self . mosaic_prob = 1.0 self . mixup_prob = 1.0 self . hsv_prob = 1.0 self . flip_prob = 0.5 self . degrees = 10.0 self . translate = 0.1 self . scale = ( 0.1 , 2 ) self . mosaic_scale = ( 0.8 , 1.6 ) self . shear = 2.0 self . perspective = 0.0 self . enable_mixup = True","title":"Config (Exp) File"},{"location":"reighns_ml_journey/sp_ppe/#steps","text":"Make the data into pascal voc format and place it under YOLOX/datasets/VOCdevkit folder. Note you need to create VOCdevkit folder first. Follow exactly the format I have inside my folder. When making the data, I hardcoded a bit, make sure eventually is same format as the one here for now https://www.youtube.com/watch?v=be_D3V9Pxlg&t=2371s with some tweaks. Download weights to weights folder YOLOX/weights mkdir weights wget.exe -P . \\w eights \\ https://github.com/Megvii-BaseDetection/storage/releases/download/0.0.1/yolox_s.pth 4. Create exp file, meaning the configuration file for the experiment. mkdir exp/custom/sp_ppe cd exp/custom/sp_ppe touch sp_ppe_voc.py cd ../../../ see code inside, this file is very important as it overwrites the base class. 4.1. Note it is very important to change coco class and voc class file. Run the training python tools/train.py -f . \\e xps \\c ustom \\s p_ppe \\s p_ppe_voc_human_only.py -d 1 -b 4 --fp16 -o -c . \\w eights \\y olox_s.pth if there is Weights & Biases: python tools/train.py -f . \\e xps \\c ustom \\s p_ppe \\s p_ppe_voc_human_only.py -d 1 -b 4 --fp16 -o -c . \\w eights \\y olox_s.pth \\ --logger wandb \\ wandb-project yolox \\ wandb-log_checkpoints True \\ wandb-num_eval_images 3 \\ where num_eval_images means at every evaluation step, the dashboard also shows 3 images from the validation set along with the predicted bounding box. Run test MODEL_PATH = 'C:\\Users\\reighns\\reighns_ml\\ml_projects\\YOLOX\\YOLOX_outputs\\sp_ppe_voc_helmet_mask_vest\\latest_ckpt.pth' TEST_IMAGE_PATH python tools/demo.py image -f . \\e xps \\c ustom \\s p_ppe \\s p_ppe_voc_helmet_mask_vest.py -c . \\Y OLOX_outputs \\s p_ppe_voc_helmet_mask_vest \\l ast_epoch_ckpt.pth --path . \\d atasets \\V OCdevkit \\V OC2012 \\s p_ppe_voc_helmet_mask_vest \\t est \\h elmet--147-_jpg.rf.ed1d0dab4fc4b4f0c4213e6569a7ef02.jpg --conf 0 .25 --nms 0 .45 --tsize 640 --save_result --device gpu","title":"Steps"},{"location":"reighns_ml_journey/sp_ppe/#weights-biases","text":"See https://docs.wandb.ai/quickstart for more details. But basically use pip install wandb wandb login however for now comment out a chunk of code in logger since it has self.cats which only works for coco.","title":"Weights &amp; Biases"},{"location":"reighns_ml_journey/sp_ppe/#model-capability","text":"We first check if model can memorize, then generalize, the former is important to check for bugs, and to ensure our model has basic capability to learn, but no way does it guarantee that it can generalize. Regardless, we still want to ensure this as if it cannot even memorize in-sample data points, it for sure cannot generalize.","title":"Model Capability"},{"location":"reighns_ml_journey/sp_ppe/#references","text":"https://docs.wandb.ai/guides/track/log/media to log bounding boxes https://www.youtube.com/watch?v=be_D3V9Pxlg&t=2371s https://github.com/awsaf49/bbox/blob/main/bbox/utils.py https://www.kaggle.com/code/ayuraj/train-yolov5-cross-validation-ensemble-w-b/notebook https://github.com/roboflow-ai/YOLOX https://www.kaggle.com/code/awsaf49/great-barrier-reef-yolov5-train/notebook#%F0%9F%93%81-Create-Folds Transfer learning for YOLOX: https://github.com/Megvii-BaseDetection/YOLOX/issues/1105 + https://github.com/Megvii-BaseDetection/YOLOX/pull/1156 Converting darknet or yolov5 datasets to COCO format for YOLOX: YOLO2COCO from Daniel https://stackoverflow.com/questions/67733876/create-pascol-voc-xml-from-csv https://pyimagesearch.com/2022/05/02/mean-average-precision-map-using-the-coco-evaluator/ https://wandb.ai/manan-goel/yolox-nano/reports/Tracking-your-YOLOX-Runs-with-Weights-Biases---VmlldzoxNzc0NjA0 https://docs.wandb.ai/guides/integrations/other/yolox https://www.kaggle.com/code/remekkinas/yolox-training-pipeline-cots-dataset-lb-0-507/notebook#6.-RUN-INFERENCE https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch https://blog.roboflow.com/how-to-train-yolox-on-a-custom-dataset/ https://www.kaggle.com/code/litaldavar/hard-head-detection-with-yolov5/notebook https://www.kaggle.com/code/billiemage/object-detection","title":"References"},{"location":"reighns_ml_journey/interview/Machine_Learning_Notations/","text":"PAC Framework Formal Definitions Input Space: \\(\\mathcal{X}\\) The input space contains the set of all possible examples/instances in a population . This is generally unknown. Output Space: \\(\\mathcal{Y}\\) The output space is the set of all possible labels/targets that corresponds to each point in \\(\\mathcal{X}\\) . Concept: Unknown Ground Truth (Target) Function: \\(f: \\mathcal{X} \\to \\mathcal{Y}\\) where \\(\\mathrm{x} \\mapsto \\mathrm{y}\\) and \\(\\mathrm{y} = f(\\mathrm{x})\\) This is a mapping from the input space to the output space. This is the so-called true function which is the underlying true relationship between each pair of point \\((\\mathrm{x}, \\mathrm{y}) \\in \\mathcal{X} \\times \\mathcal{Y}\\) . However, this function \\(f\\) is unknown to us, or else we do not need to learn anything. We try our best to find a model that best estimates the unknown ground truth function \\(f\\) . Concept Class: \\(\\mathcal{C}\\) A concept class C is a set of true functions \\(f\\) . Hypothesis class H is the set of candidates to formulate as the final output of a learning algorithm to well approximate the true function f . Hypothesis class H is chosen before seeing the data (training process). C and H can be either same or not and we can treat them independently. Distribution : \\(\\mathcal{P}\\) Reference: Learning From Data p43. The unknown distribution that generated our input space \\(\\mathcal{X}\\) . In general, instead of the mapping \\(\\mathrm{y} = f(\\mathrm{x})\\) , we can take the output \\(\\mathrm{y}\\) to be a random variable that is affected by, rather than determined by, the input \\(\\mathrm{x}\\) . Formally, we have a target distribution \\(\\mathcal{P}(\\mathrm{y} | \\mathrm{x})\\) instead of just \\(\\mathrm{y} = f(\\mathrm{x})\\) . Now we say that any point \\((\\mathrm{x}, \\mathrm{y})\\) in \\(\\mathcal{X}\\) is now generated by the joint distribution \\( \\(\\mathcal{P}(\\mathrm{x}, \\mathrm{y}) = \\mathcal{P}(\\mathrm{x})\\mathcal{P}(\\mathrm{y} | \\mathrm{x})\\) \\) Data: \\(\\mathcal{D}\\) This is the set of samples drawn from \\(\\mathcal{X} \\times \\mathcal{Y}\\) over a distribution \\(\\mathcal{P}\\) . The general notation is as follows: \\( \\(\\mathcal{D} = [(\\mathrm{x^{(1)}}, \\mathrm{y^{(1)}}), (\\mathrm{x^{(2)}}, \\mathrm{y^{(2)}}), ..., (\\mathrm{x^{(N)}}, \\mathrm{y^{(N)}}))]\\) \\) where \\(N\\) denotes the number of training samples, and each \\(\\mathrm{x}^{(i)} \\in \\mathbb{R}^{n}\\) with \\(n\\) features. In general, \\(\\mathrm{y}^{(i)} \\in \\mathbb{R}\\) and is a single label. We can split \\(\\mathcal{D}\\) into two sets respectively, where \\(\\mathrm{X}\\) consists of all the \\(\\mathrm{x}\\) , and \\(\\mathrm{Y}\\) consists of all the \\(\\mathrm{y}\\) . We will see this next. Design Matrix: \\(\\mathrm{X}\\) Let \\(\\mathrm{X}\\) be the design matrix of dimensions \\(m\u2005\\times\u2005(n\u2005+\u20051)\\) where \\(m\\) is the number of observations (training samples) and \\(n\\) independent feature/input variables. Note the inconsistency in the matrix size, I just want to point out that the second matrix, has a column of one in the first row because we usually have a bias term \\(\\mathrm{x_0}\\) , which we set to 1. \\[\\mathrm{X} = \\begin{bmatrix} (\\mathbf{x^{(1)}})^{T} \\\\ (\\mathbf{x^{(2)}})^{T} \\\\ \\vdots \\\\ (\\mathbf{x^{(m)}})^{T}\\end{bmatrix}_{m \\times n} = \\begin{bmatrix} 1 & x_1^{(1)} & x_2^{(1)} & \\cdots & x_n^{(1)} \\\\\\\\ 1 & x_1^{(2)} & x_2^{(2)} & \\cdots & x_n^{(2)} \\\\\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\\\\ 1 & x_1^{(m)} & x_2^{(m)} & \\cdots & x_n^{(m)} \\end{bmatrix}_{m \\times (n+1)} \\] Single Training Vector: \\(\\mathrm{x}\\) It is worth noting the \\(\\mathrm{x}^{(i)}\\) defined above is formally defined to be the \\(i\\) -th column of \\(\\mathrm{X}\\) , which is the \\(i\\) -th training sample, represented as a \\(n \\times 1\\) column vector . However, the way we define the Design Matrix is that each row of \\(\\mathrm{X}\\) is the transpose of \\(\\mathrm{x}^{(i)}\\) . Note \\(x^{(i)}_j\\) is the value of feature/attribute j in the ith training instance. \\[\\mathbf{x^{(i)}} = \\begin{bmatrix} x_1^{(i)} \\\\ x_2^{(i)} \\\\ \\vdots \\\\ x_n^{(i)} \\end{bmatrix}_{n \\times 1}\\] Target/Label: \\(\\mathrm{Y}\\) This is the target vector. By default, it is a column vector of size \\(m \\times 1\\) . \\[\\mathbf{y} = \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)} \\end{bmatrix}_{m \\times 1}\\] Hypothesis Set: \\(\\mathcal{H}\\) The set where it contains all possible functions to approximate our true function \\(f\\) . Note that the Hypothesis Set can be either continuous or discrete, means to say it can be either a finite or infinite set. But in reality, it is almost always infinite. Hypothesis: \\(\\mathcal{h}: \\mathrm{X} \\to \\mathrm{Y}\\) where \\(\\mathrm{x} \\mapsto \\mathrm{y}\\) Note that this \\(\\mathcal{h} \\in \\mathcal{H}\\) is the hypothesis function, The final best hypothesis function is called \\(g\\) , which approximates the true function \\(f\\) . Learning Algorithm: \\(\\mathcal{A}\\) What this does is from the set of Hypothesis \\(\\mathcal{H}\\) , the learning algorithm's role is to pick one \\(\\mathcal{h} \\in \\mathcal{H}\\) such that this \\(h\\) is the hypothesis function. More often, we also call our final hypothesis learned from \\(\\mathcal{A}\\) \\(g\\) . Hypothesis Subscript \\(\\mathcal{D}\\) : \\(h_{\\mathcal{D}}\\) This is no different from the previous hypothesis, instead the previous \\(h\\) is a shorthand for this notation. This means that the hypothesis we choose is dependent on the sample data given to us, that is to say, given a \\(\\mathcal{D}\\) , we will use \\(\\mathcal{A}\\) to learn a \\(h_{\\mathcal{D}}\\) from \\(\\mathcal{H}\\) . Generalization Error/Test Error/Out-of-Sample Error: \\(\\mathcal{E}_{\\text{out}}(h)\\) Reference from Foundations of Machine Learning . Given a hypothesis \\(h \\in \\mathcal{H}\\) , a true function \\(f \\in \\mathcal{C}\\) , and an underlying distribution \\(\\mathcal{P}\\) , the test/out-of-sample error of \\(h\\) is defined by \\( \\(\\begin{aligned}\\mathcal{E}_{\\text{out}}(h) = \\underset{x \\sim \\mathcal{P}}{\\mathrm{Pr}}[h(\\mathrm{x}) \\neq f(\\mathrm{x})]\\end{aligned}\\) \\) Note that the above equation is just the error rate between the hypothesis function \\(h\\) and the true function \\(f\\) and as a result, the test error of a hypothesis is not known because both the distribution \\(\\mathcal{P}\\) and the true function \\(f\\) are unknown. This brings us to the next best thing we can measure, the In-sample/Empirical/Training Error. More formally, in a regression setting where we Mean Squared Error, \\( \\(\\begin{aligned}\\mathcal{E}_{\\text{out}}(h) = \\mathbb{E}_{\\mathrm{x}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}) - f(\\mathrm{x}))^2 \\right] \\end{aligned}\\) \\) This is difficult and confusing to understand. To water down the formal definition, it is worth taking an example, in \\(\\mathcal{E}_{\\text{out}}(h)\\) we are only talking about the Expected Test Error over the Test Set and nothing else. Think of a test set with only one query point , we call it \\(\\mathrm{x}_{q}\\) , then the above equation is just \\( \\(\\begin{aligned}\\mathcal{E}_{\\text{out}}(h) = \\mathbb{E}_{\\mathrm{x}_{q}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}_{q}) - f(\\mathrm{x}_{q}))^2 \\right] \\end{aligned}\\) \\) over a single point over the distribution \\(\\mathrm{x}_{q}\\) . That is if \\(\\mathrm{x}_{q} = 3\\) and \\(h_{\\mathcal{D}}(\\mathrm{x}_{q}) = 2\\) and \\(f(\\mathrm{x}_{q}) = 5\\) , then \\((h_{\\mathcal{D}}(\\mathrm{x}_{q}) - f(\\mathrm{x}_{q}))^2 = 9\\) and it follows that \\( \\(\\mathcal{E}_{\\text{out}}(h) = \\mathbb{E}_{\\mathrm{x}_{q}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}_{q}) - f(\\mathrm{x}_{q}))^2 \\right] = \\mathbb{E}_{\\mathrm{x}_{q}}[9] = \\frac{9}{1} = 9\\) \\) Note that I purposely denoted the denominator to be 1 because we have only 1 test point, if we were to have 2 test point, say \\(\\mathrm{x} = [x_{p}, x_{q}] = [3, 6]\\) , then if \\(h_{\\mathcal{D}}(x_{p}) = 4\\) and \\(f(x_{p}) = 6\\) , then our \\((h_{\\mathcal{D}}(\\mathrm{x}_{p}) - f(\\mathrm{x}_{p}))^2 = 4\\) . Then our \\( \\(\\mathcal{E}_{\\text{out}}(h) = \\mathbb{E}_{\\mathrm{x}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}) - f(\\mathrm{x}))^2 \\right] = \\mathbb{E}_{\\mathrm{x}_{q}}[[9, 4]] = \\frac{1}{2} [9 + 4] = 6.5\\) \\) Note how I secretly removed the subscript in \\(\\mathrm{x}\\) , and how when there are two points, we are taking expectation over the 2 points. So if we have \\(m\\) test points, then the expectation is taken over all the test points. Till now, our hypothesis \\(h\\) is fixed over a particular sample set \\(\\mathcal{D}\\) . We will now move on to the next concept on Expected Generalization Error (adding a word Expected in front makes a lot of difference). Expected Generalization Error/Test Error/Out-of-Sample Error: \\(\\mathbb{E}_{\\mathcal{D}}[\\mathcal{E}_{\\text{out}}(h)]\\) For the previous generalization error, we are only talking a fixed hypothesis generated by one particular \\(\\mathcal{D}\\) . In order to remove this dependency, we can simply take the expectation of Generalization Error of \\(h\\) over a particular \\(\\mathcal{D}\\) by simply taking the expectation over all such \\(\\mathcal{D}_{i}\\) , \\(i = 1,2,3,...K\\) . Then the Expected Generalization Test Error is independent of any particular realization of \\(\\mathcal{D}\\) : \\[\\begin{aligned}\\mathbb{E}_{\\mathcal{D}}[\\mathcal{E}_{\\text{out}}(h)] = \\mathbb{E}_{\\mathcal{D}}[\\mathbb{E}_{\\mathrm{x}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}) - f(\\mathrm{x}))^2 \\right]] \\end{aligned}\\] In the following example, we can calculate the Expected Generalization Error, where we are using the Error to be Mean Squared Error, so in essence, we are finding the expected MSE. Empirical Error/Training Error/In-Sample Error: \\(\\mathcal{E}_{\\text{in}}(h)\\) Given a hypothesis \\(h \\in \\mathcal{H}\\) , a true function \\(f \\in \\mathcal{C}\\) , and an underlying distribution \\(\\mathcal{P}\\) , and a sample \\(\\mathrm{X}\\) drawn from \\(\\mathcal{X}\\) i.i.d with distribution \\(\\mathcal{P}\\) , the test/out-of-sample error of \\(h\\) is defined by \\( \\(\\begin{aligned}\\mathcal{E}_{\\text{in}}(h) = \\frac{1}{\\mathrm{m}}\\sum_{i=1}^{\\mathrm{m}}\\text{sign}[h(\\mathrm{x}^{(i)}) \\neq f(\\mathrm{x}^{(i)})]\\end{aligned}\\) \\) Here the sign function is mainly used for binary classification, where if \\(h\\) and \\(f\\) disagrees at any point \\(x^{(i)}\\) , then \\(\\text{sign}[h(\\mathrm{x}^{(i)}) \\neq f(\\mathrm{x}^{(i)})]\\) evaluates to 1. We take the sum of all disagreements and divide by the total number of samples. In short, that is just the misclassification/error rate. The empirical error of \\(h \\in \\mathcal{H}\\) is its average error over the sample \\(\\mathcal{X}\\) , in contrast, the generalization error is its expected error based on the distribution \\(\\mathcal{P}\\) . Take careful note here that \\(h(x^{(i)})\\) is the prediction made by our hypothesis (model), we can conventionally call it \\(\\hat{y}^{(i)}\\) whereby our \\(f(x^{(i)})\\) is our ground truth label \\(y^{(i)}\\) . I believe that this ground truth label is realized once we draw the sample from \\(\\mathcal{X}\\) even though we do not know what \\(f\\) is. An additional note here, is that the summand of the in-sample error function is not fixated to the sign function. In fact, I believe you can define any loss function to calculate the \"error\". As an example, if we are dealing with regression, then we can modify the summand to our favourite Mean Squared Error. \\[\\begin{aligned}\\mathcal{E}_{\\text{in}}(h) = \\frac{1}{\\mathrm{m}}\\sum_{i=1}^{\\mathrm{m}}[h(\\mathrm{x}^{(i)}) - f(\\mathrm{x}^{(i)})]^2\\end{aligned}\\] Average/Mean Hypothesis: \\(\\bar{h}_{\\mathcal{D}} = \\mathbb{E}_{\\mathcal{D}}[h_{\\mathcal{D}}(\\mathrm{x})]\\) This may seem confusing at first sight, but it cactually means the following: Generate many data sets \\(\\mathcal{D}_{i}\\) , \\(i = 1,2,3,..., K\\) from population \\(\\mathcal{X} \\times \\mathcal{Y}\\) over a probability distribution \\(\\mathcal{P}\\) . Apply the learning algorithm \\(\\mathcal{A}\\) to each \\(\\mathcal{D}_{i}\\) to get \\(K\\) number of hypothesis \\(h_{i} = h_{\\mathcal{D}_{i}}\\) We can then call the average function for any \\(\\mathrm{x}\\) by \\( \\(\\bar{h}_{\\mathcal{D}} = \\mathbb{E}_{\\mathcal{D}}[h_{\\mathcal{D}}(\\mathrm{x})]\\) \\) As an example, if there are 3 \\(h_{i}\\) , where \\(h_1 = 2x+1\\) , \\(h_2 = 2x+3\\) , and \\(h_3 = 3x+1\\) , then given any query point say \\(x_{q} = 3\\) , then \\(\\bar{h}(\\mathrm{x}_{q}) = \\dfrac{h_1 + h_2 + h_3}{3} = \\frac{26}{3}\\) . This function, or average hypothesis is vital for our calculation of bias and variance. Bias - Variance Decomposition This is a decomposition of the Expected Generalization Error. Formal Proof please read Learning From Data. Unless otherwise stated, we consider only the univariate case where \\(\\mathrm{x}\\) is a single test point. \\[\\begin{align*} \\mathbb{E}_{\\mathcal{D}}[\\mathcal{E}_{\\text{out}}(h)] &= \\mathbb{E}_{\\mathcal{D}}[\\mathbb{E}_{\\mathrm{x}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}) - f(\\mathrm{x}))^2 \\right]] \\\\ &= \\big(\\;\\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;] - f(x)\\;\\big)^2 + \\mathbb{E}_{\\mathcal{D}}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;])^2\\;\\big] + \\mathbb{E}\\big[(y-f(x))^2\\big] \\\\ &= \\big(\\;\\bar{h}(\\mathrm{x}) - f(x)\\;\\big)^2 + \\mathbb{E}_\\mathcal{D}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\bar{h}(\\mathrm{x}) \\;])^2\\;\\big]+ \\mathbb{E}\\big[(y-f(x))^2\\big] \\end{align*} \\] Where $\\big(\\;\\mathbb{E} {\\mathcal{D}}[\\;h }(x)\\;] - f(x)\\;\\big)^2 $ is the Bias, \\(\\mathbb{E}_\\mathcal{D}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;])^2\\;\\big]\\) is the Variance and \\(\\mathbb{E}\\big[(y-f(x))^2\\big]\\) is the irreducible error \\(\\epsilon\\) . Bias: \\(\\big(\\;\\mathbb{E}_\\mathcal{D}[\\;h_{\\mathcal{D}}(x)\\;] - f(x)\\;\\big)^2\\) In other form, we can express Bias as \\( \\(\\big(\\;\\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;] - f(x)\\;\\big)^2 = \\big(\\;\\bar{h}(\\mathrm{x}) - f(x)\\;\\big)^2\\) \\) See simulation on Bias-Variance Tradeoff to understand. If our test point is \\(x_{q} = 0.9\\) , then our bias is as such: \\[ \\widehat{\\text{bias}} \\left(\\hat{f}(0.90) \\right) = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(\\hat{f}_k^{[i]}(0.90) \\right) - f(0.90) \\] Variance: \\(\\mathbb{E}_\\mathcal{D}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;])^2\\;\\big]\\) This is more confusing, but we first express Variance as: \\[\\mathbb{E}_\\mathcal{D}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;])^2\\;\\big] = \\mathbb{E}_\\mathcal{D}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\bar{h}(\\mathrm{x}) \\;])^2\\;\\big]\\] If our test point is \\(x_{q} = 0.9\\) , then our variance is as such: \\[ \\widehat{\\text{var}} \\left(\\hat{f}(0.90) \\right) = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(\\hat{f}_k^{[i]}(0.90) - \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}}\\hat{f}_k^{[i]}(0.90) \\right)^2 \\] Loss Function: \\(\\mathcal{L}\\) Performance Metric: \\(\\mathcal{M}\\) Corresponding Intuition of Formal Definitions The learning problem is then formulated as follows: Given a training set \\(\\mathcal{D}\\) , sampled i.i.d. from \\(\\mathcal{X} \\times \\mathcal{Y}\\) over a distribution \\(\\mathcal{P}\\) where there exists a true function \\(f\\) that relates \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\) , we attempt to approximate the true relationship \\(f\\) with an optimal \\(\\mathcal{g}\\) from a hypothesis set \\(\\mathcal{H}\\) using a pre-defined learning algorithm \\(\\mathcal{A}\\) . In particular, \\(\\mathcal{g}\\) is chosen from multiple \\(h \\in \\mathcal{H}\\) where the optimal \\(g\\) gives the lowest in-sample (training) error, or more concretely, gives the lowest in-sample-validation error \\(\\mathcal{E}_{\\text{in}}(g)\\) . One thing worth noting, although prematurely at this stage, that we often look at both the in-sample error and the performance metric to decide on the optimal \\(g\\) . Input Space: \\(\\mathrm{x} \\in \\mathcal{X}\\) or \\(\\mathrm{x} \\in \\mathcal{P}\\) The input space contains the set of all possible examples/instances in a population . This is generally unknown. This is the input space, containing the set of all possible examples, or instances in a population . In general, we can think of it probabilistically and say that, a population follows a distribution \\(\\mathcal{P}\\) and we generate i.i.d samples to form our sample (often called the training set) \\(\\mathcal{D}\\) from this input space \\(\\mathcal{X}\\) . \\(\\mathcal{X} = \\(\\{0,1\\}\\) ^3\\) \\(\\mathrm{x}\\) is the feature vector and as an element of \\(\\mathcal{X}\\) , it can take on any of the following 8 distinct vectors: \\([0,0,0], [0,0,1],...,[1,1,1]\\) In further lectures, \\(\\mathcal{X}\\) can be considered as a probability distribution \\(\\mathcal{P}\\) , and picking points \\(x\\) from \\(\\mathcal{X}\\) means generating them. Output Space: \\(\\mathrm{y} \\in \\mathcal{Y}\\) This is the output space, where it corresponds to each point in \\(\\mathcal{X}\\) . This is the set of all possible labels/targets. \\(\\mathcal{y} = \\(\\{0, 1\\}\\) ^1\\) Thus, \\(\\mathrm{y}\\) can take on either 0 or 1, a binary output. Concept: Unknown Ground Truth (Target) Function: \\(f: \\mathcal{X} \\to \\mathcal{Y}\\) where \\(\\mathrm{x} \\mapsto \\mathrm{y}\\) This is a mapping from the input space to the output space. This is the so-called true function which is the underlying true relationship between each pair of point \\((\\mathrm{x}, \\mathrm{y}) \\in \\mathcal{X} \\times \\mathcal{Y}\\) . However, this function \\(f\\) is unknown to us, or else we do not need to learn anything. We try our best to find a model that best estimates the unknown ground truth function \\(f\\) . Note this is often called the \"concept\" in the PAC framework. Concept Class: \\(\\mathcal{C}\\) A concept class C is a set of true functions \\(f\\) . Hypothesis class H is the set of candidates to formulate as the final output of a learning algorithm to well approximate the true function f . Hypothesis class H is chosen before seeing the data (training process). C and H can be either same or not and we can treat them independently. Distribution : \\(\\mathcal{P}\\) Read The Deep Learning Book: Chapter 5, in particular, Section 5.2. The train and test data are generated by a probability distribution over datasets called the data generating process . We typically make a set of assumptions known collectively as the i.i.d. assumptions . These assumptions are that the examples in each dataset are independent from each other, and that the train set and test set are identically distributed, drawn from the same probability distribution as each other. This assumption allows us to describe the data generating process with a probability distribution over a single example. The same distribution is then used to generate every train example and every test example. We call that shared underlying distribution the data generating distribution, denoted pdata. This probabilistic framework and the i.i.d. assumptions allow us to mathematically study the relationship between training error and test error The unknown distribution that generated our input space \\(\\mathcal{X}\\) . The distribution can be say, a Gaussian Distribution with mean 0 and standard deviation 1, and we generate samples from More references below: https://stats.stackexchange.com/questions/515806/need-help-with-understanding-random-variables-the-data-generating-distribution https://stats.stackexchange.com/questions/481047/relationship-between-distribution-and-data-generating-process https://stats.stackexchange.com/questions/320375/what-does-it-mean-for-the-training-data-to-be-generated-by-a-probability-distrib https://datascience.stackexchange.com/questions/54346/data-generating-probability-distribution-probability-distribution-of-a-dataset https://stats.stackexchange.com/questions/542010/the-deep-learning-book-considers-the-empirical-distribution-hat-p-data-a-fu https://web.mit.edu/urban_or_book/www/book/chapter7/7.1.3.html Data: \\(\\mathcal{D}\\) This is what we usually get in a Machine Learning Problem, we have a training set, sampled from \\(\\mathcal{X}\\) with distribution \\(\\mathcal{P}\\) . The notation are usually as follows: \\( \\(\\mathcal{D} = [(\\mathrm{x^{(1)}}, \\mathrm{y^{(1)}}), (\\mathrm{x^{(2)}}, \\mathrm{y^{(2)}}), ..., (\\mathrm{x^{(N)}}, \\mathrm{y^{(N)}}))]\\) \\) OR \\( \\(\\mathcal{D} = [(\\mathrm{x_1}, \\mathrm{y_1}), (\\mathrm{x_2}, \\mathrm{y_2}), ..., (\\mathrm{x_N}, \\mathrm{y_N})]\\) \\) or The dataset where \\(\\mathrm{N}\\) is the number of samples (training samples). Note that this is actually a 2d matrix where row \\(i\\) is training sample \\(i\\) and column \\(j\\) is feature \\(j\\) . In our simple example, we define \\(\\mathrm{N} = 5\\) . The below are the five samples, for example, sample 1 \\(\\mathrm{x_1} = [0,0,0]\\) and outputs ground truth \\(\\mathrm{y} = 0\\) where white circle = 0, black circle = 1. I tend to be more used to the fact that we denote \\(m\\) to be number of samples, and \\(n\\) to be number of features, so I may use it interchangeably later. In addition, I may also denote \\(\\mathcal{D}\\) to be \\(\\mathrm{X}\\) , both are what we called the Design Matrix. Hypothesis Set: \\(\\mathcal{H}\\) The set where it contains all possible functions to approximate our true function \\(f\\) . Note that the Hypothesis Set can be either continuous or discrete, means to say it can be either a finite or infinite set. But in reality, it is almost always infinite. Read here for more information. For example: Let \\(\\mathcal{H}\\) be the set of neural networks available. Imagine there are a lot of different neural network functions inside (i.e. y = w^Tx+b is the simplest form of neural network, and it has many possible permutations because the weight vector w and b can be changed. Therefore, we decide that we should use neural network as our hypothesis set, and from there, we deploy our learning algorithm \\(\\mathcal{A}\\) , the backpropagation, to learn and output as with the so-called optimal weight set - to formulate our best approximated function \\(g \\in \\mathcal{H}\\) . Back to our example, we can actually enumerate all possible cases of the function to approximate \\(f\\) . Since the input space \\(\\mathcal{X}\\) has 8 distinct vectors, and \\(f\\) is a Boolean Function which takes in 3 Boolean Inputs, outputs 2 outputs, we have \\(2^{2^3} = 256\\) distinct Boolean Functions on 3 Boolean Inputs. Hypothesis: \\(h: \\mathcal{D} \\to \\mathcal{y}\\) where \\(\\mathrm{x} \\mapsto \\mathrm{y}\\) Note that this \\(h\\) is the hypothesis function, The final best hypothesis function is called \\(g\\) , which approximates the true function \\(f\\) . We can see that \\(\\mathcal{XOR}\\) is a function that can be used to approximate \\(f\\) . Learning Algorithm: \\(\\mathcal{A}\\) What this does is from the set of Hypothesis \\(\\mathcal{H}\\) , the learning algorithm's role is to pick one \\(\\mathcal{g} \\in \\mathcal{H}\\) such that this \\(g\\) is the hypothesis function. Intuition see Hypothesis Set. You can think of hypothesis space of linear regression as the set of linear functions of your features. For simplicity assume the feature space is the entire \\(\\mathbb R^d\\) . Then, the hypothesis space of linear regression is \\[\\mathcal F = \\{f: \\mathbb R^d \\to \\mathbb R :\\; f(x) = \\beta^T x, \\quad \\forall x \\in \\mathbb R^d,\\; \\text{and some $\\beta \\in \\mathbb R^d$}\\}.\\] This class is a parametric family and can be equivalently described by the set of coefficients \\(\\{ \\beta:\\; \\beta \\in \\mathbb R^d\\}\\) . Basic Setup of the Learning Problem Basic Setup of the Learning Problem with Probability Distribution \\(\\mathcal{P}\\) Theorems Expected Training Error = Expected Test Error The Deep Learning Book p111 Foundation of Machine Learning p12-13 The Deep Learning Book: One immediate connection we can observe between the training and test error is that the expected training error of a randomly selected model is equal to the expected test error of that model. Suppose we have a probability distribution p(x, y) and we sample from it repeatedly to generate the train set and the test set. For some fixed value w, the expected training set error is exactly the same as the expected test set error, because both expectations are formed using the same dataset sampling process. The only difference between the two conditions is the name we assign to the dataset we sample. Of course, when we use a machine learning algorithm, we do not fix the parameters ahead of time, then sample both datasets. We sample the training set, then use it to choose the parameters to reduce training set error, then sample the test set. Under this process, the expected test error is greater than or equal to the expected value of training error. The factors determining how well a machine learning algorithm will perform are its ability to: 1. Make the training error small. 2. Make the gap between training and test error small. These two factors correspond to the two central challenges in machine learning: underfitting and overfitting . Underfitting occurs when the model is not able to obtain a sufficiently low error value on the training set. Overfitting occurs when the gap between the training error and test error is too large. Mathematical Notations Sign function \\[\\operatorname{sign}(x) = \\begin{cases} 1 & x > 0 \\\\ 0 & x = 0 \\\\ -1 & x < 0 \\tag{1.1} \\end{cases}\\] Line Equation In the case of two variables, any linear equation can be in the form of \\[ax+by+c=0 \\tag{1.2}\\] In the case of two points: \\((x_1, y_1), (x_2, y_2)\\) , to find the equation of the line, we find the slope \\(m = \\dfrac{y_2-y_1}{x_2-x_1}\\) . Then use one pair of point \\((x_1, y_1)\\) to solve for the intercept \\(c\\) . Finally, we have \\(y = mx + c\\) . \\(R^{n}\\) space This is called the n-dimensional space, and thus any element of \\(R^{n}\\) is a n-tuple: \\[(x_1, x_2,...,x_n) \\tag{1.3}\\] In Machine Learning, if your dataset has 2 features, then any training instance \\(x^{(i)}\\) is inside \\(R^{2}\\) space and represents \\((x_{1}^{(i)}, x_{2}^{(i)})\\) Readings and References Foundations of Machine Learning : Chapter 2, p24-27 The Deep Learning Book: Chapter 5.1, 5.2 Learning From Data: Chapter 1 Concept Class Probability Distribution \\(\\mathcal{P}\\) https://stats.stackexchange.com/questions/515806/need-help-with-understanding-random-variables-the-data-generating-distribution https://stats.stackexchange.com/questions/481047/relationship-between-distribution-and-data-generating-process https://stats.stackexchange.com/questions/320375/what-does-it-mean-for-the-training-data-to-be-generated-by-a-probability-distrib https://datascience.stackexchange.com/questions/54346/data-generating-probability-distribution-probability-distribution-of-a-dataset https://stats.stackexchange.com/questions/542010/the-deep-learning-book-considers-the-empirical-distribution-hat-p-data-a-fu https://web.mit.edu/urban_or_book/www/book/chapter7/7.1.3.html Hypothesis Set https://stats.stackexchange.com/questions/183989/what-exactly-is-a-hypothesis-space-in-machine-learning Expected Training Error = Expected Test Error The Deep Learning Book: p111 Foundations of Machine Learning: p12-13","title":"PAC Framework"},{"location":"reighns_ml_journey/interview/Machine_Learning_Notations/#pac-framework","text":"","title":"PAC Framework"},{"location":"reighns_ml_journey/interview/Machine_Learning_Notations/#formal-definitions","text":"","title":"Formal Definitions"},{"location":"reighns_ml_journey/interview/Machine_Learning_Notations/#input-space-mathcalx","text":"The input space contains the set of all possible examples/instances in a population . This is generally unknown.","title":"Input Space: \\(\\mathcal{X}\\)"},{"location":"reighns_ml_journey/interview/Machine_Learning_Notations/#output-space-mathcaly","text":"The output space is the set of all possible labels/targets that corresponds to each point in \\(\\mathcal{X}\\) .","title":"Output Space: \\(\\mathcal{Y}\\)"},{"location":"reighns_ml_journey/interview/Machine_Learning_Notations/#concept-unknown-ground-truth-target-function-f-mathcalx-to-mathcaly-where-mathrmx-mapsto-mathrmy-and-mathrmy-fmathrmx","text":"This is a mapping from the input space to the output space. This is the so-called true function which is the underlying true relationship between each pair of point \\((\\mathrm{x}, \\mathrm{y}) \\in \\mathcal{X} \\times \\mathcal{Y}\\) . However, this function \\(f\\) is unknown to us, or else we do not need to learn anything. We try our best to find a model that best estimates the unknown ground truth function \\(f\\) .","title":"Concept: Unknown Ground Truth (Target) Function: \\(f: \\mathcal{X} \\to \\mathcal{Y}\\) where \\(\\mathrm{x} \\mapsto \\mathrm{y}\\) and \\(\\mathrm{y} = f(\\mathrm{x})\\)"},{"location":"reighns_ml_journey/interview/Machine_Learning_Notations/#concept-class-mathcalc","text":"A concept class C is a set of true functions \\(f\\) . Hypothesis class H is the set of candidates to formulate as the final output of a learning algorithm to well approximate the true function f . Hypothesis class H is chosen before seeing the data (training process). C and H can be either same or not and we can treat them independently.","title":"Concept Class: \\(\\mathcal{C}\\)"},{"location":"reighns_ml_journey/interview/Machine_Learning_Notations/#distribution-mathcalp","text":"Reference: Learning From Data p43. The unknown distribution that generated our input space \\(\\mathcal{X}\\) . In general, instead of the mapping \\(\\mathrm{y} = f(\\mathrm{x})\\) , we can take the output \\(\\mathrm{y}\\) to be a random variable that is affected by, rather than determined by, the input \\(\\mathrm{x}\\) . Formally, we have a target distribution \\(\\mathcal{P}(\\mathrm{y} | \\mathrm{x})\\) instead of just \\(\\mathrm{y} = f(\\mathrm{x})\\) . Now we say that any point \\((\\mathrm{x}, \\mathrm{y})\\) in \\(\\mathcal{X}\\) is now generated by the joint distribution \\( \\(\\mathcal{P}(\\mathrm{x}, \\mathrm{y}) = \\mathcal{P}(\\mathrm{x})\\mathcal{P}(\\mathrm{y} | \\mathrm{x})\\) \\)","title":"Distribution: \\(\\mathcal{P}\\)"},{"location":"reighns_ml_journey/interview/Machine_Learning_Notations/#data-mathcald","text":"This is the set of samples drawn from \\(\\mathcal{X} \\times \\mathcal{Y}\\) over a distribution \\(\\mathcal{P}\\) . The general notation is as follows: \\( \\(\\mathcal{D} = [(\\mathrm{x^{(1)}}, \\mathrm{y^{(1)}}), (\\mathrm{x^{(2)}}, \\mathrm{y^{(2)}}), ..., (\\mathrm{x^{(N)}}, \\mathrm{y^{(N)}}))]\\) \\) where \\(N\\) denotes the number of training samples, and each \\(\\mathrm{x}^{(i)} \\in \\mathbb{R}^{n}\\) with \\(n\\) features. In general, \\(\\mathrm{y}^{(i)} \\in \\mathbb{R}\\) and is a single label. We can split \\(\\mathcal{D}\\) into two sets respectively, where \\(\\mathrm{X}\\) consists of all the \\(\\mathrm{x}\\) , and \\(\\mathrm{Y}\\) consists of all the \\(\\mathrm{y}\\) . We will see this next.","title":"Data: \\(\\mathcal{D}\\)"},{"location":"reighns_ml_journey/interview/Machine_Learning_Notations/#design-matrix-mathrmx","text":"Let \\(\\mathrm{X}\\) be the design matrix of dimensions \\(m\u2005\\times\u2005(n\u2005+\u20051)\\) where \\(m\\) is the number of observations (training samples) and \\(n\\) independent feature/input variables. Note the inconsistency in the matrix size, I just want to point out that the second matrix, has a column of one in the first row because we usually have a bias term \\(\\mathrm{x_0}\\) , which we set to 1. \\[\\mathrm{X} = \\begin{bmatrix} (\\mathbf{x^{(1)}})^{T} \\\\ (\\mathbf{x^{(2)}})^{T} \\\\ \\vdots \\\\ (\\mathbf{x^{(m)}})^{T}\\end{bmatrix}_{m \\times n} = \\begin{bmatrix} 1 & x_1^{(1)} & x_2^{(1)} & \\cdots & x_n^{(1)} \\\\\\\\ 1 & x_1^{(2)} & x_2^{(2)} & \\cdots & x_n^{(2)} \\\\\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\\\\ 1 & x_1^{(m)} & x_2^{(m)} & \\cdots & x_n^{(m)} \\end{bmatrix}_{m \\times (n+1)} \\]","title":"Design Matrix: \\(\\mathrm{X}\\)"},{"location":"reighns_ml_journey/interview/Machine_Learning_Notations/#single-training-vector-mathrmx","text":"It is worth noting the \\(\\mathrm{x}^{(i)}\\) defined above is formally defined to be the \\(i\\) -th column of \\(\\mathrm{X}\\) , which is the \\(i\\) -th training sample, represented as a \\(n \\times 1\\) column vector . However, the way we define the Design Matrix is that each row of \\(\\mathrm{X}\\) is the transpose of \\(\\mathrm{x}^{(i)}\\) . Note \\(x^{(i)}_j\\) is the value of feature/attribute j in the ith training instance. \\[\\mathbf{x^{(i)}} = \\begin{bmatrix} x_1^{(i)} \\\\ x_2^{(i)} \\\\ \\vdots \\\\ x_n^{(i)} \\end{bmatrix}_{n \\times 1}\\]","title":"Single Training Vector: \\(\\mathrm{x}\\)"},{"location":"reighns_ml_journey/interview/Machine_Learning_Notations/#targetlabel-mathrmy","text":"This is the target vector. By default, it is a column vector of size \\(m \\times 1\\) . \\[\\mathbf{y} = \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)} \\end{bmatrix}_{m \\times 1}\\]","title":"Target/Label: \\(\\mathrm{Y}\\)"},{"location":"reighns_ml_journey/interview/Machine_Learning_Notations/#hypothesis-set-mathcalh","text":"The set where it contains all possible functions to approximate our true function \\(f\\) . Note that the Hypothesis Set can be either continuous or discrete, means to say it can be either a finite or infinite set. But in reality, it is almost always infinite.","title":"Hypothesis Set: \\(\\mathcal{H}\\)"},{"location":"reighns_ml_journey/interview/Machine_Learning_Notations/#hypothesis-mathcalh-mathrmx-to-mathrmy-where-mathrmx-mapsto-mathrmy","text":"Note that this \\(\\mathcal{h} \\in \\mathcal{H}\\) is the hypothesis function, The final best hypothesis function is called \\(g\\) , which approximates the true function \\(f\\) .","title":"Hypothesis: \\(\\mathcal{h}: \\mathrm{X} \\to \\mathrm{Y}\\) where \\(\\mathrm{x} \\mapsto \\mathrm{y}\\)"},{"location":"reighns_ml_journey/interview/Machine_Learning_Notations/#learning-algorithm-mathcala","text":"What this does is from the set of Hypothesis \\(\\mathcal{H}\\) , the learning algorithm's role is to pick one \\(\\mathcal{h} \\in \\mathcal{H}\\) such that this \\(h\\) is the hypothesis function. More often, we also call our final hypothesis learned from \\(\\mathcal{A}\\) \\(g\\) .","title":"Learning Algorithm: \\(\\mathcal{A}\\)"},{"location":"reighns_ml_journey/interview/Machine_Learning_Notations/#hypothesis-subscript-mathcald-h_mathcald","text":"This is no different from the previous hypothesis, instead the previous \\(h\\) is a shorthand for this notation. This means that the hypothesis we choose is dependent on the sample data given to us, that is to say, given a \\(\\mathcal{D}\\) , we will use \\(\\mathcal{A}\\) to learn a \\(h_{\\mathcal{D}}\\) from \\(\\mathcal{H}\\) .","title":"Hypothesis Subscript \\(\\mathcal{D}\\): \\(h_{\\mathcal{D}}\\)"},{"location":"reighns_ml_journey/interview/Machine_Learning_Notations/#generalization-errortest-errorout-of-sample-error-mathcale_textouth","text":"Reference from Foundations of Machine Learning . Given a hypothesis \\(h \\in \\mathcal{H}\\) , a true function \\(f \\in \\mathcal{C}\\) , and an underlying distribution \\(\\mathcal{P}\\) , the test/out-of-sample error of \\(h\\) is defined by \\( \\(\\begin{aligned}\\mathcal{E}_{\\text{out}}(h) = \\underset{x \\sim \\mathcal{P}}{\\mathrm{Pr}}[h(\\mathrm{x}) \\neq f(\\mathrm{x})]\\end{aligned}\\) \\) Note that the above equation is just the error rate between the hypothesis function \\(h\\) and the true function \\(f\\) and as a result, the test error of a hypothesis is not known because both the distribution \\(\\mathcal{P}\\) and the true function \\(f\\) are unknown. This brings us to the next best thing we can measure, the In-sample/Empirical/Training Error. More formally, in a regression setting where we Mean Squared Error, \\( \\(\\begin{aligned}\\mathcal{E}_{\\text{out}}(h) = \\mathbb{E}_{\\mathrm{x}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}) - f(\\mathrm{x}))^2 \\right] \\end{aligned}\\) \\) This is difficult and confusing to understand. To water down the formal definition, it is worth taking an example, in \\(\\mathcal{E}_{\\text{out}}(h)\\) we are only talking about the Expected Test Error over the Test Set and nothing else. Think of a test set with only one query point , we call it \\(\\mathrm{x}_{q}\\) , then the above equation is just \\( \\(\\begin{aligned}\\mathcal{E}_{\\text{out}}(h) = \\mathbb{E}_{\\mathrm{x}_{q}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}_{q}) - f(\\mathrm{x}_{q}))^2 \\right] \\end{aligned}\\) \\) over a single point over the distribution \\(\\mathrm{x}_{q}\\) . That is if \\(\\mathrm{x}_{q} = 3\\) and \\(h_{\\mathcal{D}}(\\mathrm{x}_{q}) = 2\\) and \\(f(\\mathrm{x}_{q}) = 5\\) , then \\((h_{\\mathcal{D}}(\\mathrm{x}_{q}) - f(\\mathrm{x}_{q}))^2 = 9\\) and it follows that \\( \\(\\mathcal{E}_{\\text{out}}(h) = \\mathbb{E}_{\\mathrm{x}_{q}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}_{q}) - f(\\mathrm{x}_{q}))^2 \\right] = \\mathbb{E}_{\\mathrm{x}_{q}}[9] = \\frac{9}{1} = 9\\) \\) Note that I purposely denoted the denominator to be 1 because we have only 1 test point, if we were to have 2 test point, say \\(\\mathrm{x} = [x_{p}, x_{q}] = [3, 6]\\) , then if \\(h_{\\mathcal{D}}(x_{p}) = 4\\) and \\(f(x_{p}) = 6\\) , then our \\((h_{\\mathcal{D}}(\\mathrm{x}_{p}) - f(\\mathrm{x}_{p}))^2 = 4\\) . Then our \\( \\(\\mathcal{E}_{\\text{out}}(h) = \\mathbb{E}_{\\mathrm{x}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}) - f(\\mathrm{x}))^2 \\right] = \\mathbb{E}_{\\mathrm{x}_{q}}[[9, 4]] = \\frac{1}{2} [9 + 4] = 6.5\\) \\) Note how I secretly removed the subscript in \\(\\mathrm{x}\\) , and how when there are two points, we are taking expectation over the 2 points. So if we have \\(m\\) test points, then the expectation is taken over all the test points. Till now, our hypothesis \\(h\\) is fixed over a particular sample set \\(\\mathcal{D}\\) . We will now move on to the next concept on Expected Generalization Error (adding a word Expected in front makes a lot of difference).","title":"Generalization Error/Test Error/Out-of-Sample Error: \\(\\mathcal{E}_{\\text{out}}(h)\\)"},{"location":"reighns_ml_journey/interview/Machine_Learning_Notations/#expected-generalization-errortest-errorout-of-sample-error-mathbbe_mathcaldmathcale_textouth","text":"For the previous generalization error, we are only talking a fixed hypothesis generated by one particular \\(\\mathcal{D}\\) . In order to remove this dependency, we can simply take the expectation of Generalization Error of \\(h\\) over a particular \\(\\mathcal{D}\\) by simply taking the expectation over all such \\(\\mathcal{D}_{i}\\) , \\(i = 1,2,3,...K\\) . Then the Expected Generalization Test Error is independent of any particular realization of \\(\\mathcal{D}\\) : \\[\\begin{aligned}\\mathbb{E}_{\\mathcal{D}}[\\mathcal{E}_{\\text{out}}(h)] = \\mathbb{E}_{\\mathcal{D}}[\\mathbb{E}_{\\mathrm{x}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}) - f(\\mathrm{x}))^2 \\right]] \\end{aligned}\\] In the following example, we can calculate the Expected Generalization Error, where we are using the Error to be Mean Squared Error, so in essence, we are finding the expected MSE.","title":"Expected Generalization Error/Test Error/Out-of-Sample Error: \\(\\mathbb{E}_{\\mathcal{D}}[\\mathcal{E}_{\\text{out}}(h)]\\)"},{"location":"reighns_ml_journey/interview/Machine_Learning_Notations/#empirical-errortraining-errorin-sample-error-mathcale_textinh","text":"Given a hypothesis \\(h \\in \\mathcal{H}\\) , a true function \\(f \\in \\mathcal{C}\\) , and an underlying distribution \\(\\mathcal{P}\\) , and a sample \\(\\mathrm{X}\\) drawn from \\(\\mathcal{X}\\) i.i.d with distribution \\(\\mathcal{P}\\) , the test/out-of-sample error of \\(h\\) is defined by \\( \\(\\begin{aligned}\\mathcal{E}_{\\text{in}}(h) = \\frac{1}{\\mathrm{m}}\\sum_{i=1}^{\\mathrm{m}}\\text{sign}[h(\\mathrm{x}^{(i)}) \\neq f(\\mathrm{x}^{(i)})]\\end{aligned}\\) \\) Here the sign function is mainly used for binary classification, where if \\(h\\) and \\(f\\) disagrees at any point \\(x^{(i)}\\) , then \\(\\text{sign}[h(\\mathrm{x}^{(i)}) \\neq f(\\mathrm{x}^{(i)})]\\) evaluates to 1. We take the sum of all disagreements and divide by the total number of samples. In short, that is just the misclassification/error rate. The empirical error of \\(h \\in \\mathcal{H}\\) is its average error over the sample \\(\\mathcal{X}\\) , in contrast, the generalization error is its expected error based on the distribution \\(\\mathcal{P}\\) . Take careful note here that \\(h(x^{(i)})\\) is the prediction made by our hypothesis (model), we can conventionally call it \\(\\hat{y}^{(i)}\\) whereby our \\(f(x^{(i)})\\) is our ground truth label \\(y^{(i)}\\) . I believe that this ground truth label is realized once we draw the sample from \\(\\mathcal{X}\\) even though we do not know what \\(f\\) is. An additional note here, is that the summand of the in-sample error function is not fixated to the sign function. In fact, I believe you can define any loss function to calculate the \"error\". As an example, if we are dealing with regression, then we can modify the summand to our favourite Mean Squared Error. \\[\\begin{aligned}\\mathcal{E}_{\\text{in}}(h) = \\frac{1}{\\mathrm{m}}\\sum_{i=1}^{\\mathrm{m}}[h(\\mathrm{x}^{(i)}) - f(\\mathrm{x}^{(i)})]^2\\end{aligned}\\]","title":"Empirical Error/Training Error/In-Sample Error: \\(\\mathcal{E}_{\\text{in}}(h)\\)"},{"location":"reighns_ml_journey/interview/Machine_Learning_Notations/#averagemean-hypothesis-barh_mathcald-mathbbe_mathcaldh_mathcaldmathrmx","text":"This may seem confusing at first sight, but it cactually means the following: Generate many data sets \\(\\mathcal{D}_{i}\\) , \\(i = 1,2,3,..., K\\) from population \\(\\mathcal{X} \\times \\mathcal{Y}\\) over a probability distribution \\(\\mathcal{P}\\) . Apply the learning algorithm \\(\\mathcal{A}\\) to each \\(\\mathcal{D}_{i}\\) to get \\(K\\) number of hypothesis \\(h_{i} = h_{\\mathcal{D}_{i}}\\) We can then call the average function for any \\(\\mathrm{x}\\) by \\( \\(\\bar{h}_{\\mathcal{D}} = \\mathbb{E}_{\\mathcal{D}}[h_{\\mathcal{D}}(\\mathrm{x})]\\) \\) As an example, if there are 3 \\(h_{i}\\) , where \\(h_1 = 2x+1\\) , \\(h_2 = 2x+3\\) , and \\(h_3 = 3x+1\\) , then given any query point say \\(x_{q} = 3\\) , then \\(\\bar{h}(\\mathrm{x}_{q}) = \\dfrac{h_1 + h_2 + h_3}{3} = \\frac{26}{3}\\) . This function, or average hypothesis is vital for our calculation of bias and variance.","title":"Average/Mean Hypothesis: \\(\\bar{h}_{\\mathcal{D}} = \\mathbb{E}_{\\mathcal{D}}[h_{\\mathcal{D}}(\\mathrm{x})]\\)"},{"location":"reighns_ml_journey/interview/Machine_Learning_Notations/#bias-variance-decomposition","text":"This is a decomposition of the Expected Generalization Error. Formal Proof please read Learning From Data. Unless otherwise stated, we consider only the univariate case where \\(\\mathrm{x}\\) is a single test point. \\[\\begin{align*} \\mathbb{E}_{\\mathcal{D}}[\\mathcal{E}_{\\text{out}}(h)] &= \\mathbb{E}_{\\mathcal{D}}[\\mathbb{E}_{\\mathrm{x}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}) - f(\\mathrm{x}))^2 \\right]] \\\\ &= \\big(\\;\\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;] - f(x)\\;\\big)^2 + \\mathbb{E}_{\\mathcal{D}}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;])^2\\;\\big] + \\mathbb{E}\\big[(y-f(x))^2\\big] \\\\ &= \\big(\\;\\bar{h}(\\mathrm{x}) - f(x)\\;\\big)^2 + \\mathbb{E}_\\mathcal{D}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\bar{h}(\\mathrm{x}) \\;])^2\\;\\big]+ \\mathbb{E}\\big[(y-f(x))^2\\big] \\end{align*} \\] Where $\\big(\\;\\mathbb{E} {\\mathcal{D}}[\\;h }(x)\\;] - f(x)\\;\\big)^2 $ is the Bias, \\(\\mathbb{E}_\\mathcal{D}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;])^2\\;\\big]\\) is the Variance and \\(\\mathbb{E}\\big[(y-f(x))^2\\big]\\) is the irreducible error \\(\\epsilon\\) .","title":"Bias - Variance Decomposition"},{"location":"reighns_ml_journey/interview/Machine_Learning_Notations/#bias-bigmathbbe_mathcaldh_mathcaldx-fxbig2","text":"In other form, we can express Bias as \\( \\(\\big(\\;\\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;] - f(x)\\;\\big)^2 = \\big(\\;\\bar{h}(\\mathrm{x}) - f(x)\\;\\big)^2\\) \\) See simulation on Bias-Variance Tradeoff to understand. If our test point is \\(x_{q} = 0.9\\) , then our bias is as such: \\[ \\widehat{\\text{bias}} \\left(\\hat{f}(0.90) \\right) = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(\\hat{f}_k^{[i]}(0.90) \\right) - f(0.90) \\]","title":"Bias: \\(\\big(\\;\\mathbb{E}_\\mathcal{D}[\\;h_{\\mathcal{D}}(x)\\;] - f(x)\\;\\big)^2\\)"},{"location":"reighns_ml_journey/interview/Machine_Learning_Notations/#variance-mathbbe_mathcaldbigh_mathcaldx-mathbbe_mathcaldh_mathcaldx2big","text":"This is more confusing, but we first express Variance as: \\[\\mathbb{E}_\\mathcal{D}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;])^2\\;\\big] = \\mathbb{E}_\\mathcal{D}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\bar{h}(\\mathrm{x}) \\;])^2\\;\\big]\\] If our test point is \\(x_{q} = 0.9\\) , then our variance is as such: \\[ \\widehat{\\text{var}} \\left(\\hat{f}(0.90) \\right) = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(\\hat{f}_k^{[i]}(0.90) - \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}}\\hat{f}_k^{[i]}(0.90) \\right)^2 \\] Loss Function: \\(\\mathcal{L}\\) Performance Metric: \\(\\mathcal{M}\\)","title":"Variance: \\(\\mathbb{E}_\\mathcal{D}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;])^2\\;\\big]\\)"},{"location":"reighns_ml_journey/interview/Machine_Learning_Notations/#corresponding-intuition-of-formal-definitions","text":"The learning problem is then formulated as follows: Given a training set \\(\\mathcal{D}\\) , sampled i.i.d. from \\(\\mathcal{X} \\times \\mathcal{Y}\\) over a distribution \\(\\mathcal{P}\\) where there exists a true function \\(f\\) that relates \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\) , we attempt to approximate the true relationship \\(f\\) with an optimal \\(\\mathcal{g}\\) from a hypothesis set \\(\\mathcal{H}\\) using a pre-defined learning algorithm \\(\\mathcal{A}\\) . In particular, \\(\\mathcal{g}\\) is chosen from multiple \\(h \\in \\mathcal{H}\\) where the optimal \\(g\\) gives the lowest in-sample (training) error, or more concretely, gives the lowest in-sample-validation error \\(\\mathcal{E}_{\\text{in}}(g)\\) . One thing worth noting, although prematurely at this stage, that we often look at both the in-sample error and the performance metric to decide on the optimal \\(g\\) . Input Space: \\(\\mathrm{x} \\in \\mathcal{X}\\) or \\(\\mathrm{x} \\in \\mathcal{P}\\) The input space contains the set of all possible examples/instances in a population . This is generally unknown. This is the input space, containing the set of all possible examples, or instances in a population . In general, we can think of it probabilistically and say that, a population follows a distribution \\(\\mathcal{P}\\) and we generate i.i.d samples to form our sample (often called the training set) \\(\\mathcal{D}\\) from this input space \\(\\mathcal{X}\\) . \\(\\mathcal{X} = \\(\\{0,1\\}\\) ^3\\) \\(\\mathrm{x}\\) is the feature vector and as an element of \\(\\mathcal{X}\\) , it can take on any of the following 8 distinct vectors: \\([0,0,0], [0,0,1],...,[1,1,1]\\) In further lectures, \\(\\mathcal{X}\\) can be considered as a probability distribution \\(\\mathcal{P}\\) , and picking points \\(x\\) from \\(\\mathcal{X}\\) means generating them. Output Space: \\(\\mathrm{y} \\in \\mathcal{Y}\\) This is the output space, where it corresponds to each point in \\(\\mathcal{X}\\) . This is the set of all possible labels/targets. \\(\\mathcal{y} = \\(\\{0, 1\\}\\) ^1\\) Thus, \\(\\mathrm{y}\\) can take on either 0 or 1, a binary output. Concept: Unknown Ground Truth (Target) Function: \\(f: \\mathcal{X} \\to \\mathcal{Y}\\) where \\(\\mathrm{x} \\mapsto \\mathrm{y}\\) This is a mapping from the input space to the output space. This is the so-called true function which is the underlying true relationship between each pair of point \\((\\mathrm{x}, \\mathrm{y}) \\in \\mathcal{X} \\times \\mathcal{Y}\\) . However, this function \\(f\\) is unknown to us, or else we do not need to learn anything. We try our best to find a model that best estimates the unknown ground truth function \\(f\\) . Note this is often called the \"concept\" in the PAC framework. Concept Class: \\(\\mathcal{C}\\) A concept class C is a set of true functions \\(f\\) . Hypothesis class H is the set of candidates to formulate as the final output of a learning algorithm to well approximate the true function f . Hypothesis class H is chosen before seeing the data (training process). C and H can be either same or not and we can treat them independently. Distribution : \\(\\mathcal{P}\\) Read The Deep Learning Book: Chapter 5, in particular, Section 5.2. The train and test data are generated by a probability distribution over datasets called the data generating process . We typically make a set of assumptions known collectively as the i.i.d. assumptions . These assumptions are that the examples in each dataset are independent from each other, and that the train set and test set are identically distributed, drawn from the same probability distribution as each other. This assumption allows us to describe the data generating process with a probability distribution over a single example. The same distribution is then used to generate every train example and every test example. We call that shared underlying distribution the data generating distribution, denoted pdata. This probabilistic framework and the i.i.d. assumptions allow us to mathematically study the relationship between training error and test error The unknown distribution that generated our input space \\(\\mathcal{X}\\) . The distribution can be say, a Gaussian Distribution with mean 0 and standard deviation 1, and we generate samples from More references below: https://stats.stackexchange.com/questions/515806/need-help-with-understanding-random-variables-the-data-generating-distribution https://stats.stackexchange.com/questions/481047/relationship-between-distribution-and-data-generating-process https://stats.stackexchange.com/questions/320375/what-does-it-mean-for-the-training-data-to-be-generated-by-a-probability-distrib https://datascience.stackexchange.com/questions/54346/data-generating-probability-distribution-probability-distribution-of-a-dataset https://stats.stackexchange.com/questions/542010/the-deep-learning-book-considers-the-empirical-distribution-hat-p-data-a-fu https://web.mit.edu/urban_or_book/www/book/chapter7/7.1.3.html Data: \\(\\mathcal{D}\\) This is what we usually get in a Machine Learning Problem, we have a training set, sampled from \\(\\mathcal{X}\\) with distribution \\(\\mathcal{P}\\) . The notation are usually as follows: \\( \\(\\mathcal{D} = [(\\mathrm{x^{(1)}}, \\mathrm{y^{(1)}}), (\\mathrm{x^{(2)}}, \\mathrm{y^{(2)}}), ..., (\\mathrm{x^{(N)}}, \\mathrm{y^{(N)}}))]\\) \\) OR \\( \\(\\mathcal{D} = [(\\mathrm{x_1}, \\mathrm{y_1}), (\\mathrm{x_2}, \\mathrm{y_2}), ..., (\\mathrm{x_N}, \\mathrm{y_N})]\\) \\) or The dataset where \\(\\mathrm{N}\\) is the number of samples (training samples). Note that this is actually a 2d matrix where row \\(i\\) is training sample \\(i\\) and column \\(j\\) is feature \\(j\\) . In our simple example, we define \\(\\mathrm{N} = 5\\) . The below are the five samples, for example, sample 1 \\(\\mathrm{x_1} = [0,0,0]\\) and outputs ground truth \\(\\mathrm{y} = 0\\) where white circle = 0, black circle = 1. I tend to be more used to the fact that we denote \\(m\\) to be number of samples, and \\(n\\) to be number of features, so I may use it interchangeably later. In addition, I may also denote \\(\\mathcal{D}\\) to be \\(\\mathrm{X}\\) , both are what we called the Design Matrix. Hypothesis Set: \\(\\mathcal{H}\\) The set where it contains all possible functions to approximate our true function \\(f\\) . Note that the Hypothesis Set can be either continuous or discrete, means to say it can be either a finite or infinite set. But in reality, it is almost always infinite. Read here for more information. For example: Let \\(\\mathcal{H}\\) be the set of neural networks available. Imagine there are a lot of different neural network functions inside (i.e. y = w^Tx+b is the simplest form of neural network, and it has many possible permutations because the weight vector w and b can be changed. Therefore, we decide that we should use neural network as our hypothesis set, and from there, we deploy our learning algorithm \\(\\mathcal{A}\\) , the backpropagation, to learn and output as with the so-called optimal weight set - to formulate our best approximated function \\(g \\in \\mathcal{H}\\) . Back to our example, we can actually enumerate all possible cases of the function to approximate \\(f\\) . Since the input space \\(\\mathcal{X}\\) has 8 distinct vectors, and \\(f\\) is a Boolean Function which takes in 3 Boolean Inputs, outputs 2 outputs, we have \\(2^{2^3} = 256\\) distinct Boolean Functions on 3 Boolean Inputs. Hypothesis: \\(h: \\mathcal{D} \\to \\mathcal{y}\\) where \\(\\mathrm{x} \\mapsto \\mathrm{y}\\) Note that this \\(h\\) is the hypothesis function, The final best hypothesis function is called \\(g\\) , which approximates the true function \\(f\\) . We can see that \\(\\mathcal{XOR}\\) is a function that can be used to approximate \\(f\\) . Learning Algorithm: \\(\\mathcal{A}\\) What this does is from the set of Hypothesis \\(\\mathcal{H}\\) , the learning algorithm's role is to pick one \\(\\mathcal{g} \\in \\mathcal{H}\\) such that this \\(g\\) is the hypothesis function. Intuition see Hypothesis Set. You can think of hypothesis space of linear regression as the set of linear functions of your features. For simplicity assume the feature space is the entire \\(\\mathbb R^d\\) . Then, the hypothesis space of linear regression is \\[\\mathcal F = \\{f: \\mathbb R^d \\to \\mathbb R :\\; f(x) = \\beta^T x, \\quad \\forall x \\in \\mathbb R^d,\\; \\text{and some $\\beta \\in \\mathbb R^d$}\\}.\\] This class is a parametric family and can be equivalently described by the set of coefficients \\(\\{ \\beta:\\; \\beta \\in \\mathbb R^d\\}\\) . Basic Setup of the Learning Problem Basic Setup of the Learning Problem with Probability Distribution \\(\\mathcal{P}\\)","title":"Corresponding Intuition of Formal Definitions"},{"location":"reighns_ml_journey/interview/Machine_Learning_Notations/#theorems","text":"","title":"Theorems"},{"location":"reighns_ml_journey/interview/Machine_Learning_Notations/#expected-training-error-expected-test-error","text":"The Deep Learning Book p111 Foundation of Machine Learning p12-13 The Deep Learning Book: One immediate connection we can observe between the training and test error is that the expected training error of a randomly selected model is equal to the expected test error of that model. Suppose we have a probability distribution p(x, y) and we sample from it repeatedly to generate the train set and the test set. For some fixed value w, the expected training set error is exactly the same as the expected test set error, because both expectations are formed using the same dataset sampling process. The only difference between the two conditions is the name we assign to the dataset we sample. Of course, when we use a machine learning algorithm, we do not fix the parameters ahead of time, then sample both datasets. We sample the training set, then use it to choose the parameters to reduce training set error, then sample the test set. Under this process, the expected test error is greater than or equal to the expected value of training error. The factors determining how well a machine learning algorithm will perform are its ability to: 1. Make the training error small. 2. Make the gap between training and test error small. These two factors correspond to the two central challenges in machine learning: underfitting and overfitting . Underfitting occurs when the model is not able to obtain a sufficiently low error value on the training set. Overfitting occurs when the gap between the training error and test error is too large.","title":"Expected Training Error = Expected Test Error"},{"location":"reighns_ml_journey/interview/Machine_Learning_Notations/#mathematical-notations","text":"","title":"Mathematical Notations"},{"location":"reighns_ml_journey/interview/Machine_Learning_Notations/#sign-function","text":"\\[\\operatorname{sign}(x) = \\begin{cases} 1 & x > 0 \\\\ 0 & x = 0 \\\\ -1 & x < 0 \\tag{1.1} \\end{cases}\\]","title":"Sign function"},{"location":"reighns_ml_journey/interview/Machine_Learning_Notations/#line-equation","text":"In the case of two variables, any linear equation can be in the form of \\[ax+by+c=0 \\tag{1.2}\\] In the case of two points: \\((x_1, y_1), (x_2, y_2)\\) , to find the equation of the line, we find the slope \\(m = \\dfrac{y_2-y_1}{x_2-x_1}\\) . Then use one pair of point \\((x_1, y_1)\\) to solve for the intercept \\(c\\) . Finally, we have \\(y = mx + c\\) .","title":"Line Equation"},{"location":"reighns_ml_journey/interview/Machine_Learning_Notations/#rnspace","text":"This is called the n-dimensional space, and thus any element of \\(R^{n}\\) is a n-tuple: \\[(x_1, x_2,...,x_n) \\tag{1.3}\\] In Machine Learning, if your dataset has 2 features, then any training instance \\(x^{(i)}\\) is inside \\(R^{2}\\) space and represents \\((x_{1}^{(i)}, x_{2}^{(i)})\\)","title":"\\(R^{n}\\)space"},{"location":"reighns_ml_journey/interview/Machine_Learning_Notations/#readings-and-references","text":"Foundations of Machine Learning : Chapter 2, p24-27 The Deep Learning Book: Chapter 5.1, 5.2 Learning From Data: Chapter 1 Concept Class Probability Distribution \\(\\mathcal{P}\\) https://stats.stackexchange.com/questions/515806/need-help-with-understanding-random-variables-the-data-generating-distribution https://stats.stackexchange.com/questions/481047/relationship-between-distribution-and-data-generating-process https://stats.stackexchange.com/questions/320375/what-does-it-mean-for-the-training-data-to-be-generated-by-a-probability-distrib https://datascience.stackexchange.com/questions/54346/data-generating-probability-distribution-probability-distribution-of-a-dataset https://stats.stackexchange.com/questions/542010/the-deep-learning-book-considers-the-empirical-distribution-hat-p-data-a-fu https://web.mit.edu/urban_or_book/www/book/chapter7/7.1.3.html Hypothesis Set https://stats.stackexchange.com/questions/183989/what-exactly-is-a-hypothesis-space-in-machine-learning Expected Training Error = Expected Test Error The Deep Learning Book: p111 Foundations of Machine Learning: p12-13","title":"Readings and References"},{"location":"reighns_ml_journey/interview/interview_handbook/","text":"\\[ \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\betaa}{\\mathbf{\\beta}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\z}{\\mathbf{z}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\C}{\\mathbf{C}} \\newcommand{\\X}{\\mathbf{X}} \\newcommand{\\rank}{\\textbf{rank}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\U}{\\mathrm{U}} \\newcommand{\\V}{\\mathrm{V}} \\newcommand{\\W}{\\mathrm{W}} \\newcommand{\\L}{\\mathcal{L}} \\newcommand{\\P}{\\mathbb{P}} \\] Why is Softmax Layer Monotonic? https://stackoverflow.com/questions/56096598/is-softmax-used-when-only-the-most-probable-class-will-be-used#:~:text=But%20softmax%20is%20a%20monotonic,layer%2C%20leaving%20the%20softmax%20out. Data Imbalance https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/ Robinhood Question 1 First, check if we can collect more data for the minority class, this may not always be feasible, but worth a try to ask first. Make sure the metrics defined for such problem is sensible, for example, using accuracy is not good and one should explain why this is the case by using the ZeroR example. One should also mention that AUROC, precision-recall metrics that are not so misleading. Over/Under sampling: delete majority class randomly till convergence to desired distribution, or resample with replacement minority class, Smote Bias-Variance Tradeoff Definition Intuition and Examples My code on understanding the math and stats behind. Association with Overfit-Underfit How to tackle Bias-Variance Tradeoff In the Ace the data science interview book, author mentioned how to approach case specific question. Example high bias -> increase model complexity high var -> increase data etc Overfitting and Underfitting What is overfitting? https://sebastianraschka.com/faq/docs/overfitting.html How to tackle Overfitting? Regularization ... Regularization Briefly desribe Regularization in Logistic Regression https://sebastianraschka.com/faq/docs/regularized-logistic-regression-performance.html The visualization in the example is insane coupled with clear and intuitive explanation! Initially, you see the global minimum of the convex function is in the center of the graph, which is unregularized, we want to make the model a bit less \"accurate\" so we can have it generalize, so you see the small circle there is the \"L2-term\", now you can still find minimum, but with constraint to be in the small L2 circle. Why is the L1 constraint a diamond and L2 constraint a circle? Consider a naive vector \\(\\v = \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\in \\R^2\\) . Assume all vectors start from the origin. Define L2 norm as \\(\\Vert \\v \\Vert_2 = \\sqrt{x^2 + y^2}\\) and then the set of vectors (coordinates) that satisfies this definition of L2 norm can be rewritten as: \\[ \\left\\{(x, y) ~|~ x^2+y^2 = r^2 \\right\\} \\] where this definition means that if \\(\\Vert \\v \\Vert_2 = 1\\) , then all vectors \\(\\v\\) having this norm must lie on the unit circle. See the image from ISLR, consider a simple linear equation with two independent variables \\(\\y = \\beta_1 \\x_1 + \\beta_2 \\x_2\\) , then the beta vector \\(\\betaa = \\begin{bmatrix} \\beta_1 \\\\ \\beta_2 \\end{bmatrix}\\) not has to fulfil a constraint such that say for example L2 norm (ridge) of \\(\\betaa\\) must be less than or equals to \\(r = 1\\) . Geomtrically, you can see the circle that the \\(\\betaa\\) must fall within and note that it may or may not contain the \"true optimal \\(\\betaa\\) \". ISLR See pp.242-244 from ISLR for more intuition . https://medium.com/uwaterloo-voice/a-deep-dive-into-regularization-eec8ab648bce https://www.mathworks.com/matlabcentral/answers/347583-how-to-plot-the-l2-norm-circle https://medium.com/@kiprono_65591/regularization-a-technique-used-to-prevent-over-fitting-886d5b361700 https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/ Preprocessing Techniques should also check cs3244. Feature Scaling An article on which models may need scale: https://www.thekerneltrip.com/statistics/when-scale-my-data/ Sebastian god: https://sebastianraschka.com/Articles/2014_about_feature_scaling.html More math: https://stats.stackexchange.com/questions/29781/when-conducting-multiple-regression-when-should-you-center-your-predictor-varia https://stats.stackexchange.com/questions/48360/is-standardization-needed-before-fitting-logistic-regression says If you use logistic regression with LASSO or ridge regression (as Weka Logistic class does) you should. As Hastie,Tibshirani and Friedman points out (page 82 of the pdf or at page 63 of the book): The ridge solutions are not equivariant under scaling of the inputs, and so one normally standardizes the inputs before solving. Encoding https://stats.stackexchange.com/questions/288095/what-algorithms-require-one-hot-encoding Model Selection, Evaluation Must read: Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning by Seb Some summary from the paper. 1.1 Performance Estimation: Generalization Performance vs. Model Selection Why evaluate the predictive performance of a model? Relative performance is accurate if we assume bias affects all the models we trained. For example, if we trained 3 models, \\(M_1, M_2, M_3\\) , and their validation accuracy is ranked as \\(M_2: 75\\% > M_1: 70\\% > M_3: 65\\%\\) , then if we add pessismistic bias of \\(10\\%\\) , the three models still ranked in the same order relatively: \\(M_2: 65\\% > M_1: 60\\% > M_3: 55\\%\\) . But do not think a \\(65\\%\\) validation accuracy translates to \\(65\\%\\) accuracy on the unseen test set. 1.2 Assumptions and Terminology 1.5 Holdout validation: v impt idea 3.9: Talks about whether feature selection in cv loop or out, data leak or ok. section 4: Hypothesis testing and algorithm comparison! What is Model Selection? The process of finding the best-performing model from a set of models that were produced by different hyperparameter settings is called model selection. Gradient Descent https://realpython.com/gradient-descent-algorithm-python/ Batch Gradient Descent Stochastic Gradient Descent Exploding and Vanishing Gradients https://www.analyticsvidhya.com/blog/2021/06/the-challenge-of-vanishing-exploding-gradients-in-deep-neural-networks/ Metrics What models produce well calibrated probabilities? https://stats.stackexchange.com/questions/208867/why-does-logistic-regression-produce-well-calibrated-models Why do we sometimes use \"multi-head\" to evaluate multi-label classification? https://debuggercafe.com/multi-head-deep-learning-models-for-multi-label-classification/ When should one use Sigmoid or Softmax in a Neural Network? Sigmoid vs Softmax I've noticed people often get directed to this question when searching whether to use sigmoid vs softmax in neural networks. If you are one of those people building a neural network classifier, here is how to decide whether to apply sigmoid or softmax to the raw output values from your network: If you have a multi-label classification problem = there is more than one \"right answer\" = the outputs are NOT mutually exclusive, then use a sigmoid function on each raw output independently. The sigmoid will allow you to have high probability for all of your classes, some of them, or none of them. Example: classifying diseases in a chest x-ray image. The image might contain pneumonia, emphysema, and/or cancer, or none of those findings. If you have a multi-class classification problem = there is only one \"right answer\" = the outputs are mutually exclusive, then use a softmax function. The softmax will enforce that the sum of the probabilities of your output classes are equal to one, so in order to increase the probability of a particular class, your model must correspondingly decrease the probability of at least one of the other classes. Example: classifying images from the MNIST data set of handwritten digits. A single picture of a digit has only one true identity - the picture cannot be a 7 and an 8 at the same time. # micro-averaged AUC roc_auc_score ( y_true . flatten (), y_pred . flatten ()) # macro-averaged AUC np . mean ([ roc_auc_score ( y_true [:, i ], y_pred [:, i ]) for i in range ( 11 )]) https://scikit-learn.org/stable/modules/multiclass.html#multilabel-classification https://glassboxmedicine.com/2019/05/26/classification-sigmoid-vs-softmax/ https://scikit-learn.org/stable/modules/multiclass.html#multilabel-classification Probability and Statistics Why do you multiply probabilities? Also appear in Chapter 2 summary. Consider rolling a dice twice, what is the probability that you roll a 5 and 6 respectively. We all know the answer is \\(\\dfrac{1}{6} \\times \\dfrac{1}{6} = \\dfrac{1}{36}\\) . But why? This can be first understood that our denominator is the total outcomes in our sample space \\(\\S\\) . This is \\(36\\) , why? By our counting principle on multiplication, we know that if we have \\(6\\) choices in roll \\(1\\) and \\(6\\) choices in roll 2, then the cross-product is \\(6 \\times 6 = 36\\) total choices. One can enumerate \\(\\{(1,1), (1,2), \\ldots, (6,6)\\}\\) to see why. Now the numerator is also related to the counting principle of multiplication as well! In roll 1, rolling a 5 is 1 choice, rolling a 6 next is 1 choice, so total there is a only one combination choice \\(1 \\times 1\\) ! Now if we reframe the problem to what is the probability that you roll a 1, 2 or 3 in the first roll and 2 or 3 in the second roll. Then of course our denominator don't change as \\(36\\) , but our numerator changes, since in roll 1 we have 3 choices, and roll 2 have 2 choices, by the multiplicative principle we have a total of \\(3 \\times 2 = 6\\) choices, and so our probability is \\(\\dfrac{6}{36}\\) now. You can verify that there are indeed \\(6\\) choices manually. Now the most important part is we can use this if both events are independent! If not we need to be careful! . What is Priori, Posterior and Conditional Common terms in ML world. Suppose there are three types of players in a tennis tournament: A, B, and C. Fifty percent of the contestants in the tournament are A players, 25% are B players, and 25% are C players. Your chance of beating the contestants depends on the class of the player, as follows: 0.3 against an A player 0.4 against a B player 0.5 against a C player If you play a match in this tournament, what is the probability of your winning the match? Supposing that you have won a match, what is the probability that you played against an A player? Let \\(W\\) be the event that you win and \\(A\\) be the event that you played vs player \\(A\\) , then Conditional: \\(\\P(W~|~A)\\) = given you played player \\(A\\) , what is your probability of winning? Priori: \\(\\P(A)\\) = without entering the game , what is your probability of facing player \\(A\\) ? Posterior: \\(\\P(A~|~W)\\) = after entering the game and winning the match , what is your probability that you have actually played with \\(A\\) ? Machine Learning: In many practical engineering problems, the question of interest is often the last one. That is, supposing that you have observed something, what is the most likely cause of that event? For example, supposing we have observed this particular dataset, what is the best Gaussian model that would fit the dataset? Questions like these require some analysis of conditional probability, prior probability, and posterior probability. Generic Questions Usually generic and fundamental questions. These questions are \"less code oriented\", and usually requires a short and sweet answer, alongside with an example to illustrate. What are common pitfalls that lead to Data Leakage in a pipeline? https://scikit-learn.org/stable/common_pitfalls.html See also numerous question I asked on cross-validated. What is the meaning of \"Train/Test\" Distribution in the context of Machine Learning? https://towardsdatascience.com/understanding-entropy-the-golden-measurement-of-machine-learning-4ea97c663dc3#:~:text=By%20using%20entropy%20in%20machine,be%20desired%20in%20model%2Dbuilding. https://www.kdnuggets.com/2019/01/when-your-training-testing-data-different-distributions.html Let us talk this in a less formal way without invoking formal definitions of \"probability distributions\". Train and Test Distribution Let us say we are imposed a Machine Learning problem to predict cats vs dogs. We are handed a training set, and there is also a hidden test set that we do not know of. Let us pretend we browse through each image in the training set \\(\\mathcal{D}\\) , and plot out the pixel distribution of each image, we observe that each image pixel distribution follows a gaussian curve with mean 0 and std 1 (i.e. each image in \\(\\mathcal{D}\\) is generated from \\(\\mathcal{N}(0,1)\\) ). Then we trained a good model using training set, but performed badly on the test set \\(\\mathcal{D}_{test}\\) , and after we also plot the pixel density of the test set, we discovered that this set follows a very different distributions, say follow \\(\\mathcal{N}(255, 255)\\) , and each image has 10 times the resolution of the training set. We can therefore conclude, that, both train and test set have different \"probability distributions\". Distribution from your model Let us also continue our example from the cat vs dog, this time we assume both train, test and every other images for dogs and cats are generated from \\(\\mathcal{N}(0, 1)\\) . That is to say, we know our population distribution, and if we do know, then why do we need a Machine Learning model at all? We can just approximate all images with the known population distribution with the parameters. Even simpler, if we have a dataset that is beknownst to us with a perfect linear relationship that can be predicted perfectly by a simple linear regression model, (i.e. the population parameter is known), then why do we even need linear regression? However, this world is far from perfect, and we do not know the population parameters of the underlying distribution of the dataset given to us, hence we use models to predict it. Thus, we can analogously say that if a training set \\(\\mathcal{D}\\) follows a distribution \\(p\\) , then our model follows a distribution \\(q\\) , where in the simplest case in linear regression, we know our \\(q\\) follows a distribution with 2 parameters, \\(m\\) the slope, \\(c\\) the bias. With that in mind, our machine learning models can be phrased as using \\(q\\) to approximate \\(q\\) . And this is often used in Cross-Entropy loss. Parametric and Non-Parametric Models. ISLR (p21-24) https://stats.stackexchange.com/questions/268638/what-exactly-is-the-difference-between-a-parametric-and-non-parametric-model https://sebastianraschka.com/faq/docs/parametric_vs_nonparametric.html https://sebastianraschka.com/faq/docs/parametric_vs_nonparametric.html: The term \u201cnon-parametric\u201d might sound a bit confusing at first: non-parametric does not mean that they have NO parameters! On the contrary, non-parametric models (can) become more and more complex with an increasing amount of data. So, in a parametric model, we have a finite number of parameters, and in nonparametric models, the number of parameters is (potentially) infinite. Or in other words, in nonparametric models, the complexity of the model grows with the number of training data; in parametric models, we have a fixed number of parameters (or a fixed structure if you will). Linear models such as linear regression, logistic regression, and linear Support Vector Machines are typical examples of a parametric \u201clearners;\u201d here, we have a fixed size of parameters (the weight coefficient.) In contrast, K-nearest neighbor, decision trees, or RBF kernel SVMs are considered as non-parametric learning algorithms since the number of parameters grows with the size of the training set. \u2013 K-nearest neighbor and decision trees, that makes sense, but why is an RBF kernel SVM non-parametric whereas a linear SVM is parametric? In the RBF kernel SVM, we construct the kernel matrix by computing the pair-wise distances between the training points, which makes it non-parametric. In the field of statistics, the term parametric is also associated with a specified probability distribution that you \u201cassume\u201d your data follows, and this distribution comes with the finite number of parameters (for example, the mean and standard deviation of a normal distribution); you don\u2019t make/have these assumptions in non-parametric models. So, in intuitive terms, we can think of a non-parametric model as a \u201cdistribution\u201d or (quasi) assumption-free model. However, keep in mind that the definitions of \u201cparametric\u201d and \u201cnon-parametric\u201d are \u201ca bit ambiguous\u201d at best; according to the \u201cThe Handbook of Nonparametric Statistics 1 (1962) on p. 2: \u201cA precise and universally acceptable definition of the term \u2018nonparametric\u2019 is not presently available. The viewpoint adopted in this handbook is that a statistical procedure is of a nonparametric type if it has properties which are satisfied to a reasonable approximation when some assumptions that are at least of a moderately general nature hold.\" Supervised and Unsupervised Learning. ISLR (p26-28) Tradeoff between Flexibility and Interpretability of Machine Learning Models. ISLR (p24-26) Reducible and Irreducible Errors. ISLR (p18-19) Definition Formal Definition Example (Intuition) We can use features \\(\\X\\) to predict \\(\\y\\) , for example, use age of house, number of bedrooms, etc to predict the price of the house . The irreducible errors can be random events such as Elon Musk tweeting something good about a particular house, which our model cannot account for. Describe Ensembling in laymen terms. Fran\u00e7ois Chollet (Creator of Keras) explained it as : Ensembling relies on the assumption that different good models trained independently are likely to be good for different reasons: each model looks at slightly different aspects of the data to make its predictions, getting part of the \u201ctruth\u201d but not all of it. You may be familiar with the ancient parable of the blind men and the elephant: a group of blind men come across an elephant for the first time and try to understand what the elephant is by touching it. Each man touches a different part of the elephant\u2019s body\u2014just one part, such as the trunk or a leg. Then the men describe toeach other what an elephant is: \u201cIt\u2019s like a snake,\u201d \u201cLike a pillar or a tree,\u201d and so on.The blind men are essentially machine-learning models trying to understand the manifold of the training data, each from its own perspective, using its own assumptions(provided by the unique architecture of the model and the unique random weight initialization). Each of them gets part of the truth of the data, but not the whole truth. By pooling their perspectives together, you can get a far more accurate description of the data. The elephant is a combination of parts: not any single blind man gets it quiteright, but, interviewed together, they can tell a fairly accurate story. Let\u2019s use classification as an example. The easiest way to pool the predictions of a setof classifiers (to ensemble the classifiers) is to average their predictions at inference time:Use four different models to compute initial predictions. preds_a = model_a.predict(x_val) preds_b = model_b.predict(x_val) preds_c = model_c.predict(x_val) preds_d = model_d.predict(x_val) This new prediction array should be more accurate than any of the initial ones. final_preds = 0.25 * (preds_a + preds_b + preds_c + preds_d) This will work only if the classifiers are more or less equally good. If one of them is significantly worse than the others, the final predictions may not be as good as the best classifier of the group.A smarter way to ensemble classifiers is to do a weighted average, where the weights are learned on the validation data typically, the better classifiers are given a higher weight, and the worse classifiers are given a lower weight. To search for a good set of ensembling weights, you can use random search or a simple optimization algorithm such as Nelder-Mead: preds_a = model_a.predict(x_val) preds_b = model_b.predict(x_val) preds_c = model_c.predict(x_val) preds_d = model_d.predict(x_val) These weights (0.5, 0.25,0.1, 0.15) are assumed to be learned empirically. final_preds = 0.5 * preds_a + 0.25 * preds_b + 0.1 * preds_c + 0.15 * preds_d Ensembling Theory Bagging Cross-Validation Why does Cross-Validation reduce Variance? Model Evaluation https://www.ritchieng.com/applying-machine-learning/ Linear Regression Polynomial Regression https://zerowithdot.com/polynomial-regression-in-python/ Logistic Regression Decision Tree Describe DT briefly Suprvised Learning: will need corresponding label Non-parametric Convolutional Neural Networks # !pip install torchinfo import timm import torch import torchvision from typing import Dict , Union , Callable , OrderedDict , Tuple import os , random import numpy as np import torch.nn as nn from torchinfo import summary def seed_all ( seed : int = 1992 ) -> None : \"\"\"Seed all random number generators.\"\"\" print ( f \"Using Seed Number { seed } \" ) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) # set PYTHONHASHSEED env var at fixed value torch . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) torch . cuda . manual_seed ( seed ) # pytorch (both CPU and CUDA) np . random . seed ( seed ) # for numpy pseudo-random generator # set fixed value for python built-in pseudo-random generator random . seed ( seed ) torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = True torch . backends . cudnn . enabled = True def seed_worker ( _worker_id ) -> None : \"\"\"Seed a worker with the given ID.\"\"\" worker_seed = torch . initial_seed () % 2 ** 32 np . random . seed ( worker_seed ) random . seed ( worker_seed ) seed_all ( seed = 1992 ) Using Seed Number 1992 Padding What is the role of zero padding? Pooling Global Average Pooling https://rwightman.github.io/pytorch-image-models/feature_extraction/#penultimate-layer-features-pre-classifier-features import torch import timm model = timm . create_model ( 'resnet18' , pretrained = True , num_classes = 1000 ) batch_size = 2 image_shape = ( 3 , 224 , 224 ) input_image_tensor = torch . rand ( size = ( batch_size , * image_shape )) # print(summary(model, (2, 3, 224, 224))) model.reset_classifier(num_classes=0, global_pool=\"\") means we do not want global average pooling and thus the shape at the last conv layer (penultimate layer) is \\((2, 512, 7, 7)\\) model.reset_classifier(num_classes=0, global_pool=\"avg\") means we do want global average pooling and thus the shape at the last conv layer (penultimate layer) is \\((2, 512)\\) whereby for each and every of the 512 feature maps \\(f_i\\) , we average \\(f_i\\) across all pixels (i.e. if \\(f_i\\) is 3 by 3 then average means add all \\(3 \\times 3 = 9\\) pixels and average) and concat to become one \\(512\\) vector. model = timm . create_model ( 'resnet18' , pretrained = True , num_classes = 1000 ) o = model ( input_image_tensor ) print ( f 'Original shape: { o . shape } ' ) model . reset_classifier ( num_classes = 0 , global_pool = \"\" ) o = model ( input_image_tensor ) print ( f 'Unpooled shape: { o . shape } ' ) Original shape: torch.Size([2, 1000]) Unpooled shape: torch.Size([2, 512, 7, 7]) model = timm . create_model ( 'resnet18' , pretrained = True , num_classes = 1000 ) o = model ( input_image_tensor ) print ( f 'Original shape: { o . shape } ' ) model . reset_classifier ( num_classes = 0 , global_pool = \"avg\" ) o = model ( input_image_tensor ) print ( f 'Pooled shape: { o . shape } ' ) Original shape: torch.Size([2, 1000]) Pooled shape: torch.Size([2, 512]) Image Normalization in Transfer Learning Image Normalization 1x1 Convolution A Gentle Introduction to 1\u00d71 Convolutions to Manage Model Complexity Intuition Pooling layers are designed to downscale feature maps and systematically halve the width and height of feature maps in the network. Nevertheless, pooling layers do not change the number of filters in the model which is the depth or number of channels . The intuition of 1x1 convolution is that we can think of it as a downsample operation on the feature maps but this time shrinking the depth/channels instead of the width/height . Downsample Feature Maps with 1x1 Convolution In Andrew Ng's example, let us define: Batch size of 1; A conv layer of with kernel size of 5, same padding of 2 and stride of 1, and an output depth of 32; The input shape is a stack of feature maps of 192 depth/channels, each of 28 by 28; It follows that the learnable params of the this conv layer is \\(32 \\times 5 \\times 5 \\times 192 + 32 = 153, 632\\) . It follows that the output shape is of \\((32, 28, 28)\\) , which is a stack of feature maps of 32 depth, each of 28 by 28 size. model = nn . Sequential ( nn . Conv2d ( in_channels = 192 , out_channels = 32 , kernel_size = 5 , padding = 2 , stride = 1 ), ) batch_size = 1 input_image = torch . rand ( size = ( 192 , 28 , 28 )) print ( summary ( model , ( batch_size , * input_image . shape ))) ========================================================================================== Layer (type:depth-idx) Output Shape Param # ========================================================================================== Sequential -- -- \u251c\u2500Conv2d: 1-1 [1, 32, 28, 28] 153,632 ========================================================================================== Total params: 153,632 Trainable params: 153,632 Non-trainable params: 0 Total mult-adds (M): 120.45 ========================================================================================== Input size (MB): 0.60 Forward/backward pass size (MB): 0.20 Params size (MB): 0.61 Estimated Total Size (MB): 1.42 ========================================================================================== Now if we design a intermediate layer that holds 1x1 convolution that first decreases the depth from 192 to 16 instead of 32, then connect back with another conv layer of kernel size 5 and depth 32, we can recover the same output shape but at a much leseer computational cost. As we can see below, the params reduced almost 10 folds. Commonly, we call this intermediate layer a bottleneck layer . model = nn . Sequential ( nn . Conv2d ( in_channels = 192 , out_channels = 16 , kernel_size = 1 , padding = 0 , stride = 1 ), nn . Conv2d ( in_channels = 16 , out_channels = 32 , kernel_size = 5 , padding = 2 , stride = 1 ), ) batch_size = 1 input_image = torch . rand ( size = ( 192 , 28 , 28 )) print ( summary ( model , ( batch_size , * input_image . shape ))) ========================================================================================== Layer (type:depth-idx) Output Shape Param # ========================================================================================== Sequential -- -- \u251c\u2500Conv2d: 1-1 [1, 16, 28, 28] 3,088 \u251c\u2500Conv2d: 1-2 [1, 32, 28, 28] 12,832 ========================================================================================== Total params: 15,920 Trainable params: 15,920 Non-trainable params: 0 Total mult-adds (M): 12.48 ========================================================================================== Input size (MB): 0.60 Forward/backward pass size (MB): 0.30 Params size (MB): 0.06 Estimated Total Size (MB): 0.97 ========================================================================================== Upsample Feature Maps with 1x1 Convolution Similarly, we can increase the depth/channel of the feature maps while maintaining the width and height . 1x1 Convolution is Equivalent to a FC-layer Downsampling Convolutional Stride (Downsampling) batch_size = 1 input_image = torch . rand ( size = ( 3 , 224 , 224 )) model = nn . Sequential ( nn . Conv2d ( in_channels = 3 , out_channels = 64 , kernel_size = 7 , padding = 3 , stride = 1 , bias = False ), ) print ( summary ( model , ( batch_size , * input_image . shape ))) ========================================================================================== Layer (type:depth-idx) Output Shape Param # ========================================================================================== Sequential -- -- \u251c\u2500Conv2d: 1-1 [1, 64, 224, 224] 9,408 ========================================================================================== Total params: 9,408 Trainable params: 9,408 Non-trainable params: 0 Total mult-adds (M): 472.06 ========================================================================================== Input size (MB): 0.60 Forward/backward pass size (MB): 25.69 Params size (MB): 0.04 Estimated Total Size (MB): 26.33 ========================================================================================== model = nn . Sequential ( nn . Conv2d ( in_channels = 3 , out_channels = 64 , kernel_size = 7 , padding = 3 , stride = 2 , bias = False ), ) print ( summary ( model , ( batch_size , * input_image . shape ))) ========================================================================================== Layer (type:depth-idx) Output Shape Param # ========================================================================================== Sequential -- -- \u251c\u2500Conv2d: 1-1 [1, 64, 112, 112] 9,408 ========================================================================================== Total params: 9,408 Trainable params: 9,408 Non-trainable params: 0 Total mult-adds (M): 118.01 ========================================================================================== Input size (MB): 0.60 Forward/backward pass size (MB): 6.42 Params size (MB): 0.04 Estimated Total Size (MB): 7.06 ========================================================================================== This is considered a downsampling operation due to our stride changed from 2 to 1. The output shape of the feature maps is from (64, 224, 224) to (64, 112, 112), effectively halving the width and height of the feature maps. Pooling (Downsampling) https://www.quora.com/What-is-a-downsampling-layer-in-Convolutional-Neural-Network-CNN Pooling as a form of Downsampling. Source model = nn . Sequential ( nn . Conv2d ( in_channels = 3 , out_channels = 64 , kernel_size = 7 , padding = 3 , stride = 1 , bias = False ) ) print ( summary ( model , ( batch_size , * input_image . shape ))) ========================================================================================== Layer (type:depth-idx) Output Shape Param # ========================================================================================== Sequential -- -- \u251c\u2500Conv2d: 1-1 [1, 64, 224, 224] 9,408 ========================================================================================== Total params: 9,408 Trainable params: 9,408 Non-trainable params: 0 Total mult-adds (M): 472.06 ========================================================================================== Input size (MB): 0.60 Forward/backward pass size (MB): 25.69 Params size (MB): 0.04 Estimated Total Size (MB): 26.33 ========================================================================================== model = nn . Sequential ( nn . Conv2d ( in_channels = 3 , out_channels = 64 , kernel_size = 7 , padding = 3 , stride = 1 , bias = False ), nn . MaxPool2d ( kernel_size = 2 , stride = 2 ), ) print ( summary ( model , ( batch_size , * input_image . shape ))) ========================================================================================== Layer (type:depth-idx) Output Shape Param # ========================================================================================== Sequential -- -- \u251c\u2500Conv2d: 1-1 [1, 64, 224, 224] 9,408 \u251c\u2500MaxPool2d: 1-2 [1, 64, 112, 112] -- ========================================================================================== Total params: 9,408 Trainable params: 9,408 Non-trainable params: 0 Total mult-adds (M): 472.06 ========================================================================================== Input size (MB): 0.60 Forward/backward pass size (MB): 25.69 Params size (MB): 0.04 Estimated Total Size (MB): 26.33 ========================================================================================== Spatial pooling layers such as MaxPool2d with a size of 2 and stride 2 also reduces the feature map by a factor of 2 by 2. Notice that both downsampling methods have the same number of trainable parameters and also the exact same output shape , so what's the difference? Maybe the additional MaxPool2d is an additional operation that is not needed? Let's see two examples. Convolutional Stride vs Pooling for Downsampling The advantage of the convolution layer is that it can learn certain properties that you might not think of while you add pooling layer. Pooling is a fixed operation and convolution can be learned. On the other hand, pooling is a cheaper operation than convolution, both in terms of the amount of computation that you need to do and number of parameters that you need to store (no parameters for pooling layer). There are examples when one of them is better choice than the other. Example when the convolution with strides is better than pooling The first layer in the ResNet uses convolution with strides. This is a great example of when striding gives you an advantage. This layer by itself significantly reduces the amount of computation that has to be done by the network in the subsequent layers. It compresses multiple 3x3 convolution (3 to be exact) in to one 7x7 convolution, to make sure that it has exactly the same receptive field as 3 convolution layers (even though it is less powerful in terms of what it can learn). At the same time this layer applies stride=2 that downsamples the image. Because this first layer in ResNet does convolution and downsampling at the same time, the operation becomes significantly cheaper computationally. If you use stride=1 and pooling for downsampling, then you will end up with convolution that does 4 times more computation + extra computation for the next pooling layer. The same trick was used in SqueezeNet and some other neural network architectures. Example where pooling is better than convolution In the NIPS 2018, there was a new architecture presented called FishNet . One thing that they try is to fix the problems with the residual connections used in the ResNet. In the ResNet, in few places, they put 1x1 convolution in the skip connection when downsampling was applied to the image. This convolution layer makes gradient propagation harder. One of the major changes in their paper is that they get rid of the convolutions in the residual connections and replaced them with pooling and simple upscales/identities/concatenations. This solution fixes problem with gradient propagation in very deep networks. From the FishNet paper (Section 3.2) The layers in the head are composed of concatenation, convolution with identity mapping, and max-pooling. Therefore, the gradient propagation problem from the previous backbone network in the tail are solved with the FishNet by 1) excluding I-conv at the head; and 2) using concatenation at the body and the head. Pooling vs. stride for downsampling Feature Maps Why are the last few Feature Maps more Useful in Feature extraction? The backbone network (\"convolution and pooling\") is responsible for extracting a feature map from the image that contains higher level summarized information. Each head uses this feature map as input to predict its desired outcome. The main intuition why feature maps of the last few layers (last layer usually) are important is one needs to recognize the earlier conv layer's feature maps find simple features like shapes, sizes, edges from an image, while the deep conv layers will be of more abstract features in an image. As a result, we really just want the abstract feature maps as they are more class specific to the image instead of the earlier layers which gives generic shapes . Object Detection (YOLO) Object Detection (Bounding Boxes) Different conversions https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/ https://github.com/awsaf49/bbox/blob/main/bbox/utils.py Kevin Huo & Nick Singh: Ace The Data Science Interview, 2021. (pp. 118) \u21a9","title":"Interview handbook"},{"location":"reighns_ml_journey/interview/interview_handbook/#why-is-softmax-layer-monotonic","text":"https://stackoverflow.com/questions/56096598/is-softmax-used-when-only-the-most-probable-class-will-be-used#:~:text=But%20softmax%20is%20a%20monotonic,layer%2C%20leaving%20the%20softmax%20out.","title":"Why is Softmax Layer Monotonic?"},{"location":"reighns_ml_journey/interview/interview_handbook/#data-imbalance","text":"https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/","title":"Data Imbalance"},{"location":"reighns_ml_journey/interview/interview_handbook/#robinhood-question-1","text":"First, check if we can collect more data for the minority class, this may not always be feasible, but worth a try to ask first. Make sure the metrics defined for such problem is sensible, for example, using accuracy is not good and one should explain why this is the case by using the ZeroR example. One should also mention that AUROC, precision-recall metrics that are not so misleading. Over/Under sampling: delete majority class randomly till convergence to desired distribution, or resample with replacement minority class, Smote","title":"Robinhood Question 1"},{"location":"reighns_ml_journey/interview/interview_handbook/#bias-variance-tradeoff","text":"Definition Intuition and Examples My code on understanding the math and stats behind.","title":"Bias-Variance Tradeoff"},{"location":"reighns_ml_journey/interview/interview_handbook/#association-with-overfit-underfit","text":"","title":"Association with Overfit-Underfit"},{"location":"reighns_ml_journey/interview/interview_handbook/#how-to-tackle-bias-variance-tradeoff","text":"In the Ace the data science interview book, author mentioned how to approach case specific question. Example high bias -> increase model complexity high var -> increase data etc","title":"How to tackle Bias-Variance Tradeoff"},{"location":"reighns_ml_journey/interview/interview_handbook/#overfitting-and-underfitting","text":"","title":"Overfitting and Underfitting"},{"location":"reighns_ml_journey/interview/interview_handbook/#what-is-overfitting","text":"https://sebastianraschka.com/faq/docs/overfitting.html","title":"What is overfitting?"},{"location":"reighns_ml_journey/interview/interview_handbook/#how-to-tackle-overfitting","text":"","title":"How to tackle Overfitting?"},{"location":"reighns_ml_journey/interview/interview_handbook/#regularization","text":"...","title":"Regularization"},{"location":"reighns_ml_journey/interview/interview_handbook/#regularization_1","text":"","title":"Regularization"},{"location":"reighns_ml_journey/interview/interview_handbook/#briefly-desribe-regularization-in-logistic-regression","text":"https://sebastianraschka.com/faq/docs/regularized-logistic-regression-performance.html The visualization in the example is insane coupled with clear and intuitive explanation! Initially, you see the global minimum of the convex function is in the center of the graph, which is unregularized, we want to make the model a bit less \"accurate\" so we can have it generalize, so you see the small circle there is the \"L2-term\", now you can still find minimum, but with constraint to be in the small L2 circle.","title":"Briefly desribe Regularization in Logistic Regression"},{"location":"reighns_ml_journey/interview/interview_handbook/#why-is-the-l1-constraint-a-diamond-and-l2-constraint-a-circle","text":"Consider a naive vector \\(\\v = \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\in \\R^2\\) . Assume all vectors start from the origin. Define L2 norm as \\(\\Vert \\v \\Vert_2 = \\sqrt{x^2 + y^2}\\) and then the set of vectors (coordinates) that satisfies this definition of L2 norm can be rewritten as: \\[ \\left\\{(x, y) ~|~ x^2+y^2 = r^2 \\right\\} \\] where this definition means that if \\(\\Vert \\v \\Vert_2 = 1\\) , then all vectors \\(\\v\\) having this norm must lie on the unit circle. See the image from ISLR, consider a simple linear equation with two independent variables \\(\\y = \\beta_1 \\x_1 + \\beta_2 \\x_2\\) , then the beta vector \\(\\betaa = \\begin{bmatrix} \\beta_1 \\\\ \\beta_2 \\end{bmatrix}\\) not has to fulfil a constraint such that say for example L2 norm (ridge) of \\(\\betaa\\) must be less than or equals to \\(r = 1\\) . Geomtrically, you can see the circle that the \\(\\betaa\\) must fall within and note that it may or may not contain the \"true optimal \\(\\betaa\\) \". ISLR See pp.242-244 from ISLR for more intuition . https://medium.com/uwaterloo-voice/a-deep-dive-into-regularization-eec8ab648bce https://www.mathworks.com/matlabcentral/answers/347583-how-to-plot-the-l2-norm-circle https://medium.com/@kiprono_65591/regularization-a-technique-used-to-prevent-over-fitting-886d5b361700 https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/","title":"Why is the L1 constraint a diamond and L2 constraint a circle?"},{"location":"reighns_ml_journey/interview/interview_handbook/#preprocessing-techniques","text":"should also check cs3244.","title":"Preprocessing Techniques"},{"location":"reighns_ml_journey/interview/interview_handbook/#feature-scaling","text":"An article on which models may need scale: https://www.thekerneltrip.com/statistics/when-scale-my-data/ Sebastian god: https://sebastianraschka.com/Articles/2014_about_feature_scaling.html More math: https://stats.stackexchange.com/questions/29781/when-conducting-multiple-regression-when-should-you-center-your-predictor-varia https://stats.stackexchange.com/questions/48360/is-standardization-needed-before-fitting-logistic-regression says If you use logistic regression with LASSO or ridge regression (as Weka Logistic class does) you should. As Hastie,Tibshirani and Friedman points out (page 82 of the pdf or at page 63 of the book): The ridge solutions are not equivariant under scaling of the inputs, and so one normally standardizes the inputs before solving.","title":"Feature Scaling"},{"location":"reighns_ml_journey/interview/interview_handbook/#encoding","text":"https://stats.stackexchange.com/questions/288095/what-algorithms-require-one-hot-encoding","title":"Encoding"},{"location":"reighns_ml_journey/interview/interview_handbook/#model-selection-evaluation","text":"Must read: Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning by Seb Some summary from the paper. 1.1 Performance Estimation: Generalization Performance vs. Model Selection Why evaluate the predictive performance of a model? Relative performance is accurate if we assume bias affects all the models we trained. For example, if we trained 3 models, \\(M_1, M_2, M_3\\) , and their validation accuracy is ranked as \\(M_2: 75\\% > M_1: 70\\% > M_3: 65\\%\\) , then if we add pessismistic bias of \\(10\\%\\) , the three models still ranked in the same order relatively: \\(M_2: 65\\% > M_1: 60\\% > M_3: 55\\%\\) . But do not think a \\(65\\%\\) validation accuracy translates to \\(65\\%\\) accuracy on the unseen test set. 1.2 Assumptions and Terminology 1.5 Holdout validation: v impt idea 3.9: Talks about whether feature selection in cv loop or out, data leak or ok. section 4: Hypothesis testing and algorithm comparison!","title":"Model Selection, Evaluation"},{"location":"reighns_ml_journey/interview/interview_handbook/#what-is-model-selection","text":"The process of finding the best-performing model from a set of models that were produced by different hyperparameter settings is called model selection.","title":"What is Model Selection?"},{"location":"reighns_ml_journey/interview/interview_handbook/#gradient-descent","text":"https://realpython.com/gradient-descent-algorithm-python/","title":"Gradient Descent"},{"location":"reighns_ml_journey/interview/interview_handbook/#batch-gradient-descent","text":"","title":"Batch Gradient Descent"},{"location":"reighns_ml_journey/interview/interview_handbook/#stochastic-gradient-descent","text":"","title":"Stochastic Gradient Descent"},{"location":"reighns_ml_journey/interview/interview_handbook/#exploding-and-vanishing-gradients","text":"https://www.analyticsvidhya.com/blog/2021/06/the-challenge-of-vanishing-exploding-gradients-in-deep-neural-networks/","title":"Exploding and Vanishing Gradients"},{"location":"reighns_ml_journey/interview/interview_handbook/#metrics","text":"","title":"Metrics"},{"location":"reighns_ml_journey/interview/interview_handbook/#what-models-produce-well-calibrated-probabilities","text":"https://stats.stackexchange.com/questions/208867/why-does-logistic-regression-produce-well-calibrated-models","title":"What models produce well calibrated probabilities?"},{"location":"reighns_ml_journey/interview/interview_handbook/#why-do-we-sometimes-use-multi-head-to-evaluate-multi-label-classification","text":"https://debuggercafe.com/multi-head-deep-learning-models-for-multi-label-classification/","title":"Why do we sometimes use \"multi-head\" to evaluate multi-label classification?"},{"location":"reighns_ml_journey/interview/interview_handbook/#when-should-one-use-sigmoid-or-softmax-in-a-neural-network","text":"Sigmoid vs Softmax I've noticed people often get directed to this question when searching whether to use sigmoid vs softmax in neural networks. If you are one of those people building a neural network classifier, here is how to decide whether to apply sigmoid or softmax to the raw output values from your network: If you have a multi-label classification problem = there is more than one \"right answer\" = the outputs are NOT mutually exclusive, then use a sigmoid function on each raw output independently. The sigmoid will allow you to have high probability for all of your classes, some of them, or none of them. Example: classifying diseases in a chest x-ray image. The image might contain pneumonia, emphysema, and/or cancer, or none of those findings. If you have a multi-class classification problem = there is only one \"right answer\" = the outputs are mutually exclusive, then use a softmax function. The softmax will enforce that the sum of the probabilities of your output classes are equal to one, so in order to increase the probability of a particular class, your model must correspondingly decrease the probability of at least one of the other classes. Example: classifying images from the MNIST data set of handwritten digits. A single picture of a digit has only one true identity - the picture cannot be a 7 and an 8 at the same time. # micro-averaged AUC roc_auc_score ( y_true . flatten (), y_pred . flatten ()) # macro-averaged AUC np . mean ([ roc_auc_score ( y_true [:, i ], y_pred [:, i ]) for i in range ( 11 )]) https://scikit-learn.org/stable/modules/multiclass.html#multilabel-classification https://glassboxmedicine.com/2019/05/26/classification-sigmoid-vs-softmax/ https://scikit-learn.org/stable/modules/multiclass.html#multilabel-classification","title":"When should one use Sigmoid or Softmax in a Neural Network?"},{"location":"reighns_ml_journey/interview/interview_handbook/#probability-and-statistics","text":"","title":"Probability and Statistics"},{"location":"reighns_ml_journey/interview/interview_handbook/#why-do-you-multiply-probabilities","text":"Also appear in Chapter 2 summary. Consider rolling a dice twice, what is the probability that you roll a 5 and 6 respectively. We all know the answer is \\(\\dfrac{1}{6} \\times \\dfrac{1}{6} = \\dfrac{1}{36}\\) . But why? This can be first understood that our denominator is the total outcomes in our sample space \\(\\S\\) . This is \\(36\\) , why? By our counting principle on multiplication, we know that if we have \\(6\\) choices in roll \\(1\\) and \\(6\\) choices in roll 2, then the cross-product is \\(6 \\times 6 = 36\\) total choices. One can enumerate \\(\\{(1,1), (1,2), \\ldots, (6,6)\\}\\) to see why. Now the numerator is also related to the counting principle of multiplication as well! In roll 1, rolling a 5 is 1 choice, rolling a 6 next is 1 choice, so total there is a only one combination choice \\(1 \\times 1\\) ! Now if we reframe the problem to what is the probability that you roll a 1, 2 or 3 in the first roll and 2 or 3 in the second roll. Then of course our denominator don't change as \\(36\\) , but our numerator changes, since in roll 1 we have 3 choices, and roll 2 have 2 choices, by the multiplicative principle we have a total of \\(3 \\times 2 = 6\\) choices, and so our probability is \\(\\dfrac{6}{36}\\) now. You can verify that there are indeed \\(6\\) choices manually. Now the most important part is we can use this if both events are independent! If not we need to be careful! .","title":"Why do you multiply probabilities?"},{"location":"reighns_ml_journey/interview/interview_handbook/#what-is-priori-posterior-and-conditional","text":"Common terms in ML world. Suppose there are three types of players in a tennis tournament: A, B, and C. Fifty percent of the contestants in the tournament are A players, 25% are B players, and 25% are C players. Your chance of beating the contestants depends on the class of the player, as follows: 0.3 against an A player 0.4 against a B player 0.5 against a C player If you play a match in this tournament, what is the probability of your winning the match? Supposing that you have won a match, what is the probability that you played against an A player? Let \\(W\\) be the event that you win and \\(A\\) be the event that you played vs player \\(A\\) , then Conditional: \\(\\P(W~|~A)\\) = given you played player \\(A\\) , what is your probability of winning? Priori: \\(\\P(A)\\) = without entering the game , what is your probability of facing player \\(A\\) ? Posterior: \\(\\P(A~|~W)\\) = after entering the game and winning the match , what is your probability that you have actually played with \\(A\\) ? Machine Learning: In many practical engineering problems, the question of interest is often the last one. That is, supposing that you have observed something, what is the most likely cause of that event? For example, supposing we have observed this particular dataset, what is the best Gaussian model that would fit the dataset? Questions like these require some analysis of conditional probability, prior probability, and posterior probability.","title":"What is Priori, Posterior and Conditional"},{"location":"reighns_ml_journey/interview/interview_handbook/#generic-questions","text":"Usually generic and fundamental questions. These questions are \"less code oriented\", and usually requires a short and sweet answer, alongside with an example to illustrate.","title":"Generic Questions"},{"location":"reighns_ml_journey/interview/interview_handbook/#what-are-common-pitfalls-that-lead-to-data-leakage-in-a-pipeline","text":"https://scikit-learn.org/stable/common_pitfalls.html See also numerous question I asked on cross-validated.","title":"What are common pitfalls that lead to Data Leakage in a pipeline?"},{"location":"reighns_ml_journey/interview/interview_handbook/#what-is-the-meaning-of-traintest-distribution-in-the-context-of-machine-learning","text":"https://towardsdatascience.com/understanding-entropy-the-golden-measurement-of-machine-learning-4ea97c663dc3#:~:text=By%20using%20entropy%20in%20machine,be%20desired%20in%20model%2Dbuilding. https://www.kdnuggets.com/2019/01/when-your-training-testing-data-different-distributions.html Let us talk this in a less formal way without invoking formal definitions of \"probability distributions\".","title":"What is the meaning of \"Train/Test\" Distribution in the context of Machine Learning?"},{"location":"reighns_ml_journey/interview/interview_handbook/#train-and-test-distribution","text":"Let us say we are imposed a Machine Learning problem to predict cats vs dogs. We are handed a training set, and there is also a hidden test set that we do not know of. Let us pretend we browse through each image in the training set \\(\\mathcal{D}\\) , and plot out the pixel distribution of each image, we observe that each image pixel distribution follows a gaussian curve with mean 0 and std 1 (i.e. each image in \\(\\mathcal{D}\\) is generated from \\(\\mathcal{N}(0,1)\\) ). Then we trained a good model using training set, but performed badly on the test set \\(\\mathcal{D}_{test}\\) , and after we also plot the pixel density of the test set, we discovered that this set follows a very different distributions, say follow \\(\\mathcal{N}(255, 255)\\) , and each image has 10 times the resolution of the training set. We can therefore conclude, that, both train and test set have different \"probability distributions\".","title":"Train and Test Distribution"},{"location":"reighns_ml_journey/interview/interview_handbook/#distribution-from-your-model","text":"Let us also continue our example from the cat vs dog, this time we assume both train, test and every other images for dogs and cats are generated from \\(\\mathcal{N}(0, 1)\\) . That is to say, we know our population distribution, and if we do know, then why do we need a Machine Learning model at all? We can just approximate all images with the known population distribution with the parameters. Even simpler, if we have a dataset that is beknownst to us with a perfect linear relationship that can be predicted perfectly by a simple linear regression model, (i.e. the population parameter is known), then why do we even need linear regression? However, this world is far from perfect, and we do not know the population parameters of the underlying distribution of the dataset given to us, hence we use models to predict it. Thus, we can analogously say that if a training set \\(\\mathcal{D}\\) follows a distribution \\(p\\) , then our model follows a distribution \\(q\\) , where in the simplest case in linear regression, we know our \\(q\\) follows a distribution with 2 parameters, \\(m\\) the slope, \\(c\\) the bias. With that in mind, our machine learning models can be phrased as using \\(q\\) to approximate \\(q\\) . And this is often used in Cross-Entropy loss.","title":"Distribution from your model"},{"location":"reighns_ml_journey/interview/interview_handbook/#parametric-and-non-parametric-models","text":"ISLR (p21-24) https://stats.stackexchange.com/questions/268638/what-exactly-is-the-difference-between-a-parametric-and-non-parametric-model https://sebastianraschka.com/faq/docs/parametric_vs_nonparametric.html https://sebastianraschka.com/faq/docs/parametric_vs_nonparametric.html: The term \u201cnon-parametric\u201d might sound a bit confusing at first: non-parametric does not mean that they have NO parameters! On the contrary, non-parametric models (can) become more and more complex with an increasing amount of data. So, in a parametric model, we have a finite number of parameters, and in nonparametric models, the number of parameters is (potentially) infinite. Or in other words, in nonparametric models, the complexity of the model grows with the number of training data; in parametric models, we have a fixed number of parameters (or a fixed structure if you will). Linear models such as linear regression, logistic regression, and linear Support Vector Machines are typical examples of a parametric \u201clearners;\u201d here, we have a fixed size of parameters (the weight coefficient.) In contrast, K-nearest neighbor, decision trees, or RBF kernel SVMs are considered as non-parametric learning algorithms since the number of parameters grows with the size of the training set. \u2013 K-nearest neighbor and decision trees, that makes sense, but why is an RBF kernel SVM non-parametric whereas a linear SVM is parametric? In the RBF kernel SVM, we construct the kernel matrix by computing the pair-wise distances between the training points, which makes it non-parametric. In the field of statistics, the term parametric is also associated with a specified probability distribution that you \u201cassume\u201d your data follows, and this distribution comes with the finite number of parameters (for example, the mean and standard deviation of a normal distribution); you don\u2019t make/have these assumptions in non-parametric models. So, in intuitive terms, we can think of a non-parametric model as a \u201cdistribution\u201d or (quasi) assumption-free model. However, keep in mind that the definitions of \u201cparametric\u201d and \u201cnon-parametric\u201d are \u201ca bit ambiguous\u201d at best; according to the \u201cThe Handbook of Nonparametric Statistics 1 (1962) on p. 2: \u201cA precise and universally acceptable definition of the term \u2018nonparametric\u2019 is not presently available. The viewpoint adopted in this handbook is that a statistical procedure is of a nonparametric type if it has properties which are satisfied to a reasonable approximation when some assumptions that are at least of a moderately general nature hold.\"","title":"Parametric and Non-Parametric Models."},{"location":"reighns_ml_journey/interview/interview_handbook/#supervised-and-unsupervised-learning","text":"ISLR (p26-28)","title":"Supervised and Unsupervised Learning."},{"location":"reighns_ml_journey/interview/interview_handbook/#tradeoff-between-flexibility-and-interpretability-of-machine-learning-models","text":"ISLR (p24-26)","title":"Tradeoff between Flexibility and Interpretability of Machine Learning Models."},{"location":"reighns_ml_journey/interview/interview_handbook/#reducible-and-irreducible-errors","text":"ISLR (p18-19)","title":"Reducible and Irreducible Errors."},{"location":"reighns_ml_journey/interview/interview_handbook/#definition","text":"Formal Definition","title":"Definition"},{"location":"reighns_ml_journey/interview/interview_handbook/#example-intuition","text":"We can use features \\(\\X\\) to predict \\(\\y\\) , for example, use age of house, number of bedrooms, etc to predict the price of the house . The irreducible errors can be random events such as Elon Musk tweeting something good about a particular house, which our model cannot account for.","title":"Example (Intuition)"},{"location":"reighns_ml_journey/interview/interview_handbook/#describe-ensembling-in-laymen-terms","text":"Fran\u00e7ois Chollet (Creator of Keras) explained it as : Ensembling relies on the assumption that different good models trained independently are likely to be good for different reasons: each model looks at slightly different aspects of the data to make its predictions, getting part of the \u201ctruth\u201d but not all of it. You may be familiar with the ancient parable of the blind men and the elephant: a group of blind men come across an elephant for the first time and try to understand what the elephant is by touching it. Each man touches a different part of the elephant\u2019s body\u2014just one part, such as the trunk or a leg. Then the men describe toeach other what an elephant is: \u201cIt\u2019s like a snake,\u201d \u201cLike a pillar or a tree,\u201d and so on.The blind men are essentially machine-learning models trying to understand the manifold of the training data, each from its own perspective, using its own assumptions(provided by the unique architecture of the model and the unique random weight initialization). Each of them gets part of the truth of the data, but not the whole truth. By pooling their perspectives together, you can get a far more accurate description of the data. The elephant is a combination of parts: not any single blind man gets it quiteright, but, interviewed together, they can tell a fairly accurate story. Let\u2019s use classification as an example. The easiest way to pool the predictions of a setof classifiers (to ensemble the classifiers) is to average their predictions at inference time:Use four different models to compute initial predictions. preds_a = model_a.predict(x_val) preds_b = model_b.predict(x_val) preds_c = model_c.predict(x_val) preds_d = model_d.predict(x_val) This new prediction array should be more accurate than any of the initial ones. final_preds = 0.25 * (preds_a + preds_b + preds_c + preds_d) This will work only if the classifiers are more or less equally good. If one of them is significantly worse than the others, the final predictions may not be as good as the best classifier of the group.A smarter way to ensemble classifiers is to do a weighted average, where the weights are learned on the validation data typically, the better classifiers are given a higher weight, and the worse classifiers are given a lower weight. To search for a good set of ensembling weights, you can use random search or a simple optimization algorithm such as Nelder-Mead: preds_a = model_a.predict(x_val) preds_b = model_b.predict(x_val) preds_c = model_c.predict(x_val) preds_d = model_d.predict(x_val) These weights (0.5, 0.25,0.1, 0.15) are assumed to be learned empirically. final_preds = 0.5 * preds_a + 0.25 * preds_b + 0.1 * preds_c + 0.15 * preds_d","title":"Describe Ensembling in laymen terms."},{"location":"reighns_ml_journey/interview/interview_handbook/#ensembling-theory","text":"","title":"Ensembling Theory"},{"location":"reighns_ml_journey/interview/interview_handbook/#bagging","text":"","title":"Bagging"},{"location":"reighns_ml_journey/interview/interview_handbook/#cross-validation","text":"","title":"Cross-Validation"},{"location":"reighns_ml_journey/interview/interview_handbook/#why-does-cross-validation-reduce-variance","text":"","title":"Why does Cross-Validation reduce Variance?"},{"location":"reighns_ml_journey/interview/interview_handbook/#model-evaluation","text":"https://www.ritchieng.com/applying-machine-learning/","title":"Model Evaluation"},{"location":"reighns_ml_journey/interview/interview_handbook/#linear-regression","text":"","title":"Linear Regression"},{"location":"reighns_ml_journey/interview/interview_handbook/#polynomial-regression","text":"https://zerowithdot.com/polynomial-regression-in-python/","title":"Polynomial Regression"},{"location":"reighns_ml_journey/interview/interview_handbook/#logistic-regression","text":"","title":"Logistic Regression"},{"location":"reighns_ml_journey/interview/interview_handbook/#decision-tree","text":"","title":"Decision Tree"},{"location":"reighns_ml_journey/interview/interview_handbook/#describe-dt-briefly","text":"Suprvised Learning: will need corresponding label Non-parametric","title":"Describe DT briefly"},{"location":"reighns_ml_journey/interview/interview_handbook/#convolutional-neural-networks","text":"# !pip install torchinfo import timm import torch import torchvision from typing import Dict , Union , Callable , OrderedDict , Tuple import os , random import numpy as np import torch.nn as nn from torchinfo import summary def seed_all ( seed : int = 1992 ) -> None : \"\"\"Seed all random number generators.\"\"\" print ( f \"Using Seed Number { seed } \" ) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) # set PYTHONHASHSEED env var at fixed value torch . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) torch . cuda . manual_seed ( seed ) # pytorch (both CPU and CUDA) np . random . seed ( seed ) # for numpy pseudo-random generator # set fixed value for python built-in pseudo-random generator random . seed ( seed ) torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = True torch . backends . cudnn . enabled = True def seed_worker ( _worker_id ) -> None : \"\"\"Seed a worker with the given ID.\"\"\" worker_seed = torch . initial_seed () % 2 ** 32 np . random . seed ( worker_seed ) random . seed ( worker_seed ) seed_all ( seed = 1992 ) Using Seed Number 1992","title":"Convolutional Neural Networks"},{"location":"reighns_ml_journey/interview/interview_handbook/#padding","text":"","title":"Padding"},{"location":"reighns_ml_journey/interview/interview_handbook/#what-is-the-role-of-zero-padding","text":"","title":"What is the role of zero padding?"},{"location":"reighns_ml_journey/interview/interview_handbook/#pooling","text":"","title":"Pooling"},{"location":"reighns_ml_journey/interview/interview_handbook/#global-average-pooling","text":"https://rwightman.github.io/pytorch-image-models/feature_extraction/#penultimate-layer-features-pre-classifier-features import torch import timm model = timm . create_model ( 'resnet18' , pretrained = True , num_classes = 1000 ) batch_size = 2 image_shape = ( 3 , 224 , 224 ) input_image_tensor = torch . rand ( size = ( batch_size , * image_shape )) # print(summary(model, (2, 3, 224, 224))) model.reset_classifier(num_classes=0, global_pool=\"\") means we do not want global average pooling and thus the shape at the last conv layer (penultimate layer) is \\((2, 512, 7, 7)\\) model.reset_classifier(num_classes=0, global_pool=\"avg\") means we do want global average pooling and thus the shape at the last conv layer (penultimate layer) is \\((2, 512)\\) whereby for each and every of the 512 feature maps \\(f_i\\) , we average \\(f_i\\) across all pixels (i.e. if \\(f_i\\) is 3 by 3 then average means add all \\(3 \\times 3 = 9\\) pixels and average) and concat to become one \\(512\\) vector. model = timm . create_model ( 'resnet18' , pretrained = True , num_classes = 1000 ) o = model ( input_image_tensor ) print ( f 'Original shape: { o . shape } ' ) model . reset_classifier ( num_classes = 0 , global_pool = \"\" ) o = model ( input_image_tensor ) print ( f 'Unpooled shape: { o . shape } ' ) Original shape: torch.Size([2, 1000]) Unpooled shape: torch.Size([2, 512, 7, 7]) model = timm . create_model ( 'resnet18' , pretrained = True , num_classes = 1000 ) o = model ( input_image_tensor ) print ( f 'Original shape: { o . shape } ' ) model . reset_classifier ( num_classes = 0 , global_pool = \"avg\" ) o = model ( input_image_tensor ) print ( f 'Pooled shape: { o . shape } ' ) Original shape: torch.Size([2, 1000]) Pooled shape: torch.Size([2, 512])","title":"Global Average Pooling"},{"location":"reighns_ml_journey/interview/interview_handbook/#image-normalization-in-transfer-learning","text":"Image Normalization","title":"Image Normalization in Transfer Learning"},{"location":"reighns_ml_journey/interview/interview_handbook/#1x1-convolution","text":"A Gentle Introduction to 1\u00d71 Convolutions to Manage Model Complexity","title":"1x1 Convolution"},{"location":"reighns_ml_journey/interview/interview_handbook/#intuition","text":"Pooling layers are designed to downscale feature maps and systematically halve the width and height of feature maps in the network. Nevertheless, pooling layers do not change the number of filters in the model which is the depth or number of channels . The intuition of 1x1 convolution is that we can think of it as a downsample operation on the feature maps but this time shrinking the depth/channels instead of the width/height .","title":"Intuition"},{"location":"reighns_ml_journey/interview/interview_handbook/#downsample-feature-maps-with-1x1-convolution","text":"In Andrew Ng's example, let us define: Batch size of 1; A conv layer of with kernel size of 5, same padding of 2 and stride of 1, and an output depth of 32; The input shape is a stack of feature maps of 192 depth/channels, each of 28 by 28; It follows that the learnable params of the this conv layer is \\(32 \\times 5 \\times 5 \\times 192 + 32 = 153, 632\\) . It follows that the output shape is of \\((32, 28, 28)\\) , which is a stack of feature maps of 32 depth, each of 28 by 28 size. model = nn . Sequential ( nn . Conv2d ( in_channels = 192 , out_channels = 32 , kernel_size = 5 , padding = 2 , stride = 1 ), ) batch_size = 1 input_image = torch . rand ( size = ( 192 , 28 , 28 )) print ( summary ( model , ( batch_size , * input_image . shape ))) ========================================================================================== Layer (type:depth-idx) Output Shape Param # ========================================================================================== Sequential -- -- \u251c\u2500Conv2d: 1-1 [1, 32, 28, 28] 153,632 ========================================================================================== Total params: 153,632 Trainable params: 153,632 Non-trainable params: 0 Total mult-adds (M): 120.45 ========================================================================================== Input size (MB): 0.60 Forward/backward pass size (MB): 0.20 Params size (MB): 0.61 Estimated Total Size (MB): 1.42 ========================================================================================== Now if we design a intermediate layer that holds 1x1 convolution that first decreases the depth from 192 to 16 instead of 32, then connect back with another conv layer of kernel size 5 and depth 32, we can recover the same output shape but at a much leseer computational cost. As we can see below, the params reduced almost 10 folds. Commonly, we call this intermediate layer a bottleneck layer . model = nn . Sequential ( nn . Conv2d ( in_channels = 192 , out_channels = 16 , kernel_size = 1 , padding = 0 , stride = 1 ), nn . Conv2d ( in_channels = 16 , out_channels = 32 , kernel_size = 5 , padding = 2 , stride = 1 ), ) batch_size = 1 input_image = torch . rand ( size = ( 192 , 28 , 28 )) print ( summary ( model , ( batch_size , * input_image . shape ))) ========================================================================================== Layer (type:depth-idx) Output Shape Param # ========================================================================================== Sequential -- -- \u251c\u2500Conv2d: 1-1 [1, 16, 28, 28] 3,088 \u251c\u2500Conv2d: 1-2 [1, 32, 28, 28] 12,832 ========================================================================================== Total params: 15,920 Trainable params: 15,920 Non-trainable params: 0 Total mult-adds (M): 12.48 ========================================================================================== Input size (MB): 0.60 Forward/backward pass size (MB): 0.30 Params size (MB): 0.06 Estimated Total Size (MB): 0.97 ==========================================================================================","title":"Downsample Feature Maps with 1x1 Convolution"},{"location":"reighns_ml_journey/interview/interview_handbook/#upsample-feature-maps-with-1x1-convolution","text":"Similarly, we can increase the depth/channel of the feature maps while maintaining the width and height .","title":"Upsample Feature Maps with 1x1 Convolution"},{"location":"reighns_ml_journey/interview/interview_handbook/#1x1-convolution-is-equivalent-to-a-fc-layer","text":"","title":"1x1 Convolution is Equivalent to a FC-layer"},{"location":"reighns_ml_journey/interview/interview_handbook/#downsampling","text":"","title":"Downsampling"},{"location":"reighns_ml_journey/interview/interview_handbook/#convolutional-stride-downsampling","text":"batch_size = 1 input_image = torch . rand ( size = ( 3 , 224 , 224 )) model = nn . Sequential ( nn . Conv2d ( in_channels = 3 , out_channels = 64 , kernel_size = 7 , padding = 3 , stride = 1 , bias = False ), ) print ( summary ( model , ( batch_size , * input_image . shape ))) ========================================================================================== Layer (type:depth-idx) Output Shape Param # ========================================================================================== Sequential -- -- \u251c\u2500Conv2d: 1-1 [1, 64, 224, 224] 9,408 ========================================================================================== Total params: 9,408 Trainable params: 9,408 Non-trainable params: 0 Total mult-adds (M): 472.06 ========================================================================================== Input size (MB): 0.60 Forward/backward pass size (MB): 25.69 Params size (MB): 0.04 Estimated Total Size (MB): 26.33 ========================================================================================== model = nn . Sequential ( nn . Conv2d ( in_channels = 3 , out_channels = 64 , kernel_size = 7 , padding = 3 , stride = 2 , bias = False ), ) print ( summary ( model , ( batch_size , * input_image . shape ))) ========================================================================================== Layer (type:depth-idx) Output Shape Param # ========================================================================================== Sequential -- -- \u251c\u2500Conv2d: 1-1 [1, 64, 112, 112] 9,408 ========================================================================================== Total params: 9,408 Trainable params: 9,408 Non-trainable params: 0 Total mult-adds (M): 118.01 ========================================================================================== Input size (MB): 0.60 Forward/backward pass size (MB): 6.42 Params size (MB): 0.04 Estimated Total Size (MB): 7.06 ========================================================================================== This is considered a downsampling operation due to our stride changed from 2 to 1. The output shape of the feature maps is from (64, 224, 224) to (64, 112, 112), effectively halving the width and height of the feature maps.","title":"Convolutional Stride (Downsampling)"},{"location":"reighns_ml_journey/interview/interview_handbook/#pooling-downsampling","text":"https://www.quora.com/What-is-a-downsampling-layer-in-Convolutional-Neural-Network-CNN Pooling as a form of Downsampling. Source model = nn . Sequential ( nn . Conv2d ( in_channels = 3 , out_channels = 64 , kernel_size = 7 , padding = 3 , stride = 1 , bias = False ) ) print ( summary ( model , ( batch_size , * input_image . shape ))) ========================================================================================== Layer (type:depth-idx) Output Shape Param # ========================================================================================== Sequential -- -- \u251c\u2500Conv2d: 1-1 [1, 64, 224, 224] 9,408 ========================================================================================== Total params: 9,408 Trainable params: 9,408 Non-trainable params: 0 Total mult-adds (M): 472.06 ========================================================================================== Input size (MB): 0.60 Forward/backward pass size (MB): 25.69 Params size (MB): 0.04 Estimated Total Size (MB): 26.33 ========================================================================================== model = nn . Sequential ( nn . Conv2d ( in_channels = 3 , out_channels = 64 , kernel_size = 7 , padding = 3 , stride = 1 , bias = False ), nn . MaxPool2d ( kernel_size = 2 , stride = 2 ), ) print ( summary ( model , ( batch_size , * input_image . shape ))) ========================================================================================== Layer (type:depth-idx) Output Shape Param # ========================================================================================== Sequential -- -- \u251c\u2500Conv2d: 1-1 [1, 64, 224, 224] 9,408 \u251c\u2500MaxPool2d: 1-2 [1, 64, 112, 112] -- ========================================================================================== Total params: 9,408 Trainable params: 9,408 Non-trainable params: 0 Total mult-adds (M): 472.06 ========================================================================================== Input size (MB): 0.60 Forward/backward pass size (MB): 25.69 Params size (MB): 0.04 Estimated Total Size (MB): 26.33 ========================================================================================== Spatial pooling layers such as MaxPool2d with a size of 2 and stride 2 also reduces the feature map by a factor of 2 by 2. Notice that both downsampling methods have the same number of trainable parameters and also the exact same output shape , so what's the difference? Maybe the additional MaxPool2d is an additional operation that is not needed? Let's see two examples.","title":"Pooling (Downsampling)"},{"location":"reighns_ml_journey/interview/interview_handbook/#convolutional-stride-vs-pooling-for-downsampling","text":"The advantage of the convolution layer is that it can learn certain properties that you might not think of while you add pooling layer. Pooling is a fixed operation and convolution can be learned. On the other hand, pooling is a cheaper operation than convolution, both in terms of the amount of computation that you need to do and number of parameters that you need to store (no parameters for pooling layer). There are examples when one of them is better choice than the other.","title":"Convolutional Stride vs Pooling for Downsampling"},{"location":"reighns_ml_journey/interview/interview_handbook/#example-when-the-convolution-with-strides-is-better-than-pooling","text":"The first layer in the ResNet uses convolution with strides. This is a great example of when striding gives you an advantage. This layer by itself significantly reduces the amount of computation that has to be done by the network in the subsequent layers. It compresses multiple 3x3 convolution (3 to be exact) in to one 7x7 convolution, to make sure that it has exactly the same receptive field as 3 convolution layers (even though it is less powerful in terms of what it can learn). At the same time this layer applies stride=2 that downsamples the image. Because this first layer in ResNet does convolution and downsampling at the same time, the operation becomes significantly cheaper computationally. If you use stride=1 and pooling for downsampling, then you will end up with convolution that does 4 times more computation + extra computation for the next pooling layer. The same trick was used in SqueezeNet and some other neural network architectures.","title":"Example when the convolution with strides is better than pooling"},{"location":"reighns_ml_journey/interview/interview_handbook/#example-where-pooling-is-better-than-convolution","text":"In the NIPS 2018, there was a new architecture presented called FishNet . One thing that they try is to fix the problems with the residual connections used in the ResNet. In the ResNet, in few places, they put 1x1 convolution in the skip connection when downsampling was applied to the image. This convolution layer makes gradient propagation harder. One of the major changes in their paper is that they get rid of the convolutions in the residual connections and replaced them with pooling and simple upscales/identities/concatenations. This solution fixes problem with gradient propagation in very deep networks. From the FishNet paper (Section 3.2) The layers in the head are composed of concatenation, convolution with identity mapping, and max-pooling. Therefore, the gradient propagation problem from the previous backbone network in the tail are solved with the FishNet by 1) excluding I-conv at the head; and 2) using concatenation at the body and the head. Pooling vs. stride for downsampling","title":"Example where pooling is better than convolution"},{"location":"reighns_ml_journey/interview/interview_handbook/#feature-maps","text":"","title":"Feature Maps"},{"location":"reighns_ml_journey/interview/interview_handbook/#why-are-the-last-few-feature-maps-more-useful-in-feature-extraction","text":"The backbone network (\"convolution and pooling\") is responsible for extracting a feature map from the image that contains higher level summarized information. Each head uses this feature map as input to predict its desired outcome. The main intuition why feature maps of the last few layers (last layer usually) are important is one needs to recognize the earlier conv layer's feature maps find simple features like shapes, sizes, edges from an image, while the deep conv layers will be of more abstract features in an image. As a result, we really just want the abstract feature maps as they are more class specific to the image instead of the earlier layers which gives generic shapes .","title":"Why are the last few Feature Maps more Useful in Feature extraction?"},{"location":"reighns_ml_journey/interview/interview_handbook/#object-detection-yolo","text":"","title":"Object Detection (YOLO)"},{"location":"reighns_ml_journey/interview/interview_handbook/#object-detection-bounding-boxes","text":"Different conversions https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/ https://github.com/awsaf49/bbox/blob/main/bbox/utils.py Kevin Huo & Nick Singh: Ace The Data Science Interview, 2021. (pp. 118) \u21a9","title":"Object Detection (Bounding Boxes)"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/","text":"16th January 2021: Shifted my notebooks onto GitHub for easier integration, in particular to track changes. References: Reference 1 Reference 2 Reference 3 from google.colab import drive drive . mount ( '/content/drive/' ) Mounted at /content/drive/ ! unzip '/content/drive/My Drive/Cassava/zip-folder/Cassava-JPEG-128x128.zip' - d '/content/' \u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m inflating: /content/kaggle/train_images_jpeg/3952546193.jpg inflating: /content/kaggle/train_images_jpeg/3952799769.jpg inflating: /content/kaggle/train_images_jpeg/3953140534.jpg inflating: /content/kaggle/train_images_jpeg/3953222407.jpg inflating: /content/kaggle/train_images_jpeg/3953247024.jpg inflating: /content/kaggle/train_images_jpeg/3953327881.jpg inflating: /content/kaggle/train_images_jpeg/3953331047.jpg inflating: /content/kaggle/train_images_jpeg/3953514366.jpg inflating: /content/kaggle/train_images_jpeg/3953530273.jpg inflating: /content/kaggle/train_images_jpeg/3953560426.jpg inflating: /content/kaggle/train_images_jpeg/3953648207.jpg inflating: /content/kaggle/train_images_jpeg/3953857113.jpg inflating: /content/kaggle/train_images_jpeg/3953901317.jpg inflating: /content/kaggle/train_images_jpeg/3954180556.jpg inflating: /content/kaggle/train_images_jpeg/3954239221.jpg inflating: /content/kaggle/train_images_jpeg/3954387963.jpg inflating: /content/kaggle/train_images_jpeg/3954399974.jpg inflating: /content/kaggle/train_images_jpeg/3954487465.jpg inflating: /content/kaggle/train_images_jpeg/3954910918.jpg inflating: /content/kaggle/train_images_jpeg/3955391972.jpg inflating: /content/kaggle/train_images_jpeg/3955442838.jpg inflating: /content/kaggle/train_images_jpeg/3955739563.jpg inflating: /content/kaggle/train_images_jpeg/3955931830.jpg inflating: /content/kaggle/train_images_jpeg/3955972139.jpg inflating: /content/kaggle/train_images_jpeg/3956075690.jpg inflating: /content/kaggle/train_images_jpeg/3956077728.jpg inflating: /content/kaggle/train_images_jpeg/3956155774.jpg inflating: /content/kaggle/train_images_jpeg/3956271103.jpg inflating: /content/kaggle/train_images_jpeg/3956372146.jpg inflating: /content/kaggle/train_images_jpeg/3956407201.jpg inflating: /content/kaggle/train_images_jpeg/3956550570.jpg inflating: /content/kaggle/train_images_jpeg/3956605397.jpg inflating: /content/kaggle/train_images_jpeg/3956778160.jpg inflating: /content/kaggle/train_images_jpeg/3957562076.jpg inflating: /content/kaggle/train_images_jpeg/3957612771.jpg inflating: /content/kaggle/train_images_jpeg/3957807023.jpg inflating: /content/kaggle/train_images_jpeg/3957819631.jpg inflating: /content/kaggle/train_images_jpeg/395820132.jpg inflating: /content/kaggle/train_images_jpeg/3958304403.jpg inflating: /content/kaggle/train_images_jpeg/395852128.jpg inflating: /content/kaggle/train_images_jpeg/395853907.jpg inflating: /content/kaggle/train_images_jpeg/3958714206.jpg inflating: /content/kaggle/train_images_jpeg/3958986545.jpg inflating: /content/kaggle/train_images_jpeg/3959033347.jpg inflating: /content/kaggle/train_images_jpeg/3959151804.jpg inflating: /content/kaggle/train_images_jpeg/3959266436.jpg inflating: /content/kaggle/train_images_jpeg/3959470813.jpg inflating: /content/kaggle/train_images_jpeg/3959970856.jpg inflating: /content/kaggle/train_images_jpeg/3960301906.jpg inflating: /content/kaggle/train_images_jpeg/396042395.jpg inflating: /content/kaggle/train_images_jpeg/396060343.jpg inflating: /content/kaggle/train_images_jpeg/3960669389.jpg inflating: /content/kaggle/train_images_jpeg/3960791119.jpg inflating: /content/kaggle/train_images_jpeg/3961078700.jpg inflating: /content/kaggle/train_images_jpeg/3961117682.jpg inflating: /content/kaggle/train_images_jpeg/3961136833.jpg inflating: /content/kaggle/train_images_jpeg/396152951.jpg inflating: /content/kaggle/train_images_jpeg/3961645455.jpg inflating: /content/kaggle/train_images_jpeg/3962057162.jpg inflating: /content/kaggle/train_images_jpeg/3962294427.jpg inflating: /content/kaggle/train_images_jpeg/3962424026.jpg inflating: /content/kaggle/train_images_jpeg/3962684748.jpg inflating: /content/kaggle/train_images_jpeg/3962830486.jpg inflating: /content/kaggle/train_images_jpeg/3962941156.jpg inflating: /content/kaggle/train_images_jpeg/3962965208.jpg inflating: /content/kaggle/train_images_jpeg/3963041758.jpg inflating: /content/kaggle/train_images_jpeg/3963163426.jpg inflating: /content/kaggle/train_images_jpeg/3963194620.jpg inflating: /content/kaggle/train_images_jpeg/3963293417.jpg inflating: /content/kaggle/train_images_jpeg/3963421928.jpg inflating: /content/kaggle/train_images_jpeg/396350327.jpg inflating: /content/kaggle/train_images_jpeg/3963572251.jpg inflating: /content/kaggle/train_images_jpeg/3963604072.jpg inflating: /content/kaggle/train_images_jpeg/3963914024.jpg inflating: /content/kaggle/train_images_jpeg/3963917355.jpg inflating: /content/kaggle/train_images_jpeg/3963919295.jpg inflating: /content/kaggle/train_images_jpeg/3963996017.jpg inflating: /content/kaggle/train_images_jpeg/3964029612.jpg inflating: /content/kaggle/train_images_jpeg/3964066128.jpg inflating: /content/kaggle/train_images_jpeg/3964069198.jpg inflating: /content/kaggle/train_images_jpeg/3964074425.jpg inflating: /content/kaggle/train_images_jpeg/3964080612.jpg inflating: /content/kaggle/train_images_jpeg/3964124529.jpg inflating: /content/kaggle/train_images_jpeg/3964194408.jpg inflating: /content/kaggle/train_images_jpeg/3964240864.jpg inflating: /content/kaggle/train_images_jpeg/3964316261.jpg inflating: /content/kaggle/train_images_jpeg/3964372330.jpg inflating: /content/kaggle/train_images_jpeg/3964576371.jpg inflating: /content/kaggle/train_images_jpeg/3964588616.jpg inflating: /content/kaggle/train_images_jpeg/3964816822.jpg inflating: /content/kaggle/train_images_jpeg/3964842875.jpg inflating: /content/kaggle/train_images_jpeg/3964876844.jpg inflating: /content/kaggle/train_images_jpeg/3964970132.jpg inflating: /content/kaggle/train_images_jpeg/396499078.jpg inflating: /content/kaggle/train_images_jpeg/3965026649.jpg inflating: /content/kaggle/train_images_jpeg/3965161877.jpg inflating: /content/kaggle/train_images_jpeg/3965169748.jpg inflating: /content/kaggle/train_images_jpeg/396528848.jpg inflating: /content/kaggle/train_images_jpeg/3966039850.jpg inflating: /content/kaggle/train_images_jpeg/3966169079.jpg inflating: /content/kaggle/train_images_jpeg/396625328.jpg inflating: /content/kaggle/train_images_jpeg/3966256467.jpg inflating: /content/kaggle/train_images_jpeg/3966311153.jpg inflating: /content/kaggle/train_images_jpeg/3966397461.jpg inflating: /content/kaggle/train_images_jpeg/3966432707.jpg inflating: /content/kaggle/train_images_jpeg/3966618744.jpg inflating: /content/kaggle/train_images_jpeg/3966914560.jpg inflating: /content/kaggle/train_images_jpeg/3966975834.jpg inflating: /content/kaggle/train_images_jpeg/3967002927.jpg inflating: /content/kaggle/train_images_jpeg/3967037274.jpg inflating: /content/kaggle/train_images_jpeg/3967050100.jpg inflating: /content/kaggle/train_images_jpeg/3967891639.jpg inflating: /content/kaggle/train_images_jpeg/3968048683.jpg inflating: /content/kaggle/train_images_jpeg/3968083899.jpg inflating: /content/kaggle/train_images_jpeg/396829878.jpg inflating: /content/kaggle/train_images_jpeg/3968384941.jpg inflating: /content/kaggle/train_images_jpeg/3968735847.jpg inflating: /content/kaggle/train_images_jpeg/3968955392.jpg inflating: /content/kaggle/train_images_jpeg/3968956447.jpg inflating: /content/kaggle/train_images_jpeg/3969087697.jpg inflating: /content/kaggle/train_images_jpeg/3969288877.jpg inflating: /content/kaggle/train_images_jpeg/3969515.jpg inflating: /content/kaggle/train_images_jpeg/3969588604.jpg inflating: /content/kaggle/train_images_jpeg/3969787615.jpg inflating: /content/kaggle/train_images_jpeg/3969928849.jpg inflating: /content/kaggle/train_images_jpeg/3970119375.jpg inflating: /content/kaggle/train_images_jpeg/397081522.jpg inflating: /content/kaggle/train_images_jpeg/3971043257.jpg inflating: /content/kaggle/train_images_jpeg/3971124978.jpg inflating: /content/kaggle/train_images_jpeg/3971240417.jpg inflating: /content/kaggle/train_images_jpeg/3971401456.jpg inflating: /content/kaggle/train_images_jpeg/3971597689.jpg inflating: /content/kaggle/train_images_jpeg/3971663926.jpg inflating: /content/kaggle/train_images_jpeg/3971676147.jpg inflating: /content/kaggle/train_images_jpeg/3972035105.jpg inflating: /content/kaggle/train_images_jpeg/3972201692.jpg inflating: /content/kaggle/train_images_jpeg/3972207361.jpg inflating: /content/kaggle/train_images_jpeg/397242175.jpg inflating: /content/kaggle/train_images_jpeg/3972834452.jpg inflating: /content/kaggle/train_images_jpeg/3973042447.jpg inflating: /content/kaggle/train_images_jpeg/3973123989.jpg inflating: /content/kaggle/train_images_jpeg/3973350310.jpg inflating: /content/kaggle/train_images_jpeg/3973412468.jpg inflating: /content/kaggle/train_images_jpeg/3973452259.jpg inflating: /content/kaggle/train_images_jpeg/3973659068.jpg inflating: /content/kaggle/train_images_jpeg/3973759314.jpg inflating: /content/kaggle/train_images_jpeg/3973772550.jpg inflating: /content/kaggle/train_images_jpeg/3973846840.jpg inflating: /content/kaggle/train_images_jpeg/3973974283.jpg inflating: /content/kaggle/train_images_jpeg/3974310104.jpg inflating: /content/kaggle/train_images_jpeg/3974406563.jpg inflating: /content/kaggle/train_images_jpeg/3974455402.jpg inflating: /content/kaggle/train_images_jpeg/3974644689.jpg inflating: /content/kaggle/train_images_jpeg/397477697.jpg inflating: /content/kaggle/train_images_jpeg/3975444429.jpg inflating: /content/kaggle/train_images_jpeg/3975487347.jpg inflating: /content/kaggle/train_images_jpeg/3975766951.jpg inflating: /content/kaggle/train_images_jpeg/3975873583.jpg inflating: /content/kaggle/train_images_jpeg/3976044474.jpg inflating: /content/kaggle/train_images_jpeg/3976294368.jpg inflating: /content/kaggle/train_images_jpeg/3976477069.jpg inflating: /content/kaggle/train_images_jpeg/3976765860.jpg inflating: /content/kaggle/train_images_jpeg/3977257761.jpg inflating: /content/kaggle/train_images_jpeg/3977307384.jpg inflating: /content/kaggle/train_images_jpeg/3977320696.jpg inflating: /content/kaggle/train_images_jpeg/3977561724.jpg inflating: /content/kaggle/train_images_jpeg/3977718689.jpg inflating: /content/kaggle/train_images_jpeg/3977934674.jpg inflating: /content/kaggle/train_images_jpeg/3977938536.jpg inflating: /content/kaggle/train_images_jpeg/3978175497.jpg inflating: /content/kaggle/train_images_jpeg/3978296365.jpg inflating: /content/kaggle/train_images_jpeg/3978345491.jpg inflating: /content/kaggle/train_images_jpeg/3978498326.jpg inflating: /content/kaggle/train_images_jpeg/3978633568.jpg inflating: /content/kaggle/train_images_jpeg/397896685.jpg inflating: /content/kaggle/train_images_jpeg/3979130498.jpg inflating: /content/kaggle/train_images_jpeg/3979670694.jpg inflating: /content/kaggle/train_images_jpeg/3979826725.jpg inflating: /content/kaggle/train_images_jpeg/3980236832.jpg inflating: /content/kaggle/train_images_jpeg/3980286599.jpg inflating: /content/kaggle/train_images_jpeg/3980578262.jpg inflating: /content/kaggle/train_images_jpeg/3980819954.jpg inflating: /content/kaggle/train_images_jpeg/398109991.jpg inflating: /content/kaggle/train_images_jpeg/3981217700.jpg inflating: /content/kaggle/train_images_jpeg/3981449307.jpg inflating: /content/kaggle/train_images_jpeg/3981645736.jpg inflating: /content/kaggle/train_images_jpeg/3982368414.jpg inflating: /content/kaggle/train_images_jpeg/3982901070.jpg inflating: /content/kaggle/train_images_jpeg/3982972908.jpg inflating: /content/kaggle/train_images_jpeg/3983156415.jpg inflating: /content/kaggle/train_images_jpeg/3983257977.jpg inflating: /content/kaggle/train_images_jpeg/3983529541.jpg inflating: /content/kaggle/train_images_jpeg/3983622976.jpg inflating: /content/kaggle/train_images_jpeg/3984046455.jpg inflating: /content/kaggle/train_images_jpeg/3984082727.jpg inflating: /content/kaggle/train_images_jpeg/3984375685.jpg inflating: /content/kaggle/train_images_jpeg/3984703972.jpg inflating: /content/kaggle/train_images_jpeg/398530204.jpg inflating: /content/kaggle/train_images_jpeg/3985367860.jpg inflating: /content/kaggle/train_images_jpeg/3985416531.jpg inflating: /content/kaggle/train_images_jpeg/3985539982.jpg inflating: /content/kaggle/train_images_jpeg/3985545704.jpg inflating: /content/kaggle/train_images_jpeg/3985864340.jpg inflating: /content/kaggle/train_images_jpeg/3986073933.jpg inflating: /content/kaggle/train_images_jpeg/3986159422.jpg inflating: /content/kaggle/train_images_jpeg/3986218390.jpg inflating: /content/kaggle/train_images_jpeg/3986415473.jpg inflating: /content/kaggle/train_images_jpeg/3986526512.jpg inflating: /content/kaggle/train_images_jpeg/3986617581.jpg inflating: /content/kaggle/train_images_jpeg/3986748411.jpg inflating: /content/kaggle/train_images_jpeg/3986774650.jpg inflating: /content/kaggle/train_images_jpeg/3986797492.jpg inflating: /content/kaggle/train_images_jpeg/3986821371.jpg inflating: /content/kaggle/train_images_jpeg/3986994681.jpg inflating: /content/kaggle/train_images_jpeg/3987026948.jpg inflating: /content/kaggle/train_images_jpeg/3987029837.jpg inflating: /content/kaggle/train_images_jpeg/3987145170.jpg inflating: /content/kaggle/train_images_jpeg/3987160874.jpg inflating: /content/kaggle/train_images_jpeg/3987321075.jpg inflating: /content/kaggle/train_images_jpeg/3987329576.jpg inflating: /content/kaggle/train_images_jpeg/3987330899.jpg inflating: /content/kaggle/train_images_jpeg/3987528887.jpg inflating: /content/kaggle/train_images_jpeg/3987800643.jpg inflating: /content/kaggle/train_images_jpeg/3988179260.jpg inflating: /content/kaggle/train_images_jpeg/3988520173.jpg inflating: /content/kaggle/train_images_jpeg/3988625744.jpg inflating: /content/kaggle/train_images_jpeg/3988748153.jpg inflating: /content/kaggle/train_images_jpeg/3988816967.jpg inflating: /content/kaggle/train_images_jpeg/3988908835.jpg inflating: /content/kaggle/train_images_jpeg/3989425098.jpg inflating: /content/kaggle/train_images_jpeg/3989450757.jpg inflating: /content/kaggle/train_images_jpeg/3989567826.jpg inflating: /content/kaggle/train_images_jpeg/3989761123.jpg inflating: /content/kaggle/train_images_jpeg/399009476.jpg inflating: /content/kaggle/train_images_jpeg/399128521.jpg inflating: /content/kaggle/train_images_jpeg/3991445025.jpg inflating: /content/kaggle/train_images_jpeg/3991703391.jpg inflating: /content/kaggle/train_images_jpeg/3991899434.jpg inflating: /content/kaggle/train_images_jpeg/3992156989.jpg inflating: /content/kaggle/train_images_jpeg/3992168079.jpg inflating: /content/kaggle/train_images_jpeg/3992333811.jpg inflating: /content/kaggle/train_images_jpeg/3992509961.jpg inflating: /content/kaggle/train_images_jpeg/3992628804.jpg inflating: /content/kaggle/train_images_jpeg/3993385288.jpg inflating: /content/kaggle/train_images_jpeg/3993406091.jpg inflating: /content/kaggle/train_images_jpeg/3993445930.jpg inflating: /content/kaggle/train_images_jpeg/3993544013.jpg inflating: /content/kaggle/train_images_jpeg/3993617081.jpg inflating: /content/kaggle/train_images_jpeg/3994140306.jpg inflating: /content/kaggle/train_images_jpeg/3994489797.jpg inflating: /content/kaggle/train_images_jpeg/3994624875.jpg inflating: /content/kaggle/train_images_jpeg/3994849052.jpg inflating: /content/kaggle/train_images_jpeg/3994869632.jpg inflating: /content/kaggle/train_images_jpeg/3995159118.jpg inflating: /content/kaggle/train_images_jpeg/3995459350.jpg inflating: /content/kaggle/train_images_jpeg/3995498206.jpg inflating: /content/kaggle/train_images_jpeg/3995539083.jpg inflating: /content/kaggle/train_images_jpeg/3995611090.jpg inflating: /content/kaggle/train_images_jpeg/3995611401.jpg inflating: /content/kaggle/train_images_jpeg/3995652879.jpg inflating: /content/kaggle/train_images_jpeg/3995855836.jpg inflating: /content/kaggle/train_images_jpeg/3995859683.jpg inflating: /content/kaggle/train_images_jpeg/3995871788.jpg inflating: /content/kaggle/train_images_jpeg/3995905691.jpg inflating: /content/kaggle/train_images_jpeg/3995991109.jpg inflating: /content/kaggle/train_images_jpeg/3996218398.jpg inflating: /content/kaggle/train_images_jpeg/3996410280.jpg inflating: /content/kaggle/train_images_jpeg/3996497470.jpg inflating: /content/kaggle/train_images_jpeg/3996512301.jpg inflating: /content/kaggle/train_images_jpeg/399652571.jpg inflating: /content/kaggle/train_images_jpeg/3997023855.jpg inflating: /content/kaggle/train_images_jpeg/3997092474.jpg inflating: /content/kaggle/train_images_jpeg/3997582442.jpg inflating: /content/kaggle/train_images_jpeg/399780928.jpg inflating: /content/kaggle/train_images_jpeg/3997854029.jpg inflating: /content/kaggle/train_images_jpeg/3998061368.jpg inflating: /content/kaggle/train_images_jpeg/3998293147.jpg inflating: /content/kaggle/train_images_jpeg/3998419018.jpg inflating: /content/kaggle/train_images_jpeg/3998452657.jpg inflating: /content/kaggle/train_images_jpeg/3998460342.jpg inflating: /content/kaggle/train_images_jpeg/3998485682.jpg inflating: /content/kaggle/train_images_jpeg/3998876448.jpg inflating: /content/kaggle/train_images_jpeg/3998978709.jpg inflating: /content/kaggle/train_images_jpeg/3998982675.jpg inflating: /content/kaggle/train_images_jpeg/3999057498.jpg inflating: /content/kaggle/train_images_jpeg/3999223206.jpg inflating: /content/kaggle/train_images_jpeg/3999758372.jpg inflating: /content/kaggle/train_images_jpeg/3999919472.jpg inflating: /content/kaggle/train_images_jpeg/4000041992.jpg inflating: /content/kaggle/train_images_jpeg/4000198689.jpg inflating: /content/kaggle/train_images_jpeg/4000520733.jpg inflating: /content/kaggle/train_images_jpeg/4000584857.jpg inflating: /content/kaggle/train_images_jpeg/400079356.jpg inflating: /content/kaggle/train_images_jpeg/4001352115.jpg inflating: /content/kaggle/train_images_jpeg/4001662386.jpg inflating: /content/kaggle/train_images_jpeg/4002244325.jpg inflating: /content/kaggle/train_images_jpeg/4002359446.jpg inflating: /content/kaggle/train_images_jpeg/4002980483.jpg inflating: /content/kaggle/train_images_jpeg/4003064199.jpg inflating: /content/kaggle/train_images_jpeg/4003333385.jpg inflating: /content/kaggle/train_images_jpeg/4003454599.jpg inflating: /content/kaggle/train_images_jpeg/4003743131.jpg inflating: /content/kaggle/train_images_jpeg/4003954623.jpg inflating: /content/kaggle/train_images_jpeg/4004133979.jpg inflating: /content/kaggle/train_images_jpeg/4004256594.jpg inflating: /content/kaggle/train_images_jpeg/4004395307.jpg inflating: /content/kaggle/train_images_jpeg/4004655752.jpg inflating: /content/kaggle/train_images_jpeg/4004943921.jpg inflating: /content/kaggle/train_images_jpeg/4005310479.jpg inflating: /content/kaggle/train_images_jpeg/4005501772.jpg inflating: /content/kaggle/train_images_jpeg/4005530868.jpg inflating: /content/kaggle/train_images_jpeg/4005695464.jpg inflating: /content/kaggle/train_images_jpeg/400576594.jpg inflating: /content/kaggle/train_images_jpeg/4005806149.jpg inflating: /content/kaggle/train_images_jpeg/4006299018.jpg inflating: /content/kaggle/train_images_jpeg/4006518224.jpg inflating: /content/kaggle/train_images_jpeg/4006579451.jpg inflating: /content/kaggle/train_images_jpeg/400659016.jpg inflating: /content/kaggle/train_images_jpeg/4006771974.jpg inflating: /content/kaggle/train_images_jpeg/400686072.jpg inflating: /content/kaggle/train_images_jpeg/4007062391.jpg inflating: /content/kaggle/train_images_jpeg/4007185339.jpg inflating: /content/kaggle/train_images_jpeg/4007199956.jpg inflating: /content/kaggle/train_images_jpeg/4007234267.jpg inflating: /content/kaggle/train_images_jpeg/4007496530.jpg inflating: /content/kaggle/train_images_jpeg/4007863717.jpg inflating: /content/kaggle/train_images_jpeg/400794559.jpg inflating: /content/kaggle/train_images_jpeg/4008326221.jpg inflating: /content/kaggle/train_images_jpeg/4008495271.jpg inflating: /content/kaggle/train_images_jpeg/4009113191.jpg inflating: /content/kaggle/train_images_jpeg/4009310147.jpg inflating: /content/kaggle/train_images_jpeg/4009415549.jpg inflating: /content/kaggle/train_images_jpeg/4009427061.jpg inflating: /content/kaggle/train_images_jpeg/4009888434.jpg inflating: /content/kaggle/train_images_jpeg/4010033110.jpg inflating: /content/kaggle/train_images_jpeg/4010091989.jpg inflating: /content/kaggle/train_images_jpeg/4010133596.jpg inflating: /content/kaggle/train_images_jpeg/401014506.jpg inflating: /content/kaggle/train_images_jpeg/401103417.jpg inflating: /content/kaggle/train_images_jpeg/4011347600.jpg inflating: /content/kaggle/train_images_jpeg/4011584654.jpg inflating: /content/kaggle/train_images_jpeg/4011808410.jpg inflating: /content/kaggle/train_images_jpeg/4011855023.jpg inflating: /content/kaggle/train_images_jpeg/4012035359.jpg inflating: /content/kaggle/train_images_jpeg/4012105605.jpg inflating: /content/kaggle/train_images_jpeg/4012298941.jpg inflating: /content/kaggle/train_images_jpeg/4012991652.jpg inflating: /content/kaggle/train_images_jpeg/4013151880.jpg inflating: /content/kaggle/train_images_jpeg/4013206965.jpg inflating: /content/kaggle/train_images_jpeg/4013482004.jpg inflating: /content/kaggle/train_images_jpeg/4013610919.jpg inflating: /content/kaggle/train_images_jpeg/4013834415.jpg inflating: /content/kaggle/train_images_jpeg/4013991367.jpg inflating: /content/kaggle/train_images_jpeg/4014439076.jpg inflating: /content/kaggle/train_images_jpeg/4014877464.jpg inflating: /content/kaggle/train_images_jpeg/4014881433.jpg inflating: /content/kaggle/train_images_jpeg/4014913503.jpg inflating: /content/kaggle/train_images_jpeg/4015277516.jpg inflating: /content/kaggle/train_images_jpeg/401560785.jpg inflating: /content/kaggle/train_images_jpeg/4016073096.jpg inflating: /content/kaggle/train_images_jpeg/4016154109.jpg inflating: /content/kaggle/train_images_jpeg/4016279583.jpg inflating: /content/kaggle/train_images_jpeg/4016400274.jpg inflating: /content/kaggle/train_images_jpeg/4016407107.jpg inflating: /content/kaggle/train_images_jpeg/4016547157.jpg inflating: /content/kaggle/train_images_jpeg/4016802646.jpg inflating: /content/kaggle/train_images_jpeg/4017187341.jpg inflating: /content/kaggle/train_images_jpeg/4017354587.jpg inflating: /content/kaggle/train_images_jpeg/4017387228.jpg inflating: /content/kaggle/train_images_jpeg/4017647533.jpg inflating: /content/kaggle/train_images_jpeg/4017689394.jpg inflating: /content/kaggle/train_images_jpeg/4017822607.jpg inflating: /content/kaggle/train_images_jpeg/4017896312.jpg inflating: /content/kaggle/train_images_jpeg/401790013.jpg inflating: /content/kaggle/train_images_jpeg/4017977717.jpg inflating: /content/kaggle/train_images_jpeg/4018169789.jpg inflating: /content/kaggle/train_images_jpeg/401824702.jpg inflating: /content/kaggle/train_images_jpeg/401825012.jpg inflating: /content/kaggle/train_images_jpeg/4018250398.jpg inflating: /content/kaggle/train_images_jpeg/4018307313.jpg inflating: /content/kaggle/train_images_jpeg/4018364725.jpg inflating: /content/kaggle/train_images_jpeg/4018367133.jpg inflating: /content/kaggle/train_images_jpeg/4018898833.jpg inflating: /content/kaggle/train_images_jpeg/4018949072.jpg inflating: /content/kaggle/train_images_jpeg/4019060671.jpg inflating: /content/kaggle/train_images_jpeg/4019065947.jpg inflating: /content/kaggle/train_images_jpeg/4019067935.jpg inflating: /content/kaggle/train_images_jpeg/4019214613.jpg inflating: /content/kaggle/train_images_jpeg/4019472268.jpg inflating: /content/kaggle/train_images_jpeg/4019771530.jpg inflating: /content/kaggle/train_images_jpeg/4020055407.jpg inflating: /content/kaggle/train_images_jpeg/4020073258.jpg inflating: /content/kaggle/train_images_jpeg/4020138210.jpg inflating: /content/kaggle/train_images_jpeg/4020462779.jpg inflating: /content/kaggle/train_images_jpeg/4020967686.jpg inflating: /content/kaggle/train_images_jpeg/4021015970.jpg inflating: /content/kaggle/train_images_jpeg/4021120982.jpg inflating: /content/kaggle/train_images_jpeg/4021769307.jpg inflating: /content/kaggle/train_images_jpeg/4022000067.jpg inflating: /content/kaggle/train_images_jpeg/402211676.jpg inflating: /content/kaggle/train_images_jpeg/4022128079.jpg inflating: /content/kaggle/train_images_jpeg/4022798796.jpg inflating: /content/kaggle/train_images_jpeg/402280669.jpg inflating: /content/kaggle/train_images_jpeg/4022871563.jpg inflating: /content/kaggle/train_images_jpeg/4022972553.jpg inflating: /content/kaggle/train_images_jpeg/4022980210.jpg inflating: /content/kaggle/train_images_jpeg/4024341056.jpg inflating: /content/kaggle/train_images_jpeg/4024391744.jpg inflating: /content/kaggle/train_images_jpeg/4024425381.jpg inflating: /content/kaggle/train_images_jpeg/4024428406.jpg inflating: /content/kaggle/train_images_jpeg/4024436293.jpg inflating: /content/kaggle/train_images_jpeg/4024636592.jpg inflating: /content/kaggle/train_images_jpeg/4025247063.jpg inflating: /content/kaggle/train_images_jpeg/4025505972.jpg inflating: /content/kaggle/train_images_jpeg/4025590571.jpg inflating: /content/kaggle/train_images_jpeg/4025854027.jpg inflating: /content/kaggle/train_images_jpeg/4025993930.jpg inflating: /content/kaggle/train_images_jpeg/4026064019.jpg inflating: /content/kaggle/train_images_jpeg/402606580.jpg inflating: /content/kaggle/train_images_jpeg/4026684435.jpg inflating: /content/kaggle/train_images_jpeg/4026736343.jpg inflating: /content/kaggle/train_images_jpeg/4026910494.jpg inflating: /content/kaggle/train_images_jpeg/4027254563.jpg inflating: /content/kaggle/train_images_jpeg/4027944552.jpg inflating: /content/kaggle/train_images_jpeg/402862496.jpg inflating: /content/kaggle/train_images_jpeg/402874662.jpg inflating: /content/kaggle/train_images_jpeg/402875912.jpg inflating: /content/kaggle/train_images_jpeg/4028806922.jpg inflating: /content/kaggle/train_images_jpeg/4028883745.jpg inflating: /content/kaggle/train_images_jpeg/4028966918.jpg inflating: /content/kaggle/train_images_jpeg/4029027750.jpg inflating: /content/kaggle/train_images_jpeg/402961309.jpg inflating: /content/kaggle/train_images_jpeg/402962978.jpg inflating: /content/kaggle/train_images_jpeg/4029728754.jpg inflating: /content/kaggle/train_images_jpeg/4029880572.jpg inflating: /content/kaggle/train_images_jpeg/4029905096.jpg inflating: /content/kaggle/train_images_jpeg/4030129065.jpg inflating: /content/kaggle/train_images_jpeg/4030345152.jpg inflating: /content/kaggle/train_images_jpeg/4030356791.jpg inflating: /content/kaggle/train_images_jpeg/4030410880.jpg inflating: /content/kaggle/train_images_jpeg/403051866.jpg inflating: /content/kaggle/train_images_jpeg/4030982383.jpg inflating: /content/kaggle/train_images_jpeg/4031122706.jpg inflating: /content/kaggle/train_images_jpeg/4031188863.jpg inflating: /content/kaggle/train_images_jpeg/4031297900.jpg inflating: /content/kaggle/train_images_jpeg/4031321038.jpg inflating: /content/kaggle/train_images_jpeg/4031374252.jpg inflating: /content/kaggle/train_images_jpeg/4031511007.jpg inflating: /content/kaggle/train_images_jpeg/4031523832.jpg inflating: /content/kaggle/train_images_jpeg/4031627524.jpg inflating: /content/kaggle/train_images_jpeg/4031789905.jpg inflating: /content/kaggle/train_images_jpeg/4031880127.jpg inflating: /content/kaggle/train_images_jpeg/4031957327.jpg inflating: /content/kaggle/train_images_jpeg/4032292698.jpg inflating: /content/kaggle/train_images_jpeg/4032326580.jpg inflating: /content/kaggle/train_images_jpeg/403241496.jpg inflating: /content/kaggle/train_images_jpeg/4032774501.jpg inflating: /content/kaggle/train_images_jpeg/4032903735.jpg inflating: /content/kaggle/train_images_jpeg/403293009.jpg inflating: /content/kaggle/train_images_jpeg/4032940326.jpg inflating: /content/kaggle/train_images_jpeg/4033298149.jpg inflating: /content/kaggle/train_images_jpeg/403345253.jpg inflating: /content/kaggle/train_images_jpeg/4033546541.jpg inflating: /content/kaggle/train_images_jpeg/403372857.jpg inflating: /content/kaggle/train_images_jpeg/4033870667.jpg inflating: /content/kaggle/train_images_jpeg/4034095153.jpg inflating: /content/kaggle/train_images_jpeg/403422220.jpg inflating: /content/kaggle/train_images_jpeg/4034332345.jpg inflating: /content/kaggle/train_images_jpeg/403440184.jpg inflating: /content/kaggle/train_images_jpeg/4034495674.jpg inflating: /content/kaggle/train_images_jpeg/4034498053.jpg inflating: /content/kaggle/train_images_jpeg/4034547314.jpg inflating: /content/kaggle/train_images_jpeg/4034553253.jpg inflating: /content/kaggle/train_images_jpeg/403458333.jpg inflating: /content/kaggle/train_images_jpeg/4034653549.jpg inflating: /content/kaggle/train_images_jpeg/4034666533.jpg inflating: /content/kaggle/train_images_jpeg/403476410.jpg inflating: /content/kaggle/train_images_jpeg/4035576240.jpg inflating: /content/kaggle/train_images_jpeg/4035811730.jpg inflating: /content/kaggle/train_images_jpeg/4035835394.jpg inflating: /content/kaggle/train_images_jpeg/4036100881.jpg inflating: /content/kaggle/train_images_jpeg/4036105950.jpg inflating: /content/kaggle/train_images_jpeg/4036237704.jpg inflating: /content/kaggle/train_images_jpeg/4036527823.jpg inflating: /content/kaggle/train_images_jpeg/4036777489.jpg inflating: /content/kaggle/train_images_jpeg/403680473.jpg inflating: /content/kaggle/train_images_jpeg/4037044151.jpg inflating: /content/kaggle/train_images_jpeg/4037316351.jpg inflating: /content/kaggle/train_images_jpeg/4037341116.jpg inflating: /content/kaggle/train_images_jpeg/403735844.jpg inflating: /content/kaggle/train_images_jpeg/4037393509.jpg inflating: /content/kaggle/train_images_jpeg/4037509269.jpg inflating: /content/kaggle/train_images_jpeg/4037648479.jpg inflating: /content/kaggle/train_images_jpeg/4037735151.jpg inflating: /content/kaggle/train_images_jpeg/4037829966.jpg inflating: /content/kaggle/train_images_jpeg/4037870925.jpg inflating: /content/kaggle/train_images_jpeg/4038168935.jpg inflating: /content/kaggle/train_images_jpeg/4038284250.jpg inflating: /content/kaggle/train_images_jpeg/4038344014.jpg inflating: /content/kaggle/train_images_jpeg/4038568741.jpg inflating: /content/kaggle/train_images_jpeg/4038643368.jpg inflating: /content/kaggle/train_images_jpeg/403910552.jpg inflating: /content/kaggle/train_images_jpeg/4039408618.jpg inflating: /content/kaggle/train_images_jpeg/4039538172.jpg inflating: /content/kaggle/train_images_jpeg/403963359.jpg inflating: /content/kaggle/train_images_jpeg/4039669211.jpg inflating: /content/kaggle/train_images_jpeg/4040102956.jpg inflating: /content/kaggle/train_images_jpeg/4040156841.jpg inflating: /content/kaggle/train_images_jpeg/4040452479.jpg inflating: /content/kaggle/train_images_jpeg/404066691.jpg inflating: /content/kaggle/train_images_jpeg/4040861068.jpg inflating: /content/kaggle/train_images_jpeg/404115232.jpg inflating: /content/kaggle/train_images_jpeg/4041427827.jpg inflating: /content/kaggle/train_images_jpeg/4042037111.jpg inflating: /content/kaggle/train_images_jpeg/4042084056.jpg inflating: /content/kaggle/train_images_jpeg/4042496889.jpg inflating: /content/kaggle/train_images_jpeg/4042964772.jpg inflating: /content/kaggle/train_images_jpeg/4043076131.jpg inflating: /content/kaggle/train_images_jpeg/4043153792.jpg inflating: /content/kaggle/train_images_jpeg/4043211245.jpg inflating: /content/kaggle/train_images_jpeg/4043869250.jpg inflating: /content/kaggle/train_images_jpeg/4044063310.jpg inflating: /content/kaggle/train_images_jpeg/4044222946.jpg inflating: /content/kaggle/train_images_jpeg/4044329664.jpg inflating: /content/kaggle/train_images_jpeg/4044444164.jpg inflating: /content/kaggle/train_images_jpeg/4044550421.jpg inflating: /content/kaggle/train_images_jpeg/4044829046.jpg inflating: /content/kaggle/train_images_jpeg/4044997961.jpg inflating: /content/kaggle/train_images_jpeg/4045015795.jpg inflating: /content/kaggle/train_images_jpeg/4045136417.jpg inflating: /content/kaggle/train_images_jpeg/4045142021.jpg inflating: /content/kaggle/train_images_jpeg/4045262215.jpg inflating: /content/kaggle/train_images_jpeg/4045566880.jpg inflating: /content/kaggle/train_images_jpeg/4046068592.jpg inflating: /content/kaggle/train_images_jpeg/4046154865.jpg inflating: /content/kaggle/train_images_jpeg/4046217352.jpg inflating: /content/kaggle/train_images_jpeg/4046239145.jpg inflating: /content/kaggle/train_images_jpeg/4046290790.jpg inflating: /content/kaggle/train_images_jpeg/4046649903.jpg inflating: /content/kaggle/train_images_jpeg/4046661202.jpg inflating: /content/kaggle/train_images_jpeg/4046910331.jpg inflating: /content/kaggle/train_images_jpeg/4047167362.jpg inflating: /content/kaggle/train_images_jpeg/4047585545.jpg inflating: /content/kaggle/train_images_jpeg/4047998740.jpg inflating: /content/kaggle/train_images_jpeg/4048156987.jpg inflating: /content/kaggle/train_images_jpeg/4048354655.jpg inflating: /content/kaggle/train_images_jpeg/404837485.jpg inflating: /content/kaggle/train_images_jpeg/4048377608.jpg inflating: /content/kaggle/train_images_jpeg/4048486399.jpg inflating: /content/kaggle/train_images_jpeg/4048519217.jpg inflating: /content/kaggle/train_images_jpeg/404853146.jpg inflating: /content/kaggle/train_images_jpeg/4048650706.jpg inflating: /content/kaggle/train_images_jpeg/4048732920.jpg inflating: /content/kaggle/train_images_jpeg/404883957.jpg inflating: /content/kaggle/train_images_jpeg/4049425598.jpg inflating: /content/kaggle/train_images_jpeg/4049438636.jpg inflating: /content/kaggle/train_images_jpeg/4049710650.jpg inflating: /content/kaggle/train_images_jpeg/4049743612.jpg inflating: /content/kaggle/train_images_jpeg/4049843068.jpg inflating: /content/kaggle/train_images_jpeg/4050218395.jpg inflating: /content/kaggle/train_images_jpeg/4050620489.jpg inflating: /content/kaggle/train_images_jpeg/4050718274.jpg inflating: /content/kaggle/train_images_jpeg/4050750401.jpg inflating: /content/kaggle/train_images_jpeg/4050904412.jpg inflating: /content/kaggle/train_images_jpeg/4051003869.jpg inflating: /content/kaggle/train_images_jpeg/4051205328.jpg inflating: /content/kaggle/train_images_jpeg/4051216052.jpg inflating: /content/kaggle/train_images_jpeg/4051367500.jpg inflating: /content/kaggle/train_images_jpeg/4052176327.jpg inflating: /content/kaggle/train_images_jpeg/4052185416.jpg inflating: /content/kaggle/train_images_jpeg/4052298208.jpg inflating: /content/kaggle/train_images_jpeg/4052486714.jpg inflating: /content/kaggle/train_images_jpeg/4052560823.jpg inflating: /content/kaggle/train_images_jpeg/4052896742.jpg inflating: /content/kaggle/train_images_jpeg/4052938438.jpg inflating: /content/kaggle/train_images_jpeg/4053192372.jpg inflating: /content/kaggle/train_images_jpeg/405323789.jpg inflating: /content/kaggle/train_images_jpeg/4053510295.jpg inflating: /content/kaggle/train_images_jpeg/4053511214.jpg inflating: /content/kaggle/train_images_jpeg/4053727516.jpg inflating: /content/kaggle/train_images_jpeg/405406521.jpg inflating: /content/kaggle/train_images_jpeg/4054194563.jpg inflating: /content/kaggle/train_images_jpeg/4054246073.jpg inflating: /content/kaggle/train_images_jpeg/4054632356.jpg inflating: /content/kaggle/train_images_jpeg/4054666662.jpg inflating: /content/kaggle/train_images_jpeg/4054950426.jpg inflating: /content/kaggle/train_images_jpeg/405521670.jpg inflating: /content/kaggle/train_images_jpeg/4056070889.jpg inflating: /content/kaggle/train_images_jpeg/4056167943.jpg inflating: /content/kaggle/train_images_jpeg/4056643222.jpg inflating: /content/kaggle/train_images_jpeg/4056695916.jpg inflating: /content/kaggle/train_images_jpeg/4056735912.jpg inflating: /content/kaggle/train_images_jpeg/405683848.jpg inflating: /content/kaggle/train_images_jpeg/4056868775.jpg inflating: /content/kaggle/train_images_jpeg/405707941.jpg inflating: /content/kaggle/train_images_jpeg/4057082743.jpg inflating: /content/kaggle/train_images_jpeg/405720625.jpg inflating: /content/kaggle/train_images_jpeg/4057315544.jpg inflating: /content/kaggle/train_images_jpeg/4057359447.jpg inflating: /content/kaggle/train_images_jpeg/4057637286.jpg inflating: /content/kaggle/train_images_jpeg/4057824685.jpg inflating: /content/kaggle/train_images_jpeg/4057831785.jpg inflating: /content/kaggle/train_images_jpeg/405787230.jpg inflating: /content/kaggle/train_images_jpeg/4057892749.jpg inflating: /content/kaggle/train_images_jpeg/4058131705.jpg inflating: /content/kaggle/train_images_jpeg/4058472696.jpg inflating: /content/kaggle/train_images_jpeg/4058647833.jpg inflating: /content/kaggle/train_images_jpeg/4058785345.jpg inflating: /content/kaggle/train_images_jpeg/4059009090.jpg inflating: /content/kaggle/train_images_jpeg/4059169921.jpg inflating: /content/kaggle/train_images_jpeg/4059451569.jpg inflating: /content/kaggle/train_images_jpeg/4059603470.jpg inflating: /content/kaggle/train_images_jpeg/4059786587.jpg inflating: /content/kaggle/train_images_jpeg/40598577.jpg inflating: /content/kaggle/train_images_jpeg/4059993044.jpg inflating: /content/kaggle/train_images_jpeg/4060179434.jpg inflating: /content/kaggle/train_images_jpeg/406021295.jpg inflating: /content/kaggle/train_images_jpeg/4060265637.jpg inflating: /content/kaggle/train_images_jpeg/4060304349.jpg inflating: /content/kaggle/train_images_jpeg/4060346540.jpg inflating: /content/kaggle/train_images_jpeg/4060346776.jpg inflating: /content/kaggle/train_images_jpeg/4060450564.jpg inflating: /content/kaggle/train_images_jpeg/4060707407.jpg inflating: /content/kaggle/train_images_jpeg/4060802139.jpg inflating: /content/kaggle/train_images_jpeg/4060945579.jpg inflating: /content/kaggle/train_images_jpeg/4060977828.jpg inflating: /content/kaggle/train_images_jpeg/4060987360.jpg inflating: /content/kaggle/train_images_jpeg/4061563490.jpg inflating: /content/kaggle/train_images_jpeg/4061646597.jpg inflating: /content/kaggle/train_images_jpeg/4061670540.jpg inflating: /content/kaggle/train_images_jpeg/4061773647.jpg inflating: /content/kaggle/train_images_jpeg/4062190855.jpg inflating: /content/kaggle/train_images_jpeg/4062287710.jpg inflating: /content/kaggle/train_images_jpeg/4062558073.jpg inflating: /content/kaggle/train_images_jpeg/4062580069.jpg inflating: /content/kaggle/train_images_jpeg/4062600395.jpg inflating: /content/kaggle/train_images_jpeg/4062679165.jpg inflating: /content/kaggle/train_images_jpeg/4063056293.jpg inflating: /content/kaggle/train_images_jpeg/4063216337.jpg inflating: /content/kaggle/train_images_jpeg/4063439592.jpg inflating: /content/kaggle/train_images_jpeg/4063529695.jpg inflating: /content/kaggle/train_images_jpeg/4063867745.jpg inflating: /content/kaggle/train_images_jpeg/4063897288.jpg inflating: /content/kaggle/train_images_jpeg/4064049916.jpg inflating: /content/kaggle/train_images_jpeg/4064148193.jpg inflating: /content/kaggle/train_images_jpeg/4064395542.jpg inflating: /content/kaggle/train_images_jpeg/4064456640.jpg inflating: /content/kaggle/train_images_jpeg/4064710747.jpg inflating: /content/kaggle/train_images_jpeg/4064862687.jpg inflating: /content/kaggle/train_images_jpeg/4065072100.jpg inflating: /content/kaggle/train_images_jpeg/4065661474.jpg inflating: /content/kaggle/train_images_jpeg/4065788460.jpg inflating: /content/kaggle/train_images_jpeg/406648571.jpg inflating: /content/kaggle/train_images_jpeg/4066829254.jpg inflating: /content/kaggle/train_images_jpeg/4067721259.jpg inflating: /content/kaggle/train_images_jpeg/4067764811.jpg inflating: /content/kaggle/train_images_jpeg/4067939346.jpg inflating: /content/kaggle/train_images_jpeg/4068103331.jpg inflating: /content/kaggle/train_images_jpeg/4068398781.jpg inflating: /content/kaggle/train_images_jpeg/4068404919.jpg inflating: /content/kaggle/train_images_jpeg/4068888153.jpg inflating: /content/kaggle/train_images_jpeg/406899084.jpg inflating: /content/kaggle/train_images_jpeg/4069208912.jpg inflating: /content/kaggle/train_images_jpeg/40696329.jpg inflating: /content/kaggle/train_images_jpeg/4069710475.jpg inflating: /content/kaggle/train_images_jpeg/4069795713.jpg inflating: /content/kaggle/train_images_jpeg/4070109402.jpg inflating: /content/kaggle/train_images_jpeg/4070357374.jpg inflating: /content/kaggle/train_images_jpeg/407043133.jpg inflating: /content/kaggle/train_images_jpeg/4070631340.jpg inflating: /content/kaggle/train_images_jpeg/4071283452.jpg inflating: /content/kaggle/train_images_jpeg/4071582691.jpg inflating: /content/kaggle/train_images_jpeg/4071612105.jpg inflating: /content/kaggle/train_images_jpeg/4071618930.jpg inflating: /content/kaggle/train_images_jpeg/4071806046.jpg inflating: /content/kaggle/train_images_jpeg/4071831340.jpg inflating: /content/kaggle/train_images_jpeg/4072033035.jpg inflating: /content/kaggle/train_images_jpeg/4072193966.jpg inflating: /content/kaggle/train_images_jpeg/4072724949.jpg inflating: /content/kaggle/train_images_jpeg/4072800453.jpg inflating: /content/kaggle/train_images_jpeg/4072965036.jpg inflating: /content/kaggle/train_images_jpeg/4073303739.jpg inflating: /content/kaggle/train_images_jpeg/4073445610.jpg inflating: /content/kaggle/train_images_jpeg/4073523035.jpg inflating: /content/kaggle/train_images_jpeg/4073637668.jpg inflating: /content/kaggle/train_images_jpeg/4073688156.jpg inflating: /content/kaggle/train_images_jpeg/407426699.jpg inflating: /content/kaggle/train_images_jpeg/4074421218.jpg inflating: /content/kaggle/train_images_jpeg/4074718278.jpg inflating: /content/kaggle/train_images_jpeg/4074847622.jpg inflating: /content/kaggle/train_images_jpeg/4075237855.jpg inflating: /content/kaggle/train_images_jpeg/4075320498.jpg inflating: /content/kaggle/train_images_jpeg/4075412353.jpg inflating: /content/kaggle/train_images_jpeg/4075436598.jpg inflating: /content/kaggle/train_images_jpeg/4075854821.jpg inflating: /content/kaggle/train_images_jpeg/4076400056.jpg inflating: /content/kaggle/train_images_jpeg/4076735370.jpg inflating: /content/kaggle/train_images_jpeg/4076823454.jpg inflating: /content/kaggle/train_images_jpeg/4077138673.jpg inflating: /content/kaggle/train_images_jpeg/4077296935.jpg inflating: /content/kaggle/train_images_jpeg/4077418003.jpg inflating: /content/kaggle/train_images_jpeg/4077473431.jpg inflating: /content/kaggle/train_images_jpeg/4077715021.jpg inflating: /content/kaggle/train_images_jpeg/407801447.jpg inflating: /content/kaggle/train_images_jpeg/4078601864.jpg inflating: /content/kaggle/train_images_jpeg/4079242692.jpg inflating: /content/kaggle/train_images_jpeg/4079437688.jpg inflating: /content/kaggle/train_images_jpeg/40795473.jpg inflating: /content/kaggle/train_images_jpeg/4079941284.jpg inflating: /content/kaggle/train_images_jpeg/4080358205.jpg inflating: /content/kaggle/train_images_jpeg/4080368773.jpg inflating: /content/kaggle/train_images_jpeg/4080443505.jpg inflating: /content/kaggle/train_images_jpeg/408051106.jpg inflating: /content/kaggle/train_images_jpeg/4080548789.jpg inflating: /content/kaggle/train_images_jpeg/4080639262.jpg inflating: /content/kaggle/train_images_jpeg/4080805940.jpg inflating: /content/kaggle/train_images_jpeg/408081310.jpg inflating: /content/kaggle/train_images_jpeg/4080945037.jpg inflating: /content/kaggle/train_images_jpeg/4080972605.jpg inflating: /content/kaggle/train_images_jpeg/4081070840.jpg inflating: /content/kaggle/train_images_jpeg/4081152393.jpg inflating: /content/kaggle/train_images_jpeg/4081341161.jpg inflating: /content/kaggle/train_images_jpeg/408144068.jpg inflating: /content/kaggle/train_images_jpeg/4081836555.jpg inflating: /content/kaggle/train_images_jpeg/4082024517.jpg inflating: /content/kaggle/train_images_jpeg/4082420465.jpg inflating: /content/kaggle/train_images_jpeg/4082694328.jpg inflating: /content/kaggle/train_images_jpeg/4083175669.jpg inflating: /content/kaggle/train_images_jpeg/4083517071.jpg inflating: /content/kaggle/train_images_jpeg/408355226.jpg inflating: /content/kaggle/train_images_jpeg/4083589127.jpg inflating: /content/kaggle/train_images_jpeg/4083644073.jpg inflating: /content/kaggle/train_images_jpeg/4083711449.jpg inflating: /content/kaggle/train_images_jpeg/4083726805.jpg inflating: /content/kaggle/train_images_jpeg/4083737751.jpg inflating: /content/kaggle/train_images_jpeg/4083768019.jpg inflating: /content/kaggle/train_images_jpeg/4083830732.jpg inflating: /content/kaggle/train_images_jpeg/4083849263.jpg inflating: /content/kaggle/train_images_jpeg/4084071481.jpg inflating: /content/kaggle/train_images_jpeg/408414905.jpg inflating: /content/kaggle/train_images_jpeg/4084209845.jpg inflating: /content/kaggle/train_images_jpeg/4084470563.jpg inflating: /content/kaggle/train_images_jpeg/4084600785.jpg inflating: /content/kaggle/train_images_jpeg/4084618799.jpg inflating: /content/kaggle/train_images_jpeg/4084681419.jpg inflating: /content/kaggle/train_images_jpeg/4084684483.jpg inflating: /content/kaggle/train_images_jpeg/4085008913.jpg inflating: /content/kaggle/train_images_jpeg/4085837229.jpg inflating: /content/kaggle/train_images_jpeg/4086398319.jpg inflating: /content/kaggle/train_images_jpeg/4086759203.jpg inflating: /content/kaggle/train_images_jpeg/4086892324.jpg inflating: /content/kaggle/train_images_jpeg/408698793.jpg inflating: /content/kaggle/train_images_jpeg/4087247373.jpg inflating: /content/kaggle/train_images_jpeg/4087448407.jpg inflating: /content/kaggle/train_images_jpeg/4087523720.jpg inflating: /content/kaggle/train_images_jpeg/4087569473.jpg inflating: /content/kaggle/train_images_jpeg/4088071692.jpg inflating: /content/kaggle/train_images_jpeg/4088233611.jpg inflating: /content/kaggle/train_images_jpeg/4088249542.jpg inflating: /content/kaggle/train_images_jpeg/4088464085.jpg inflating: /content/kaggle/train_images_jpeg/4088770277.jpg inflating: /content/kaggle/train_images_jpeg/4089218356.jpg inflating: /content/kaggle/train_images_jpeg/4089587580.jpg inflating: /content/kaggle/train_images_jpeg/4089645917.jpg inflating: /content/kaggle/train_images_jpeg/4089661162.jpg inflating: /content/kaggle/train_images_jpeg/4089766352.jpg inflating: /content/kaggle/train_images_jpeg/4089978663.jpg inflating: /content/kaggle/train_images_jpeg/4090024132.jpg inflating: /content/kaggle/train_images_jpeg/4090496115.jpg inflating: /content/kaggle/train_images_jpeg/4090594888.jpg inflating: /content/kaggle/train_images_jpeg/4090740611.jpg inflating: /content/kaggle/train_images_jpeg/409126983.jpg inflating: /content/kaggle/train_images_jpeg/4091333216.jpg inflating: /content/kaggle/train_images_jpeg/4091446019.jpg inflating: /content/kaggle/train_images_jpeg/4091571824.jpg inflating: /content/kaggle/train_images_jpeg/4091618832.jpg inflating: /content/kaggle/train_images_jpeg/4091663475.jpg inflating: /content/kaggle/train_images_jpeg/4091748271.jpg inflating: /content/kaggle/train_images_jpeg/4092163942.jpg inflating: /content/kaggle/train_images_jpeg/4092211517.jpg inflating: /content/kaggle/train_images_jpeg/4092403597.jpg inflating: /content/kaggle/train_images_jpeg/4092496590.jpg inflating: /content/kaggle/train_images_jpeg/4092575256.jpg inflating: /content/kaggle/train_images_jpeg/4092623035.jpg inflating: /content/kaggle/train_images_jpeg/4092816326.jpg inflating: /content/kaggle/train_images_jpeg/409358481.jpg inflating: /content/kaggle/train_images_jpeg/4093904731.jpg inflating: /content/kaggle/train_images_jpeg/409443978.jpg inflating: /content/kaggle/train_images_jpeg/4094696582.jpg inflating: /content/kaggle/train_images_jpeg/409474529.jpg inflating: /content/kaggle/train_images_jpeg/4094778981.jpg inflating: /content/kaggle/train_images_jpeg/4095625090.jpg inflating: /content/kaggle/train_images_jpeg/4095719634.jpg inflating: /content/kaggle/train_images_jpeg/4095924422.jpg inflating: /content/kaggle/train_images_jpeg/40960944.jpg inflating: /content/kaggle/train_images_jpeg/4096334049.jpg inflating: /content/kaggle/train_images_jpeg/4096337072.jpg inflating: /content/kaggle/train_images_jpeg/4096675395.jpg inflating: /content/kaggle/train_images_jpeg/4096774182.jpg inflating: /content/kaggle/train_images_jpeg/4096966485.jpg inflating: /content/kaggle/train_images_jpeg/409723324.jpg inflating: /content/kaggle/train_images_jpeg/4097262272.jpg inflating: /content/kaggle/train_images_jpeg/4098206524.jpg inflating: /content/kaggle/train_images_jpeg/4098341362.jpg inflating: /content/kaggle/train_images_jpeg/4098383673.jpg inflating: /content/kaggle/train_images_jpeg/4098458638.jpg inflating: /content/kaggle/train_images_jpeg/4098473118.jpg inflating: /content/kaggle/train_images_jpeg/4098892948.jpg inflating: /content/kaggle/train_images_jpeg/4098951453.jpg inflating: /content/kaggle/train_images_jpeg/4099004871.jpg inflating: /content/kaggle/train_images_jpeg/409901220.jpg inflating: /content/kaggle/train_images_jpeg/4099957665.jpg inflating: /content/kaggle/train_images_jpeg/4100265698.jpg inflating: /content/kaggle/train_images_jpeg/4100436529.jpg inflating: /content/kaggle/train_images_jpeg/4100445719.jpg inflating: /content/kaggle/train_images_jpeg/4100598870.jpg inflating: /content/kaggle/train_images_jpeg/4100817891.jpg inflating: /content/kaggle/train_images_jpeg/4101194273.jpg inflating: /content/kaggle/train_images_jpeg/4101440622.jpg inflating: /content/kaggle/train_images_jpeg/4102169055.jpg inflating: /content/kaggle/train_images_jpeg/4102201143.jpg inflating: /content/kaggle/train_images_jpeg/4102423093.jpg inflating: /content/kaggle/train_images_jpeg/4102442898.jpg inflating: /content/kaggle/train_images_jpeg/410264599.jpg inflating: /content/kaggle/train_images_jpeg/4102729978.jpg inflating: /content/kaggle/train_images_jpeg/4102750438.jpg inflating: /content/kaggle/train_images_jpeg/4102833148.jpg inflating: /content/kaggle/train_images_jpeg/4102856795.jpg inflating: /content/kaggle/train_images_jpeg/4102864004.jpg inflating: /content/kaggle/train_images_jpeg/410291053.jpg inflating: /content/kaggle/train_images_jpeg/4102968836.jpg inflating: /content/kaggle/train_images_jpeg/4103177818.jpg inflating: /content/kaggle/train_images_jpeg/4103274356.jpg inflating: /content/kaggle/train_images_jpeg/4103428960.jpg inflating: /content/kaggle/train_images_jpeg/4103433070.jpg inflating: /content/kaggle/train_images_jpeg/410355412.jpg inflating: /content/kaggle/train_images_jpeg/4103654228.jpg inflating: /content/kaggle/train_images_jpeg/4103773008.jpg inflating: /content/kaggle/train_images_jpeg/4103867773.jpg inflating: /content/kaggle/train_images_jpeg/410414425.jpg inflating: /content/kaggle/train_images_jpeg/4104305706.jpg inflating: /content/kaggle/train_images_jpeg/4104529237.jpg inflating: /content/kaggle/train_images_jpeg/410476072.jpg inflating: /content/kaggle/train_images_jpeg/4105402505.jpg inflating: /content/kaggle/train_images_jpeg/410556634.jpg inflating: /content/kaggle/train_images_jpeg/4105662619.jpg inflating: /content/kaggle/train_images_jpeg/4106389238.jpg inflating: /content/kaggle/train_images_jpeg/4106517442.jpg inflating: /content/kaggle/train_images_jpeg/4106720331.jpg inflating: /content/kaggle/train_images_jpeg/4106981326.jpg inflating: /content/kaggle/train_images_jpeg/4107144871.jpg inflating: /content/kaggle/train_images_jpeg/4107344800.jpg inflating: /content/kaggle/train_images_jpeg/4107485206.jpg inflating: /content/kaggle/train_images_jpeg/4107585642.jpg inflating: /content/kaggle/train_images_jpeg/4107677180.jpg inflating: /content/kaggle/train_images_jpeg/410771727.jpg inflating: /content/kaggle/train_images_jpeg/4107720653.jpg inflating: /content/kaggle/train_images_jpeg/4108340746.jpg inflating: /content/kaggle/train_images_jpeg/4108393166.jpg inflating: /content/kaggle/train_images_jpeg/410863155.jpg inflating: /content/kaggle/train_images_jpeg/4108766138.jpg inflating: /content/kaggle/train_images_jpeg/410880003.jpg inflating: /content/kaggle/train_images_jpeg/4108844904.jpg inflating: /content/kaggle/train_images_jpeg/410890316.jpg inflating: /content/kaggle/train_images_jpeg/4109154808.jpg inflating: /content/kaggle/train_images_jpeg/4109287296.jpg inflating: /content/kaggle/train_images_jpeg/4109440762.jpg inflating: /content/kaggle/train_images_jpeg/4109731442.jpg inflating: /content/kaggle/train_images_jpeg/4109907649.jpg inflating: /content/kaggle/train_images_jpeg/4109950736.jpg inflating: /content/kaggle/train_images_jpeg/411062197.jpg inflating: /content/kaggle/train_images_jpeg/4110644267.jpg inflating: /content/kaggle/train_images_jpeg/4110692796.jpg inflating: /content/kaggle/train_images_jpeg/4110987286.jpg inflating: /content/kaggle/train_images_jpeg/4111161962.jpg inflating: /content/kaggle/train_images_jpeg/4111265654.jpg inflating: /content/kaggle/train_images_jpeg/4111579304.jpg inflating: /content/kaggle/train_images_jpeg/4111582298.jpg inflating: /content/kaggle/train_images_jpeg/4112011876.jpg inflating: /content/kaggle/train_images_jpeg/4112721304.jpg inflating: /content/kaggle/train_images_jpeg/4113552103.jpg inflating: /content/kaggle/train_images_jpeg/4113678968.jpg inflating: /content/kaggle/train_images_jpeg/4113877744.jpg inflating: /content/kaggle/train_images_jpeg/4113975266.jpg inflating: /content/kaggle/train_images_jpeg/4114035268.jpg inflating: /content/kaggle/train_images_jpeg/4114133209.jpg inflating: /content/kaggle/train_images_jpeg/4114136102.jpg inflating: /content/kaggle/train_images_jpeg/4114142974.jpg inflating: /content/kaggle/train_images_jpeg/4114788959.jpg inflating: /content/kaggle/train_images_jpeg/4115111691.jpg inflating: /content/kaggle/train_images_jpeg/4115342208.jpg inflating: /content/kaggle/train_images_jpeg/4115513115.jpg inflating: /content/kaggle/train_images_jpeg/4116136963.jpg inflating: /content/kaggle/train_images_jpeg/4116414929.jpg inflating: /content/kaggle/train_images_jpeg/4116600053.jpg inflating: /content/kaggle/train_images_jpeg/4116832468.jpg inflating: /content/kaggle/train_images_jpeg/4117129692.jpg inflating: /content/kaggle/train_images_jpeg/4117169270.jpg inflating: /content/kaggle/train_images_jpeg/4117178950.jpg inflating: /content/kaggle/train_images_jpeg/4117276057.jpg inflating: /content/kaggle/train_images_jpeg/4117539669.jpg inflating: /content/kaggle/train_images_jpeg/4117744697.jpg inflating: /content/kaggle/train_images_jpeg/4118106145.jpg inflating: /content/kaggle/train_images_jpeg/4118609397.jpg inflating: /content/kaggle/train_images_jpeg/4118708857.jpg inflating: /content/kaggle/train_images_jpeg/4118954331.jpg inflating: /content/kaggle/train_images_jpeg/4119019208.jpg inflating: /content/kaggle/train_images_jpeg/4119076771.jpg inflating: /content/kaggle/train_images_jpeg/4119270485.jpg inflating: /content/kaggle/train_images_jpeg/4119371139.jpg inflating: /content/kaggle/train_images_jpeg/4119520540.jpg inflating: /content/kaggle/train_images_jpeg/411955232.jpg inflating: /content/kaggle/train_images_jpeg/4120101206.jpg inflating: /content/kaggle/train_images_jpeg/4120352368.jpg inflating: /content/kaggle/train_images_jpeg/4120621808.jpg inflating: /content/kaggle/train_images_jpeg/4120735558.jpg inflating: /content/kaggle/train_images_jpeg/4120741228.jpg inflating: /content/kaggle/train_images_jpeg/4120845845.jpg inflating: /content/kaggle/train_images_jpeg/4120960004.jpg inflating: /content/kaggle/train_images_jpeg/4120963447.jpg inflating: /content/kaggle/train_images_jpeg/4121034467.jpg inflating: /content/kaggle/train_images_jpeg/4121046251.jpg inflating: /content/kaggle/train_images_jpeg/4121231239.jpg inflating: /content/kaggle/train_images_jpeg/4121252698.jpg inflating: /content/kaggle/train_images_jpeg/4121566836.jpg inflating: /content/kaggle/train_images_jpeg/4121873151.jpg inflating: /content/kaggle/train_images_jpeg/4122060123.jpg inflating: /content/kaggle/train_images_jpeg/4122391127.jpg inflating: /content/kaggle/train_images_jpeg/4122820167.jpg inflating: /content/kaggle/train_images_jpeg/4122834254.jpg inflating: /content/kaggle/train_images_jpeg/4122883459.jpg inflating: /content/kaggle/train_images_jpeg/4123166218.jpg inflating: /content/kaggle/train_images_jpeg/412355658.jpg inflating: /content/kaggle/train_images_jpeg/4123594620.jpg inflating: /content/kaggle/train_images_jpeg/4123656006.jpg inflating: /content/kaggle/train_images_jpeg/412390858.jpg inflating: /content/kaggle/train_images_jpeg/4123940124.jpg inflating: /content/kaggle/train_images_jpeg/4124205703.jpg inflating: /content/kaggle/train_images_jpeg/4124483418.jpg inflating: /content/kaggle/train_images_jpeg/4124651871.jpg inflating: /content/kaggle/train_images_jpeg/4125025056.jpg inflating: /content/kaggle/train_images_jpeg/4125173435.jpg inflating: /content/kaggle/train_images_jpeg/4125318611.jpg inflating: /content/kaggle/train_images_jpeg/4126019985.jpg inflating: /content/kaggle/train_images_jpeg/4126193524.jpg inflating: /content/kaggle/train_images_jpeg/4126692105.jpg inflating: /content/kaggle/train_images_jpeg/4126694609.jpg inflating: /content/kaggle/train_images_jpeg/4126983355.jpg inflating: /content/kaggle/train_images_jpeg/4127132722.jpg inflating: /content/kaggle/train_images_jpeg/4127310091.jpg inflating: /content/kaggle/train_images_jpeg/4127363037.jpg inflating: /content/kaggle/train_images_jpeg/4127820008.jpg inflating: /content/kaggle/train_images_jpeg/4128295692.jpg inflating: /content/kaggle/train_images_jpeg/4128588977.jpg inflating: /content/kaggle/train_images_jpeg/4128599330.jpg inflating: /content/kaggle/train_images_jpeg/4128762392.jpg inflating: /content/kaggle/train_images_jpeg/4129031753.jpg inflating: /content/kaggle/train_images_jpeg/4129857747.jpg inflating: /content/kaggle/train_images_jpeg/4130015523.jpg inflating: /content/kaggle/train_images_jpeg/4130177618.jpg inflating: /content/kaggle/train_images_jpeg/4130203885.jpg inflating: /content/kaggle/train_images_jpeg/4130341996.jpg inflating: /content/kaggle/train_images_jpeg/413051976.jpg inflating: /content/kaggle/train_images_jpeg/4130557422.jpg inflating: /content/kaggle/train_images_jpeg/4130607163.jpg inflating: /content/kaggle/train_images_jpeg/4130960215.jpg inflating: /content/kaggle/train_images_jpeg/4131012471.jpg inflating: /content/kaggle/train_images_jpeg/4131123441.jpg inflating: /content/kaggle/train_images_jpeg/413123167.jpg inflating: /content/kaggle/train_images_jpeg/4131260506.jpg inflating: /content/kaggle/train_images_jpeg/4131457294.jpg inflating: /content/kaggle/train_images_jpeg/4132347499.jpg inflating: /content/kaggle/train_images_jpeg/4132381633.jpg inflating: /content/kaggle/train_images_jpeg/4132540951.jpg inflating: /content/kaggle/train_images_jpeg/41327518.jpg inflating: /content/kaggle/train_images_jpeg/4133329748.jpg inflating: /content/kaggle/train_images_jpeg/4133546667.jpg inflating: /content/kaggle/train_images_jpeg/4133624649.jpg inflating: /content/kaggle/train_images_jpeg/4133641764.jpg inflating: /content/kaggle/train_images_jpeg/4133934896.jpg inflating: /content/kaggle/train_images_jpeg/4134468341.jpg inflating: /content/kaggle/train_images_jpeg/4134583704.jpg inflating: /content/kaggle/train_images_jpeg/4134617519.jpg inflating: /content/kaggle/train_images_jpeg/4134659314.jpg inflating: /content/kaggle/train_images_jpeg/4135070493.jpg inflating: /content/kaggle/train_images_jpeg/4135122260.jpg inflating: /content/kaggle/train_images_jpeg/4135213760.jpg inflating: /content/kaggle/train_images_jpeg/41357060.jpg inflating: /content/kaggle/train_images_jpeg/4135889078.jpg inflating: /content/kaggle/train_images_jpeg/4135953143.jpg inflating: /content/kaggle/train_images_jpeg/4136065352.jpg inflating: /content/kaggle/train_images_jpeg/4136137862.jpg inflating: /content/kaggle/train_images_jpeg/4136626919.jpg inflating: /content/kaggle/train_images_jpeg/4136833908.jpg inflating: /content/kaggle/train_images_jpeg/4136929395.jpg inflating: /content/kaggle/train_images_jpeg/4136963668.jpg inflating: /content/kaggle/train_images_jpeg/4137525990.jpg inflating: /content/kaggle/train_images_jpeg/4137700702.jpg inflating: /content/kaggle/train_images_jpeg/4137735351.jpg inflating: /content/kaggle/train_images_jpeg/4137790344.jpg inflating: /content/kaggle/train_images_jpeg/41378433.jpg inflating: /content/kaggle/train_images_jpeg/4137861888.jpg inflating: /content/kaggle/train_images_jpeg/4138481373.jpg inflating: /content/kaggle/train_images_jpeg/4138769021.jpg inflating: /content/kaggle/train_images_jpeg/4138957556.jpg inflating: /content/kaggle/train_images_jpeg/413906882.jpg inflating: /content/kaggle/train_images_jpeg/413912363.jpg inflating: /content/kaggle/train_images_jpeg/413920422.jpg inflating: /content/kaggle/train_images_jpeg/4139268407.jpg inflating: /content/kaggle/train_images_jpeg/4140015912.jpg inflating: /content/kaggle/train_images_jpeg/4140207015.jpg inflating: /content/kaggle/train_images_jpeg/4140381716.jpg inflating: /content/kaggle/train_images_jpeg/41404383.jpg inflating: /content/kaggle/train_images_jpeg/4140557884.jpg inflating: /content/kaggle/train_images_jpeg/4140868783.jpg inflating: /content/kaggle/train_images_jpeg/4141439714.jpg inflating: /content/kaggle/train_images_jpeg/4141594059.jpg inflating: /content/kaggle/train_images_jpeg/4141630996.jpg inflating: /content/kaggle/train_images_jpeg/414171398.jpg inflating: /content/kaggle/train_images_jpeg/4141721005.jpg inflating: /content/kaggle/train_images_jpeg/4141801603.jpg inflating: /content/kaggle/train_images_jpeg/4141898495.jpg inflating: /content/kaggle/train_images_jpeg/4142137699.jpg inflating: /content/kaggle/train_images_jpeg/4142266385.jpg inflating: /content/kaggle/train_images_jpeg/414229694.jpg inflating: /content/kaggle/train_images_jpeg/4142424247.jpg inflating: /content/kaggle/train_images_jpeg/4142462733.jpg inflating: /content/kaggle/train_images_jpeg/4142500062.jpg inflating: /content/kaggle/train_images_jpeg/4142699923.jpg inflating: /content/kaggle/train_images_jpeg/4142864830.jpg inflating: /content/kaggle/train_images_jpeg/4142973523.jpg inflating: /content/kaggle/train_images_jpeg/414320641.jpg inflating: /content/kaggle/train_images_jpeg/4143264926.jpg inflating: /content/kaggle/train_images_jpeg/4143432365.jpg inflating: /content/kaggle/train_images_jpeg/414363375.jpg inflating: /content/kaggle/train_images_jpeg/4143889304.jpg inflating: /content/kaggle/train_images_jpeg/4144053251.jpg inflating: /content/kaggle/train_images_jpeg/4144134428.jpg inflating: /content/kaggle/train_images_jpeg/4144248686.jpg inflating: /content/kaggle/train_images_jpeg/4144297967.jpg inflating: /content/kaggle/train_images_jpeg/4144515023.jpg inflating: /content/kaggle/train_images_jpeg/4144868742.jpg inflating: /content/kaggle/train_images_jpeg/4145051602.jpg inflating: /content/kaggle/train_images_jpeg/4145160333.jpg inflating: /content/kaggle/train_images_jpeg/4145471533.jpg inflating: /content/kaggle/train_images_jpeg/4145595052.jpg inflating: /content/kaggle/train_images_jpeg/4145634320.jpg inflating: /content/kaggle/train_images_jpeg/4145708140.jpg inflating: /content/kaggle/train_images_jpeg/4145817013.jpg inflating: /content/kaggle/train_images_jpeg/4145869943.jpg inflating: /content/kaggle/train_images_jpeg/414587008.jpg inflating: /content/kaggle/train_images_jpeg/4146003606.jpg inflating: /content/kaggle/train_images_jpeg/4146091086.jpg inflating: /content/kaggle/train_images_jpeg/4146139706.jpg inflating: /content/kaggle/train_images_jpeg/4146567066.jpg inflating: /content/kaggle/train_images_jpeg/4146743223.jpg inflating: /content/kaggle/train_images_jpeg/4146786218.jpg inflating: /content/kaggle/train_images_jpeg/4146910494.jpg inflating: /content/kaggle/train_images_jpeg/4147019519.jpg inflating: /content/kaggle/train_images_jpeg/4147446952.jpg inflating: /content/kaggle/train_images_jpeg/4147456412.jpg inflating: /content/kaggle/train_images_jpeg/4147565807.jpg inflating: /content/kaggle/train_images_jpeg/4147670856.jpg inflating: /content/kaggle/train_images_jpeg/4147695010.jpg inflating: /content/kaggle/train_images_jpeg/4147986236.jpg inflating: /content/kaggle/train_images_jpeg/4147991798.jpg inflating: /content/kaggle/train_images_jpeg/4148250496.jpg inflating: /content/kaggle/train_images_jpeg/4148306870.jpg inflating: /content/kaggle/train_images_jpeg/4148349078.jpg inflating: /content/kaggle/train_images_jpeg/4148374755.jpg inflating: /content/kaggle/train_images_jpeg/4148746293.jpg inflating: /content/kaggle/train_images_jpeg/4148790087.jpg inflating: /content/kaggle/train_images_jpeg/4148883240.jpg inflating: /content/kaggle/train_images_jpeg/4148970869.jpg inflating: /content/kaggle/train_images_jpeg/4148972063.jpg inflating: /content/kaggle/train_images_jpeg/4149005618.jpg inflating: /content/kaggle/train_images_jpeg/4149019611.jpg inflating: /content/kaggle/train_images_jpeg/4149032443.jpg inflating: /content/kaggle/train_images_jpeg/4149062810.jpg inflating: /content/kaggle/train_images_jpeg/4149113936.jpg inflating: /content/kaggle/train_images_jpeg/414926193.jpg inflating: /content/kaggle/train_images_jpeg/4149439273.jpg inflating: /content/kaggle/train_images_jpeg/4149442480.jpg inflating: /content/kaggle/train_images_jpeg/4149713311.jpg inflating: /content/kaggle/train_images_jpeg/4149713617.jpg inflating: /content/kaggle/train_images_jpeg/4149719008.jpg inflating: /content/kaggle/train_images_jpeg/4150333729.jpg inflating: /content/kaggle/train_images_jpeg/415073532.jpg inflating: /content/kaggle/train_images_jpeg/4150882054.jpg inflating: /content/kaggle/train_images_jpeg/415127331.jpg inflating: /content/kaggle/train_images_jpeg/4151340541.jpg inflating: /content/kaggle/train_images_jpeg/4151371064.jpg inflating: /content/kaggle/train_images_jpeg/4151450216.jpg inflating: /content/kaggle/train_images_jpeg/4151543321.jpg inflating: /content/kaggle/train_images_jpeg/4152070738.jpg inflating: /content/kaggle/train_images_jpeg/4152202956.jpg inflating: /content/kaggle/train_images_jpeg/4152354751.jpg inflating: /content/kaggle/train_images_jpeg/4152813711.jpg inflating: /content/kaggle/train_images_jpeg/4152969433.jpg inflating: /content/kaggle/train_images_jpeg/4153020952.jpg inflating: /content/kaggle/train_images_jpeg/4153074724.jpg inflating: /content/kaggle/train_images_jpeg/4153204450.jpg inflating: /content/kaggle/train_images_jpeg/4153230948.jpg inflating: /content/kaggle/train_images_jpeg/4153465850.jpg inflating: /content/kaggle/train_images_jpeg/4153590251.jpg inflating: /content/kaggle/train_images_jpeg/4153904483.jpg inflating: /content/kaggle/train_images_jpeg/4153984677.jpg inflating: /content/kaggle/train_images_jpeg/4154410133.jpg inflating: /content/kaggle/train_images_jpeg/4154571034.jpg inflating: /content/kaggle/train_images_jpeg/4154577723.jpg inflating: /content/kaggle/train_images_jpeg/415504810.jpg inflating: /content/kaggle/train_images_jpeg/4155077237.jpg inflating: /content/kaggle/train_images_jpeg/4155175371.jpg inflating: /content/kaggle/train_images_jpeg/4155925395.jpg inflating: /content/kaggle/train_images_jpeg/415601411.jpg inflating: /content/kaggle/train_images_jpeg/4156066738.jpg inflating: /content/kaggle/train_images_jpeg/4156138691.jpg inflating: /content/kaggle/train_images_jpeg/4156232104.jpg inflating: /content/kaggle/train_images_jpeg/4156293959.jpg inflating: /content/kaggle/train_images_jpeg/4156350478.jpg inflating: /content/kaggle/train_images_jpeg/4156518225.jpg inflating: /content/kaggle/train_images_jpeg/4156520758.jpg inflating: /content/kaggle/train_images_jpeg/4156615923.jpg inflating: /content/kaggle/train_images_jpeg/4156754691.jpg inflating: /content/kaggle/train_images_jpeg/4156956690.jpg inflating: /content/kaggle/train_images_jpeg/4157431857.jpg inflating: /content/kaggle/train_images_jpeg/4157582812.jpg inflating: /content/kaggle/train_images_jpeg/4157808497.jpg inflating: /content/kaggle/train_images_jpeg/4157923017.jpg inflating: /content/kaggle/train_images_jpeg/415809799.jpg inflating: /content/kaggle/train_images_jpeg/4158544304.jpg inflating: /content/kaggle/train_images_jpeg/4158622141.jpg inflating: /content/kaggle/train_images_jpeg/415901273.jpg inflating: /content/kaggle/train_images_jpeg/4159206286.jpg inflating: /content/kaggle/train_images_jpeg/4159264325.jpg inflating: /content/kaggle/train_images_jpeg/4159550001.jpg inflating: /content/kaggle/train_images_jpeg/4159698994.jpg inflating: /content/kaggle/train_images_jpeg/4159778976.jpg inflating: /content/kaggle/train_images_jpeg/4159967753.jpg inflating: /content/kaggle/train_images_jpeg/4160459669.jpg inflating: /content/kaggle/train_images_jpeg/416062259.jpg inflating: /content/kaggle/train_images_jpeg/41606397.jpg inflating: /content/kaggle/train_images_jpeg/4160794520.jpg inflating: /content/kaggle/train_images_jpeg/4161456491.jpg inflating: /content/kaggle/train_images_jpeg/4161605185.jpg inflating: /content/kaggle/train_images_jpeg/4161798280.jpg inflating: /content/kaggle/train_images_jpeg/4162062319.jpg inflating: /content/kaggle/train_images_jpeg/4162289609.jpg inflating: /content/kaggle/train_images_jpeg/4162370694.jpg inflating: /content/kaggle/train_images_jpeg/416260433.jpg inflating: /content/kaggle/train_images_jpeg/416297686.jpg inflating: /content/kaggle/train_images_jpeg/4163008109.jpg inflating: /content/kaggle/train_images_jpeg/416350020.jpg inflating: /content/kaggle/train_images_jpeg/4163993800.jpg inflating: /content/kaggle/train_images_jpeg/4164175215.jpg inflating: /content/kaggle/train_images_jpeg/4165205071.jpg inflating: /content/kaggle/train_images_jpeg/4165290692.jpg inflating: /content/kaggle/train_images_jpeg/4165383936.jpg inflating: /content/kaggle/train_images_jpeg/4165541319.jpg inflating: /content/kaggle/train_images_jpeg/4165714007.jpg inflating: /content/kaggle/train_images_jpeg/4165918071.jpg inflating: /content/kaggle/train_images_jpeg/4165966394.jpg inflating: /content/kaggle/train_images_jpeg/4166172905.jpg inflating: /content/kaggle/train_images_jpeg/4166257739.jpg inflating: /content/kaggle/train_images_jpeg/4166481906.jpg inflating: /content/kaggle/train_images_jpeg/4166602274.jpg inflating: /content/kaggle/train_images_jpeg/4166762.jpg inflating: /content/kaggle/train_images_jpeg/4166830303.jpg inflating: /content/kaggle/train_images_jpeg/4167054860.jpg inflating: /content/kaggle/train_images_jpeg/4167185859.jpg inflating: /content/kaggle/train_images_jpeg/4167436274.jpg inflating: /content/kaggle/train_images_jpeg/4167938990.jpg inflating: /content/kaggle/train_images_jpeg/4168229813.jpg inflating: /content/kaggle/train_images_jpeg/4168406521.jpg inflating: /content/kaggle/train_images_jpeg/4168463468.jpg inflating: /content/kaggle/train_images_jpeg/4168645136.jpg inflating: /content/kaggle/train_images_jpeg/416874515.jpg inflating: /content/kaggle/train_images_jpeg/4168975711.jpg inflating: /content/kaggle/train_images_jpeg/4169005982.jpg inflating: /content/kaggle/train_images_jpeg/4169071977.jpg inflating: /content/kaggle/train_images_jpeg/4169121477.jpg inflating: /content/kaggle/train_images_jpeg/4169133281.jpg inflating: /content/kaggle/train_images_jpeg/4169428267.jpg inflating: /content/kaggle/train_images_jpeg/4169508703.jpg inflating: /content/kaggle/train_images_jpeg/4169558391.jpg inflating: /content/kaggle/train_images_jpeg/4169577123.jpg inflating: /content/kaggle/train_images_jpeg/4169595131.jpg inflating: /content/kaggle/train_images_jpeg/4169606060.jpg inflating: /content/kaggle/train_images_jpeg/4169747301.jpg inflating: /content/kaggle/train_images_jpeg/4169857735.jpg inflating: /content/kaggle/train_images_jpeg/4169866451.jpg inflating: /content/kaggle/train_images_jpeg/4169894391.jpg inflating: /content/kaggle/train_images_jpeg/4170279280.jpg inflating: /content/kaggle/train_images_jpeg/417059979.jpg inflating: /content/kaggle/train_images_jpeg/4170665280.jpg inflating: /content/kaggle/train_images_jpeg/417083161.jpg inflating: /content/kaggle/train_images_jpeg/4170892667.jpg inflating: /content/kaggle/train_images_jpeg/4171069852.jpg inflating: /content/kaggle/train_images_jpeg/4171083166.jpg inflating: /content/kaggle/train_images_jpeg/4171117342.jpg inflating: /content/kaggle/train_images_jpeg/4171305326.jpg inflating: /content/kaggle/train_images_jpeg/4171327555.jpg inflating: /content/kaggle/train_images_jpeg/417135798.jpg inflating: /content/kaggle/train_images_jpeg/4171363767.jpg inflating: /content/kaggle/train_images_jpeg/4171404904.jpg inflating: /content/kaggle/train_images_jpeg/4171429454.jpg inflating: /content/kaggle/train_images_jpeg/4171468415.jpg inflating: /content/kaggle/train_images_jpeg/4171475132.jpg inflating: /content/kaggle/train_images_jpeg/4171842238.jpg inflating: /content/kaggle/train_images_jpeg/4172116548.jpg inflating: /content/kaggle/train_images_jpeg/4172181287.jpg inflating: /content/kaggle/train_images_jpeg/4172211620.jpg inflating: /content/kaggle/train_images_jpeg/4172221636.jpg inflating: /content/kaggle/train_images_jpeg/4172276292.jpg inflating: /content/kaggle/train_images_jpeg/4172444687.jpg inflating: /content/kaggle/train_images_jpeg/4172480899.jpg inflating: /content/kaggle/train_images_jpeg/4172651490.jpg inflating: /content/kaggle/train_images_jpeg/4172721678.jpg inflating: /content/kaggle/train_images_jpeg/4173133461.jpg inflating: /content/kaggle/train_images_jpeg/4173419470.jpg inflating: /content/kaggle/train_images_jpeg/4173430293.jpg inflating: /content/kaggle/train_images_jpeg/4173509764.jpg inflating: /content/kaggle/train_images_jpeg/4173521076.jpg inflating: /content/kaggle/train_images_jpeg/4173578063.jpg inflating: /content/kaggle/train_images_jpeg/4173585663.jpg inflating: /content/kaggle/train_images_jpeg/4173729579.jpg inflating: /content/kaggle/train_images_jpeg/4173960352.jpg inflating: /content/kaggle/train_images_jpeg/4174034867.jpg inflating: /content/kaggle/train_images_jpeg/4174350664.jpg inflating: /content/kaggle/train_images_jpeg/4174883420.jpg inflating: /content/kaggle/train_images_jpeg/4175172679.jpg inflating: /content/kaggle/train_images_jpeg/4175214629.jpg inflating: /content/kaggle/train_images_jpeg/4175366577.jpg inflating: /content/kaggle/train_images_jpeg/4175374809.jpg inflating: /content/kaggle/train_images_jpeg/4175405033.jpg inflating: /content/kaggle/train_images_jpeg/4175483651.jpg inflating: /content/kaggle/train_images_jpeg/4175723076.jpg inflating: /content/kaggle/train_images_jpeg/4175864678.jpg inflating: /content/kaggle/train_images_jpeg/4175885310.jpg inflating: /content/kaggle/train_images_jpeg/4175951103.jpg inflating: /content/kaggle/train_images_jpeg/4175967929.jpg inflating: /content/kaggle/train_images_jpeg/4176018336.jpg inflating: /content/kaggle/train_images_jpeg/4176445921.jpg inflating: /content/kaggle/train_images_jpeg/4176553783.jpg inflating: /content/kaggle/train_images_jpeg/4176585618.jpg inflating: /content/kaggle/train_images_jpeg/4176726961.jpg inflating: /content/kaggle/train_images_jpeg/4177084912.jpg inflating: /content/kaggle/train_images_jpeg/4177148396.jpg inflating: /content/kaggle/train_images_jpeg/4177200098.jpg inflating: /content/kaggle/train_images_jpeg/417726822.jpg inflating: /content/kaggle/train_images_jpeg/4177391802.jpg inflating: /content/kaggle/train_images_jpeg/4177392079.jpg inflating: /content/kaggle/train_images_jpeg/4177483982.jpg inflating: /content/kaggle/train_images_jpeg/4177519290.jpg inflating: /content/kaggle/train_images_jpeg/4177958446.jpg inflating: /content/kaggle/train_images_jpeg/4178125933.jpg inflating: /content/kaggle/train_images_jpeg/4178245302.jpg inflating: /content/kaggle/train_images_jpeg/417840962.jpg inflating: /content/kaggle/train_images_jpeg/4178594597.jpg inflating: /content/kaggle/train_images_jpeg/4179147529.jpg inflating: /content/kaggle/train_images_jpeg/4179303014.jpg inflating: /content/kaggle/train_images_jpeg/4179442683.jpg inflating: /content/kaggle/train_images_jpeg/4179444530.jpg inflating: /content/kaggle/train_images_jpeg/4179630738.jpg inflating: /content/kaggle/train_images_jpeg/4179735226.jpg inflating: /content/kaggle/train_images_jpeg/4180192243.jpg inflating: /content/kaggle/train_images_jpeg/4180586995.jpg inflating: /content/kaggle/train_images_jpeg/4180626813.jpg inflating: /content/kaggle/train_images_jpeg/4181129288.jpg inflating: /content/kaggle/train_images_jpeg/4181351157.jpg inflating: /content/kaggle/train_images_jpeg/4181746841.jpg inflating: /content/kaggle/train_images_jpeg/4182063113.jpg inflating: /content/kaggle/train_images_jpeg/4182229432.jpg inflating: /content/kaggle/train_images_jpeg/4182496583.jpg inflating: /content/kaggle/train_images_jpeg/4182510325.jpg inflating: /content/kaggle/train_images_jpeg/418261395.jpg inflating: /content/kaggle/train_images_jpeg/4182626844.jpg inflating: /content/kaggle/train_images_jpeg/4182634633.jpg inflating: /content/kaggle/train_images_jpeg/4182646483.jpg inflating: /content/kaggle/train_images_jpeg/4182745953.jpg inflating: /content/kaggle/train_images_jpeg/4182889279.jpg inflating: /content/kaggle/train_images_jpeg/4182898688.jpg inflating: /content/kaggle/train_images_jpeg/4182987199.jpg inflating: /content/kaggle/train_images_jpeg/4183077936.jpg inflating: /content/kaggle/train_images_jpeg/4183078751.jpg inflating: /content/kaggle/train_images_jpeg/4183084622.jpg inflating: /content/kaggle/train_images_jpeg/4183253873.jpg inflating: /content/kaggle/train_images_jpeg/4183312671.jpg inflating: /content/kaggle/train_images_jpeg/418334255.jpg inflating: /content/kaggle/train_images_jpeg/4183847559.jpg inflating: /content/kaggle/train_images_jpeg/4183866544.jpg inflating: /content/kaggle/train_images_jpeg/4183926297.jpg inflating: /content/kaggle/train_images_jpeg/4184062100.jpg inflating: /content/kaggle/train_images_jpeg/4184461961.jpg inflating: /content/kaggle/train_images_jpeg/4184485788.jpg inflating: /content/kaggle/train_images_jpeg/4184902338.jpg inflating: /content/kaggle/train_images_jpeg/4185096944.jpg inflating: /content/kaggle/train_images_jpeg/4185505558.jpg inflating: /content/kaggle/train_images_jpeg/4186416539.jpg inflating: /content/kaggle/train_images_jpeg/4186436193.jpg inflating: /content/kaggle/train_images_jpeg/4186528669.jpg inflating: /content/kaggle/train_images_jpeg/4186603766.jpg inflating: /content/kaggle/train_images_jpeg/4186633206.jpg inflating: /content/kaggle/train_images_jpeg/4186640212.jpg inflating: /content/kaggle/train_images_jpeg/4186680625.jpg inflating: /content/kaggle/train_images_jpeg/418673724.jpg inflating: /content/kaggle/train_images_jpeg/4186901068.jpg inflating: /content/kaggle/train_images_jpeg/4186913818.jpg inflating: /content/kaggle/train_images_jpeg/4187343844.jpg inflating: /content/kaggle/train_images_jpeg/4188111262.jpg inflating: /content/kaggle/train_images_jpeg/4188170872.jpg inflating: /content/kaggle/train_images_jpeg/4188219605.jpg inflating: /content/kaggle/train_images_jpeg/4188255023.jpg inflating: /content/kaggle/train_images_jpeg/4188579631.jpg inflating: /content/kaggle/train_images_jpeg/418911240.jpg inflating: /content/kaggle/train_images_jpeg/4189307730.jpg inflating: /content/kaggle/train_images_jpeg/4189434450.jpg inflating: /content/kaggle/train_images_jpeg/4189463484.jpg inflating: /content/kaggle/train_images_jpeg/4189636250.jpg inflating: /content/kaggle/train_images_jpeg/4189821383.jpg inflating: /content/kaggle/train_images_jpeg/4190467935.jpg inflating: /content/kaggle/train_images_jpeg/4191284579.jpg inflating: /content/kaggle/train_images_jpeg/4191409437.jpg inflating: /content/kaggle/train_images_jpeg/4191778272.jpg inflating: /content/kaggle/train_images_jpeg/4191979346.jpg inflating: /content/kaggle/train_images_jpeg/4192173547.jpg inflating: /content/kaggle/train_images_jpeg/4192202317.jpg inflating: /content/kaggle/train_images_jpeg/4192503187.jpg inflating: /content/kaggle/train_images_jpeg/4192665082.jpg inflating: /content/kaggle/train_images_jpeg/4192717491.jpg inflating: /content/kaggle/train_images_jpeg/4192792225.jpg inflating: /content/kaggle/train_images_jpeg/4192933342.jpg inflating: /content/kaggle/train_images_jpeg/4193039618.jpg inflating: /content/kaggle/train_images_jpeg/4193527713.jpg inflating: /content/kaggle/train_images_jpeg/4193627922.jpg inflating: /content/kaggle/train_images_jpeg/4193821272.jpg inflating: /content/kaggle/train_images_jpeg/4193954616.jpg inflating: /content/kaggle/train_images_jpeg/4194395982.jpg inflating: /content/kaggle/train_images_jpeg/4194767619.jpg inflating: /content/kaggle/train_images_jpeg/4194771487.jpg inflating: /content/kaggle/train_images_jpeg/4194820497.jpg inflating: /content/kaggle/train_images_jpeg/4195040582.jpg inflating: /content/kaggle/train_images_jpeg/4195112641.jpg inflating: /content/kaggle/train_images_jpeg/4195244088.jpg inflating: /content/kaggle/train_images_jpeg/4195466993.jpg inflating: /content/kaggle/train_images_jpeg/4195586692.jpg inflating: /content/kaggle/train_images_jpeg/4195598174.jpg inflating: /content/kaggle/train_images_jpeg/4196010375.jpg inflating: /content/kaggle/train_images_jpeg/4196311493.jpg inflating: /content/kaggle/train_images_jpeg/4196557563.jpg inflating: /content/kaggle/train_images_jpeg/4196856438.jpg inflating: /content/kaggle/train_images_jpeg/4196928486.jpg inflating: /content/kaggle/train_images_jpeg/4196953609.jpg inflating: /content/kaggle/train_images_jpeg/419708385.jpg inflating: /content/kaggle/train_images_jpeg/4197342074.jpg inflating: /content/kaggle/train_images_jpeg/4197537350.jpg inflating: /content/kaggle/train_images_jpeg/4197541916.jpg inflating: /content/kaggle/train_images_jpeg/4197748508.jpg inflating: /content/kaggle/train_images_jpeg/4197924159.jpg inflating: /content/kaggle/train_images_jpeg/4198242632.jpg inflating: /content/kaggle/train_images_jpeg/4198510978.jpg inflating: /content/kaggle/train_images_jpeg/4198716327.jpg inflating: /content/kaggle/train_images_jpeg/4199179186.jpg inflating: /content/kaggle/train_images_jpeg/4199257644.jpg inflating: /content/kaggle/train_images_jpeg/4199323027.jpg inflating: /content/kaggle/train_images_jpeg/4199546662.jpg inflating: /content/kaggle/train_images_jpeg/4199549961.jpg inflating: /content/kaggle/train_images_jpeg/419974195.jpg inflating: /content/kaggle/train_images_jpeg/4200117481.jpg inflating: /content/kaggle/train_images_jpeg/4200249723.jpg inflating: /content/kaggle/train_images_jpeg/4200538063.jpg inflating: /content/kaggle/train_images_jpeg/4200872179.jpg inflating: /content/kaggle/train_images_jpeg/4200880538.jpg inflating: /content/kaggle/train_images_jpeg/4200969502.jpg inflating: /content/kaggle/train_images_jpeg/4201017527.jpg inflating: /content/kaggle/train_images_jpeg/4201154907.jpg inflating: /content/kaggle/train_images_jpeg/4201636225.jpg inflating: /content/kaggle/train_images_jpeg/4201637914.jpg inflating: /content/kaggle/train_images_jpeg/4201965605.jpg inflating: /content/kaggle/train_images_jpeg/4202098451.jpg inflating: /content/kaggle/train_images_jpeg/4202232655.jpg inflating: /content/kaggle/train_images_jpeg/4202339262.jpg inflating: /content/kaggle/train_images_jpeg/4202436858.jpg inflating: /content/kaggle/train_images_jpeg/4202441426.jpg inflating: /content/kaggle/train_images_jpeg/42026505.jpg inflating: /content/kaggle/train_images_jpeg/4203360884.jpg inflating: /content/kaggle/train_images_jpeg/4203623611.jpg inflating: /content/kaggle/train_images_jpeg/4203754534.jpg inflating: /content/kaggle/train_images_jpeg/4203886176.jpg inflating: /content/kaggle/train_images_jpeg/4203927586.jpg inflating: /content/kaggle/train_images_jpeg/4204029505.jpg inflating: /content/kaggle/train_images_jpeg/4204052226.jpg inflating: /content/kaggle/train_images_jpeg/4204435196.jpg inflating: /content/kaggle/train_images_jpeg/4204628073.jpg inflating: /content/kaggle/train_images_jpeg/4204930835.jpg inflating: /content/kaggle/train_images_jpeg/420499628.jpg inflating: /content/kaggle/train_images_jpeg/4205231348.jpg inflating: /content/kaggle/train_images_jpeg/4205256960.jpg inflating: /content/kaggle/train_images_jpeg/4205544766.jpg inflating: /content/kaggle/train_images_jpeg/4205804371.jpg inflating: /content/kaggle/train_images_jpeg/4205935925.jpg inflating: /content/kaggle/train_images_jpeg/4206104015.jpg inflating: /content/kaggle/train_images_jpeg/4206128911.jpg inflating: /content/kaggle/train_images_jpeg/4206288054.jpg inflating: /content/kaggle/train_images_jpeg/420653130.jpg inflating: /content/kaggle/train_images_jpeg/4206587190.jpg inflating: /content/kaggle/train_images_jpeg/4206778919.jpg inflating: /content/kaggle/train_images_jpeg/4206947168.jpg inflating: /content/kaggle/train_images_jpeg/4206993197.jpg inflating: /content/kaggle/train_images_jpeg/4207293267.jpg inflating: /content/kaggle/train_images_jpeg/420749600.jpg inflating: /content/kaggle/train_images_jpeg/4207507257.jpg inflating: /content/kaggle/train_images_jpeg/4207531803.jpg inflating: /content/kaggle/train_images_jpeg/4207597104.jpg inflating: /content/kaggle/train_images_jpeg/4207597254.jpg inflating: /content/kaggle/train_images_jpeg/4207857980.jpg inflating: /content/kaggle/train_images_jpeg/4208333966.jpg inflating: /content/kaggle/train_images_jpeg/4208451715.jpg inflating: /content/kaggle/train_images_jpeg/4208508755.jpg inflating: /content/kaggle/train_images_jpeg/4208715814.jpg inflating: /content/kaggle/train_images_jpeg/4208734427.jpg inflating: /content/kaggle/train_images_jpeg/4209027570.jpg inflating: /content/kaggle/train_images_jpeg/42091153.jpg inflating: /content/kaggle/train_images_jpeg/4209232605.jpg inflating: /content/kaggle/train_images_jpeg/4209575445.jpg inflating: /content/kaggle/train_images_jpeg/4209628430.jpg inflating: /content/kaggle/train_images_jpeg/4209668687.jpg inflating: /content/kaggle/train_images_jpeg/4209715863.jpg inflating: /content/kaggle/train_images_jpeg/4209772803.jpg inflating: /content/kaggle/train_images_jpeg/420979598.jpg inflating: /content/kaggle/train_images_jpeg/4210199349.jpg inflating: /content/kaggle/train_images_jpeg/4210262767.jpg inflating: /content/kaggle/train_images_jpeg/4210272961.jpg inflating: /content/kaggle/train_images_jpeg/4210295166.jpg inflating: /content/kaggle/train_images_jpeg/421035788.jpg inflating: /content/kaggle/train_images_jpeg/4210428578.jpg inflating: /content/kaggle/train_images_jpeg/4210616939.jpg inflating: /content/kaggle/train_images_jpeg/421071700.jpg inflating: /content/kaggle/train_images_jpeg/4210825389.jpg inflating: /content/kaggle/train_images_jpeg/4210831538.jpg inflating: /content/kaggle/train_images_jpeg/4210844423.jpg inflating: /content/kaggle/train_images_jpeg/4210867586.jpg inflating: /content/kaggle/train_images_jpeg/4210957612.jpg inflating: /content/kaggle/train_images_jpeg/4211138249.jpg inflating: /content/kaggle/train_images_jpeg/4211450087.jpg inflating: /content/kaggle/train_images_jpeg/4211652237.jpg inflating: /content/kaggle/train_images_jpeg/4211828884.jpg inflating: /content/kaggle/train_images_jpeg/4211960085.jpg inflating: /content/kaggle/train_images_jpeg/4212247949.jpg inflating: /content/kaggle/train_images_jpeg/4212381540.jpg inflating: /content/kaggle/train_images_jpeg/4212642108.jpg inflating: /content/kaggle/train_images_jpeg/4213060082.jpg inflating: /content/kaggle/train_images_jpeg/4213221330.jpg inflating: /content/kaggle/train_images_jpeg/4213312070.jpg inflating: /content/kaggle/train_images_jpeg/4213411330.jpg inflating: /content/kaggle/train_images_jpeg/4213471018.jpg inflating: /content/kaggle/train_images_jpeg/4213479544.jpg inflating: /content/kaggle/train_images_jpeg/4213496821.jpg inflating: /content/kaggle/train_images_jpeg/4213525466.jpg inflating: /content/kaggle/train_images_jpeg/4213617078.jpg inflating: /content/kaggle/train_images_jpeg/4213734588.jpg inflating: /content/kaggle/train_images_jpeg/4213863678.jpg inflating: /content/kaggle/train_images_jpeg/4214126617.jpg inflating: /content/kaggle/train_images_jpeg/4214231918.jpg inflating: /content/kaggle/train_images_jpeg/4214625845.jpg inflating: /content/kaggle/train_images_jpeg/4214804223.jpg inflating: /content/kaggle/train_images_jpeg/42150813.jpg inflating: /content/kaggle/train_images_jpeg/4215244581.jpg inflating: /content/kaggle/train_images_jpeg/4215295571.jpg inflating: /content/kaggle/train_images_jpeg/4215365960.jpg inflating: /content/kaggle/train_images_jpeg/4215422065.jpg inflating: /content/kaggle/train_images_jpeg/4215590346.jpg inflating: /content/kaggle/train_images_jpeg/4215606048.jpg inflating: /content/kaggle/train_images_jpeg/4215655540.jpg inflating: /content/kaggle/train_images_jpeg/4215887355.jpg inflating: /content/kaggle/train_images_jpeg/4216163881.jpg inflating: /content/kaggle/train_images_jpeg/4216173282.jpg inflating: /content/kaggle/train_images_jpeg/421638370.jpg inflating: /content/kaggle/train_images_jpeg/4217597340.jpg inflating: /content/kaggle/train_images_jpeg/4217871378.jpg inflating: /content/kaggle/train_images_jpeg/4218379987.jpg inflating: /content/kaggle/train_images_jpeg/4218669271.jpg inflating: /content/kaggle/train_images_jpeg/4219325672.jpg inflating: /content/kaggle/train_images_jpeg/4219389723.jpg inflating: /content/kaggle/train_images_jpeg/4219528096.jpg inflating: /content/kaggle/train_images_jpeg/4219779875.jpg inflating: /content/kaggle/train_images_jpeg/4220427697.jpg inflating: /content/kaggle/train_images_jpeg/4220488715.jpg inflating: /content/kaggle/train_images_jpeg/4220602838.jpg inflating: /content/kaggle/train_images_jpeg/4220858043.jpg inflating: /content/kaggle/train_images_jpeg/4221079095.jpg inflating: /content/kaggle/train_images_jpeg/4221104214.jpg inflating: /content/kaggle/train_images_jpeg/4221172112.jpg inflating: /content/kaggle/train_images_jpeg/4221245537.jpg inflating: /content/kaggle/train_images_jpeg/4221337280.jpg inflating: /content/kaggle/train_images_jpeg/4221848010.jpg inflating: /content/kaggle/train_images_jpeg/4221898488.jpg inflating: /content/kaggle/train_images_jpeg/4221918564.jpg inflating: /content/kaggle/train_images_jpeg/4221969635.jpg inflating: /content/kaggle/train_images_jpeg/4222040143.jpg inflating: /content/kaggle/train_images_jpeg/4222242.jpg inflating: /content/kaggle/train_images_jpeg/4222358832.jpg inflating: /content/kaggle/train_images_jpeg/4222447484.jpg inflating: /content/kaggle/train_images_jpeg/4222515459.jpg inflating: /content/kaggle/train_images_jpeg/4222869193.jpg inflating: /content/kaggle/train_images_jpeg/4222895181.jpg inflating: /content/kaggle/train_images_jpeg/4222936048.jpg inflating: /content/kaggle/train_images_jpeg/4222984856.jpg inflating: /content/kaggle/train_images_jpeg/4223217189.jpg inflating: /content/kaggle/train_images_jpeg/4223297410.jpg inflating: /content/kaggle/train_images_jpeg/4223515593.jpg inflating: /content/kaggle/train_images_jpeg/4223598705.jpg inflating: /content/kaggle/train_images_jpeg/4223665750.jpg inflating: /content/kaggle/train_images_jpeg/4223800974.jpg inflating: /content/kaggle/train_images_jpeg/4224082964.jpg inflating: /content/kaggle/train_images_jpeg/4224255718.jpg inflating: /content/kaggle/train_images_jpeg/4224427146.jpg inflating: /content/kaggle/train_images_jpeg/4224453437.jpg inflating: /content/kaggle/train_images_jpeg/4224616597.jpg inflating: /content/kaggle/train_images_jpeg/4224712607.jpg inflating: /content/kaggle/train_images_jpeg/4224873954.jpg inflating: /content/kaggle/train_images_jpeg/4225063396.jpg inflating: /content/kaggle/train_images_jpeg/4225133358.jpg inflating: /content/kaggle/train_images_jpeg/4225259568.jpg inflating: /content/kaggle/train_images_jpeg/4225506393.jpg inflating: /content/kaggle/train_images_jpeg/4225572023.jpg inflating: /content/kaggle/train_images_jpeg/422596234.jpg inflating: /content/kaggle/train_images_jpeg/4226051437.jpg inflating: /content/kaggle/train_images_jpeg/4226241214.jpg inflating: /content/kaggle/train_images_jpeg/422632532.jpg inflating: /content/kaggle/train_images_jpeg/4226417214.jpg inflating: /content/kaggle/train_images_jpeg/422690051.jpg inflating: /content/kaggle/train_images_jpeg/4227424714.jpg inflating: /content/kaggle/train_images_jpeg/422752547.jpg inflating: /content/kaggle/train_images_jpeg/4227613635.jpg inflating: /content/kaggle/train_images_jpeg/4227756572.jpg inflating: /content/kaggle/train_images_jpeg/4228006175.jpg inflating: /content/kaggle/train_images_jpeg/4228467711.jpg inflating: /content/kaggle/train_images_jpeg/4228501282.jpg inflating: /content/kaggle/train_images_jpeg/4228517372.jpg inflating: /content/kaggle/train_images_jpeg/4228621377.jpg inflating: /content/kaggle/train_images_jpeg/4228732666.jpg inflating: /content/kaggle/train_images_jpeg/4228743901.jpg inflating: /content/kaggle/train_images_jpeg/4228769241.jpg inflating: /content/kaggle/train_images_jpeg/42290082.jpg inflating: /content/kaggle/train_images_jpeg/4229009634.jpg inflating: /content/kaggle/train_images_jpeg/4230253873.jpg inflating: /content/kaggle/train_images_jpeg/4230502495.jpg inflating: /content/kaggle/train_images_jpeg/4230605387.jpg inflating: /content/kaggle/train_images_jpeg/4230970262.jpg inflating: /content/kaggle/train_images_jpeg/423114651.jpg inflating: /content/kaggle/train_images_jpeg/4231168569.jpg inflating: /content/kaggle/train_images_jpeg/4231979613.jpg inflating: /content/kaggle/train_images_jpeg/4232363601.jpg inflating: /content/kaggle/train_images_jpeg/423248664.jpg inflating: /content/kaggle/train_images_jpeg/4232654944.jpg inflating: /content/kaggle/train_images_jpeg/423272178.jpg inflating: /content/kaggle/train_images_jpeg/423288187.jpg inflating: /content/kaggle/train_images_jpeg/4232966905.jpg inflating: /content/kaggle/train_images_jpeg/4233314988.jpg inflating: /content/kaggle/train_images_jpeg/4233592574.jpg inflating: /content/kaggle/train_images_jpeg/4233694735.jpg inflating: /content/kaggle/train_images_jpeg/423373963.jpg inflating: /content/kaggle/train_images_jpeg/4233818337.jpg inflating: /content/kaggle/train_images_jpeg/4233882902.jpg inflating: /content/kaggle/train_images_jpeg/4234424797.jpg inflating: /content/kaggle/train_images_jpeg/4234497061.jpg inflating: /content/kaggle/train_images_jpeg/4234605337.jpg inflating: /content/kaggle/train_images_jpeg/4234858756.jpg inflating: /content/kaggle/train_images_jpeg/4235208276.jpg inflating: /content/kaggle/train_images_jpeg/4235664377.jpg inflating: /content/kaggle/train_images_jpeg/4235958385.jpg inflating: /content/kaggle/train_images_jpeg/4236479992.jpg inflating: /content/kaggle/train_images_jpeg/4237027288.jpg inflating: /content/kaggle/train_images_jpeg/4237100244.jpg inflating: /content/kaggle/train_images_jpeg/4237476054.jpg inflating: /content/kaggle/train_images_jpeg/4237501920.jpg inflating: /content/kaggle/train_images_jpeg/423771558.jpg inflating: /content/kaggle/train_images_jpeg/4237851473.jpg inflating: /content/kaggle/train_images_jpeg/4239036041.jpg inflating: /content/kaggle/train_images_jpeg/4239047736.jpg inflating: /content/kaggle/train_images_jpeg/4239074071.jpg inflating: /content/kaggle/train_images_jpeg/423907510.jpg inflating: /content/kaggle/train_images_jpeg/4239164688.jpg inflating: /content/kaggle/train_images_jpeg/4239180232.jpg inflating: /content/kaggle/train_images_jpeg/4239262882.jpg inflating: /content/kaggle/train_images_jpeg/4240036351.jpg inflating: /content/kaggle/train_images_jpeg/4240281500.jpg inflating: /content/kaggle/train_images_jpeg/4240296796.jpg inflating: /content/kaggle/train_images_jpeg/424041602.jpg inflating: /content/kaggle/train_images_jpeg/4240447884.jpg inflating: /content/kaggle/train_images_jpeg/4241010942.jpg inflating: /content/kaggle/train_images_jpeg/4241128101.jpg inflating: /content/kaggle/train_images_jpeg/424168659.jpg inflating: /content/kaggle/train_images_jpeg/4241697639.jpg inflating: /content/kaggle/train_images_jpeg/4241803356.jpg inflating: /content/kaggle/train_images_jpeg/4242194730.jpg inflating: /content/kaggle/train_images_jpeg/4242296013.jpg inflating: /content/kaggle/train_images_jpeg/4242325516.jpg inflating: /content/kaggle/train_images_jpeg/4242340626.jpg inflating: /content/kaggle/train_images_jpeg/424246187.jpg inflating: /content/kaggle/train_images_jpeg/424257956.jpg inflating: /content/kaggle/train_images_jpeg/4242793741.jpg inflating: /content/kaggle/train_images_jpeg/4242899751.jpg inflating: /content/kaggle/train_images_jpeg/4243068188.jpg inflating: /content/kaggle/train_images_jpeg/4243138480.jpg inflating: /content/kaggle/train_images_jpeg/4243169950.jpg inflating: /content/kaggle/train_images_jpeg/4244470722.jpg inflating: /content/kaggle/train_images_jpeg/4244950360.jpg inflating: /content/kaggle/train_images_jpeg/4245177289.jpg inflating: /content/kaggle/train_images_jpeg/424538562.jpg inflating: /content/kaggle/train_images_jpeg/424543130.jpg inflating: /content/kaggle/train_images_jpeg/4245498205.jpg inflating: /content/kaggle/train_images_jpeg/4245562546.jpg inflating: /content/kaggle/train_images_jpeg/4245683638.jpg inflating: /content/kaggle/train_images_jpeg/4245699755.jpg inflating: /content/kaggle/train_images_jpeg/4246064356.jpg inflating: /content/kaggle/train_images_jpeg/4246281634.jpg inflating: /content/kaggle/train_images_jpeg/424650710.jpg inflating: /content/kaggle/train_images_jpeg/424686742.jpg inflating: /content/kaggle/train_images_jpeg/424691365.jpg inflating: /content/kaggle/train_images_jpeg/4246986532.jpg inflating: /content/kaggle/train_images_jpeg/4247295410.jpg inflating: /content/kaggle/train_images_jpeg/4247400003.jpg inflating: /content/kaggle/train_images_jpeg/4247589581.jpg inflating: /content/kaggle/train_images_jpeg/4247710206.jpg inflating: /content/kaggle/train_images_jpeg/4247839780.jpg inflating: /content/kaggle/train_images_jpeg/4248003612.jpg inflating: /content/kaggle/train_images_jpeg/4248062879.jpg inflating: /content/kaggle/train_images_jpeg/4248105895.jpg inflating: /content/kaggle/train_images_jpeg/4248150807.jpg inflating: /content/kaggle/train_images_jpeg/4248249660.jpg inflating: /content/kaggle/train_images_jpeg/4248329912.jpg inflating: /content/kaggle/train_images_jpeg/4248343921.jpg inflating: /content/kaggle/train_images_jpeg/4248695502.jpg inflating: /content/kaggle/train_images_jpeg/4248770671.jpg inflating: /content/kaggle/train_images_jpeg/4249429876.jpg inflating: /content/kaggle/train_images_jpeg/4249694195.jpg inflating: /content/kaggle/train_images_jpeg/4249699505.jpg inflating: /content/kaggle/train_images_jpeg/4249930536.jpg inflating: /content/kaggle/train_images_jpeg/424999624.jpg inflating: /content/kaggle/train_images_jpeg/4250119562.jpg inflating: /content/kaggle/train_images_jpeg/425020385.jpg inflating: /content/kaggle/train_images_jpeg/4250210058.jpg inflating: /content/kaggle/train_images_jpeg/4250231606.jpg inflating: /content/kaggle/train_images_jpeg/425031851.jpg inflating: /content/kaggle/train_images_jpeg/4250418714.jpg inflating: /content/kaggle/train_images_jpeg/4250520416.jpg inflating: /content/kaggle/train_images_jpeg/4250538490.jpg inflating: /content/kaggle/train_images_jpeg/4250554784.jpg inflating: /content/kaggle/train_images_jpeg/4250623520.jpg inflating: /content/kaggle/train_images_jpeg/4250668297.jpg inflating: /content/kaggle/train_images_jpeg/4250729106.jpg inflating: /content/kaggle/train_images_jpeg/4250754910.jpg inflating: /content/kaggle/train_images_jpeg/425082747.jpg inflating: /content/kaggle/train_images_jpeg/4250885951.jpg inflating: /content/kaggle/train_images_jpeg/4251001492.jpg inflating: /content/kaggle/train_images_jpeg/4251271803.jpg inflating: /content/kaggle/train_images_jpeg/4251608120.jpg inflating: /content/kaggle/train_images_jpeg/4251678390.jpg inflating: /content/kaggle/train_images_jpeg/4251933055.jpg inflating: /content/kaggle/train_images_jpeg/4252015282.jpg inflating: /content/kaggle/train_images_jpeg/4252058382.jpg inflating: /content/kaggle/train_images_jpeg/4252095621.jpg inflating: /content/kaggle/train_images_jpeg/4252192689.jpg inflating: /content/kaggle/train_images_jpeg/4252235677.jpg inflating: /content/kaggle/train_images_jpeg/4252236416.jpg inflating: /content/kaggle/train_images_jpeg/425235104.jpg inflating: /content/kaggle/train_images_jpeg/4252410554.jpg inflating: /content/kaggle/train_images_jpeg/425244758.jpg inflating: /content/kaggle/train_images_jpeg/4252535713.jpg inflating: /content/kaggle/train_images_jpeg/4252539994.jpg inflating: /content/kaggle/train_images_jpeg/4253189039.jpg inflating: /content/kaggle/train_images_jpeg/4253520478.jpg inflating: /content/kaggle/train_images_jpeg/4253753819.jpg inflating: /content/kaggle/train_images_jpeg/4253761974.jpg inflating: /content/kaggle/train_images_jpeg/4253799258.jpg inflating: /content/kaggle/train_images_jpeg/425389648.jpg inflating: /content/kaggle/train_images_jpeg/4254213032.jpg inflating: /content/kaggle/train_images_jpeg/4254533142.jpg inflating: /content/kaggle/train_images_jpeg/4254813390.jpg inflating: /content/kaggle/train_images_jpeg/4254971723.jpg inflating: /content/kaggle/train_images_jpeg/4254996610.jpg inflating: /content/kaggle/train_images_jpeg/4255100884.jpg inflating: /content/kaggle/train_images_jpeg/4255196582.jpg inflating: /content/kaggle/train_images_jpeg/4255258797.jpg inflating: /content/kaggle/train_images_jpeg/4255275522.jpg inflating: /content/kaggle/train_images_jpeg/4255452857.jpg inflating: /content/kaggle/train_images_jpeg/4255490685.jpg inflating: /content/kaggle/train_images_jpeg/4255738008.jpg inflating: /content/kaggle/train_images_jpeg/4255843801.jpg inflating: /content/kaggle/train_images_jpeg/4256202412.jpg inflating: /content/kaggle/train_images_jpeg/4256268171.jpg inflating: /content/kaggle/train_images_jpeg/4256542670.jpg inflating: /content/kaggle/train_images_jpeg/4256620425.jpg inflating: /content/kaggle/train_images_jpeg/4256663928.jpg inflating: /content/kaggle/train_images_jpeg/425678500.jpg inflating: /content/kaggle/train_images_jpeg/4256882986.jpg inflating: /content/kaggle/train_images_jpeg/4256968855.jpg inflating: /content/kaggle/train_images_jpeg/4257203192.jpg inflating: /content/kaggle/train_images_jpeg/4257261494.jpg inflating: /content/kaggle/train_images_jpeg/4257577206.jpg inflating: /content/kaggle/train_images_jpeg/4257897572.jpg inflating: /content/kaggle/train_images_jpeg/42586742.jpg inflating: /content/kaggle/train_images_jpeg/4258860412.jpg inflating: /content/kaggle/train_images_jpeg/4258928980.jpg inflating: /content/kaggle/train_images_jpeg/4259637871.jpg inflating: /content/kaggle/train_images_jpeg/425994591.jpg inflating: /content/kaggle/train_images_jpeg/426011031.jpg inflating: /content/kaggle/train_images_jpeg/4260449976.jpg inflating: /content/kaggle/train_images_jpeg/4260526608.jpg inflating: /content/kaggle/train_images_jpeg/4260532551.jpg inflating: /content/kaggle/train_images_jpeg/42609166.jpg inflating: /content/kaggle/train_images_jpeg/4261141445.jpg inflating: /content/kaggle/train_images_jpeg/4261150820.jpg inflating: /content/kaggle/train_images_jpeg/4261199097.jpg inflating: /content/kaggle/train_images_jpeg/426129077.jpg inflating: /content/kaggle/train_images_jpeg/4261378787.jpg inflating: /content/kaggle/train_images_jpeg/4261671268.jpg inflating: /content/kaggle/train_images_jpeg/426170022.jpg inflating: /content/kaggle/train_images_jpeg/4262071206.jpg inflating: /content/kaggle/train_images_jpeg/4262086916.jpg inflating: /content/kaggle/train_images_jpeg/4262259136.jpg inflating: /content/kaggle/train_images_jpeg/4262410116.jpg inflating: /content/kaggle/train_images_jpeg/4262559239.jpg inflating: /content/kaggle/train_images_jpeg/4262567845.jpg inflating: /content/kaggle/train_images_jpeg/4262629224.jpg inflating: /content/kaggle/train_images_jpeg/4262860938.jpg inflating: /content/kaggle/train_images_jpeg/4263534897.jpg inflating: /content/kaggle/train_images_jpeg/4263725317.jpg inflating: /content/kaggle/train_images_jpeg/4263858320.jpg inflating: /content/kaggle/train_images_jpeg/4264274445.jpg inflating: /content/kaggle/train_images_jpeg/426501539.jpg inflating: /content/kaggle/train_images_jpeg/426528709.jpg inflating: /content/kaggle/train_images_jpeg/426530715.jpg inflating: /content/kaggle/train_images_jpeg/4265695955.jpg inflating: /content/kaggle/train_images_jpeg/4265793200.jpg inflating: /content/kaggle/train_images_jpeg/4265846658.jpg inflating: /content/kaggle/train_images_jpeg/4265873256.jpg inflating: /content/kaggle/train_images_jpeg/4266410348.jpg inflating: /content/kaggle/train_images_jpeg/4266856745.jpg inflating: /content/kaggle/train_images_jpeg/4266883021.jpg inflating: /content/kaggle/train_images_jpeg/4266959009.jpg inflating: /content/kaggle/train_images_jpeg/4267085223.jpg inflating: /content/kaggle/train_images_jpeg/4267160725.jpg inflating: /content/kaggle/train_images_jpeg/4267171540.jpg inflating: /content/kaggle/train_images_jpeg/4267414655.jpg inflating: /content/kaggle/train_images_jpeg/4267819868.jpg inflating: /content/kaggle/train_images_jpeg/4267879183.jpg inflating: /content/kaggle/train_images_jpeg/4268211199.jpg inflating: /content/kaggle/train_images_jpeg/4268256477.jpg inflating: /content/kaggle/train_images_jpeg/4268397225.jpg inflating: /content/kaggle/train_images_jpeg/4268549566.jpg inflating: /content/kaggle/train_images_jpeg/42688414.jpg inflating: /content/kaggle/train_images_jpeg/4269113447.jpg inflating: /content/kaggle/train_images_jpeg/4269208386.jpg inflating: /content/kaggle/train_images_jpeg/4269789999.jpg inflating: /content/kaggle/train_images_jpeg/4269823150.jpg inflating: /content/kaggle/train_images_jpeg/4269828095.jpg inflating: /content/kaggle/train_images_jpeg/4269977129.jpg inflating: /content/kaggle/train_images_jpeg/4270123145.jpg inflating: /content/kaggle/train_images_jpeg/4270603433.jpg inflating: /content/kaggle/train_images_jpeg/4270811965.jpg inflating: /content/kaggle/train_images_jpeg/4270972443.jpg inflating: /content/kaggle/train_images_jpeg/4270993331.jpg inflating: /content/kaggle/train_images_jpeg/4271000778.jpg inflating: /content/kaggle/train_images_jpeg/4271208803.jpg inflating: /content/kaggle/train_images_jpeg/4271963761.jpg inflating: /content/kaggle/train_images_jpeg/427201982.jpg inflating: /content/kaggle/train_images_jpeg/427226135.jpg inflating: /content/kaggle/train_images_jpeg/4272507898.jpg inflating: /content/kaggle/train_images_jpeg/4272684860.jpg inflating: /content/kaggle/train_images_jpeg/4272738876.jpg inflating: /content/kaggle/train_images_jpeg/427286737.jpg inflating: /content/kaggle/train_images_jpeg/4272950907.jpg inflating: /content/kaggle/train_images_jpeg/427331923.jpg inflating: /content/kaggle/train_images_jpeg/4273381406.jpg inflating: /content/kaggle/train_images_jpeg/4273424873.jpg inflating: /content/kaggle/train_images_jpeg/4273616869.jpg inflating: /content/kaggle/train_images_jpeg/4273727278.jpg inflating: /content/kaggle/train_images_jpeg/4273998923.jpg inflating: /content/kaggle/train_images_jpeg/4274006379.jpg inflating: /content/kaggle/train_images_jpeg/4274049119.jpg inflating: /content/kaggle/train_images_jpeg/4274564396.jpg inflating: /content/kaggle/train_images_jpeg/4274689672.jpg inflating: /content/kaggle/train_images_jpeg/4274749483.jpg inflating: /content/kaggle/train_images_jpeg/427487173.jpg inflating: /content/kaggle/train_images_jpeg/4275116781.jpg inflating: /content/kaggle/train_images_jpeg/427517931.jpg inflating: /content/kaggle/train_images_jpeg/4275236690.jpg inflating: /content/kaggle/train_images_jpeg/4275245124.jpg inflating: /content/kaggle/train_images_jpeg/4275284230.jpg inflating: /content/kaggle/train_images_jpeg/427529521.jpg inflating: /content/kaggle/train_images_jpeg/4275296346.jpg inflating: /content/kaggle/train_images_jpeg/4275452291.jpg inflating: /content/kaggle/train_images_jpeg/4275645735.jpg inflating: /content/kaggle/train_images_jpeg/4275773216.jpg inflating: /content/kaggle/train_images_jpeg/4276075687.jpg inflating: /content/kaggle/train_images_jpeg/4276437960.jpg inflating: /content/kaggle/train_images_jpeg/4276465485.jpg inflating: /content/kaggle/train_images_jpeg/4276501403.jpg inflating: /content/kaggle/train_images_jpeg/427669101.jpg inflating: /content/kaggle/train_images_jpeg/4276702530.jpg inflating: /content/kaggle/train_images_jpeg/4276709796.jpg inflating: /content/kaggle/train_images_jpeg/4276754883.jpg inflating: /content/kaggle/train_images_jpeg/4276789803.jpg inflating: /content/kaggle/train_images_jpeg/4277021360.jpg inflating: /content/kaggle/train_images_jpeg/4277157912.jpg inflating: /content/kaggle/train_images_jpeg/4277201548.jpg inflating: /content/kaggle/train_images_jpeg/4277213356.jpg inflating: /content/kaggle/train_images_jpeg/4277729347.jpg inflating: /content/kaggle/train_images_jpeg/427782419.jpg inflating: /content/kaggle/train_images_jpeg/4278159410.jpg inflating: /content/kaggle/train_images_jpeg/4278290945.jpg inflating: /content/kaggle/train_images_jpeg/4278349650.jpg inflating: /content/kaggle/train_images_jpeg/4278355555.jpg inflating: /content/kaggle/train_images_jpeg/4278469660.jpg inflating: /content/kaggle/train_images_jpeg/4278686402.jpg inflating: /content/kaggle/train_images_jpeg/4278878757.jpg inflating: /content/kaggle/train_images_jpeg/4279143511.jpg inflating: /content/kaggle/train_images_jpeg/4279177464.jpg inflating: /content/kaggle/train_images_jpeg/4279248558.jpg inflating: /content/kaggle/train_images_jpeg/4279337211.jpg inflating: /content/kaggle/train_images_jpeg/4279357084.jpg inflating: /content/kaggle/train_images_jpeg/4279465930.jpg inflating: /content/kaggle/train_images_jpeg/4279471194.jpg inflating: /content/kaggle/train_images_jpeg/4279868467.jpg inflating: /content/kaggle/train_images_jpeg/4280091186.jpg inflating: /content/kaggle/train_images_jpeg/4280523848.jpg inflating: /content/kaggle/train_images_jpeg/4280698838.jpg inflating: /content/kaggle/train_images_jpeg/4280760906.jpg inflating: /content/kaggle/train_images_jpeg/4280947903.jpg inflating: /content/kaggle/train_images_jpeg/4281502207.jpg inflating: /content/kaggle/train_images_jpeg/4281504647.jpg inflating: /content/kaggle/train_images_jpeg/4281654118.jpg inflating: /content/kaggle/train_images_jpeg/428166614.jpg inflating: /content/kaggle/train_images_jpeg/4282010677.jpg inflating: /content/kaggle/train_images_jpeg/4282408832.jpg inflating: /content/kaggle/train_images_jpeg/4282442229.jpg inflating: /content/kaggle/train_images_jpeg/4282530176.jpg inflating: /content/kaggle/train_images_jpeg/4282727554.jpg inflating: /content/kaggle/train_images_jpeg/4282894767.jpg inflating: /content/kaggle/train_images_jpeg/4282908664.jpg inflating: /content/kaggle/train_images_jpeg/4283054120.jpg inflating: /content/kaggle/train_images_jpeg/4283076582.jpg inflating: /content/kaggle/train_images_jpeg/4283270414.jpg inflating: /content/kaggle/train_images_jpeg/4283277874.jpg inflating: /content/kaggle/train_images_jpeg/4283521063.jpg inflating: /content/kaggle/train_images_jpeg/4284057693.jpg inflating: /content/kaggle/train_images_jpeg/4284191566.jpg inflating: /content/kaggle/train_images_jpeg/4284260544.jpg inflating: /content/kaggle/train_images_jpeg/4284398134.jpg inflating: /content/kaggle/train_images_jpeg/4284426558.jpg inflating: /content/kaggle/train_images_jpeg/4284428305.jpg inflating: /content/kaggle/train_images_jpeg/4284674042.jpg inflating: /content/kaggle/train_images_jpeg/4284813323.jpg inflating: /content/kaggle/train_images_jpeg/4285226176.jpg inflating: /content/kaggle/train_images_jpeg/4285379899.jpg inflating: /content/kaggle/train_images_jpeg/4285401686.jpg inflating: /content/kaggle/train_images_jpeg/4285755413.jpg inflating: /content/kaggle/train_images_jpeg/4286158151.jpg inflating: /content/kaggle/train_images_jpeg/4286169099.jpg inflating: /content/kaggle/train_images_jpeg/4286174060.jpg inflating: /content/kaggle/train_images_jpeg/4286239894.jpg inflating: /content/kaggle/train_images_jpeg/4286284700.jpg inflating: /content/kaggle/train_images_jpeg/428641617.jpg inflating: /content/kaggle/train_images_jpeg/4286577227.jpg inflating: /content/kaggle/train_images_jpeg/4286700834.jpg inflating: /content/kaggle/train_images_jpeg/428692692.jpg inflating: /content/kaggle/train_images_jpeg/4287033165.jpg inflating: /content/kaggle/train_images_jpeg/4287071486.jpg inflating: /content/kaggle/train_images_jpeg/428725949.jpg inflating: /content/kaggle/train_images_jpeg/4287278405.jpg inflating: /content/kaggle/train_images_jpeg/4287286739.jpg inflating: /content/kaggle/train_images_jpeg/4287369745.jpg inflating: /content/kaggle/train_images_jpeg/4287423419.jpg inflating: /content/kaggle/train_images_jpeg/4287448555.jpg inflating: /content/kaggle/train_images_jpeg/4287820275.jpg inflating: /content/kaggle/train_images_jpeg/4288246700.jpg inflating: /content/kaggle/train_images_jpeg/4288249349.jpg inflating: /content/kaggle/train_images_jpeg/4288369732.jpg inflating: /content/kaggle/train_images_jpeg/4288418406.jpg inflating: /content/kaggle/train_images_jpeg/4288554103.jpg inflating: /content/kaggle/train_images_jpeg/4289523242.jpg inflating: /content/kaggle/train_images_jpeg/4289669666.jpg inflating: /content/kaggle/train_images_jpeg/4290607578.jpg inflating: /content/kaggle/train_images_jpeg/429071612.jpg inflating: /content/kaggle/train_images_jpeg/4290786696.jpg inflating: /content/kaggle/train_images_jpeg/4290827656.jpg inflating: /content/kaggle/train_images_jpeg/4290868604.jpg inflating: /content/kaggle/train_images_jpeg/4290883718.jpg inflating: /content/kaggle/train_images_jpeg/4291020124.jpg inflating: /content/kaggle/train_images_jpeg/4291517626.jpg inflating: /content/kaggle/train_images_jpeg/4291622548.jpg inflating: /content/kaggle/train_images_jpeg/4291798124.jpg inflating: /content/kaggle/train_images_jpeg/4291850520.jpg inflating: /content/kaggle/train_images_jpeg/4291924776.jpg inflating: /content/kaggle/train_images_jpeg/429193832.jpg inflating: /content/kaggle/train_images_jpeg/4291943687.jpg inflating: /content/kaggle/train_images_jpeg/4292043437.jpg inflating: /content/kaggle/train_images_jpeg/4292224219.jpg inflating: /content/kaggle/train_images_jpeg/4292271303.jpg inflating: /content/kaggle/train_images_jpeg/429230321.jpg inflating: /content/kaggle/train_images_jpeg/4292333499.jpg inflating: /content/kaggle/train_images_jpeg/4292377658.jpg inflating: /content/kaggle/train_images_jpeg/4292386497.jpg inflating: /content/kaggle/train_images_jpeg/4292850114.jpg inflating: /content/kaggle/train_images_jpeg/4292988075.jpg inflating: /content/kaggle/train_images_jpeg/4293129441.jpg inflating: /content/kaggle/train_images_jpeg/4293589004.jpg inflating: /content/kaggle/train_images_jpeg/4293661491.jpg inflating: /content/kaggle/train_images_jpeg/4293817306.jpg inflating: /content/kaggle/train_images_jpeg/4293860103.jpg inflating: /content/kaggle/train_images_jpeg/429424915.jpg inflating: /content/kaggle/train_images_jpeg/4294756187.jpg inflating: /content/kaggle/train_images_jpeg/42968835.jpg inflating: /content/kaggle/train_images_jpeg/429832892.jpg inflating: /content/kaggle/train_images_jpeg/429982747.jpg inflating: /content/kaggle/train_images_jpeg/430094135.jpg inflating: /content/kaggle/train_images_jpeg/430916626.jpg inflating: /content/kaggle/train_images_jpeg/431233168.jpg inflating: /content/kaggle/train_images_jpeg/431411749.jpg inflating: /content/kaggle/train_images_jpeg/431419127.jpg inflating: /content/kaggle/train_images_jpeg/431535307.jpg inflating: /content/kaggle/train_images_jpeg/431731696.jpg inflating: /content/kaggle/train_images_jpeg/431851340.jpg inflating: /content/kaggle/train_images_jpeg/431956415.jpg inflating: /content/kaggle/train_images_jpeg/432302938.jpg inflating: /content/kaggle/train_images_jpeg/432371484.jpg inflating: /content/kaggle/train_images_jpeg/432428117.jpg inflating: /content/kaggle/train_images_jpeg/432656934.jpg inflating: /content/kaggle/train_images_jpeg/43315915.jpg inflating: /content/kaggle/train_images_jpeg/433273878.jpg inflating: /content/kaggle/train_images_jpeg/433569439.jpg inflating: /content/kaggle/train_images_jpeg/433676551.jpg inflating: /content/kaggle/train_images_jpeg/43400769.jpg inflating: /content/kaggle/train_images_jpeg/434025087.jpg inflating: /content/kaggle/train_images_jpeg/434128804.jpg inflating: /content/kaggle/train_images_jpeg/434367485.jpg inflating: /content/kaggle/train_images_jpeg/434900248.jpg inflating: /content/kaggle/train_images_jpeg/435059376.jpg inflating: /content/kaggle/train_images_jpeg/435094576.jpg inflating: /content/kaggle/train_images_jpeg/43523745.jpg inflating: /content/kaggle/train_images_jpeg/435303871.jpg inflating: /content/kaggle/train_images_jpeg/435404397.jpg inflating: /content/kaggle/train_images_jpeg/435467528.jpg inflating: /content/kaggle/train_images_jpeg/435476831.jpg inflating: /content/kaggle/train_images_jpeg/4359365.jpg inflating: /content/kaggle/train_images_jpeg/435951726.jpg inflating: /content/kaggle/train_images_jpeg/435952255.jpg inflating: /content/kaggle/train_images_jpeg/436004250.jpg inflating: /content/kaggle/train_images_jpeg/436011038.jpg inflating: /content/kaggle/train_images_jpeg/436031847.jpg inflating: /content/kaggle/train_images_jpeg/436314229.jpg inflating: /content/kaggle/train_images_jpeg/4365857.jpg inflating: /content/kaggle/train_images_jpeg/436868168.jpg inflating: /content/kaggle/train_images_jpeg/437121347.jpg inflating: /content/kaggle/train_images_jpeg/43727066.jpg inflating: /content/kaggle/train_images_jpeg/437398008.jpg inflating: /content/kaggle/train_images_jpeg/437629432.jpg inflating: /content/kaggle/train_images_jpeg/437673562.jpg inflating: /content/kaggle/train_images_jpeg/437931902.jpg inflating: /content/kaggle/train_images_jpeg/437958298.jpg inflating: /content/kaggle/train_images_jpeg/438095815.jpg inflating: /content/kaggle/train_images_jpeg/438286664.jpg inflating: /content/kaggle/train_images_jpeg/438352262.jpg inflating: /content/kaggle/train_images_jpeg/438557972.jpg inflating: /content/kaggle/train_images_jpeg/439049574.jpg inflating: /content/kaggle/train_images_jpeg/439346642.jpg inflating: /content/kaggle/train_images_jpeg/43958226.jpg inflating: /content/kaggle/train_images_jpeg/439915243.jpg inflating: /content/kaggle/train_images_jpeg/440064041.jpg inflating: /content/kaggle/train_images_jpeg/440065824.jpg inflating: /content/kaggle/train_images_jpeg/440608001.jpg inflating: /content/kaggle/train_images_jpeg/440896922.jpg inflating: /content/kaggle/train_images_jpeg/441313044.jpg inflating: /content/kaggle/train_images_jpeg/441408374.jpg inflating: /content/kaggle/train_images_jpeg/441501298.jpg inflating: /content/kaggle/train_images_jpeg/441540366.jpg inflating: /content/kaggle/train_images_jpeg/441579945.jpg inflating: /content/kaggle/train_images_jpeg/441630949.jpg inflating: /content/kaggle/train_images_jpeg/442164564.jpg inflating: /content/kaggle/train_images_jpeg/442172384.jpg inflating: /content/kaggle/train_images_jpeg/442179764.jpg inflating: /content/kaggle/train_images_jpeg/442207015.jpg inflating: /content/kaggle/train_images_jpeg/442386946.jpg inflating: /content/kaggle/train_images_jpeg/442397332.jpg inflating: /content/kaggle/train_images_jpeg/442454672.jpg inflating: /content/kaggle/train_images_jpeg/442646143.jpg inflating: /content/kaggle/train_images_jpeg/442743040.jpg inflating: /content/kaggle/train_images_jpeg/442743258.jpg inflating: /content/kaggle/train_images_jpeg/442849128.jpg inflating: /content/kaggle/train_images_jpeg/442906284.jpg inflating: /content/kaggle/train_images_jpeg/442989383.jpg inflating: /content/kaggle/train_images_jpeg/443211656.jpg inflating: /content/kaggle/train_images_jpeg/443215881.jpg inflating: /content/kaggle/train_images_jpeg/443656391.jpg inflating: /content/kaggle/train_images_jpeg/443817384.jpg inflating: /content/kaggle/train_images_jpeg/444154867.jpg inflating: /content/kaggle/train_images_jpeg/444416814.jpg inflating: /content/kaggle/train_images_jpeg/444631842.jpg inflating: /content/kaggle/train_images_jpeg/444715052.jpg inflating: /content/kaggle/train_images_jpeg/445058593.jpg inflating: /content/kaggle/train_images_jpeg/445321069.jpg inflating: /content/kaggle/train_images_jpeg/445341591.jpg inflating: /content/kaggle/train_images_jpeg/445689900.jpg inflating: /content/kaggle/train_images_jpeg/445879019.jpg inflating: /content/kaggle/train_images_jpeg/446451403.jpg inflating: /content/kaggle/train_images_jpeg/446511089.jpg inflating: /content/kaggle/train_images_jpeg/446546740.jpg inflating: /content/kaggle/train_images_jpeg/446547261.jpg inflating: /content/kaggle/train_images_jpeg/446585225.jpg inflating: /content/kaggle/train_images_jpeg/446913360.jpg inflating: /content/kaggle/train_images_jpeg/446986430.jpg inflating: /content/kaggle/train_images_jpeg/447048984.jpg inflating: /content/kaggle/train_images_jpeg/447053671.jpg inflating: /content/kaggle/train_images_jpeg/447327937.jpg inflating: /content/kaggle/train_images_jpeg/447702060.jpg inflating: /content/kaggle/train_images_jpeg/44779769.jpg inflating: /content/kaggle/train_images_jpeg/447826793.jpg inflating: /content/kaggle/train_images_jpeg/447847284.jpg inflating: /content/kaggle/train_images_jpeg/44786547.jpg inflating: /content/kaggle/train_images_jpeg/448026259.jpg inflating: /content/kaggle/train_images_jpeg/448572806.jpg inflating: /content/kaggle/train_images_jpeg/448840430.jpg inflating: /content/kaggle/train_images_jpeg/448853755.jpg inflating: /content/kaggle/train_images_jpeg/449389274.jpg inflating: /content/kaggle/train_images_jpeg/449531366.jpg inflating: /content/kaggle/train_images_jpeg/450167036.jpg inflating: /content/kaggle/train_images_jpeg/450217405.jpg inflating: /content/kaggle/train_images_jpeg/450239051.jpg inflating: /content/kaggle/train_images_jpeg/450609144.jpg inflating: /content/kaggle/train_images_jpeg/451058398.jpg inflating: /content/kaggle/train_images_jpeg/451261982.jpg inflating: /content/kaggle/train_images_jpeg/451285147.jpg inflating: /content/kaggle/train_images_jpeg/451431780.jpg inflating: /content/kaggle/train_images_jpeg/451433123.jpg inflating: /content/kaggle/train_images_jpeg/45143723.jpg inflating: /content/kaggle/train_images_jpeg/451482977.jpg inflating: /content/kaggle/train_images_jpeg/451500275.jpg inflating: /content/kaggle/train_images_jpeg/451532153.jpg inflating: /content/kaggle/train_images_jpeg/451612458.jpg inflating: /content/kaggle/train_images_jpeg/451825559.jpg inflating: /content/kaggle/train_images_jpeg/451922996.jpg inflating: /content/kaggle/train_images_jpeg/4522938.jpg inflating: /content/kaggle/train_images_jpeg/452327589.jpg inflating: /content/kaggle/train_images_jpeg/452338583.jpg inflating: /content/kaggle/train_images_jpeg/452351258.jpg inflating: /content/kaggle/train_images_jpeg/452420525.jpg inflating: /content/kaggle/train_images_jpeg/452781312.jpg inflating: /content/kaggle/train_images_jpeg/454183787.jpg inflating: /content/kaggle/train_images_jpeg/454246300.jpg inflating: /content/kaggle/train_images_jpeg/454637792.jpg inflating: /content/kaggle/train_images_jpeg/454710225.jpg inflating: /content/kaggle/train_images_jpeg/454771505.jpg inflating: /content/kaggle/train_images_jpeg/4551119.jpg inflating: /content/kaggle/train_images_jpeg/455136283.jpg inflating: /content/kaggle/train_images_jpeg/455188005.jpg inflating: /content/kaggle/train_images_jpeg/455328033.jpg inflating: /content/kaggle/train_images_jpeg/455503561.jpg inflating: /content/kaggle/train_images_jpeg/455970803.jpg inflating: /content/kaggle/train_images_jpeg/456001532.jpg inflating: /content/kaggle/train_images_jpeg/456107080.jpg inflating: /content/kaggle/train_images_jpeg/456224979.jpg inflating: /content/kaggle/train_images_jpeg/456278844.jpg inflating: /content/kaggle/train_images_jpeg/456464390.jpg inflating: /content/kaggle/train_images_jpeg/456647345.jpg inflating: /content/kaggle/train_images_jpeg/456963939.jpg inflating: /content/kaggle/train_images_jpeg/457102375.jpg inflating: /content/kaggle/train_images_jpeg/457376738.jpg inflating: /content/kaggle/train_images_jpeg/457405364.jpg inflating: /content/kaggle/train_images_jpeg/457446390.jpg inflating: /content/kaggle/train_images_jpeg/45753456.jpg inflating: /content/kaggle/train_images_jpeg/45787177.jpg inflating: /content/kaggle/train_images_jpeg/457931044.jpg inflating: /content/kaggle/train_images_jpeg/457946090.jpg inflating: /content/kaggle/train_images_jpeg/457979538.jpg inflating: /content/kaggle/train_images_jpeg/458078692.jpg inflating: /content/kaggle/train_images_jpeg/458083471.jpg inflating: /content/kaggle/train_images_jpeg/458086405.jpg inflating: /content/kaggle/train_images_jpeg/458113547.jpg inflating: /content/kaggle/train_images_jpeg/458279026.jpg inflating: /content/kaggle/train_images_jpeg/458282782.jpg inflating: /content/kaggle/train_images_jpeg/458696690.jpg inflating: /content/kaggle/train_images_jpeg/458698920.jpg inflating: /content/kaggle/train_images_jpeg/459018171.jpg inflating: /content/kaggle/train_images_jpeg/459025811.jpg inflating: /content/kaggle/train_images_jpeg/459069338.jpg inflating: /content/kaggle/train_images_jpeg/459155800.jpg inflating: /content/kaggle/train_images_jpeg/459183399.jpg inflating: /content/kaggle/train_images_jpeg/459241736.jpg inflating: /content/kaggle/train_images_jpeg/459331011.jpg inflating: /content/kaggle/train_images_jpeg/459566103.jpg inflating: /content/kaggle/train_images_jpeg/459566440.jpg inflating: /content/kaggle/train_images_jpeg/459715600.jpg inflating: /content/kaggle/train_images_jpeg/459727273.jpg inflating: /content/kaggle/train_images_jpeg/459889345.jpg inflating: /content/kaggle/train_images_jpeg/460359141.jpg inflating: /content/kaggle/train_images_jpeg/460660800.jpg inflating: /content/kaggle/train_images_jpeg/460695084.jpg inflating: /content/kaggle/train_images_jpeg/46087551.jpg inflating: /content/kaggle/train_images_jpeg/460888118.jpg inflating: /content/kaggle/train_images_jpeg/461042131.jpg inflating: /content/kaggle/train_images_jpeg/461442834.jpg inflating: /content/kaggle/train_images_jpeg/461505973.jpg inflating: /content/kaggle/train_images_jpeg/461517299.jpg inflating: /content/kaggle/train_images_jpeg/461574055.jpg inflating: /content/kaggle/train_images_jpeg/461654762.jpg inflating: /content/kaggle/train_images_jpeg/462009251.jpg inflating: /content/kaggle/train_images_jpeg/462053910.jpg inflating: /content/kaggle/train_images_jpeg/462079305.jpg inflating: /content/kaggle/train_images_jpeg/462192848.jpg inflating: /content/kaggle/train_images_jpeg/462402577.jpg inflating: /content/kaggle/train_images_jpeg/462491927.jpg inflating: /content/kaggle/train_images_jpeg/462652564.jpg inflating: /content/kaggle/train_images_jpeg/462768820.jpg inflating: /content/kaggle/train_images_jpeg/462938008.jpg inflating: /content/kaggle/train_images_jpeg/463033778.jpg inflating: /content/kaggle/train_images_jpeg/463306990.jpg inflating: /content/kaggle/train_images_jpeg/463752462.jpg inflating: /content/kaggle/train_images_jpeg/463938223.jpg inflating: /content/kaggle/train_images_jpeg/464089326.jpg inflating: /content/kaggle/train_images_jpeg/464249040.jpg inflating: /content/kaggle/train_images_jpeg/464299112.jpg inflating: /content/kaggle/train_images_jpeg/464335035.jpg inflating: /content/kaggle/train_images_jpeg/46435039.jpg inflating: /content/kaggle/train_images_jpeg/464695635.jpg inflating: /content/kaggle/train_images_jpeg/465394840.jpg inflating: /content/kaggle/train_images_jpeg/465786805.jpg inflating: /content/kaggle/train_images_jpeg/466394178.jpg inflating: /content/kaggle/train_images_jpeg/466665230.jpg inflating: /content/kaggle/train_images_jpeg/466809222.jpg inflating: /content/kaggle/train_images_jpeg/467000681.jpg inflating: /content/kaggle/train_images_jpeg/467037759.jpg inflating: /content/kaggle/train_images_jpeg/467561327.jpg inflating: /content/kaggle/train_images_jpeg/467965750.jpg inflating: /content/kaggle/train_images_jpeg/468599121.jpg inflating: /content/kaggle/train_images_jpeg/469109506.jpg inflating: /content/kaggle/train_images_jpeg/469245584.jpg inflating: /content/kaggle/train_images_jpeg/469487.jpg inflating: /content/kaggle/train_images_jpeg/469514672.jpg inflating: /content/kaggle/train_images_jpeg/469559658.jpg inflating: /content/kaggle/train_images_jpeg/469627647.jpg inflating: /content/kaggle/train_images_jpeg/469651575.jpg inflating: /content/kaggle/train_images_jpeg/46965268.jpg inflating: /content/kaggle/train_images_jpeg/469662229.jpg inflating: /content/kaggle/train_images_jpeg/469729622.jpg inflating: /content/kaggle/train_images_jpeg/469774939.jpg inflating: /content/kaggle/train_images_jpeg/469798275.jpg inflating: /content/kaggle/train_images_jpeg/470010237.jpg inflating: /content/kaggle/train_images_jpeg/470266437.jpg inflating: /content/kaggle/train_images_jpeg/470378049.jpg inflating: /content/kaggle/train_images_jpeg/470635054.jpg inflating: /content/kaggle/train_images_jpeg/471126071.jpg inflating: /content/kaggle/train_images_jpeg/471570254.jpg inflating: /content/kaggle/train_images_jpeg/471647021.jpg inflating: /content/kaggle/train_images_jpeg/471719560.jpg inflating: /content/kaggle/train_images_jpeg/471733925.jpg inflating: /content/kaggle/train_images_jpeg/471759171.jpg inflating: /content/kaggle/train_images_jpeg/471808107.jpg inflating: /content/kaggle/train_images_jpeg/471996434.jpg inflating: /content/kaggle/train_images_jpeg/472269630.jpg inflating: /content/kaggle/train_images_jpeg/472300747.jpg inflating: /content/kaggle/train_images_jpeg/472370398.jpg inflating: /content/kaggle/train_images_jpeg/472465250.jpg inflating: /content/kaggle/train_images_jpeg/472489554.jpg inflating: /content/kaggle/train_images_jpeg/472536044.jpg inflating: /content/kaggle/train_images_jpeg/472839300.jpg inflating: /content/kaggle/train_images_jpeg/473238027.jpg inflating: /content/kaggle/train_images_jpeg/473358865.jpg inflating: /content/kaggle/train_images_jpeg/473737593.jpg inflating: /content/kaggle/train_images_jpeg/473806024.jpg inflating: /content/kaggle/train_images_jpeg/473823142.jpg inflating: /content/kaggle/train_images_jpeg/473895860.jpg inflating: /content/kaggle/train_images_jpeg/473941755.jpg inflating: /content/kaggle/train_images_jpeg/474097045.jpg inflating: /content/kaggle/train_images_jpeg/474493920.jpg inflating: /content/kaggle/train_images_jpeg/475006659.jpg inflating: /content/kaggle/train_images_jpeg/475084135.jpg inflating: /content/kaggle/train_images_jpeg/475097203.jpg inflating: /content/kaggle/train_images_jpeg/475228141.jpg inflating: /content/kaggle/train_images_jpeg/475418863.jpg inflating: /content/kaggle/train_images_jpeg/475593787.jpg inflating: /content/kaggle/train_images_jpeg/475756576.jpg inflating: /content/kaggle/train_images_jpeg/476012362.jpg inflating: /content/kaggle/train_images_jpeg/476113846.jpg inflating: /content/kaggle/train_images_jpeg/476295659.jpg inflating: /content/kaggle/train_images_jpeg/476297006.jpg inflating: /content/kaggle/train_images_jpeg/476487552.jpg inflating: /content/kaggle/train_images_jpeg/476858432.jpg inflating: /content/kaggle/train_images_jpeg/477466097.jpg inflating: /content/kaggle/train_images_jpeg/477484606.jpg inflating: /content/kaggle/train_images_jpeg/477523981.jpg inflating: /content/kaggle/train_images_jpeg/477888939.jpg inflating: /content/kaggle/train_images_jpeg/478036306.jpg inflating: /content/kaggle/train_images_jpeg/478370518.jpg inflating: /content/kaggle/train_images_jpeg/478400081.jpg inflating: /content/kaggle/train_images_jpeg/478426425.jpg inflating: /content/kaggle/train_images_jpeg/4785038.jpg inflating: /content/kaggle/train_images_jpeg/478546048.jpg inflating: /content/kaggle/train_images_jpeg/478554372.jpg inflating: /content/kaggle/train_images_jpeg/478676678.jpg inflating: /content/kaggle/train_images_jpeg/478688374.jpg inflating: /content/kaggle/train_images_jpeg/478757157.jpg inflating: /content/kaggle/train_images_jpeg/478838530.jpg inflating: /content/kaggle/train_images_jpeg/479283253.jpg inflating: /content/kaggle/train_images_jpeg/479472063.jpg inflating: /content/kaggle/train_images_jpeg/479609263.jpg inflating: /content/kaggle/train_images_jpeg/479659706.jpg inflating: /content/kaggle/train_images_jpeg/479691451.jpg inflating: /content/kaggle/train_images_jpeg/480419575.jpg inflating: /content/kaggle/train_images_jpeg/480420375.jpg inflating: /content/kaggle/train_images_jpeg/480492521.jpg inflating: /content/kaggle/train_images_jpeg/480754210.jpg inflating: /content/kaggle/train_images_jpeg/480764320.jpg inflating: /content/kaggle/train_images_jpeg/480985864.jpg inflating: /content/kaggle/train_images_jpeg/48105658.jpg inflating: /content/kaggle/train_images_jpeg/481101110.jpg inflating: /content/kaggle/train_images_jpeg/481152398.jpg inflating: /content/kaggle/train_images_jpeg/481202063.jpg inflating: /content/kaggle/train_images_jpeg/48153077.jpg inflating: /content/kaggle/train_images_jpeg/481585072.jpg inflating: /content/kaggle/train_images_jpeg/481972599.jpg inflating: /content/kaggle/train_images_jpeg/482191440.jpg inflating: /content/kaggle/train_images_jpeg/482335767.jpg inflating: /content/kaggle/train_images_jpeg/482396705.jpg inflating: /content/kaggle/train_images_jpeg/482843238.jpg inflating: /content/kaggle/train_images_jpeg/483058921.jpg inflating: /content/kaggle/train_images_jpeg/483070348.jpg inflating: /content/kaggle/train_images_jpeg/483132634.jpg inflating: /content/kaggle/train_images_jpeg/483199386.jpg inflating: /content/kaggle/train_images_jpeg/483210216.jpg inflating: /content/kaggle/train_images_jpeg/483372035.jpg inflating: /content/kaggle/train_images_jpeg/483398598.jpg inflating: /content/kaggle/train_images_jpeg/483417793.jpg inflating: /content/kaggle/train_images_jpeg/483725147.jpg inflating: /content/kaggle/train_images_jpeg/483774797.jpg inflating: /content/kaggle/train_images_jpeg/483986538.jpg inflating: /content/kaggle/train_images_jpeg/484161659.jpg inflating: /content/kaggle/train_images_jpeg/484219848.jpg inflating: /content/kaggle/train_images_jpeg/484286450.jpg inflating: /content/kaggle/train_images_jpeg/484462927.jpg inflating: /content/kaggle/train_images_jpeg/484625436.jpg inflating: /content/kaggle/train_images_jpeg/484710612.jpg inflating: /content/kaggle/train_images_jpeg/4851187.jpg inflating: /content/kaggle/train_images_jpeg/485289472.jpg inflating: /content/kaggle/train_images_jpeg/485328354.jpg inflating: /content/kaggle/train_images_jpeg/485496455.jpg inflating: /content/kaggle/train_images_jpeg/486088821.jpg inflating: /content/kaggle/train_images_jpeg/486182898.jpg inflating: /content/kaggle/train_images_jpeg/486370102.jpg inflating: /content/kaggle/train_images_jpeg/486378688.jpg inflating: /content/kaggle/train_images_jpeg/486827297.jpg inflating: /content/kaggle/train_images_jpeg/487107070.jpg inflating: /content/kaggle/train_images_jpeg/487175432.jpg inflating: /content/kaggle/train_images_jpeg/48759144.jpg inflating: /content/kaggle/train_images_jpeg/487710367.jpg inflating: /content/kaggle/train_images_jpeg/487884059.jpg inflating: /content/kaggle/train_images_jpeg/488165307.jpg inflating: /content/kaggle/train_images_jpeg/488245279.jpg inflating: /content/kaggle/train_images_jpeg/488438522.jpg inflating: /content/kaggle/train_images_jpeg/488450580.jpg inflating: /content/kaggle/train_images_jpeg/488530530.jpg inflating: /content/kaggle/train_images_jpeg/488621358.jpg inflating: /content/kaggle/train_images_jpeg/488710990.jpg inflating: /content/kaggle/train_images_jpeg/488981840.jpg inflating: /content/kaggle/train_images_jpeg/489343827.jpg inflating: /content/kaggle/train_images_jpeg/489369440.jpg inflating: /content/kaggle/train_images_jpeg/489562516.jpg inflating: /content/kaggle/train_images_jpeg/489607845.jpg inflating: /content/kaggle/train_images_jpeg/489859226.jpg inflating: /content/kaggle/train_images_jpeg/489869560.jpg inflating: /content/kaggle/train_images_jpeg/490262929.jpg inflating: /content/kaggle/train_images_jpeg/490343607.jpg inflating: /content/kaggle/train_images_jpeg/490603548.jpg inflating: /content/kaggle/train_images_jpeg/490649765.jpg inflating: /content/kaggle/train_images_jpeg/490760030.jpg inflating: /content/kaggle/train_images_jpeg/490933333.jpg inflating: /content/kaggle/train_images_jpeg/491244285.jpg inflating: /content/kaggle/train_images_jpeg/491341032.jpg inflating: /content/kaggle/train_images_jpeg/491402925.jpg inflating: /content/kaggle/train_images_jpeg/491516744.jpg inflating: /content/kaggle/train_images_jpeg/491673903.jpg inflating: /content/kaggle/train_images_jpeg/491901266.jpg inflating: /content/kaggle/train_images_jpeg/492140725.jpg inflating: /content/kaggle/train_images_jpeg/492151497.jpg inflating: /content/kaggle/train_images_jpeg/49247184.jpg inflating: /content/kaggle/train_images_jpeg/492645288.jpg inflating: /content/kaggle/train_images_jpeg/493667259.jpg inflating: /content/kaggle/train_images_jpeg/493875479.jpg inflating: /content/kaggle/train_images_jpeg/494174224.jpg inflating: /content/kaggle/train_images_jpeg/494373942.jpg inflating: /content/kaggle/train_images_jpeg/49440106.jpg inflating: /content/kaggle/train_images_jpeg/494558399.jpg inflating: /content/kaggle/train_images_jpeg/494596361.jpg inflating: /content/kaggle/train_images_jpeg/495190803.jpg inflating: /content/kaggle/train_images_jpeg/495290160.jpg inflating: /content/kaggle/train_images_jpeg/49533395.jpg inflating: /content/kaggle/train_images_jpeg/495499602.jpg inflating: /content/kaggle/train_images_jpeg/495769274.jpg inflating: /content/kaggle/train_images_jpeg/495897960.jpg inflating: /content/kaggle/train_images_jpeg/495907528.jpg inflating: /content/kaggle/train_images_jpeg/496190872.jpg inflating: /content/kaggle/train_images_jpeg/496222891.jpg inflating: /content/kaggle/train_images_jpeg/496284862.jpg inflating: /content/kaggle/train_images_jpeg/496523270.jpg inflating: /content/kaggle/train_images_jpeg/496631673.jpg inflating: /content/kaggle/train_images_jpeg/496814023.jpg inflating: /content/kaggle/train_images_jpeg/496924131.jpg inflating: /content/kaggle/train_images_jpeg/496949674.jpg inflating: /content/kaggle/train_images_jpeg/49697815.jpg inflating: /content/kaggle/train_images_jpeg/497021480.jpg inflating: /content/kaggle/train_images_jpeg/49703968.jpg inflating: /content/kaggle/train_images_jpeg/497447638.jpg inflating: /content/kaggle/train_images_jpeg/497483114.jpg inflating: /content/kaggle/train_images_jpeg/497620321.jpg inflating: /content/kaggle/train_images_jpeg/497624056.jpg inflating: /content/kaggle/train_images_jpeg/497685909.jpg inflating: /content/kaggle/train_images_jpeg/498197766.jpg inflating: /content/kaggle/train_images_jpeg/498240070.jpg inflating: /content/kaggle/train_images_jpeg/498242244.jpg inflating: /content/kaggle/train_images_jpeg/498277992.jpg inflating: /content/kaggle/train_images_jpeg/498454187.jpg inflating: /content/kaggle/train_images_jpeg/498491072.jpg inflating: /content/kaggle/train_images_jpeg/49856658.jpg inflating: /content/kaggle/train_images_jpeg/498735095.jpg inflating: /content/kaggle/train_images_jpeg/498850777.jpg inflating: /content/kaggle/train_images_jpeg/498928968.jpg inflating: /content/kaggle/train_images_jpeg/499130995.jpg inflating: /content/kaggle/train_images_jpeg/499152575.jpg inflating: /content/kaggle/train_images_jpeg/499195591.jpg inflating: /content/kaggle/train_images_jpeg/49920337.jpg inflating: /content/kaggle/train_images_jpeg/499499899.jpg inflating: /content/kaggle/train_images_jpeg/499562675.jpg inflating: /content/kaggle/train_images_jpeg/499671530.jpg inflating: /content/kaggle/train_images_jpeg/499934842.jpg inflating: /content/kaggle/train_images_jpeg/500165253.jpg inflating: /content/kaggle/train_images_jpeg/500317925.jpg inflating: /content/kaggle/train_images_jpeg/500440865.jpg inflating: /content/kaggle/train_images_jpeg/500785791.jpg inflating: /content/kaggle/train_images_jpeg/501052709.jpg inflating: /content/kaggle/train_images_jpeg/501215014.jpg inflating: /content/kaggle/train_images_jpeg/501394346.jpg inflating: /content/kaggle/train_images_jpeg/501634061.jpg inflating: /content/kaggle/train_images_jpeg/50223703.jpg inflating: /content/kaggle/train_images_jpeg/502274488.jpg inflating: /content/kaggle/train_images_jpeg/502320523.jpg inflating: /content/kaggle/train_images_jpeg/502783701.jpg inflating: /content/kaggle/train_images_jpeg/503037836.jpg inflating: /content/kaggle/train_images_jpeg/503224990.jpg inflating: /content/kaggle/train_images_jpeg/503366497.jpg inflating: /content/kaggle/train_images_jpeg/503851333.jpg inflating: /content/kaggle/train_images_jpeg/503867520.jpg inflating: /content/kaggle/train_images_jpeg/503880370.jpg inflating: /content/kaggle/train_images_jpeg/503916638.jpg inflating: /content/kaggle/train_images_jpeg/504065374.jpg inflating: /content/kaggle/train_images_jpeg/504400425.jpg inflating: /content/kaggle/train_images_jpeg/50441514.jpg inflating: /content/kaggle/train_images_jpeg/504689064.jpg inflating: /content/kaggle/train_images_jpeg/504875275.jpg inflating: /content/kaggle/train_images_jpeg/50496855.jpg inflating: /content/kaggle/train_images_jpeg/505261633.jpg inflating: /content/kaggle/train_images_jpeg/505355209.jpg inflating: /content/kaggle/train_images_jpeg/505693686.jpg inflating: /content/kaggle/train_images_jpeg/505820957.jpg inflating: /content/kaggle/train_images_jpeg/50589657.jpg inflating: /content/kaggle/train_images_jpeg/506080526.jpg inflating: /content/kaggle/train_images_jpeg/506567755.jpg inflating: /content/kaggle/train_images_jpeg/506664858.jpg inflating: /content/kaggle/train_images_jpeg/506713297.jpg inflating: /content/kaggle/train_images_jpeg/506774842.jpg inflating: /content/kaggle/train_images_jpeg/507004978.jpg inflating: /content/kaggle/train_images_jpeg/507089312.jpg inflating: /content/kaggle/train_images_jpeg/507279911.jpg inflating: /content/kaggle/train_images_jpeg/508104541.jpg inflating: /content/kaggle/train_images_jpeg/508241911.jpg inflating: /content/kaggle/train_images_jpeg/508250510.jpg inflating: /content/kaggle/train_images_jpeg/508378289.jpg inflating: /content/kaggle/train_images_jpeg/508653988.jpg inflating: /content/kaggle/train_images_jpeg/508678905.jpg inflating: /content/kaggle/train_images_jpeg/5088626.jpg inflating: /content/kaggle/train_images_jpeg/509573595.jpg inflating: /content/kaggle/train_images_jpeg/509720498.jpg inflating: /content/kaggle/train_images_jpeg/509958138.jpg inflating: /content/kaggle/train_images_jpeg/510134831.jpg inflating: /content/kaggle/train_images_jpeg/510336475.jpg inflating: /content/kaggle/train_images_jpeg/510338633.jpg inflating: /content/kaggle/train_images_jpeg/51063556.jpg inflating: /content/kaggle/train_images_jpeg/510655855.jpg inflating: /content/kaggle/train_images_jpeg/510749381.jpg inflating: /content/kaggle/train_images_jpeg/51075448.jpg inflating: /content/kaggle/train_images_jpeg/510847291.jpg inflating: /content/kaggle/train_images_jpeg/510873412.jpg inflating: /content/kaggle/train_images_jpeg/510931414.jpg inflating: /content/kaggle/train_images_jpeg/511089845.jpg inflating: /content/kaggle/train_images_jpeg/511322441.jpg inflating: /content/kaggle/train_images_jpeg/511472935.jpg inflating: /content/kaggle/train_images_jpeg/511494516.jpg inflating: /content/kaggle/train_images_jpeg/511654743.jpg inflating: /content/kaggle/train_images_jpeg/511675910.jpg inflating: /content/kaggle/train_images_jpeg/511932063.jpg inflating: /content/kaggle/train_images_jpeg/511993936.jpg inflating: /content/kaggle/train_images_jpeg/512152604.jpg inflating: /content/kaggle/train_images_jpeg/512168162.jpg inflating: /content/kaggle/train_images_jpeg/512575634.jpg inflating: /content/kaggle/train_images_jpeg/512651880.jpg inflating: /content/kaggle/train_images_jpeg/512811593.jpg inflating: /content/kaggle/train_images_jpeg/51284514.jpg inflating: /content/kaggle/train_images_jpeg/512994798.jpg inflating: /content/kaggle/train_images_jpeg/513308138.jpg inflating: /content/kaggle/train_images_jpeg/513364056.jpg inflating: /content/kaggle/train_images_jpeg/513549647.jpg inflating: /content/kaggle/train_images_jpeg/51370822.jpg inflating: /content/kaggle/train_images_jpeg/513780423.jpg inflating: /content/kaggle/train_images_jpeg/513916180.jpg inflating: /content/kaggle/train_images_jpeg/513986084.jpg inflating: /content/kaggle/train_images_jpeg/514040790.jpg inflating: /content/kaggle/train_images_jpeg/514282454.jpg inflating: /content/kaggle/train_images_jpeg/514376645.jpg inflating: /content/kaggle/train_images_jpeg/51460409.jpg inflating: /content/kaggle/train_images_jpeg/514638622.jpg inflating: /content/kaggle/train_images_jpeg/514840131.jpg inflating: /content/kaggle/train_images_jpeg/514995034.jpg inflating: /content/kaggle/train_images_jpeg/515083070.jpg inflating: /content/kaggle/train_images_jpeg/515191532.jpg inflating: /content/kaggle/train_images_jpeg/515241983.jpg inflating: /content/kaggle/train_images_jpeg/515316460.jpg inflating: /content/kaggle/train_images_jpeg/515391369.jpg inflating: /content/kaggle/train_images_jpeg/515448717.jpg inflating: /content/kaggle/train_images_jpeg/515738750.jpg inflating: /content/kaggle/train_images_jpeg/51624303.jpg inflating: /content/kaggle/train_images_jpeg/516331845.jpg inflating: /content/kaggle/train_images_jpeg/516333196.jpg inflating: /content/kaggle/train_images_jpeg/516746229.jpg inflating: /content/kaggle/train_images_jpeg/517018049.jpg inflating: /content/kaggle/train_images_jpeg/517170612.jpg inflating: /content/kaggle/train_images_jpeg/517295595.jpg inflating: /content/kaggle/train_images_jpeg/51748275.jpg inflating: /content/kaggle/train_images_jpeg/518232429.jpg inflating: /content/kaggle/train_images_jpeg/518284284.jpg inflating: /content/kaggle/train_images_jpeg/518398469.jpg inflating: /content/kaggle/train_images_jpeg/518719429.jpg inflating: /content/kaggle/train_images_jpeg/518891556.jpg inflating: /content/kaggle/train_images_jpeg/519050764.jpg inflating: /content/kaggle/train_images_jpeg/519080705.jpg inflating: /content/kaggle/train_images_jpeg/519341092.jpg inflating: /content/kaggle/train_images_jpeg/519363437.jpg inflating: /content/kaggle/train_images_jpeg/519373224.jpg inflating: /content/kaggle/train_images_jpeg/519516742.jpg inflating: /content/kaggle/train_images_jpeg/519569660.jpg inflating: /content/kaggle/train_images_jpeg/520111872.jpg inflating: /content/kaggle/train_images_jpeg/520184661.jpg inflating: /content/kaggle/train_images_jpeg/520310239.jpg inflating: /content/kaggle/train_images_jpeg/520827506.jpg inflating: /content/kaggle/train_images_jpeg/520992069.jpg inflating: /content/kaggle/train_images_jpeg/521262022.jpg inflating: /content/kaggle/train_images_jpeg/522043720.jpg inflating: /content/kaggle/train_images_jpeg/522172661.jpg inflating: /content/kaggle/train_images_jpeg/522209459.jpg inflating: /content/kaggle/train_images_jpeg/522535695.jpg inflating: /content/kaggle/train_images_jpeg/522793998.jpg inflating: /content/kaggle/train_images_jpeg/522854384.jpg inflating: /content/kaggle/train_images_jpeg/523005537.jpg inflating: /content/kaggle/train_images_jpeg/523032468.jpg inflating: /content/kaggle/train_images_jpeg/523149736.jpg inflating: /content/kaggle/train_images_jpeg/523201478.jpg inflating: /content/kaggle/train_images_jpeg/523467512.jpg inflating: /content/kaggle/train_images_jpeg/524130379.jpg inflating: /content/kaggle/train_images_jpeg/52417145.jpg inflating: /content/kaggle/train_images_jpeg/524188630.jpg inflating: /content/kaggle/train_images_jpeg/524234621.jpg inflating: /content/kaggle/train_images_jpeg/524701920.jpg inflating: /content/kaggle/train_images_jpeg/52480515.jpg inflating: /content/kaggle/train_images_jpeg/525349447.jpg inflating: /content/kaggle/train_images_jpeg/525691978.jpg inflating: /content/kaggle/train_images_jpeg/525742373.jpg inflating: /content/kaggle/train_images_jpeg/525960141.jpg inflating: /content/kaggle/train_images_jpeg/526290781.jpg inflating: /content/kaggle/train_images_jpeg/52672633.jpg inflating: /content/kaggle/train_images_jpeg/526842311.jpg inflating: /content/kaggle/train_images_jpeg/527207274.jpg inflating: /content/kaggle/train_images_jpeg/527287896.jpg inflating: /content/kaggle/train_images_jpeg/52729180.jpg inflating: /content/kaggle/train_images_jpeg/527304483.jpg inflating: /content/kaggle/train_images_jpeg/527458103.jpg inflating: /content/kaggle/train_images_jpeg/527474755.jpg inflating: /content/kaggle/train_images_jpeg/527619168.jpg inflating: /content/kaggle/train_images_jpeg/527648295.jpg inflating: /content/kaggle/train_images_jpeg/527923260.jpg inflating: /content/kaggle/train_images_jpeg/528179991.jpg inflating: /content/kaggle/train_images_jpeg/528487165.jpg inflating: /content/kaggle/train_images_jpeg/528632958.jpg inflating: /content/kaggle/train_images_jpeg/528691632.jpg inflating: /content/kaggle/train_images_jpeg/52883488.jpg inflating: /content/kaggle/train_images_jpeg/529324614.jpg inflating: /content/kaggle/train_images_jpeg/529520501.jpg inflating: /content/kaggle/train_images_jpeg/529572554.jpg inflating: /content/kaggle/train_images_jpeg/529641068.jpg inflating: /content/kaggle/train_images_jpeg/530166145.jpg inflating: /content/kaggle/train_images_jpeg/530221019.jpg inflating: /content/kaggle/train_images_jpeg/53025412.jpg inflating: /content/kaggle/train_images_jpeg/530355388.jpg inflating: /content/kaggle/train_images_jpeg/530632304.jpg inflating: /content/kaggle/train_images_jpeg/53065520.jpg inflating: /content/kaggle/train_images_jpeg/530867664.jpg inflating: /content/kaggle/train_images_jpeg/531185235.jpg inflating: /content/kaggle/train_images_jpeg/53126023.jpg inflating: /content/kaggle/train_images_jpeg/531614810.jpg inflating: /content/kaggle/train_images_jpeg/53164927.jpg inflating: /content/kaggle/train_images_jpeg/531653208.jpg inflating: /content/kaggle/train_images_jpeg/53200283.jpg inflating: /content/kaggle/train_images_jpeg/532038171.jpg inflating: /content/kaggle/train_images_jpeg/532081653.jpg inflating: /content/kaggle/train_images_jpeg/532103360.jpg inflating: /content/kaggle/train_images_jpeg/532201178.jpg inflating: /content/kaggle/train_images_jpeg/532235062.jpg inflating: /content/kaggle/train_images_jpeg/532255691.jpg inflating: /content/kaggle/train_images_jpeg/532304081.jpg inflating: /content/kaggle/train_images_jpeg/532464272.jpg inflating: /content/kaggle/train_images_jpeg/532508558.jpg inflating: /content/kaggle/train_images_jpeg/532692632.jpg inflating: /content/kaggle/train_images_jpeg/533133014.jpg inflating: /content/kaggle/train_images_jpeg/533486476.jpg inflating: /content/kaggle/train_images_jpeg/533613162.jpg inflating: /content/kaggle/train_images_jpeg/534270890.jpg inflating: /content/kaggle/train_images_jpeg/534358787.jpg inflating: /content/kaggle/train_images_jpeg/534364866.jpg inflating: /content/kaggle/train_images_jpeg/534568222.jpg inflating: /content/kaggle/train_images_jpeg/534784883.jpg inflating: /content/kaggle/train_images_jpeg/534932324.jpg inflating: /content/kaggle/train_images_jpeg/534969562.jpg inflating: /content/kaggle/train_images_jpeg/53515353.jpg inflating: /content/kaggle/train_images_jpeg/535314273.jpg inflating: /content/kaggle/train_images_jpeg/535503922.jpg inflating: /content/kaggle/train_images_jpeg/535534657.jpg inflating: /content/kaggle/train_images_jpeg/535537385.jpg inflating: /content/kaggle/train_images_jpeg/53615554.jpg inflating: /content/kaggle/train_images_jpeg/536966524.jpg inflating: /content/kaggle/train_images_jpeg/537247997.jpg inflating: /content/kaggle/train_images_jpeg/537279050.jpg inflating: /content/kaggle/train_images_jpeg/537462269.jpg inflating: /content/kaggle/train_images_jpeg/537704740.jpg inflating: /content/kaggle/train_images_jpeg/537771989.jpg inflating: /content/kaggle/train_images_jpeg/538079327.jpg inflating: /content/kaggle/train_images_jpeg/53822293.jpg inflating: /content/kaggle/train_images_jpeg/53835845.jpg inflating: /content/kaggle/train_images_jpeg/538938122.jpg inflating: /content/kaggle/train_images_jpeg/539156114.jpg inflating: /content/kaggle/train_images_jpeg/539212497.jpg inflating: /content/kaggle/train_images_jpeg/539448425.jpg inflating: /content/kaggle/train_images_jpeg/53955129.jpg inflating: /content/kaggle/train_images_jpeg/539640759.jpg inflating: /content/kaggle/train_images_jpeg/539706836.jpg inflating: /content/kaggle/train_images_jpeg/540248010.jpg inflating: /content/kaggle/train_images_jpeg/540729599.jpg inflating: /content/kaggle/train_images_jpeg/540762921.jpg inflating: /content/kaggle/train_images_jpeg/540817686.jpg inflating: /content/kaggle/train_images_jpeg/540860171.jpg inflating: /content/kaggle/train_images_jpeg/541016077.jpg inflating: /content/kaggle/train_images_jpeg/541216971.jpg inflating: /content/kaggle/train_images_jpeg/54133224.jpg inflating: /content/kaggle/train_images_jpeg/541581535.jpg inflating: /content/kaggle/train_images_jpeg/541684272.jpg inflating: /content/kaggle/train_images_jpeg/541812079.jpg inflating: /content/kaggle/train_images_jpeg/541997227.jpg inflating: /content/kaggle/train_images_jpeg/542067115.jpg inflating: /content/kaggle/train_images_jpeg/542129392.jpg inflating: /content/kaggle/train_images_jpeg/542215511.jpg inflating: /content/kaggle/train_images_jpeg/542432519.jpg inflating: /content/kaggle/train_images_jpeg/542560691.jpg inflating: /content/kaggle/train_images_jpeg/542786109.jpg inflating: /content/kaggle/train_images_jpeg/542875481.jpg inflating: /content/kaggle/train_images_jpeg/543032315.jpg inflating: /content/kaggle/train_images_jpeg/5430583.jpg inflating: /content/kaggle/train_images_jpeg/54321323.jpg inflating: /content/kaggle/train_images_jpeg/543312121.jpg inflating: /content/kaggle/train_images_jpeg/543499151.jpg inflating: /content/kaggle/train_images_jpeg/543515988.jpg inflating: /content/kaggle/train_images_jpeg/543588730.jpg inflating: /content/kaggle/train_images_jpeg/543674498.jpg inflating: /content/kaggle/train_images_jpeg/543908014.jpg inflating: /content/kaggle/train_images_jpeg/543908861.jpg inflating: /content/kaggle/train_images_jpeg/544308898.jpg inflating: /content/kaggle/train_images_jpeg/544310124.jpg inflating: /content/kaggle/train_images_jpeg/544346867.jpg inflating: /content/kaggle/train_images_jpeg/544498440.jpg inflating: /content/kaggle/train_images_jpeg/544530143.jpg inflating: /content/kaggle/train_images_jpeg/544532198.jpg inflating: /content/kaggle/train_images_jpeg/544545158.jpg inflating: /content/kaggle/train_images_jpeg/544761733.jpg inflating: /content/kaggle/train_images_jpeg/544785304.jpg inflating: /content/kaggle/train_images_jpeg/544841387.jpg inflating: /content/kaggle/train_images_jpeg/544885077.jpg inflating: /content/kaggle/train_images_jpeg/545003862.jpg inflating: /content/kaggle/train_images_jpeg/545074586.jpg inflating: /content/kaggle/train_images_jpeg/545206870.jpg inflating: /content/kaggle/train_images_jpeg/545499966.jpg inflating: /content/kaggle/train_images_jpeg/545763549.jpg inflating: /content/kaggle/train_images_jpeg/545818494.jpg inflating: /content/kaggle/train_images_jpeg/545843686.jpg inflating: /content/kaggle/train_images_jpeg/545897624.jpg inflating: /content/kaggle/train_images_jpeg/54600142.jpg inflating: /content/kaggle/train_images_jpeg/546003679.jpg inflating: /content/kaggle/train_images_jpeg/546383916.jpg inflating: /content/kaggle/train_images_jpeg/546421963.jpg inflating: /content/kaggle/train_images_jpeg/546528869.jpg inflating: /content/kaggle/train_images_jpeg/546602360.jpg inflating: /content/kaggle/train_images_jpeg/546912616.jpg inflating: /content/kaggle/train_images_jpeg/546931175.jpg inflating: /content/kaggle/train_images_jpeg/546947132.jpg inflating: /content/kaggle/train_images_jpeg/547059239.jpg inflating: /content/kaggle/train_images_jpeg/547270823.jpg inflating: /content/kaggle/train_images_jpeg/547271949.jpg inflating: /content/kaggle/train_images_jpeg/547866885.jpg inflating: /content/kaggle/train_images_jpeg/54787315.jpg inflating: /content/kaggle/train_images_jpeg/548402353.jpg inflating: /content/kaggle/train_images_jpeg/548412421.jpg inflating: /content/kaggle/train_images_jpeg/548597854.jpg inflating: /content/kaggle/train_images_jpeg/548717343.jpg inflating: /content/kaggle/train_images_jpeg/548963583.jpg inflating: /content/kaggle/train_images_jpeg/549334343.jpg inflating: /content/kaggle/train_images_jpeg/549529055.jpg inflating: /content/kaggle/train_images_jpeg/549591801.jpg inflating: /content/kaggle/train_images_jpeg/549750394.jpg inflating: /content/kaggle/train_images_jpeg/549830599.jpg inflating: /content/kaggle/train_images_jpeg/549854027.jpg inflating: /content/kaggle/train_images_jpeg/550133495.jpg inflating: /content/kaggle/train_images_jpeg/550429661.jpg inflating: /content/kaggle/train_images_jpeg/550691642.jpg inflating: /content/kaggle/train_images_jpeg/550742055.jpg inflating: /content/kaggle/train_images_jpeg/550916089.jpg inflating: /content/kaggle/train_images_jpeg/550983047.jpg inflating: /content/kaggle/train_images_jpeg/551051657.jpg inflating: /content/kaggle/train_images_jpeg/5511383.jpg inflating: /content/kaggle/train_images_jpeg/551310653.jpg inflating: /content/kaggle/train_images_jpeg/551716959.jpg inflating: /content/kaggle/train_images_jpeg/551875095.jpg inflating: /content/kaggle/train_images_jpeg/552169332.jpg inflating: /content/kaggle/train_images_jpeg/55220429.jpg inflating: /content/kaggle/train_images_jpeg/552242420.jpg inflating: /content/kaggle/train_images_jpeg/552389666.jpg inflating: /content/kaggle/train_images_jpeg/553154853.jpg inflating: /content/kaggle/train_images_jpeg/553195372.jpg inflating: /content/kaggle/train_images_jpeg/55319830.jpg inflating: /content/kaggle/train_images_jpeg/553204190.jpg inflating: /content/kaggle/train_images_jpeg/553294224.jpg inflating: /content/kaggle/train_images_jpeg/553826173.jpg inflating: /content/kaggle/train_images_jpeg/554118057.jpg inflating: /content/kaggle/train_images_jpeg/554189180.jpg inflating: /content/kaggle/train_images_jpeg/554488826.jpg inflating: /content/kaggle/train_images_jpeg/554618944.jpg inflating: /content/kaggle/train_images_jpeg/5546511.jpg inflating: /content/kaggle/train_images_jpeg/554934001.jpg inflating: /content/kaggle/train_images_jpeg/555028236.jpg inflating: /content/kaggle/train_images_jpeg/555296352.jpg inflating: /content/kaggle/train_images_jpeg/555882055.jpg inflating: /content/kaggle/train_images_jpeg/556066757.jpg inflating: /content/kaggle/train_images_jpeg/556170087.jpg inflating: /content/kaggle/train_images_jpeg/556223341.jpg inflating: /content/kaggle/train_images_jpeg/556542792.jpg inflating: /content/kaggle/train_images_jpeg/556624815.jpg inflating: /content/kaggle/train_images_jpeg/556665949.jpg inflating: /content/kaggle/train_images_jpeg/55670030.jpg inflating: /content/kaggle/train_images_jpeg/557056564.jpg inflating: /content/kaggle/train_images_jpeg/557774617.jpg inflating: /content/kaggle/train_images_jpeg/55799003.jpg inflating: /content/kaggle/train_images_jpeg/559509734.jpg inflating: /content/kaggle/train_images_jpeg/559693099.jpg inflating: /content/kaggle/train_images_jpeg/559984441.jpg inflating: /content/kaggle/train_images_jpeg/560477016.jpg inflating: /content/kaggle/train_images_jpeg/560592460.jpg inflating: /content/kaggle/train_images_jpeg/560888503.jpg inflating: /content/kaggle/train_images_jpeg/561131295.jpg inflating: /content/kaggle/train_images_jpeg/561201584.jpg inflating: /content/kaggle/train_images_jpeg/56135051.jpg inflating: /content/kaggle/train_images_jpeg/561385001.jpg inflating: /content/kaggle/train_images_jpeg/561444501.jpg inflating: /content/kaggle/train_images_jpeg/561647799.jpg inflating: /content/kaggle/train_images_jpeg/561864770.jpg inflating: /content/kaggle/train_images_jpeg/562027159.jpg inflating: /content/kaggle/train_images_jpeg/56205477.jpg inflating: /content/kaggle/train_images_jpeg/562228915.jpg inflating: /content/kaggle/train_images_jpeg/562337477.jpg inflating: /content/kaggle/train_images_jpeg/562822735.jpg inflating: /content/kaggle/train_images_jpeg/563348054.jpg inflating: /content/kaggle/train_images_jpeg/563887175.jpg inflating: /content/kaggle/train_images_jpeg/564364376.jpg inflating: /content/kaggle/train_images_jpeg/564511381.jpg inflating: /content/kaggle/train_images_jpeg/564643932.jpg inflating: /content/kaggle/train_images_jpeg/564925067.jpg inflating: /content/kaggle/train_images_jpeg/565347582.jpg inflating: /content/kaggle/train_images_jpeg/565367936.jpg inflating: /content/kaggle/train_images_jpeg/565965080.jpg inflating: /content/kaggle/train_images_jpeg/566159695.jpg inflating: /content/kaggle/train_images_jpeg/566231170.jpg inflating: /content/kaggle/train_images_jpeg/566279368.jpg inflating: /content/kaggle/train_images_jpeg/5664536.jpg inflating: /content/kaggle/train_images_jpeg/566469674.jpg inflating: /content/kaggle/train_images_jpeg/566542293.jpg inflating: /content/kaggle/train_images_jpeg/566571284.jpg inflating: /content/kaggle/train_images_jpeg/5666225.jpg inflating: /content/kaggle/train_images_jpeg/566719664.jpg inflating: /content/kaggle/train_images_jpeg/567206056.jpg inflating: /content/kaggle/train_images_jpeg/567537807.jpg inflating: /content/kaggle/train_images_jpeg/567779495.jpg inflating: /content/kaggle/train_images_jpeg/56784677.jpg inflating: /content/kaggle/train_images_jpeg/567892299.jpg inflating: /content/kaggle/train_images_jpeg/56813625.jpg inflating: /content/kaggle/train_images_jpeg/568226171.jpg inflating: /content/kaggle/train_images_jpeg/568347975.jpg inflating: /content/kaggle/train_images_jpeg/568411429.jpg inflating: /content/kaggle/train_images_jpeg/568504809.jpg inflating: /content/kaggle/train_images_jpeg/568585262.jpg inflating: /content/kaggle/train_images_jpeg/568805661.jpg inflating: /content/kaggle/train_images_jpeg/568949064.jpg inflating: /content/kaggle/train_images_jpeg/569345273.jpg inflating: /content/kaggle/train_images_jpeg/569545786.jpg inflating: /content/kaggle/train_images_jpeg/569609163.jpg inflating: /content/kaggle/train_images_jpeg/569842713.jpg inflating: /content/kaggle/train_images_jpeg/570097057.jpg inflating: /content/kaggle/train_images_jpeg/57023383.jpg inflating: /content/kaggle/train_images_jpeg/570348622.jpg inflating: /content/kaggle/train_images_jpeg/570526420.jpg inflating: /content/kaggle/train_images_jpeg/570889595.jpg inflating: /content/kaggle/train_images_jpeg/57099592.jpg inflating: /content/kaggle/train_images_jpeg/571006343.jpg inflating: /content/kaggle/train_images_jpeg/571189248.jpg inflating: /content/kaggle/train_images_jpeg/571328280.jpg inflating: /content/kaggle/train_images_jpeg/57149651.jpg inflating: /content/kaggle/train_images_jpeg/571576955.jpg inflating: /content/kaggle/train_images_jpeg/571732176.jpg inflating: /content/kaggle/train_images_jpeg/572003019.jpg inflating: /content/kaggle/train_images_jpeg/572073587.jpg inflating: /content/kaggle/train_images_jpeg/572515769.jpg inflating: /content/kaggle/train_images_jpeg/572739477.jpg inflating: /content/kaggle/train_images_jpeg/573039129.jpg inflating: /content/kaggle/train_images_jpeg/573324267.jpg inflating: /content/kaggle/train_images_jpeg/573623784.jpg inflating: /content/kaggle/train_images_jpeg/573832722.jpg inflating: /content/kaggle/train_images_jpeg/573879422.jpg inflating: /content/kaggle/train_images_jpeg/574000840.jpg inflating: /content/kaggle/train_images_jpeg/574578186.jpg inflating: /content/kaggle/train_images_jpeg/575082952.jpg inflating: /content/kaggle/train_images_jpeg/575098680.jpg inflating: /content/kaggle/train_images_jpeg/575135832.jpg inflating: /content/kaggle/train_images_jpeg/575181306.jpg inflating: /content/kaggle/train_images_jpeg/575338550.jpg inflating: /content/kaggle/train_images_jpeg/575396227.jpg inflating: /content/kaggle/train_images_jpeg/575428243.jpg inflating: /content/kaggle/train_images_jpeg/576478448.jpg inflating: /content/kaggle/train_images_jpeg/57665451.jpg inflating: /content/kaggle/train_images_jpeg/577090506.jpg inflating: /content/kaggle/train_images_jpeg/577274696.jpg inflating: /content/kaggle/train_images_jpeg/577275229.jpg inflating: /content/kaggle/train_images_jpeg/577684636.jpg inflating: /content/kaggle/train_images_jpeg/577741069.jpg inflating: /content/kaggle/train_images_jpeg/578080012.jpg inflating: /content/kaggle/train_images_jpeg/578106393.jpg inflating: /content/kaggle/train_images_jpeg/578279237.jpg inflating: /content/kaggle/train_images_jpeg/57861623.jpg inflating: /content/kaggle/train_images_jpeg/578646228.jpg inflating: /content/kaggle/train_images_jpeg/57892387.jpg inflating: /content/kaggle/train_images_jpeg/579280251.jpg inflating: /content/kaggle/train_images_jpeg/579333790.jpg inflating: /content/kaggle/train_images_jpeg/579336785.jpg inflating: /content/kaggle/train_images_jpeg/579498969.jpg inflating: /content/kaggle/train_images_jpeg/580100046.jpg inflating: /content/kaggle/train_images_jpeg/580111608.jpg inflating: /content/kaggle/train_images_jpeg/580293785.jpg inflating: /content/kaggle/train_images_jpeg/580516102.jpg inflating: /content/kaggle/train_images_jpeg/580550239.jpg inflating: /content/kaggle/train_images_jpeg/580838214.jpg inflating: /content/kaggle/train_images_jpeg/580859644.jpg inflating: /content/kaggle/train_images_jpeg/581030887.jpg inflating: /content/kaggle/train_images_jpeg/581164870.jpg inflating: /content/kaggle/train_images_jpeg/581179733.jpg inflating: /content/kaggle/train_images_jpeg/581235562.jpg inflating: /content/kaggle/train_images_jpeg/581749808.jpg inflating: /content/kaggle/train_images_jpeg/581902083.jpg inflating: /content/kaggle/train_images_jpeg/582120514.jpg inflating: /content/kaggle/train_images_jpeg/582179912.jpg inflating: /content/kaggle/train_images_jpeg/582352615.jpg inflating: /content/kaggle/train_images_jpeg/582499050.jpg inflating: /content/kaggle/train_images_jpeg/582719754.jpg inflating: /content/kaggle/train_images_jpeg/582948478.jpg inflating: /content/kaggle/train_images_jpeg/58390189.jpg inflating: /content/kaggle/train_images_jpeg/584367116.jpg inflating: /content/kaggle/train_images_jpeg/58446146.jpg inflating: /content/kaggle/train_images_jpeg/584513475.jpg inflating: /content/kaggle/train_images_jpeg/584742887.jpg inflating: /content/kaggle/train_images_jpeg/584853244.jpg inflating: /content/kaggle/train_images_jpeg/584996062.jpg inflating: /content/kaggle/train_images_jpeg/585067161.jpg inflating: /content/kaggle/train_images_jpeg/585209164.jpg inflating: /content/kaggle/train_images_jpeg/585228910.jpg inflating: /content/kaggle/train_images_jpeg/58528063.jpg inflating: /content/kaggle/train_images_jpeg/58559275.jpg inflating: /content/kaggle/train_images_jpeg/585813976.jpg inflating: /content/kaggle/train_images_jpeg/585953153.jpg inflating: /content/kaggle/train_images_jpeg/586054705.jpg inflating: /content/kaggle/train_images_jpeg/586116935.jpg inflating: /content/kaggle/train_images_jpeg/586367885.jpg inflating: /content/kaggle/train_images_jpeg/586470698.jpg inflating: /content/kaggle/train_images_jpeg/586485536.jpg inflating: /content/kaggle/train_images_jpeg/58667011.jpg inflating: /content/kaggle/train_images_jpeg/586687554.jpg inflating: /content/kaggle/train_images_jpeg/586808428.jpg inflating: /content/kaggle/train_images_jpeg/587011215.jpg inflating: /content/kaggle/train_images_jpeg/587043810.jpg inflating: /content/kaggle/train_images_jpeg/58720038.jpg inflating: /content/kaggle/train_images_jpeg/587329916.jpg inflating: /content/kaggle/train_images_jpeg/587410113.jpg inflating: /content/kaggle/train_images_jpeg/587420634.jpg inflating: /content/kaggle/train_images_jpeg/587620501.jpg inflating: /content/kaggle/train_images_jpeg/587692768.jpg inflating: /content/kaggle/train_images_jpeg/58780369.jpg inflating: /content/kaggle/train_images_jpeg/587829607.jpg inflating: /content/kaggle/train_images_jpeg/588030213.jpg inflating: /content/kaggle/train_images_jpeg/588169966.jpg inflating: /content/kaggle/train_images_jpeg/588425651.jpg inflating: /content/kaggle/train_images_jpeg/588893932.jpg inflating: /content/kaggle/train_images_jpeg/589122135.jpg inflating: /content/kaggle/train_images_jpeg/589247571.jpg inflating: /content/kaggle/train_images_jpeg/589734688.jpg inflating: /content/kaggle/train_images_jpeg/5912799.jpg inflating: /content/kaggle/train_images_jpeg/591335475.jpg inflating: /content/kaggle/train_images_jpeg/591372512.jpg inflating: /content/kaggle/train_images_jpeg/591721064.jpg inflating: /content/kaggle/train_images_jpeg/591741499.jpg inflating: /content/kaggle/train_images_jpeg/591760666.jpg inflating: /content/kaggle/train_images_jpeg/591792170.jpg inflating: /content/kaggle/train_images_jpeg/591802054.jpg inflating: /content/kaggle/train_images_jpeg/59208991.jpg inflating: /content/kaggle/train_images_jpeg/592098292.jpg inflating: /content/kaggle/train_images_jpeg/592236552.jpg inflating: /content/kaggle/train_images_jpeg/592251506.jpg inflating: /content/kaggle/train_images_jpeg/592322076.jpg inflating: /content/kaggle/train_images_jpeg/592477386.jpg inflating: /content/kaggle/train_images_jpeg/592844774.jpg inflating: /content/kaggle/train_images_jpeg/592978315.jpg inflating: /content/kaggle/train_images_jpeg/593092759.jpg inflating: /content/kaggle/train_images_jpeg/593244106.jpg inflating: /content/kaggle/train_images_jpeg/593295166.jpg inflating: /content/kaggle/train_images_jpeg/593922825.jpg inflating: /content/kaggle/train_images_jpeg/593986608.jpg inflating: /content/kaggle/train_images_jpeg/594027454.jpg inflating: /content/kaggle/train_images_jpeg/594438051.jpg inflating: /content/kaggle/train_images_jpeg/595285213.jpg inflating: /content/kaggle/train_images_jpeg/595384865.jpg inflating: /content/kaggle/train_images_jpeg/595567358.jpg inflating: /content/kaggle/train_images_jpeg/595787428.jpg inflating: /content/kaggle/train_images_jpeg/595835562.jpg inflating: /content/kaggle/train_images_jpeg/595909449.jpg inflating: /content/kaggle/train_images_jpeg/596124693.jpg inflating: /content/kaggle/train_images_jpeg/596572459.jpg inflating: /content/kaggle/train_images_jpeg/596602212.jpg inflating: /content/kaggle/train_images_jpeg/596669967.jpg inflating: /content/kaggle/train_images_jpeg/596706595.jpg inflating: /content/kaggle/train_images_jpeg/59686251.jpg inflating: /content/kaggle/train_images_jpeg/596964772.jpg inflating: /content/kaggle/train_images_jpeg/597003847.jpg inflating: /content/kaggle/train_images_jpeg/597213250.jpg inflating: /content/kaggle/train_images_jpeg/597269294.jpg inflating: /content/kaggle/train_images_jpeg/597389720.jpg inflating: /content/kaggle/train_images_jpeg/597527186.jpg inflating: /content/kaggle/train_images_jpeg/598088658.jpg inflating: /content/kaggle/train_images_jpeg/598311175.jpg inflating: /content/kaggle/train_images_jpeg/598574502.jpg inflating: /content/kaggle/train_images_jpeg/598720270.jpg inflating: /content/kaggle/train_images_jpeg/598746218.jpg inflating: /content/kaggle/train_images_jpeg/59894282.jpg inflating: /content/kaggle/train_images_jpeg/59901561.jpg inflating: /content/kaggle/train_images_jpeg/599325048.jpg inflating: /content/kaggle/train_images_jpeg/599387864.jpg inflating: /content/kaggle/train_images_jpeg/599867773.jpg inflating: /content/kaggle/train_images_jpeg/599872588.jpg inflating: /content/kaggle/train_images_jpeg/600110842.jpg inflating: /content/kaggle/train_images_jpeg/60029813.jpg inflating: /content/kaggle/train_images_jpeg/60038493.jpg inflating: /content/kaggle/train_images_jpeg/600422438.jpg inflating: /content/kaggle/train_images_jpeg/600610735.jpg inflating: /content/kaggle/train_images_jpeg/600691544.jpg inflating: /content/kaggle/train_images_jpeg/600736721.jpg inflating: /content/kaggle/train_images_jpeg/600869262.jpg inflating: /content/kaggle/train_images_jpeg/60094859.jpg inflating: /content/kaggle/train_images_jpeg/601096842.jpg inflating: /content/kaggle/train_images_jpeg/601263353.jpg inflating: /content/kaggle/train_images_jpeg/601644904.jpg inflating: /content/kaggle/train_images_jpeg/602031471.jpg inflating: /content/kaggle/train_images_jpeg/602562857.jpg inflating: /content/kaggle/train_images_jpeg/602798038.jpg inflating: /content/kaggle/train_images_jpeg/602912992.jpg inflating: /content/kaggle/train_images_jpeg/603736760.jpg inflating: /content/kaggle/train_images_jpeg/603756866.jpg inflating: /content/kaggle/train_images_jpeg/604445173.jpg inflating: /content/kaggle/train_images_jpeg/604476990.jpg inflating: /content/kaggle/train_images_jpeg/60495599.jpg inflating: /content/kaggle/train_images_jpeg/605088351.jpg inflating: /content/kaggle/train_images_jpeg/605469419.jpg inflating: /content/kaggle/train_images_jpeg/605794963.jpg inflating: /content/kaggle/train_images_jpeg/605816056.jpg inflating: /content/kaggle/train_images_jpeg/605921029.jpg inflating: /content/kaggle/train_images_jpeg/606006336.jpg inflating: /content/kaggle/train_images_jpeg/606101170.jpg inflating: /content/kaggle/train_images_jpeg/606601717.jpg inflating: /content/kaggle/train_images_jpeg/606691167.jpg inflating: /content/kaggle/train_images_jpeg/606831482.jpg inflating: /content/kaggle/train_images_jpeg/606945473.jpg inflating: /content/kaggle/train_images_jpeg/607008468.jpg inflating: /content/kaggle/train_images_jpeg/607067900.jpg inflating: /content/kaggle/train_images_jpeg/607207422.jpg inflating: /content/kaggle/train_images_jpeg/607627857.jpg inflating: /content/kaggle/train_images_jpeg/607734947.jpg inflating: /content/kaggle/train_images_jpeg/607840807.jpg inflating: /content/kaggle/train_images_jpeg/607886591.jpg inflating: /content/kaggle/train_images_jpeg/608243264.jpg inflating: /content/kaggle/train_images_jpeg/608264330.jpg inflating: /content/kaggle/train_images_jpeg/608266906.jpg inflating: /content/kaggle/train_images_jpeg/608792631.jpg inflating: /content/kaggle/train_images_jpeg/609027712.jpg inflating: /content/kaggle/train_images_jpeg/609134640.jpg inflating: /content/kaggle/train_images_jpeg/609358362.jpg inflating: /content/kaggle/train_images_jpeg/609480856.jpg inflating: /content/kaggle/train_images_jpeg/609857978.jpg inflating: /content/kaggle/train_images_jpeg/610094717.jpg inflating: /content/kaggle/train_images_jpeg/6103.jpg inflating: /content/kaggle/train_images_jpeg/61044687.jpg inflating: /content/kaggle/train_images_jpeg/61074408.jpg inflating: /content/kaggle/train_images_jpeg/610968546.jpg inflating: /content/kaggle/train_images_jpeg/611122414.jpg inflating: /content/kaggle/train_images_jpeg/61139702.jpg inflating: /content/kaggle/train_images_jpeg/611507457.jpg inflating: /content/kaggle/train_images_jpeg/611582421.jpg inflating: /content/kaggle/train_images_jpeg/611767283.jpg inflating: /content/kaggle/train_images_jpeg/611831978.jpg inflating: /content/kaggle/train_images_jpeg/611858169.jpg inflating: /content/kaggle/train_images_jpeg/612054401.jpg inflating: /content/kaggle/train_images_jpeg/61216809.jpg inflating: /content/kaggle/train_images_jpeg/612223117.jpg inflating: /content/kaggle/train_images_jpeg/612296656.jpg inflating: /content/kaggle/train_images_jpeg/612426444.jpg inflating: /content/kaggle/train_images_jpeg/612647551.jpg inflating: /content/kaggle/train_images_jpeg/612680278.jpg inflating: /content/kaggle/train_images_jpeg/612701263.jpg inflating: /content/kaggle/train_images_jpeg/612816896.jpg inflating: /content/kaggle/train_images_jpeg/61297107.jpg inflating: /content/kaggle/train_images_jpeg/613168943.jpg inflating: /content/kaggle/train_images_jpeg/61357519.jpg inflating: /content/kaggle/train_images_jpeg/613608531.jpg inflating: /content/kaggle/train_images_jpeg/61393807.jpg inflating: /content/kaggle/train_images_jpeg/614085740.jpg inflating: /content/kaggle/train_images_jpeg/614181171.jpg inflating: /content/kaggle/train_images_jpeg/614215074.jpg inflating: /content/kaggle/train_images_jpeg/614350420.jpg inflating: /content/kaggle/train_images_jpeg/614363476.jpg inflating: /content/kaggle/train_images_jpeg/614423071.jpg inflating: /content/kaggle/train_images_jpeg/614554751.jpg inflating: /content/kaggle/train_images_jpeg/61492497.jpg inflating: /content/kaggle/train_images_jpeg/615415014.jpg inflating: /content/kaggle/train_images_jpeg/616067166.jpg inflating: /content/kaggle/train_images_jpeg/616109576.jpg inflating: /content/kaggle/train_images_jpeg/616514000.jpg inflating: /content/kaggle/train_images_jpeg/616531930.jpg inflating: /content/kaggle/train_images_jpeg/616718743.jpg inflating: /content/kaggle/train_images_jpeg/616740640.jpg inflating: /content/kaggle/train_images_jpeg/616745238.jpg inflating: /content/kaggle/train_images_jpeg/616889374.jpg inflating: /content/kaggle/train_images_jpeg/616937230.jpg inflating: /content/kaggle/train_images_jpeg/617446207.jpg inflating: /content/kaggle/train_images_jpeg/617446787.jpg inflating: /content/kaggle/train_images_jpeg/617651411.jpg inflating: /content/kaggle/train_images_jpeg/617719193.jpg inflating: /content/kaggle/train_images_jpeg/617763677.jpg inflating: /content/kaggle/train_images_jpeg/618049831.jpg inflating: /content/kaggle/train_images_jpeg/618242268.jpg inflating: /content/kaggle/train_images_jpeg/618257527.jpg inflating: /content/kaggle/train_images_jpeg/618285041.jpg inflating: /content/kaggle/train_images_jpeg/618525974.jpg inflating: /content/kaggle/train_images_jpeg/618654809.jpg inflating: /content/kaggle/train_images_jpeg/618683385.jpg inflating: /content/kaggle/train_images_jpeg/618816554.jpg inflating: /content/kaggle/train_images_jpeg/618916143.jpg inflating: /content/kaggle/train_images_jpeg/61901680.jpg inflating: /content/kaggle/train_images_jpeg/619095657.jpg inflating: /content/kaggle/train_images_jpeg/619372394.jpg inflating: /content/kaggle/train_images_jpeg/619658845.jpg inflating: /content/kaggle/train_images_jpeg/619953066.jpg inflating: /content/kaggle/train_images_jpeg/620126996.jpg inflating: /content/kaggle/train_images_jpeg/620158263.jpg inflating: /content/kaggle/train_images_jpeg/62053025.jpg inflating: /content/kaggle/train_images_jpeg/62069424.jpg inflating: /content/kaggle/train_images_jpeg/62070739.jpg inflating: /content/kaggle/train_images_jpeg/620741082.jpg inflating: /content/kaggle/train_images_jpeg/620752009.jpg inflating: /content/kaggle/train_images_jpeg/620790397.jpg inflating: /content/kaggle/train_images_jpeg/620825319.jpg inflating: /content/kaggle/train_images_jpeg/620927832.jpg inflating: /content/kaggle/train_images_jpeg/621125183.jpg inflating: /content/kaggle/train_images_jpeg/621131184.jpg inflating: /content/kaggle/train_images_jpeg/621367062.jpg inflating: /content/kaggle/train_images_jpeg/621642656.jpg inflating: /content/kaggle/train_images_jpeg/622130666.jpg inflating: /content/kaggle/train_images_jpeg/622186063.jpg inflating: /content/kaggle/train_images_jpeg/622285967.jpg inflating: /content/kaggle/train_images_jpeg/622386644.jpg inflating: /content/kaggle/train_images_jpeg/622555654.jpg inflating: /content/kaggle/train_images_jpeg/622587109.jpg inflating: /content/kaggle/train_images_jpeg/622599265.jpg inflating: /content/kaggle/train_images_jpeg/623023838.jpg inflating: /content/kaggle/train_images_jpeg/623333046.jpg inflating: /content/kaggle/train_images_jpeg/623381434.jpg inflating: /content/kaggle/train_images_jpeg/623631977.jpg inflating: /content/kaggle/train_images_jpeg/624542244.jpg inflating: /content/kaggle/train_images_jpeg/62457077.jpg inflating: /content/kaggle/train_images_jpeg/624872321.jpg inflating: /content/kaggle/train_images_jpeg/624911036.jpg inflating: /content/kaggle/train_images_jpeg/624921461.jpg inflating: /content/kaggle/train_images_jpeg/625159901.jpg inflating: /content/kaggle/train_images_jpeg/625250328.jpg inflating: /content/kaggle/train_images_jpeg/625258862.jpg inflating: /content/kaggle/train_images_jpeg/625604215.jpg inflating: /content/kaggle/train_images_jpeg/626727559.jpg inflating: /content/kaggle/train_images_jpeg/626955644.jpg inflating: /content/kaggle/train_images_jpeg/627068686.jpg inflating: /content/kaggle/train_images_jpeg/627428671.jpg inflating: /content/kaggle/train_images_jpeg/62771226.jpg inflating: /content/kaggle/train_images_jpeg/628074703.jpg inflating: /content/kaggle/train_images_jpeg/628142575.jpg inflating: /content/kaggle/train_images_jpeg/628546721.jpg inflating: /content/kaggle/train_images_jpeg/628791743.jpg inflating: /content/kaggle/train_images_jpeg/628889573.jpg inflating: /content/kaggle/train_images_jpeg/629341978.jpg inflating: /content/kaggle/train_images_jpeg/629637059.jpg inflating: /content/kaggle/train_images_jpeg/629698342.jpg inflating: /content/kaggle/train_images_jpeg/629896725.jpg inflating: /content/kaggle/train_images_jpeg/629958689.jpg inflating: /content/kaggle/train_images_jpeg/630211324.jpg inflating: /content/kaggle/train_images_jpeg/630240307.jpg inflating: /content/kaggle/train_images_jpeg/630407730.jpg inflating: /content/kaggle/train_images_jpeg/630454798.jpg inflating: /content/kaggle/train_images_jpeg/630537916.jpg inflating: /content/kaggle/train_images_jpeg/630797550.jpg inflating: /content/kaggle/train_images_jpeg/630798966.jpg inflating: /content/kaggle/train_images_jpeg/631049662.jpg inflating: /content/kaggle/train_images_jpeg/631221436.jpg inflating: /content/kaggle/train_images_jpeg/631309846.jpg inflating: /content/kaggle/train_images_jpeg/631325296.jpg inflating: /content/kaggle/train_images_jpeg/631430009.jpg inflating: /content/kaggle/train_images_jpeg/631563709.jpg inflating: /content/kaggle/train_images_jpeg/63159897.jpg inflating: /content/kaggle/train_images_jpeg/631607680.jpg inflating: /content/kaggle/train_images_jpeg/631676303.jpg inflating: /content/kaggle/train_images_jpeg/631746612.jpg inflating: /content/kaggle/train_images_jpeg/631859689.jpg inflating: /content/kaggle/train_images_jpeg/631890170.jpg inflating: /content/kaggle/train_images_jpeg/631967336.jpg inflating: /content/kaggle/train_images_jpeg/632197705.jpg inflating: /content/kaggle/train_images_jpeg/632236115.jpg inflating: /content/kaggle/train_images_jpeg/632357010.jpg inflating: /content/kaggle/train_images_jpeg/632417341.jpg inflating: /content/kaggle/train_images_jpeg/632445353.jpg inflating: /content/kaggle/train_images_jpeg/632517607.jpg inflating: /content/kaggle/train_images_jpeg/632697564.jpg inflating: /content/kaggle/train_images_jpeg/632799335.jpg inflating: /content/kaggle/train_images_jpeg/633010478.jpg inflating: /content/kaggle/train_images_jpeg/633081194.jpg inflating: /content/kaggle/train_images_jpeg/633900413.jpg inflating: /content/kaggle/train_images_jpeg/633946965.jpg inflating: /content/kaggle/train_images_jpeg/634365236.jpg inflating: /content/kaggle/train_images_jpeg/634618230.jpg inflating: /content/kaggle/train_images_jpeg/634724499.jpg inflating: /content/kaggle/train_images_jpeg/634785695.jpg inflating: /content/kaggle/train_images_jpeg/634792920.jpg inflating: /content/kaggle/train_images_jpeg/634816572.jpg inflating: /content/kaggle/train_images_jpeg/635122883.jpg inflating: /content/kaggle/train_images_jpeg/635279232.jpg inflating: /content/kaggle/train_images_jpeg/635341993.jpg inflating: /content/kaggle/train_images_jpeg/635511735.jpg inflating: /content/kaggle/train_images_jpeg/635576141.jpg inflating: /content/kaggle/train_images_jpeg/635664264.jpg inflating: /content/kaggle/train_images_jpeg/63578884.jpg inflating: /content/kaggle/train_images_jpeg/635946158.jpg inflating: /content/kaggle/train_images_jpeg/636286001.jpg inflating: /content/kaggle/train_images_jpeg/636474296.jpg inflating: /content/kaggle/train_images_jpeg/636559480.jpg inflating: /content/kaggle/train_images_jpeg/636816861.jpg inflating: /content/kaggle/train_images_jpeg/636890802.jpg inflating: /content/kaggle/train_images_jpeg/636948047.jpg inflating: /content/kaggle/train_images_jpeg/63775422.jpg inflating: /content/kaggle/train_images_jpeg/637906306.jpg inflating: /content/kaggle/train_images_jpeg/638654515.jpg inflating: /content/kaggle/train_images_jpeg/639060341.jpg inflating: /content/kaggle/train_images_jpeg/639068838.jpg inflating: /content/kaggle/train_images_jpeg/639229623.jpg inflating: /content/kaggle/train_images_jpeg/640157848.jpg inflating: /content/kaggle/train_images_jpeg/640668148.jpg inflating: /content/kaggle/train_images_jpeg/640671691.jpg inflating: /content/kaggle/train_images_jpeg/640674729.jpg inflating: /content/kaggle/train_images_jpeg/641096322.jpg inflating: /content/kaggle/train_images_jpeg/641309732.jpg inflating: /content/kaggle/train_images_jpeg/641583443.jpg inflating: /content/kaggle/train_images_jpeg/641590483.jpg inflating: /content/kaggle/train_images_jpeg/641625656.jpg inflating: /content/kaggle/train_images_jpeg/641852090.jpg inflating: /content/kaggle/train_images_jpeg/641947553.jpg inflating: /content/kaggle/train_images_jpeg/642333740.jpg inflating: /content/kaggle/train_images_jpeg/64287317.jpg inflating: /content/kaggle/train_images_jpeg/643008801.jpg inflating: /content/kaggle/train_images_jpeg/643044892.jpg inflating: /content/kaggle/train_images_jpeg/643163594.jpg inflating: /content/kaggle/train_images_jpeg/643541195.jpg inflating: /content/kaggle/train_images_jpeg/643613398.jpg inflating: /content/kaggle/train_images_jpeg/643956994.jpg inflating: /content/kaggle/train_images_jpeg/644131683.jpg inflating: /content/kaggle/train_images_jpeg/644212609.jpg inflating: /content/kaggle/train_images_jpeg/6442965.jpg inflating: /content/kaggle/train_images_jpeg/644617197.jpg inflating: /content/kaggle/train_images_jpeg/644666904.jpg inflating: /content/kaggle/train_images_jpeg/645139700.jpg inflating: /content/kaggle/train_images_jpeg/645166434.jpg inflating: /content/kaggle/train_images_jpeg/645203852.jpg inflating: /content/kaggle/train_images_jpeg/645674702.jpg inflating: /content/kaggle/train_images_jpeg/645726700.jpg inflating: /content/kaggle/train_images_jpeg/645748752.jpg inflating: /content/kaggle/train_images_jpeg/645764425.jpg inflating: /content/kaggle/train_images_jpeg/645991519.jpg inflating: /content/kaggle/train_images_jpeg/64610911.jpg inflating: /content/kaggle/train_images_jpeg/646595988.jpg inflating: /content/kaggle/train_images_jpeg/646796085.jpg inflating: /content/kaggle/train_images_jpeg/646841236.jpg inflating: /content/kaggle/train_images_jpeg/646903835.jpg inflating: /content/kaggle/train_images_jpeg/646999993.jpg inflating: /content/kaggle/train_images_jpeg/647038320.jpg inflating: /content/kaggle/train_images_jpeg/647141648.jpg inflating: /content/kaggle/train_images_jpeg/647301407.jpg inflating: /content/kaggle/train_images_jpeg/64732457.jpg inflating: /content/kaggle/train_images_jpeg/647379181.jpg inflating: /content/kaggle/train_images_jpeg/6477704.jpg inflating: /content/kaggle/train_images_jpeg/648013564.jpg inflating: /content/kaggle/train_images_jpeg/648328136.jpg inflating: /content/kaggle/train_images_jpeg/648438978.jpg inflating: /content/kaggle/train_images_jpeg/648490394.jpg inflating: /content/kaggle/train_images_jpeg/648535358.jpg inflating: /content/kaggle/train_images_jpeg/648590846.jpg inflating: /content/kaggle/train_images_jpeg/648616108.jpg inflating: /content/kaggle/train_images_jpeg/648870866.jpg inflating: /content/kaggle/train_images_jpeg/649044628.jpg inflating: /content/kaggle/train_images_jpeg/649463618.jpg inflating: /content/kaggle/train_images_jpeg/650372735.jpg inflating: /content/kaggle/train_images_jpeg/650458335.jpg inflating: /content/kaggle/train_images_jpeg/650816529.jpg inflating: /content/kaggle/train_images_jpeg/650963380.jpg inflating: /content/kaggle/train_images_jpeg/651017189.jpg inflating: /content/kaggle/train_images_jpeg/651041869.jpg inflating: /content/kaggle/train_images_jpeg/651123589.jpg inflating: /content/kaggle/train_images_jpeg/651255888.jpg inflating: /content/kaggle/train_images_jpeg/651331224.jpg inflating: /content/kaggle/train_images_jpeg/65139094.jpg inflating: /content/kaggle/train_images_jpeg/651635547.jpg inflating: /content/kaggle/train_images_jpeg/651791438.jpg inflating: /content/kaggle/train_images_jpeg/652147847.jpg inflating: /content/kaggle/train_images_jpeg/65235972.jpg inflating: /content/kaggle/train_images_jpeg/652381513.jpg inflating: /content/kaggle/train_images_jpeg/652624585.jpg inflating: /content/kaggle/train_images_jpeg/65328715.jpg inflating: /content/kaggle/train_images_jpeg/65344468.jpg inflating: /content/kaggle/train_images_jpeg/653454326.jpg inflating: /content/kaggle/train_images_jpeg/653465782.jpg inflating: /content/kaggle/train_images_jpeg/653660984.jpg inflating: /content/kaggle/train_images_jpeg/653790605.jpg inflating: /content/kaggle/train_images_jpeg/653958117.jpg inflating: /content/kaggle/train_images_jpeg/654307881.jpg inflating: /content/kaggle/train_images_jpeg/654346166.jpg inflating: /content/kaggle/train_images_jpeg/654445822.jpg inflating: /content/kaggle/train_images_jpeg/654671746.jpg inflating: /content/kaggle/train_images_jpeg/654782602.jpg inflating: /content/kaggle/train_images_jpeg/654992578.jpg inflating: /content/kaggle/train_images_jpeg/655836057.jpg inflating: /content/kaggle/train_images_jpeg/655873382.jpg inflating: /content/kaggle/train_images_jpeg/656522898.jpg inflating: /content/kaggle/train_images_jpeg/65671451.jpg inflating: /content/kaggle/train_images_jpeg/657031133.jpg inflating: /content/kaggle/train_images_jpeg/657278738.jpg inflating: /content/kaggle/train_images_jpeg/657403248.jpg inflating: /content/kaggle/train_images_jpeg/657424989.jpg inflating: /content/kaggle/train_images_jpeg/657542940.jpg inflating: /content/kaggle/train_images_jpeg/657747231.jpg inflating: /content/kaggle/train_images_jpeg/657747378.jpg inflating: /content/kaggle/train_images_jpeg/657817116.jpg inflating: /content/kaggle/train_images_jpeg/657823258.jpg inflating: /content/kaggle/train_images_jpeg/657935270.jpg inflating: /content/kaggle/train_images_jpeg/658043427.jpg inflating: /content/kaggle/train_images_jpeg/658285764.jpg inflating: /content/kaggle/train_images_jpeg/658300616.jpg inflating: /content/kaggle/train_images_jpeg/658395701.jpg inflating: /content/kaggle/train_images_jpeg/658617440.jpg inflating: /content/kaggle/train_images_jpeg/658961090.jpg inflating: /content/kaggle/train_images_jpeg/659315200.jpg inflating: /content/kaggle/train_images_jpeg/65949096.jpg inflating: /content/kaggle/train_images_jpeg/659495074.jpg inflating: /content/kaggle/train_images_jpeg/659615367.jpg inflating: /content/kaggle/train_images_jpeg/659696981.jpg inflating: /content/kaggle/train_images_jpeg/659737430.jpg inflating: /content/kaggle/train_images_jpeg/660261179.jpg inflating: /content/kaggle/train_images_jpeg/660273630.jpg inflating: /content/kaggle/train_images_jpeg/660374182.jpg inflating: /content/kaggle/train_images_jpeg/660699898.jpg inflating: /content/kaggle/train_images_jpeg/660715584.jpg inflating: /content/kaggle/train_images_jpeg/660729197.jpg inflating: /content/kaggle/train_images_jpeg/66116561.jpg inflating: /content/kaggle/train_images_jpeg/661256835.jpg inflating: /content/kaggle/train_images_jpeg/66126211.jpg inflating: /content/kaggle/train_images_jpeg/661263276.jpg inflating: /content/kaggle/train_images_jpeg/661610886.jpg inflating: /content/kaggle/train_images_jpeg/661795879.jpg inflating: /content/kaggle/train_images_jpeg/662050128.jpg inflating: /content/kaggle/train_images_jpeg/662317211.jpg inflating: /content/kaggle/train_images_jpeg/662372109.jpg inflating: /content/kaggle/train_images_jpeg/66240294.jpg inflating: /content/kaggle/train_images_jpeg/662783465.jpg inflating: /content/kaggle/train_images_jpeg/662844751.jpg inflating: /content/kaggle/train_images_jpeg/663146106.jpg inflating: /content/kaggle/train_images_jpeg/663803096.jpg inflating: /content/kaggle/train_images_jpeg/663810012.jpg inflating: /content/kaggle/train_images_jpeg/664024610.jpg inflating: /content/kaggle/train_images_jpeg/664137344.jpg inflating: /content/kaggle/train_images_jpeg/664265007.jpg inflating: /content/kaggle/train_images_jpeg/664408485.jpg inflating: /content/kaggle/train_images_jpeg/66458318.jpg inflating: /content/kaggle/train_images_jpeg/664583876.jpg inflating: /content/kaggle/train_images_jpeg/66494240.jpg inflating: /content/kaggle/train_images_jpeg/664943933.jpg inflating: /content/kaggle/train_images_jpeg/664996949.jpg inflating: /content/kaggle/train_images_jpeg/665478889.jpg inflating: /content/kaggle/train_images_jpeg/66571398.jpg inflating: /content/kaggle/train_images_jpeg/665859596.jpg inflating: /content/kaggle/train_images_jpeg/666057688.jpg inflating: /content/kaggle/train_images_jpeg/666145174.jpg inflating: /content/kaggle/train_images_jpeg/667058681.jpg inflating: /content/kaggle/train_images_jpeg/667064229.jpg inflating: /content/kaggle/train_images_jpeg/66720017.jpg inflating: /content/kaggle/train_images_jpeg/667282886.jpg inflating: /content/kaggle/train_images_jpeg/667745883.jpg inflating: /content/kaggle/train_images_jpeg/667790094.jpg inflating: /content/kaggle/train_images_jpeg/668151635.jpg inflating: /content/kaggle/train_images_jpeg/66820054.jpg inflating: /content/kaggle/train_images_jpeg/668811521.jpg inflating: /content/kaggle/train_images_jpeg/668943288.jpg inflating: /content/kaggle/train_images_jpeg/669119596.jpg inflating: /content/kaggle/train_images_jpeg/669360547.jpg inflating: /content/kaggle/train_images_jpeg/669612316.jpg inflating: /content/kaggle/train_images_jpeg/669748740.jpg inflating: /content/kaggle/train_images_jpeg/670049521.jpg inflating: /content/kaggle/train_images_jpeg/670363237.jpg inflating: /content/kaggle/train_images_jpeg/670519295.jpg inflating: /content/kaggle/train_images_jpeg/670541081.jpg inflating: /content/kaggle/train_images_jpeg/670606517.jpg inflating: /content/kaggle/train_images_jpeg/670877458.jpg inflating: /content/kaggle/train_images_jpeg/670935164.jpg inflating: /content/kaggle/train_images_jpeg/670991329.jpg inflating: /content/kaggle/train_images_jpeg/671076744.jpg inflating: /content/kaggle/train_images_jpeg/671675016.jpg inflating: /content/kaggle/train_images_jpeg/671677199.jpg inflating: /content/kaggle/train_images_jpeg/671906195.jpg inflating: /content/kaggle/train_images_jpeg/671937589.jpg inflating: /content/kaggle/train_images_jpeg/671949861.jpg inflating: /content/kaggle/train_images_jpeg/672149291.jpg inflating: /content/kaggle/train_images_jpeg/67235681.jpg inflating: /content/kaggle/train_images_jpeg/672387843.jpg inflating: /content/kaggle/train_images_jpeg/672634169.jpg inflating: /content/kaggle/train_images_jpeg/672737517.jpg inflating: /content/kaggle/train_images_jpeg/672772302.jpg inflating: /content/kaggle/train_images_jpeg/673461056.jpg inflating: /content/kaggle/train_images_jpeg/673860639.jpg inflating: /content/kaggle/train_images_jpeg/673868311.jpg inflating: /content/kaggle/train_images_jpeg/673903398.jpg inflating: /content/kaggle/train_images_jpeg/673931298.jpg inflating: /content/kaggle/train_images_jpeg/674072472.jpg inflating: /content/kaggle/train_images_jpeg/67417136.jpg inflating: /content/kaggle/train_images_jpeg/674205254.jpg inflating: /content/kaggle/train_images_jpeg/674360414.jpg inflating: /content/kaggle/train_images_jpeg/674380607.jpg inflating: /content/kaggle/train_images_jpeg/674703368.jpg inflating: /content/kaggle/train_images_jpeg/674806764.jpg inflating: /content/kaggle/train_images_jpeg/674941646.jpg inflating: /content/kaggle/train_images_jpeg/675146219.jpg inflating: /content/kaggle/train_images_jpeg/675264849.jpg inflating: /content/kaggle/train_images_jpeg/675325967.jpg inflating: /content/kaggle/train_images_jpeg/67533463.jpg inflating: /content/kaggle/train_images_jpeg/676226256.jpg inflating: /content/kaggle/train_images_jpeg/677178045.jpg inflating: /content/kaggle/train_images_jpeg/677537252.jpg inflating: /content/kaggle/train_images_jpeg/677634085.jpg inflating: /content/kaggle/train_images_jpeg/677816697.jpg inflating: /content/kaggle/train_images_jpeg/677870514.jpg inflating: /content/kaggle/train_images_jpeg/678150539.jpg inflating: /content/kaggle/train_images_jpeg/678308655.jpg inflating: /content/kaggle/train_images_jpeg/678699468.jpg inflating: /content/kaggle/train_images_jpeg/678707458.jpg inflating: /content/kaggle/train_images_jpeg/678743430.jpg inflating: /content/kaggle/train_images_jpeg/678779380.jpg inflating: /content/kaggle/train_images_jpeg/6788393.jpg inflating: /content/kaggle/train_images_jpeg/678943117.jpg inflating: /content/kaggle/train_images_jpeg/678969911.jpg inflating: /content/kaggle/train_images_jpeg/679033130.jpg inflating: /content/kaggle/train_images_jpeg/679212500.jpg inflating: /content/kaggle/train_images_jpeg/679522078.jpg inflating: /content/kaggle/train_images_jpeg/679718743.jpg inflating: /content/kaggle/train_images_jpeg/679887404.jpg inflating: /content/kaggle/train_images_jpeg/679977941.jpg inflating: /content/kaggle/train_images_jpeg/680272982.jpg inflating: /content/kaggle/train_images_jpeg/680596021.jpg inflating: /content/kaggle/train_images_jpeg/680707413.jpg inflating: /content/kaggle/train_images_jpeg/680852379.jpg inflating: /content/kaggle/train_images_jpeg/680910518.jpg inflating: /content/kaggle/train_images_jpeg/681073401.jpg inflating: /content/kaggle/train_images_jpeg/681211585.jpg inflating: /content/kaggle/train_images_jpeg/68141141.jpg inflating: /content/kaggle/train_images_jpeg/681602202.jpg inflating: /content/kaggle/train_images_jpeg/681610474.jpg inflating: /content/kaggle/train_images_jpeg/681698816.jpg inflating: /content/kaggle/train_images_jpeg/682135395.jpg inflating: /content/kaggle/train_images_jpeg/682215276.jpg inflating: /content/kaggle/train_images_jpeg/682271538.jpg inflating: /content/kaggle/train_images_jpeg/682973253.jpg inflating: /content/kaggle/train_images_jpeg/683096346.jpg inflating: /content/kaggle/train_images_jpeg/683136495.jpg inflating: /content/kaggle/train_images_jpeg/683492948.jpg inflating: /content/kaggle/train_images_jpeg/684887397.jpg inflating: /content/kaggle/train_images_jpeg/684911390.jpg inflating: /content/kaggle/train_images_jpeg/685003567.jpg inflating: /content/kaggle/train_images_jpeg/685053856.jpg inflating: /content/kaggle/train_images_jpeg/685199622.jpg inflating: /content/kaggle/train_images_jpeg/685497354.jpg inflating: /content/kaggle/train_images_jpeg/68550961.jpg inflating: /content/kaggle/train_images_jpeg/685533220.jpg inflating: /content/kaggle/train_images_jpeg/685596470.jpg inflating: /content/kaggle/train_images_jpeg/685742976.jpg inflating: /content/kaggle/train_images_jpeg/685822440.jpg inflating: /content/kaggle/train_images_jpeg/685840333.jpg inflating: /content/kaggle/train_images_jpeg/685844269.jpg inflating: /content/kaggle/train_images_jpeg/686071324.jpg inflating: /content/kaggle/train_images_jpeg/686102337.jpg inflating: /content/kaggle/train_images_jpeg/686214048.jpg inflating: /content/kaggle/train_images_jpeg/686263581.jpg inflating: /content/kaggle/train_images_jpeg/686316627.jpg inflating: /content/kaggle/train_images_jpeg/686503891.jpg inflating: /content/kaggle/train_images_jpeg/686958649.jpg inflating: /content/kaggle/train_images_jpeg/687245677.jpg inflating: /content/kaggle/train_images_jpeg/68735991.jpg inflating: /content/kaggle/train_images_jpeg/687533013.jpg inflating: /content/kaggle/train_images_jpeg/68761769.jpg inflating: /content/kaggle/train_images_jpeg/68764231.jpg inflating: /content/kaggle/train_images_jpeg/687693766.jpg inflating: /content/kaggle/train_images_jpeg/687753518.jpg inflating: /content/kaggle/train_images_jpeg/687913373.jpg inflating: /content/kaggle/train_images_jpeg/687931574.jpg inflating: /content/kaggle/train_images_jpeg/688011612.jpg inflating: /content/kaggle/train_images_jpeg/688243919.jpg inflating: /content/kaggle/train_images_jpeg/688266741.jpg inflating: /content/kaggle/train_images_jpeg/688489193.jpg inflating: /content/kaggle/train_images_jpeg/688733567.jpg inflating: /content/kaggle/train_images_jpeg/688748124.jpg inflating: /content/kaggle/train_images_jpeg/689108493.jpg inflating: /content/kaggle/train_images_jpeg/689866725.jpg inflating: /content/kaggle/train_images_jpeg/69003295.jpg inflating: /content/kaggle/train_images_jpeg/690125781.jpg inflating: /content/kaggle/train_images_jpeg/690163.jpg inflating: /content/kaggle/train_images_jpeg/690221576.jpg inflating: /content/kaggle/train_images_jpeg/690258961.jpg inflating: /content/kaggle/train_images_jpeg/69026823.jpg inflating: /content/kaggle/train_images_jpeg/690368559.jpg inflating: /content/kaggle/train_images_jpeg/690399930.jpg inflating: /content/kaggle/train_images_jpeg/690440568.jpg inflating: /content/kaggle/train_images_jpeg/690564008.jpg inflating: /content/kaggle/train_images_jpeg/690565188.jpg inflating: /content/kaggle/train_images_jpeg/690578148.jpg inflating: /content/kaggle/train_images_jpeg/690627061.jpg inflating: /content/kaggle/train_images_jpeg/690967280.jpg inflating: /content/kaggle/train_images_jpeg/69119767.jpg inflating: /content/kaggle/train_images_jpeg/691231499.jpg inflating: /content/kaggle/train_images_jpeg/691285209.jpg inflating: /content/kaggle/train_images_jpeg/691286710.jpg inflating: /content/kaggle/train_images_jpeg/691829328.jpg inflating: /content/kaggle/train_images_jpeg/692019924.jpg inflating: /content/kaggle/train_images_jpeg/692022283.jpg inflating: /content/kaggle/train_images_jpeg/692398741.jpg inflating: /content/kaggle/train_images_jpeg/692727457.jpg inflating: /content/kaggle/train_images_jpeg/692997740.jpg inflating: /content/kaggle/train_images_jpeg/693123459.jpg inflating: /content/kaggle/train_images_jpeg/693164586.jpg inflating: /content/kaggle/train_images_jpeg/693412624.jpg inflating: /content/kaggle/train_images_jpeg/693609543.jpg inflating: /content/kaggle/train_images_jpeg/693678702.jpg inflating: /content/kaggle/train_images_jpeg/693711677.jpg inflating: /content/kaggle/train_images_jpeg/694303061.jpg inflating: /content/kaggle/train_images_jpeg/694387219.jpg inflating: /content/kaggle/train_images_jpeg/694767625.jpg inflating: /content/kaggle/train_images_jpeg/695000133.jpg inflating: /content/kaggle/train_images_jpeg/695167489.jpg inflating: /content/kaggle/train_images_jpeg/695172067.jpg inflating: /content/kaggle/train_images_jpeg/695438825.jpg inflating: /content/kaggle/train_images_jpeg/695839663.jpg inflating: /content/kaggle/train_images_jpeg/695924951.jpg inflating: /content/kaggle/train_images_jpeg/696062925.jpg inflating: /content/kaggle/train_images_jpeg/696072814.jpg inflating: /content/kaggle/train_images_jpeg/696538469.jpg inflating: /content/kaggle/train_images_jpeg/696638670.jpg inflating: /content/kaggle/train_images_jpeg/696867083.jpg inflating: /content/kaggle/train_images_jpeg/697032846.jpg inflating: /content/kaggle/train_images_jpeg/697372150.jpg inflating: /content/kaggle/train_images_jpeg/697621030.jpg inflating: /content/kaggle/train_images_jpeg/697821709.jpg inflating: /content/kaggle/train_images_jpeg/697962684.jpg inflating: /content/kaggle/train_images_jpeg/69803851.jpg inflating: /content/kaggle/train_images_jpeg/698154366.jpg inflating: /content/kaggle/train_images_jpeg/698230072.jpg inflating: /content/kaggle/train_images_jpeg/698646619.jpg inflating: /content/kaggle/train_images_jpeg/698692514.jpg inflating: /content/kaggle/train_images_jpeg/69869891.jpg inflating: /content/kaggle/train_images_jpeg/698853568.jpg inflating: /content/kaggle/train_images_jpeg/698931163.jpg inflating: /content/kaggle/train_images_jpeg/699261554.jpg inflating: /content/kaggle/train_images_jpeg/699282565.jpg inflating: /content/kaggle/train_images_jpeg/699330995.jpg inflating: /content/kaggle/train_images_jpeg/699393181.jpg inflating: /content/kaggle/train_images_jpeg/699893195.jpg inflating: /content/kaggle/train_images_jpeg/699952648.jpg inflating: /content/kaggle/train_images_jpeg/699957994.jpg inflating: /content/kaggle/train_images_jpeg/700113045.jpg inflating: /content/kaggle/train_images_jpeg/700464495.jpg inflating: /content/kaggle/train_images_jpeg/700664783.jpg inflating: /content/kaggle/train_images_jpeg/700772842.jpg inflating: /content/kaggle/train_images_jpeg/700919274.jpg inflating: /content/kaggle/train_images_jpeg/700971789.jpg inflating: /content/kaggle/train_images_jpeg/701052546.jpg inflating: /content/kaggle/train_images_jpeg/701148586.jpg inflating: /content/kaggle/train_images_jpeg/701496880.jpg inflating: /content/kaggle/train_images_jpeg/701698487.jpg inflating: /content/kaggle/train_images_jpeg/701939948.jpg inflating: /content/kaggle/train_images_jpeg/702121018.jpg inflating: /content/kaggle/train_images_jpeg/702619228.jpg inflating: /content/kaggle/train_images_jpeg/702921718.jpg inflating: /content/kaggle/train_images_jpeg/702974599.jpg inflating: /content/kaggle/train_images_jpeg/702998173.jpg inflating: /content/kaggle/train_images_jpeg/703084766.jpg inflating: /content/kaggle/train_images_jpeg/703096302.jpg inflating: /content/kaggle/train_images_jpeg/703310122.jpg inflating: /content/kaggle/train_images_jpeg/703444371.jpg inflating: /content/kaggle/train_images_jpeg/704115558.jpg inflating: /content/kaggle/train_images_jpeg/70412561.jpg inflating: /content/kaggle/train_images_jpeg/704269024.jpg inflating: /content/kaggle/train_images_jpeg/704485270.jpg inflating: /content/kaggle/train_images_jpeg/704504922.jpg inflating: /content/kaggle/train_images_jpeg/70462322.jpg inflating: /content/kaggle/train_images_jpeg/704827087.jpg inflating: /content/kaggle/train_images_jpeg/704856896.jpg inflating: /content/kaggle/train_images_jpeg/705481569.jpg inflating: /content/kaggle/train_images_jpeg/705499651.jpg inflating: /content/kaggle/train_images_jpeg/705789219.jpg inflating: /content/kaggle/train_images_jpeg/705886704.jpg inflating: /content/kaggle/train_images_jpeg/705928796.jpg inflating: /content/kaggle/train_images_jpeg/70656521.jpg inflating: /content/kaggle/train_images_jpeg/706776365.jpg inflating: /content/kaggle/train_images_jpeg/706806140.jpg inflating: /content/kaggle/train_images_jpeg/706808191.jpg inflating: /content/kaggle/train_images_jpeg/70700352.jpg inflating: /content/kaggle/train_images_jpeg/707255804.jpg inflating: /content/kaggle/train_images_jpeg/707347112.jpg inflating: /content/kaggle/train_images_jpeg/707856389.jpg inflating: /content/kaggle/train_images_jpeg/707908216.jpg inflating: /content/kaggle/train_images_jpeg/708059328.jpg inflating: /content/kaggle/train_images_jpeg/70816770.jpg inflating: /content/kaggle/train_images_jpeg/708185327.jpg inflating: /content/kaggle/train_images_jpeg/708463860.jpg inflating: /content/kaggle/train_images_jpeg/708732298.jpg inflating: /content/kaggle/train_images_jpeg/708801738.jpg inflating: /content/kaggle/train_images_jpeg/708802382.jpg inflating: /content/kaggle/train_images_jpeg/708824087.jpg inflating: /content/kaggle/train_images_jpeg/708900690.jpg inflating: /content/kaggle/train_images_jpeg/709094492.jpg inflating: /content/kaggle/train_images_jpeg/709285563.jpg inflating: /content/kaggle/train_images_jpeg/709405326.jpg inflating: /content/kaggle/train_images_jpeg/70951614.jpg inflating: /content/kaggle/train_images_jpeg/709663348.jpg inflating: /content/kaggle/train_images_jpeg/710147759.jpg inflating: /content/kaggle/train_images_jpeg/710255784.jpg inflating: /content/kaggle/train_images_jpeg/710350966.jpg inflating: /content/kaggle/train_images_jpeg/710380163.jpg inflating: /content/kaggle/train_images_jpeg/710791285.jpg inflating: /content/kaggle/train_images_jpeg/710999120.jpg inflating: /content/kaggle/train_images_jpeg/711239502.jpg inflating: /content/kaggle/train_images_jpeg/711417337.jpg inflating: /content/kaggle/train_images_jpeg/711729648.jpg inflating: /content/kaggle/train_images_jpeg/711824069.jpg inflating: /content/kaggle/train_images_jpeg/712150180.jpg inflating: /content/kaggle/train_images_jpeg/712373628.jpg inflating: /content/kaggle/train_images_jpeg/712382120.jpg inflating: /content/kaggle/train_images_jpeg/712421823.jpg inflating: /content/kaggle/train_images_jpeg/712443028.jpg inflating: /content/kaggle/train_images_jpeg/712488898.jpg inflating: /content/kaggle/train_images_jpeg/712506424.jpg inflating: /content/kaggle/train_images_jpeg/712756334.jpg inflating: /content/kaggle/train_images_jpeg/713216878.jpg inflating: /content/kaggle/train_images_jpeg/713250541.jpg inflating: /content/kaggle/train_images_jpeg/713259486.jpg inflating: /content/kaggle/train_images_jpeg/713693996.jpg inflating: /content/kaggle/train_images_jpeg/714060321.jpg inflating: /content/kaggle/train_images_jpeg/714473351.jpg inflating: /content/kaggle/train_images_jpeg/7145099.jpg inflating: /content/kaggle/train_images_jpeg/714720109.jpg inflating: /content/kaggle/train_images_jpeg/714791504.jpg inflating: /content/kaggle/train_images_jpeg/715057901.jpg inflating: /content/kaggle/train_images_jpeg/715316118.jpg inflating: /content/kaggle/train_images_jpeg/715597792.jpg inflating: /content/kaggle/train_images_jpeg/715785641.jpg inflating: /content/kaggle/train_images_jpeg/715889320.jpg inflating: /content/kaggle/train_images_jpeg/715948482.jpg inflating: /content/kaggle/train_images_jpeg/716011398.jpg inflating: /content/kaggle/train_images_jpeg/7160206.jpg inflating: /content/kaggle/train_images_jpeg/716052378.jpg inflating: /content/kaggle/train_images_jpeg/71608311.jpg inflating: /content/kaggle/train_images_jpeg/716279515.jpg inflating: /content/kaggle/train_images_jpeg/716467114.jpg inflating: /content/kaggle/train_images_jpeg/716707376.jpg inflating: /content/kaggle/train_images_jpeg/716814677.jpg inflating: /content/kaggle/train_images_jpeg/717043764.jpg inflating: /content/kaggle/train_images_jpeg/717221311.jpg inflating: /content/kaggle/train_images_jpeg/717332289.jpg inflating: /content/kaggle/train_images_jpeg/717532306.jpg inflating: /content/kaggle/train_images_jpeg/717745388.jpg inflating: /content/kaggle/train_images_jpeg/717753643.jpg inflating: /content/kaggle/train_images_jpeg/717891487.jpg inflating: /content/kaggle/train_images_jpeg/717911960.jpg inflating: /content/kaggle/train_images_jpeg/718077278.jpg inflating: /content/kaggle/train_images_jpeg/718198849.jpg inflating: /content/kaggle/train_images_jpeg/718245684.jpg inflating: /content/kaggle/train_images_jpeg/718269749.jpg inflating: /content/kaggle/train_images_jpeg/718324300.jpg inflating: /content/kaggle/train_images_jpeg/71842711.jpg inflating: /content/kaggle/train_images_jpeg/718497528.jpg inflating: /content/kaggle/train_images_jpeg/718957379.jpg inflating: /content/kaggle/train_images_jpeg/71913940.jpg inflating: /content/kaggle/train_images_jpeg/719168391.jpg inflating: /content/kaggle/train_images_jpeg/719222576.jpg inflating: /content/kaggle/train_images_jpeg/719512192.jpg inflating: /content/kaggle/train_images_jpeg/719526260.jpg inflating: /content/kaggle/train_images_jpeg/719562902.jpg inflating: /content/kaggle/train_images_jpeg/719731461.jpg inflating: /content/kaggle/train_images_jpeg/719825434.jpg inflating: /content/kaggle/train_images_jpeg/719913445.jpg inflating: /content/kaggle/train_images_jpeg/720028805.jpg inflating: /content/kaggle/train_images_jpeg/720072964.jpg inflating: /content/kaggle/train_images_jpeg/720076710.jpg inflating: /content/kaggle/train_images_jpeg/72014881.jpg inflating: /content/kaggle/train_images_jpeg/720275537.jpg inflating: /content/kaggle/train_images_jpeg/720304671.jpg inflating: /content/kaggle/train_images_jpeg/720647277.jpg inflating: /content/kaggle/train_images_jpeg/720749489.jpg inflating: /content/kaggle/train_images_jpeg/720776367.jpg inflating: /content/kaggle/train_images_jpeg/721126244.jpg inflating: /content/kaggle/train_images_jpeg/721201325.jpg inflating: /content/kaggle/train_images_jpeg/721232616.jpg inflating: /content/kaggle/train_images_jpeg/721245068.jpg inflating: /content/kaggle/train_images_jpeg/721430436.jpg inflating: /content/kaggle/train_images_jpeg/72147766.jpg inflating: /content/kaggle/train_images_jpeg/721483208.jpg inflating: /content/kaggle/train_images_jpeg/722155866.jpg inflating: /content/kaggle/train_images_jpeg/722419138.jpg inflating: /content/kaggle/train_images_jpeg/722430028.jpg inflating: /content/kaggle/train_images_jpeg/72286182.jpg inflating: /content/kaggle/train_images_jpeg/72299830.jpg inflating: /content/kaggle/train_images_jpeg/723119750.jpg inflating: /content/kaggle/train_images_jpeg/723564013.jpg inflating: /content/kaggle/train_images_jpeg/723773880.jpg inflating: /content/kaggle/train_images_jpeg/723805565.jpg inflating: /content/kaggle/train_images_jpeg/723891592.jpg inflating: /content/kaggle/train_images_jpeg/723977410.jpg inflating: /content/kaggle/train_images_jpeg/724059503.jpg inflating: /content/kaggle/train_images_jpeg/724195836.jpg inflating: /content/kaggle/train_images_jpeg/724302384.jpg inflating: /content/kaggle/train_images_jpeg/724406669.jpg inflating: /content/kaggle/train_images_jpeg/724656877.jpg inflating: /content/kaggle/train_images_jpeg/725027388.jpg inflating: /content/kaggle/train_images_jpeg/725034194.jpg inflating: /content/kaggle/train_images_jpeg/725248159.jpg inflating: /content/kaggle/train_images_jpeg/725502119.jpg inflating: /content/kaggle/train_images_jpeg/725865491.jpg inflating: /content/kaggle/train_images_jpeg/726063333.jpg inflating: /content/kaggle/train_images_jpeg/726108789.jpg inflating: /content/kaggle/train_images_jpeg/726278005.jpg inflating: /content/kaggle/train_images_jpeg/726374295.jpg inflating: /content/kaggle/train_images_jpeg/726377415.jpg inflating: /content/kaggle/train_images_jpeg/72640810.jpg inflating: /content/kaggle/train_images_jpeg/726818817.jpg inflating: /content/kaggle/train_images_jpeg/726948358.jpg inflating: /content/kaggle/train_images_jpeg/72715709.jpg inflating: /content/kaggle/train_images_jpeg/727949301.jpg inflating: /content/kaggle/train_images_jpeg/728024398.jpg inflating: /content/kaggle/train_images_jpeg/728084630.jpg inflating: /content/kaggle/train_images_jpeg/728194146.jpg inflating: /content/kaggle/train_images_jpeg/728221316.jpg inflating: /content/kaggle/train_images_jpeg/728369192.jpg inflating: /content/kaggle/train_images_jpeg/728569501.jpg inflating: /content/kaggle/train_images_jpeg/728690551.jpg inflating: /content/kaggle/train_images_jpeg/7288550.jpg inflating: /content/kaggle/train_images_jpeg/729230062.jpg inflating: /content/kaggle/train_images_jpeg/72925791.jpg inflating: /content/kaggle/train_images_jpeg/729514787.jpg inflating: /content/kaggle/train_images_jpeg/729571986.jpg inflating: /content/kaggle/train_images_jpeg/730246469.jpg inflating: /content/kaggle/train_images_jpeg/730367123.jpg inflating: /content/kaggle/train_images_jpeg/730431773.jpg inflating: /content/kaggle/train_images_jpeg/730805822.jpg inflating: /content/kaggle/train_images_jpeg/731031258.jpg inflating: /content/kaggle/train_images_jpeg/731084892.jpg inflating: /content/kaggle/train_images_jpeg/731497931.jpg inflating: /content/kaggle/train_images_jpeg/731606877.jpg inflating: /content/kaggle/train_images_jpeg/731638651.jpg inflating: /content/kaggle/train_images_jpeg/731897320.jpg inflating: /content/kaggle/train_images_jpeg/732124818.jpg inflating: /content/kaggle/train_images_jpeg/732377500.jpg inflating: /content/kaggle/train_images_jpeg/732483696.jpg inflating: /content/kaggle/train_images_jpeg/732531942.jpg inflating: /content/kaggle/train_images_jpeg/732560190.jpg inflating: /content/kaggle/train_images_jpeg/732588563.jpg inflating: /content/kaggle/train_images_jpeg/732613479.jpg inflating: /content/kaggle/train_images_jpeg/73287115.jpg inflating: /content/kaggle/train_images_jpeg/732968835.jpg inflating: /content/kaggle/train_images_jpeg/733045784.jpg inflating: /content/kaggle/train_images_jpeg/733232478.jpg inflating: /content/kaggle/train_images_jpeg/733445038.jpg inflating: /content/kaggle/train_images_jpeg/733543752.jpg inflating: /content/kaggle/train_images_jpeg/733701690.jpg inflating: /content/kaggle/train_images_jpeg/733770850.jpg inflating: /content/kaggle/train_images_jpeg/734091779.jpg inflating: /content/kaggle/train_images_jpeg/734163390.jpg inflating: /content/kaggle/train_images_jpeg/734530855.jpg inflating: /content/kaggle/train_images_jpeg/734583205.jpg inflating: /content/kaggle/train_images_jpeg/734701008.jpg inflating: /content/kaggle/train_images_jpeg/735041254.jpg inflating: /content/kaggle/train_images_jpeg/73533711.jpg inflating: /content/kaggle/train_images_jpeg/735529446.jpg inflating: /content/kaggle/train_images_jpeg/736521224.jpg inflating: /content/kaggle/train_images_jpeg/736522598.jpg inflating: /content/kaggle/train_images_jpeg/736571688.jpg inflating: /content/kaggle/train_images_jpeg/736694812.jpg inflating: /content/kaggle/train_images_jpeg/736834551.jpg inflating: /content/kaggle/train_images_jpeg/736896439.jpg inflating: /content/kaggle/train_images_jpeg/736957194.jpg inflating: /content/kaggle/train_images_jpeg/737157935.jpg inflating: /content/kaggle/train_images_jpeg/73728059.jpg inflating: /content/kaggle/train_images_jpeg/737465791.jpg inflating: /content/kaggle/train_images_jpeg/737529075.jpg inflating: /content/kaggle/train_images_jpeg/737556184.jpg inflating: /content/kaggle/train_images_jpeg/737753256.jpg inflating: /content/kaggle/train_images_jpeg/738238956.jpg inflating: /content/kaggle/train_images_jpeg/738338306.jpg inflating: /content/kaggle/train_images_jpeg/738403041.jpg inflating: /content/kaggle/train_images_jpeg/738435591.jpg inflating: /content/kaggle/train_images_jpeg/738553844.jpg inflating: /content/kaggle/train_images_jpeg/738656611.jpg inflating: /content/kaggle/train_images_jpeg/739227863.jpg inflating: /content/kaggle/train_images_jpeg/739247147.jpg inflating: /content/kaggle/train_images_jpeg/739320335.jpg inflating: /content/kaggle/train_images_jpeg/739346655.jpg inflating: /content/kaggle/train_images_jpeg/740035863.jpg inflating: /content/kaggle/train_images_jpeg/740762568.jpg inflating: /content/kaggle/train_images_jpeg/741358374.jpg inflating: /content/kaggle/train_images_jpeg/741807266.jpg inflating: /content/kaggle/train_images_jpeg/742038371.jpg inflating: /content/kaggle/train_images_jpeg/742154911.jpg inflating: /content/kaggle/train_images_jpeg/742278677.jpg inflating: /content/kaggle/train_images_jpeg/742410118.jpg inflating: /content/kaggle/train_images_jpeg/742519691.jpg inflating: /content/kaggle/train_images_jpeg/742539602.jpg inflating: /content/kaggle/train_images_jpeg/742622446.jpg inflating: /content/kaggle/train_images_jpeg/742898185.jpg inflating: /content/kaggle/train_images_jpeg/743105570.jpg inflating: /content/kaggle/train_images_jpeg/743135503.jpg inflating: /content/kaggle/train_images_jpeg/743348334.jpg inflating: /content/kaggle/train_images_jpeg/743721638.jpg inflating: /content/kaggle/train_images_jpeg/743850893.jpg inflating: /content/kaggle/train_images_jpeg/743945725.jpg inflating: /content/kaggle/train_images_jpeg/743999758.jpg inflating: /content/kaggle/train_images_jpeg/744348515.jpg inflating: /content/kaggle/train_images_jpeg/744371772.jpg inflating: /content/kaggle/train_images_jpeg/744383303.jpg inflating: /content/kaggle/train_images_jpeg/744419664.jpg inflating: /content/kaggle/train_images_jpeg/744619434.jpg inflating: /content/kaggle/train_images_jpeg/744676370.jpg inflating: /content/kaggle/train_images_jpeg/744968127.jpg inflating: /content/kaggle/train_images_jpeg/744972013.jpg inflating: /content/kaggle/train_images_jpeg/745112623.jpg inflating: /content/kaggle/train_images_jpeg/745135323.jpg inflating: /content/kaggle/train_images_jpeg/745226266.jpg inflating: /content/kaggle/train_images_jpeg/745442190.jpg inflating: /content/kaggle/train_images_jpeg/74548158.jpg inflating: /content/kaggle/train_images_jpeg/745566741.jpg inflating: /content/kaggle/train_images_jpeg/74571802.jpg inflating: /content/kaggle/train_images_jpeg/745723934.jpg inflating: /content/kaggle/train_images_jpeg/745792080.jpg inflating: /content/kaggle/train_images_jpeg/746494129.jpg inflating: /content/kaggle/train_images_jpeg/74649970.jpg inflating: /content/kaggle/train_images_jpeg/746746526.jpg inflating: /content/kaggle/train_images_jpeg/746879421.jpg inflating: /content/kaggle/train_images_jpeg/747082270.jpg inflating: /content/kaggle/train_images_jpeg/747646454.jpg inflating: /content/kaggle/train_images_jpeg/747698769.jpg inflating: /content/kaggle/train_images_jpeg/747749936.jpg inflating: /content/kaggle/train_images_jpeg/747761920.jpg inflating: /content/kaggle/train_images_jpeg/747770020.jpg inflating: /content/kaggle/train_images_jpeg/748029556.jpg inflating: /content/kaggle/train_images_jpeg/748035819.jpg inflating: /content/kaggle/train_images_jpeg/748154559.jpg inflating: /content/kaggle/train_images_jpeg/748237612.jpg inflating: /content/kaggle/train_images_jpeg/748345018.jpg inflating: /content/kaggle/train_images_jpeg/748404648.jpg inflating: /content/kaggle/train_images_jpeg/748448917.jpg inflating: /content/kaggle/train_images_jpeg/748623930.jpg inflating: /content/kaggle/train_images_jpeg/748745239.jpg inflating: /content/kaggle/train_images_jpeg/749023134.jpg inflating: /content/kaggle/train_images_jpeg/749210924.jpg inflating: /content/kaggle/train_images_jpeg/749454717.jpg inflating: /content/kaggle/train_images_jpeg/74966012.jpg inflating: /content/kaggle/train_images_jpeg/750060449.jpg inflating: /content/kaggle/train_images_jpeg/750376884.jpg inflating: /content/kaggle/train_images_jpeg/750488061.jpg inflating: /content/kaggle/train_images_jpeg/750645140.jpg inflating: /content/kaggle/train_images_jpeg/75068408.jpg inflating: /content/kaggle/train_images_jpeg/750828465.jpg inflating: /content/kaggle/train_images_jpeg/751463025.jpg inflating: /content/kaggle/train_images_jpeg/751472775.jpg inflating: /content/kaggle/train_images_jpeg/752038369.jpg inflating: /content/kaggle/train_images_jpeg/752076516.jpg inflating: /content/kaggle/train_images_jpeg/75207996.jpg inflating: /content/kaggle/train_images_jpeg/752382759.jpg inflating: /content/kaggle/train_images_jpeg/752869166.jpg inflating: /content/kaggle/train_images_jpeg/752884975.jpg inflating: /content/kaggle/train_images_jpeg/753116208.jpg inflating: /content/kaggle/train_images_jpeg/753470251.jpg inflating: /content/kaggle/train_images_jpeg/753617544.jpg inflating: /content/kaggle/train_images_jpeg/753837203.jpg inflating: /content/kaggle/train_images_jpeg/753901704.jpg inflating: /content/kaggle/train_images_jpeg/754110483.jpg inflating: /content/kaggle/train_images_jpeg/754271436.jpg inflating: /content/kaggle/train_images_jpeg/754482252.jpg inflating: /content/kaggle/train_images_jpeg/755073612.jpg inflating: /content/kaggle/train_images_jpeg/755119269.jpg inflating: /content/kaggle/train_images_jpeg/755437879.jpg inflating: /content/kaggle/train_images_jpeg/755497707.jpg inflating: /content/kaggle/train_images_jpeg/755729450.jpg inflating: /content/kaggle/train_images_jpeg/755912815.jpg inflating: /content/kaggle/train_images_jpeg/755941592.jpg inflating: /content/kaggle/train_images_jpeg/75598343.jpg inflating: /content/kaggle/train_images_jpeg/756009491.jpg inflating: /content/kaggle/train_images_jpeg/756195820.jpg inflating: /content/kaggle/train_images_jpeg/756368308.jpg inflating: /content/kaggle/train_images_jpeg/756826989.jpg inflating: /content/kaggle/train_images_jpeg/756993867.jpg inflating: /content/kaggle/train_images_jpeg/757527381.jpg inflating: /content/kaggle/train_images_jpeg/758001650.jpg inflating: /content/kaggle/train_images_jpeg/758155419.jpg inflating: /content/kaggle/train_images_jpeg/758535635.jpg inflating: /content/kaggle/train_images_jpeg/758980708.jpg inflating: /content/kaggle/train_images_jpeg/75951556.jpg inflating: /content/kaggle/train_images_jpeg/759526675.jpg inflating: /content/kaggle/train_images_jpeg/759532134.jpg inflating: /content/kaggle/train_images_jpeg/759559409.jpg inflating: /content/kaggle/train_images_jpeg/759626803.jpg inflating: /content/kaggle/train_images_jpeg/759934720.jpg inflating: /content/kaggle/train_images_jpeg/76026574.jpg inflating: /content/kaggle/train_images_jpeg/760333191.jpg inflating: /content/kaggle/train_images_jpeg/760603911.jpg inflating: /content/kaggle/train_images_jpeg/760650876.jpg inflating: /content/kaggle/train_images_jpeg/760745341.jpg inflating: /content/kaggle/train_images_jpeg/76080816.jpg inflating: /content/kaggle/train_images_jpeg/760863006.jpg inflating: /content/kaggle/train_images_jpeg/760916412.jpg inflating: /content/kaggle/train_images_jpeg/761160675.jpg inflating: /content/kaggle/train_images_jpeg/761651651.jpg inflating: /content/kaggle/train_images_jpeg/761719454.jpg inflating: /content/kaggle/train_images_jpeg/761767350.jpg inflating: /content/kaggle/train_images_jpeg/762063619.jpg inflating: /content/kaggle/train_images_jpeg/762338204.jpg inflating: /content/kaggle/train_images_jpeg/762372916.jpg inflating: /content/kaggle/train_images_jpeg/762558370.jpg inflating: /content/kaggle/train_images_jpeg/762652990.jpg inflating: /content/kaggle/train_images_jpeg/763303438.jpg inflating: /content/kaggle/train_images_jpeg/7635457.jpg inflating: /content/kaggle/train_images_jpeg/763642259.jpg inflating: /content/kaggle/train_images_jpeg/76382862.jpg inflating: /content/kaggle/train_images_jpeg/763846038.jpg inflating: /content/kaggle/train_images_jpeg/764093492.jpg inflating: /content/kaggle/train_images_jpeg/764276739.jpg inflating: /content/kaggle/train_images_jpeg/764628702.jpg inflating: /content/kaggle/train_images_jpeg/764713577.jpg inflating: /content/kaggle/train_images_jpeg/764879606.jpg inflating: /content/kaggle/train_images_jpeg/764956722.jpg inflating: /content/kaggle/train_images_jpeg/765160322.jpg inflating: /content/kaggle/train_images_jpeg/765925416.jpg inflating: /content/kaggle/train_images_jpeg/76610968.jpg inflating: /content/kaggle/train_images_jpeg/766741415.jpg inflating: /content/kaggle/train_images_jpeg/766864157.jpg inflating: /content/kaggle/train_images_jpeg/766908244.jpg inflating: /content/kaggle/train_images_jpeg/767516217.jpg inflating: /content/kaggle/train_images_jpeg/76754228.jpg inflating: /content/kaggle/train_images_jpeg/767824748.jpg inflating: /content/kaggle/train_images_jpeg/76786311.jpg inflating: /content/kaggle/train_images_jpeg/767882051.jpg inflating: /content/kaggle/train_images_jpeg/767931500.jpg inflating: /content/kaggle/train_images_jpeg/768298131.jpg inflating: /content/kaggle/train_images_jpeg/768729652.jpg inflating: /content/kaggle/train_images_jpeg/768744313.jpg inflating: /content/kaggle/train_images_jpeg/768801643.jpg inflating: /content/kaggle/train_images_jpeg/76889034.jpg inflating: /content/kaggle/train_images_jpeg/768948562.jpg inflating: /content/kaggle/train_images_jpeg/76900399.jpg inflating: /content/kaggle/train_images_jpeg/769509727.jpg inflating: /content/kaggle/train_images_jpeg/769621297.jpg inflating: /content/kaggle/train_images_jpeg/769737121.jpg inflating: /content/kaggle/train_images_jpeg/770022346.jpg inflating: /content/kaggle/train_images_jpeg/770144532.jpg inflating: /content/kaggle/train_images_jpeg/770243890.jpg inflating: /content/kaggle/train_images_jpeg/770348865.jpg inflating: /content/kaggle/train_images_jpeg/770763357.jpg inflating: /content/kaggle/train_images_jpeg/770913881.jpg inflating: /content/kaggle/train_images_jpeg/771092021.jpg inflating: /content/kaggle/train_images_jpeg/771269957.jpg inflating: /content/kaggle/train_images_jpeg/771444014.jpg inflating: /content/kaggle/train_images_jpeg/771588286.jpg inflating: /content/kaggle/train_images_jpeg/772210592.jpg inflating: /content/kaggle/train_images_jpeg/772317210.jpg inflating: /content/kaggle/train_images_jpeg/772651285.jpg inflating: /content/kaggle/train_images_jpeg/773007484.jpg inflating: /content/kaggle/train_images_jpeg/773130398.jpg inflating: /content/kaggle/train_images_jpeg/773139109.jpg inflating: /content/kaggle/train_images_jpeg/773221073.jpg inflating: /content/kaggle/train_images_jpeg/773525225.jpg inflating: /content/kaggle/train_images_jpeg/773722195.jpg inflating: /content/kaggle/train_images_jpeg/77379532.jpg inflating: /content/kaggle/train_images_jpeg/77416532.jpg inflating: /content/kaggle/train_images_jpeg/774557036.jpg inflating: /content/kaggle/train_images_jpeg/774618628.jpg inflating: /content/kaggle/train_images_jpeg/774796489.jpg inflating: /content/kaggle/train_images_jpeg/774801310.jpg inflating: /content/kaggle/train_images_jpeg/774942626.jpg inflating: /content/kaggle/train_images_jpeg/775786945.jpg inflating: /content/kaggle/train_images_jpeg/775831061.jpg inflating: /content/kaggle/train_images_jpeg/775841859.jpg inflating: /content/kaggle/train_images_jpeg/776473897.jpg inflating: /content/kaggle/train_images_jpeg/776814328.jpg inflating: /content/kaggle/train_images_jpeg/777180004.jpg inflating: /content/kaggle/train_images_jpeg/777515553.jpg inflating: /content/kaggle/train_images_jpeg/778084917.jpg inflating: /content/kaggle/train_images_jpeg/778101302.jpg inflating: /content/kaggle/train_images_jpeg/778148477.jpg inflating: /content/kaggle/train_images_jpeg/778751547.jpg inflating: /content/kaggle/train_images_jpeg/77896504.jpg inflating: /content/kaggle/train_images_jpeg/779284958.jpg inflating: /content/kaggle/train_images_jpeg/779647083.jpg inflating: /content/kaggle/train_images_jpeg/779704473.jpg inflating: /content/kaggle/train_images_jpeg/779799514.jpg inflating: /content/kaggle/train_images_jpeg/77981157.jpg inflating: /content/kaggle/train_images_jpeg/779832601.jpg inflating: /content/kaggle/train_images_jpeg/779905140.jpg inflating: /content/kaggle/train_images_jpeg/780018709.jpg inflating: /content/kaggle/train_images_jpeg/780244327.jpg inflating: /content/kaggle/train_images_jpeg/780779910.jpg inflating: /content/kaggle/train_images_jpeg/780806888.jpg inflating: /content/kaggle/train_images_jpeg/780810143.jpg inflating: /content/kaggle/train_images_jpeg/780812090.jpg inflating: /content/kaggle/train_images_jpeg/78106943.jpg inflating: /content/kaggle/train_images_jpeg/781215669.jpg inflating: /content/kaggle/train_images_jpeg/781490175.jpg inflating: /content/kaggle/train_images_jpeg/781539722.jpg inflating: /content/kaggle/train_images_jpeg/781625622.jpg inflating: /content/kaggle/train_images_jpeg/781748642.jpg inflating: /content/kaggle/train_images_jpeg/781807650.jpg inflating: /content/kaggle/train_images_jpeg/781931652.jpg inflating: /content/kaggle/train_images_jpeg/782026162.jpg inflating: /content/kaggle/train_images_jpeg/782115033.jpg inflating: /content/kaggle/train_images_jpeg/782478171.jpg inflating: /content/kaggle/train_images_jpeg/782489487.jpg inflating: /content/kaggle/train_images_jpeg/782802185.jpg inflating: /content/kaggle/train_images_jpeg/7830631.jpg inflating: /content/kaggle/train_images_jpeg/783137555.jpg inflating: /content/kaggle/train_images_jpeg/78347787.jpg inflating: /content/kaggle/train_images_jpeg/783533670.jpg inflating: /content/kaggle/train_images_jpeg/783558367.jpg inflating: /content/kaggle/train_images_jpeg/783577117.jpg inflating: /content/kaggle/train_images_jpeg/783771621.jpg inflating: /content/kaggle/train_images_jpeg/784183735.jpg inflating: /content/kaggle/train_images_jpeg/785146800.jpg inflating: /content/kaggle/train_images_jpeg/785210463.jpg inflating: /content/kaggle/train_images_jpeg/785251696.jpg inflating: /content/kaggle/train_images_jpeg/785755916.jpg inflating: /content/kaggle/train_images_jpeg/786134670.jpg inflating: /content/kaggle/train_images_jpeg/786431540.jpg inflating: /content/kaggle/train_images_jpeg/786525788.jpg inflating: /content/kaggle/train_images_jpeg/786703663.jpg inflating: /content/kaggle/train_images_jpeg/787052620.jpg inflating: /content/kaggle/train_images_jpeg/787314434.jpg inflating: /content/kaggle/train_images_jpeg/787415998.jpg inflating: /content/kaggle/train_images_jpeg/78766531.jpg inflating: /content/kaggle/train_images_jpeg/787756871.jpg inflating: /content/kaggle/train_images_jpeg/787849373.jpg inflating: /content/kaggle/train_images_jpeg/787914923.jpg inflating: /content/kaggle/train_images_jpeg/788188732.jpg inflating: /content/kaggle/train_images_jpeg/788479472.jpg inflating: /content/kaggle/train_images_jpeg/789395846.jpg inflating: /content/kaggle/train_images_jpeg/78946276.jpg inflating: /content/kaggle/train_images_jpeg/790193480.jpg inflating: /content/kaggle/train_images_jpeg/790473247.jpg inflating: /content/kaggle/train_images_jpeg/790568397.jpg inflating: /content/kaggle/train_images_jpeg/790574591.jpg inflating: /content/kaggle/train_images_jpeg/790690044.jpg inflating: /content/kaggle/train_images_jpeg/790756634.jpg inflating: /content/kaggle/train_images_jpeg/791132731.jpg inflating: /content/kaggle/train_images_jpeg/791404140.jpg inflating: /content/kaggle/train_images_jpeg/79165028.jpg inflating: /content/kaggle/train_images_jpeg/791667524.jpg inflating: /content/kaggle/train_images_jpeg/791971573.jpg inflating: /content/kaggle/train_images_jpeg/792105357.jpg inflating: /content/kaggle/train_images_jpeg/792213146.jpg inflating: /content/kaggle/train_images_jpeg/792366511.jpg inflating: /content/kaggle/train_images_jpeg/792369876.jpg inflating: /content/kaggle/train_images_jpeg/792936600.jpg inflating: /content/kaggle/train_images_jpeg/792953439.jpg inflating: /content/kaggle/train_images_jpeg/792990285.jpg inflating: /content/kaggle/train_images_jpeg/793076912.jpg inflating: /content/kaggle/train_images_jpeg/793174767.jpg inflating: /content/kaggle/train_images_jpeg/793206643.jpg inflating: /content/kaggle/train_images_jpeg/793578252.jpg inflating: /content/kaggle/train_images_jpeg/793897139.jpg inflating: /content/kaggle/train_images_jpeg/793903784.jpg inflating: /content/kaggle/train_images_jpeg/794210543.jpg inflating: /content/kaggle/train_images_jpeg/794333846.jpg inflating: /content/kaggle/train_images_jpeg/794665522.jpg inflating: /content/kaggle/train_images_jpeg/794817264.jpg inflating: /content/kaggle/train_images_jpeg/794895924.jpg inflating: /content/kaggle/train_images_jpeg/795110663.jpg inflating: /content/kaggle/train_images_jpeg/795263399.jpg inflating: /content/kaggle/train_images_jpeg/795280732.jpg inflating: /content/kaggle/train_images_jpeg/795383461.jpg inflating: /content/kaggle/train_images_jpeg/795781797.jpg inflating: /content/kaggle/train_images_jpeg/795796251.jpg inflating: /content/kaggle/train_images_jpeg/796369691.jpg inflating: /content/kaggle/train_images_jpeg/796761175.jpg inflating: /content/kaggle/train_images_jpeg/796933103.jpg inflating: /content/kaggle/train_images_jpeg/797009430.jpg inflating: /content/kaggle/train_images_jpeg/797094434.jpg inflating: /content/kaggle/train_images_jpeg/797310513.jpg inflating: /content/kaggle/train_images_jpeg/797580411.jpg inflating: /content/kaggle/train_images_jpeg/797589607.jpg inflating: /content/kaggle/train_images_jpeg/798006612.jpg inflating: /content/kaggle/train_images_jpeg/798023657.jpg inflating: /content/kaggle/train_images_jpeg/798065416.jpg inflating: /content/kaggle/train_images_jpeg/798586903.jpg inflating: /content/kaggle/train_images_jpeg/79867712.jpg inflating: /content/kaggle/train_images_jpeg/799030211.jpg inflating: /content/kaggle/train_images_jpeg/799279391.jpg inflating: /content/kaggle/train_images_jpeg/800065702.jpg inflating: /content/kaggle/train_images_jpeg/800250438.jpg inflating: /content/kaggle/train_images_jpeg/800501715.jpg inflating: /content/kaggle/train_images_jpeg/800593866.jpg inflating: /content/kaggle/train_images_jpeg/801173555.jpg inflating: /content/kaggle/train_images_jpeg/801343155.jpg inflating: /content/kaggle/train_images_jpeg/801531661.jpg inflating: /content/kaggle/train_images_jpeg/801551318.jpg inflating: /content/kaggle/train_images_jpeg/801908113.jpg inflating: /content/kaggle/train_images_jpeg/802174212.jpg inflating: /content/kaggle/train_images_jpeg/802266352.jpg inflating: /content/kaggle/train_images_jpeg/802386066.jpg inflating: /content/kaggle/train_images_jpeg/802718522.jpg inflating: /content/kaggle/train_images_jpeg/803019874.jpg inflating: /content/kaggle/train_images_jpeg/803137962.jpg inflating: /content/kaggle/train_images_jpeg/803511182.jpg inflating: /content/kaggle/train_images_jpeg/803527097.jpg inflating: /content/kaggle/train_images_jpeg/803715275.jpg inflating: /content/kaggle/train_images_jpeg/803734282.jpg inflating: /content/kaggle/train_images_jpeg/803873425.jpg inflating: /content/kaggle/train_images_jpeg/804056394.jpg inflating: /content/kaggle/train_images_jpeg/804347789.jpg inflating: /content/kaggle/train_images_jpeg/804353681.jpg inflating: /content/kaggle/train_images_jpeg/804506814.jpg inflating: /content/kaggle/train_images_jpeg/804513680.jpg inflating: /content/kaggle/train_images_jpeg/804520437.jpg inflating: /content/kaggle/train_images_jpeg/804653240.jpg inflating: /content/kaggle/train_images_jpeg/80482467.jpg inflating: /content/kaggle/train_images_jpeg/804847403.jpg inflating: /content/kaggle/train_images_jpeg/80488129.jpg inflating: /content/kaggle/train_images_jpeg/804903970.jpg inflating: /content/kaggle/train_images_jpeg/805005298.jpg inflating: /content/kaggle/train_images_jpeg/805152902.jpg inflating: /content/kaggle/train_images_jpeg/805519444.jpg inflating: /content/kaggle/train_images_jpeg/805711898.jpg inflating: /content/kaggle/train_images_jpeg/805835204.jpg inflating: /content/kaggle/train_images_jpeg/805915725.jpg inflating: /content/kaggle/train_images_jpeg/806175300.jpg inflating: /content/kaggle/train_images_jpeg/806185900.jpg inflating: /content/kaggle/train_images_jpeg/806357910.jpg inflating: /content/kaggle/train_images_jpeg/806456552.jpg inflating: /content/kaggle/train_images_jpeg/806670613.jpg inflating: /content/kaggle/train_images_jpeg/806702626.jpg inflating: /content/kaggle/train_images_jpeg/806943895.jpg inflating: /content/kaggle/train_images_jpeg/807063038.jpg inflating: /content/kaggle/train_images_jpeg/807086334.jpg inflating: /content/kaggle/train_images_jpeg/807190211.jpg inflating: /content/kaggle/train_images_jpeg/807322814.jpg inflating: /content/kaggle/train_images_jpeg/807555228.jpg inflating: /content/kaggle/train_images_jpeg/807698612.jpg inflating: /content/kaggle/train_images_jpeg/807777679.jpg inflating: /content/kaggle/train_images_jpeg/807976846.jpg inflating: /content/kaggle/train_images_jpeg/808180923.jpg inflating: /content/kaggle/train_images_jpeg/808194984.jpg inflating: /content/kaggle/train_images_jpeg/808416790.jpg inflating: /content/kaggle/train_images_jpeg/808606006.jpg inflating: /content/kaggle/train_images_jpeg/808870213.jpg inflating: /content/kaggle/train_images_jpeg/809104607.jpg inflating: /content/kaggle/train_images_jpeg/809330872.jpg inflating: /content/kaggle/train_images_jpeg/809359953.jpg inflating: /content/kaggle/train_images_jpeg/809489252.jpg inflating: /content/kaggle/train_images_jpeg/809808575.jpg inflating: /content/kaggle/train_images_jpeg/80998969.jpg inflating: /content/kaggle/train_images_jpeg/81000729.jpg inflating: /content/kaggle/train_images_jpeg/810282945.jpg inflating: /content/kaggle/train_images_jpeg/810351903.jpg inflating: /content/kaggle/train_images_jpeg/810511060.jpg inflating: /content/kaggle/train_images_jpeg/810720134.jpg inflating: /content/kaggle/train_images_jpeg/810859265.jpg inflating: /content/kaggle/train_images_jpeg/810886166.jpg inflating: /content/kaggle/train_images_jpeg/811026048.jpg inflating: /content/kaggle/train_images_jpeg/811520495.jpg inflating: /content/kaggle/train_images_jpeg/81164598.jpg inflating: /content/kaggle/train_images_jpeg/811696349.jpg inflating: /content/kaggle/train_images_jpeg/811732946.jpg inflating: /content/kaggle/train_images_jpeg/811928525.jpg inflating: /content/kaggle/train_images_jpeg/812377482.jpg inflating: /content/kaggle/train_images_jpeg/812408252.jpg inflating: /content/kaggle/train_images_jpeg/812524479.jpg inflating: /content/kaggle/train_images_jpeg/812733394.jpg inflating: /content/kaggle/train_images_jpeg/8129389.jpg inflating: /content/kaggle/train_images_jpeg/813017538.jpg inflating: /content/kaggle/train_images_jpeg/813060428.jpg inflating: /content/kaggle/train_images_jpeg/813110259.jpg inflating: /content/kaggle/train_images_jpeg/813217011.jpg inflating: /content/kaggle/train_images_jpeg/813262235.jpg inflating: /content/kaggle/train_images_jpeg/813361567.jpg inflating: /content/kaggle/train_images_jpeg/813435800.jpg inflating: /content/kaggle/train_images_jpeg/813509621.jpg inflating: /content/kaggle/train_images_jpeg/813688651.jpg inflating: /content/kaggle/train_images_jpeg/813697698.jpg inflating: /content/kaggle/train_images_jpeg/813744167.jpg inflating: /content/kaggle/train_images_jpeg/813751338.jpg inflating: /content/kaggle/train_images_jpeg/814020712.jpg inflating: /content/kaggle/train_images_jpeg/814026139.jpg inflating: /content/kaggle/train_images_jpeg/814045052.jpg inflating: /content/kaggle/train_images_jpeg/814122857.jpg inflating: /content/kaggle/train_images_jpeg/814185128.jpg inflating: /content/kaggle/train_images_jpeg/814288392.jpg inflating: /content/kaggle/train_images_jpeg/814547184.jpg inflating: /content/kaggle/train_images_jpeg/814722861.jpg inflating: /content/kaggle/train_images_jpeg/815181959.jpg inflating: /content/kaggle/train_images_jpeg/815227743.jpg inflating: /content/kaggle/train_images_jpeg/815702740.jpg inflating: /content/kaggle/train_images_jpeg/815758241.jpg inflating: /content/kaggle/train_images_jpeg/815952257.jpg inflating: /content/kaggle/train_images_jpeg/816404060.jpg inflating: /content/kaggle/train_images_jpeg/816584530.jpg inflating: /content/kaggle/train_images_jpeg/816689138.jpg inflating: /content/kaggle/train_images_jpeg/816704728.jpg inflating: /content/kaggle/train_images_jpeg/816704735.jpg inflating: /content/kaggle/train_images_jpeg/816935625.jpg inflating: /content/kaggle/train_images_jpeg/817366566.jpg inflating: /content/kaggle/train_images_jpeg/817459754.jpg inflating: /content/kaggle/train_images_jpeg/817545586.jpg inflating: /content/kaggle/train_images_jpeg/817778981.jpg inflating: /content/kaggle/train_images_jpeg/817893600.jpg inflating: /content/kaggle/train_images_jpeg/818188540.jpg inflating: /content/kaggle/train_images_jpeg/81845086.jpg inflating: /content/kaggle/train_images_jpeg/819599194.jpg inflating: /content/kaggle/train_images_jpeg/819600313.jpg inflating: /content/kaggle/train_images_jpeg/819817921.jpg inflating: /content/kaggle/train_images_jpeg/820562857.jpg inflating: /content/kaggle/train_images_jpeg/82105739.jpg inflating: /content/kaggle/train_images_jpeg/821382382.jpg inflating: /content/kaggle/train_images_jpeg/821592762.jpg inflating: /content/kaggle/train_images_jpeg/821706868.jpg inflating: /content/kaggle/train_images_jpeg/821895788.jpg inflating: /content/kaggle/train_images_jpeg/822120250.jpg inflating: /content/kaggle/train_images_jpeg/822187943.jpg inflating: /content/kaggle/train_images_jpeg/822444900.jpg inflating: /content/kaggle/train_images_jpeg/822532439.jpg inflating: /content/kaggle/train_images_jpeg/822641071.jpg inflating: /content/kaggle/train_images_jpeg/82290706.jpg inflating: /content/kaggle/train_images_jpeg/823167906.jpg inflating: /content/kaggle/train_images_jpeg/823276084.jpg inflating: /content/kaggle/train_images_jpeg/823280173.jpg inflating: /content/kaggle/train_images_jpeg/823298994.jpg inflating: /content/kaggle/train_images_jpeg/823483380.jpg inflating: /content/kaggle/train_images_jpeg/823568510.jpg inflating: /content/kaggle/train_images_jpeg/823914480.jpg inflating: /content/kaggle/train_images_jpeg/824321777.jpg inflating: /content/kaggle/train_images_jpeg/824361434.jpg inflating: /content/kaggle/train_images_jpeg/824408929.jpg inflating: /content/kaggle/train_images_jpeg/824489541.jpg inflating: /content/kaggle/train_images_jpeg/824766892.jpg inflating: /content/kaggle/train_images_jpeg/824811574.jpg inflating: /content/kaggle/train_images_jpeg/825263308.jpg inflating: /content/kaggle/train_images_jpeg/82533757.jpg inflating: /content/kaggle/train_images_jpeg/825340900.jpg inflating: /content/kaggle/train_images_jpeg/825527127.jpg inflating: /content/kaggle/train_images_jpeg/825551560.jpg inflating: /content/kaggle/train_images_jpeg/825637783.jpg inflating: /content/kaggle/train_images_jpeg/825877957.jpg inflating: /content/kaggle/train_images_jpeg/826144344.jpg inflating: /content/kaggle/train_images_jpeg/826231979.jpg inflating: /content/kaggle/train_images_jpeg/826288261.jpg inflating: /content/kaggle/train_images_jpeg/826306862.jpg inflating: /content/kaggle/train_images_jpeg/826682979.jpg inflating: /content/kaggle/train_images_jpeg/827007782.jpg inflating: /content/kaggle/train_images_jpeg/827085399.jpg inflating: /content/kaggle/train_images_jpeg/827134808.jpg inflating: /content/kaggle/train_images_jpeg/827159844.jpg inflating: /content/kaggle/train_images_jpeg/827284043.jpg inflating: /content/kaggle/train_images_jpeg/827738585.jpg inflating: /content/kaggle/train_images_jpeg/827746278.jpg inflating: /content/kaggle/train_images_jpeg/828383067.jpg inflating: /content/kaggle/train_images_jpeg/828472143.jpg inflating: /content/kaggle/train_images_jpeg/82853961.jpg inflating: /content/kaggle/train_images_jpeg/828576182.jpg inflating: /content/kaggle/train_images_jpeg/828740814.jpg inflating: /content/kaggle/train_images_jpeg/829018445.jpg inflating: /content/kaggle/train_images_jpeg/829043523.jpg inflating: /content/kaggle/train_images_jpeg/829062180.jpg inflating: /content/kaggle/train_images_jpeg/829236675.jpg inflating: /content/kaggle/train_images_jpeg/829442901.jpg inflating: /content/kaggle/train_images_jpeg/829448982.jpg inflating: /content/kaggle/train_images_jpeg/82972530.jpg inflating: /content/kaggle/train_images_jpeg/829746450.jpg inflating: /content/kaggle/train_images_jpeg/829835342.jpg inflating: /content/kaggle/train_images_jpeg/829974709.jpg inflating: /content/kaggle/train_images_jpeg/830055343.jpg inflating: /content/kaggle/train_images_jpeg/830213771.jpg inflating: /content/kaggle/train_images_jpeg/830231464.jpg inflating: /content/kaggle/train_images_jpeg/830369134.jpg inflating: /content/kaggle/train_images_jpeg/830380663.jpg inflating: /content/kaggle/train_images_jpeg/830574932.jpg inflating: /content/kaggle/train_images_jpeg/830740908.jpg inflating: /content/kaggle/train_images_jpeg/830772376.jpg inflating: /content/kaggle/train_images_jpeg/830793434.jpg inflating: /content/kaggle/train_images_jpeg/830812751.jpg inflating: /content/kaggle/train_images_jpeg/831001239.jpg inflating: /content/kaggle/train_images_jpeg/831187317.jpg inflating: /content/kaggle/train_images_jpeg/832048490.jpg inflating: /content/kaggle/train_images_jpeg/832387974.jpg inflating: /content/kaggle/train_images_jpeg/832440144.jpg inflating: /content/kaggle/train_images_jpeg/832593154.jpg inflating: /content/kaggle/train_images_jpeg/832729024.jpg inflating: /content/kaggle/train_images_jpeg/832785383.jpg inflating: /content/kaggle/train_images_jpeg/832869230.jpg inflating: /content/kaggle/train_images_jpeg/833082515.jpg inflating: /content/kaggle/train_images_jpeg/833188663.jpg inflating: /content/kaggle/train_images_jpeg/83335504.jpg inflating: /content/kaggle/train_images_jpeg/83337985.jpg inflating: /content/kaggle/train_images_jpeg/833453231.jpg inflating: /content/kaggle/train_images_jpeg/833472737.jpg inflating: /content/kaggle/train_images_jpeg/833499440.jpg inflating: /content/kaggle/train_images_jpeg/833599102.jpg inflating: /content/kaggle/train_images_jpeg/833990099.jpg inflating: /content/kaggle/train_images_jpeg/833991576.jpg inflating: /content/kaggle/train_images_jpeg/834011850.jpg inflating: /content/kaggle/train_images_jpeg/834333640.jpg inflating: /content/kaggle/train_images_jpeg/834357600.jpg inflating: /content/kaggle/train_images_jpeg/834601546.jpg inflating: /content/kaggle/train_images_jpeg/835290707.jpg inflating: /content/kaggle/train_images_jpeg/836250152.jpg inflating: /content/kaggle/train_images_jpeg/836310616.jpg inflating: /content/kaggle/train_images_jpeg/836320036.jpg inflating: /content/kaggle/train_images_jpeg/836430127.jpg inflating: /content/kaggle/train_images_jpeg/836589560.jpg inflating: /content/kaggle/train_images_jpeg/83715695.jpg inflating: /content/kaggle/train_images_jpeg/837412690.jpg inflating: /content/kaggle/train_images_jpeg/837521873.jpg inflating: /content/kaggle/train_images_jpeg/837916123.jpg inflating: /content/kaggle/train_images_jpeg/838106879.jpg inflating: /content/kaggle/train_images_jpeg/838121109.jpg inflating: /content/kaggle/train_images_jpeg/838136657.jpg inflating: /content/kaggle/train_images_jpeg/83814661.jpg inflating: /content/kaggle/train_images_jpeg/83815023.jpg inflating: /content/kaggle/train_images_jpeg/838198963.jpg inflating: /content/kaggle/train_images_jpeg/838294593.jpg inflating: /content/kaggle/train_images_jpeg/838404479.jpg inflating: /content/kaggle/train_images_jpeg/83870062.jpg inflating: /content/kaggle/train_images_jpeg/838734529.jpg inflating: /content/kaggle/train_images_jpeg/839547710.jpg inflating: /content/kaggle/train_images_jpeg/839709957.jpg inflating: /content/kaggle/train_images_jpeg/839823259.jpg inflating: /content/kaggle/train_images_jpeg/840009401.jpg inflating: /content/kaggle/train_images_jpeg/84024270.jpg inflating: /content/kaggle/train_images_jpeg/840541858.jpg inflating: /content/kaggle/train_images_jpeg/840828184.jpg inflating: /content/kaggle/train_images_jpeg/840889363.jpg inflating: /content/kaggle/train_images_jpeg/841248971.jpg inflating: /content/kaggle/train_images_jpeg/841447930.jpg inflating: /content/kaggle/train_images_jpeg/841741628.jpg inflating: /content/kaggle/train_images_jpeg/842361914.jpg inflating: /content/kaggle/train_images_jpeg/842480567.jpg inflating: /content/kaggle/train_images_jpeg/84269918.jpg inflating: /content/kaggle/train_images_jpeg/842889608.jpg inflating: /content/kaggle/train_images_jpeg/842973014.jpg inflating: /content/kaggle/train_images_jpeg/843004516.jpg inflating: /content/kaggle/train_images_jpeg/843145973.jpg inflating: /content/kaggle/train_images_jpeg/84339133.jpg inflating: /content/kaggle/train_images_jpeg/843483418.jpg inflating: /content/kaggle/train_images_jpeg/843529789.jpg inflating: /content/kaggle/train_images_jpeg/843818406.jpg inflating: /content/kaggle/train_images_jpeg/844267975.jpg inflating: /content/kaggle/train_images_jpeg/844410022.jpg inflating: /content/kaggle/train_images_jpeg/844432443.jpg inflating: /content/kaggle/train_images_jpeg/845131319.jpg inflating: /content/kaggle/train_images_jpeg/845640217.jpg inflating: /content/kaggle/train_images_jpeg/845869703.jpg inflating: /content/kaggle/train_images_jpeg/845926406.jpg inflating: /content/kaggle/train_images_jpeg/84598209.jpg inflating: /content/kaggle/train_images_jpeg/846123439.jpg inflating: /content/kaggle/train_images_jpeg/846141770.jpg inflating: /content/kaggle/train_images_jpeg/846277174.jpg inflating: /content/kaggle/train_images_jpeg/84635797.jpg inflating: /content/kaggle/train_images_jpeg/846434156.jpg inflating: /content/kaggle/train_images_jpeg/846591139.jpg inflating: /content/kaggle/train_images_jpeg/846734911.jpg inflating: /content/kaggle/train_images_jpeg/846824837.jpg inflating: /content/kaggle/train_images_jpeg/84689733.jpg inflating: /content/kaggle/train_images_jpeg/847112619.jpg inflating: /content/kaggle/train_images_jpeg/847187108.jpg inflating: /content/kaggle/train_images_jpeg/847229985.jpg inflating: /content/kaggle/train_images_jpeg/847288162.jpg inflating: /content/kaggle/train_images_jpeg/847610100.jpg inflating: /content/kaggle/train_images_jpeg/847847826.jpg inflating: /content/kaggle/train_images_jpeg/84787134.jpg inflating: /content/kaggle/train_images_jpeg/847938979.jpg inflating: /content/kaggle/train_images_jpeg/847946899.jpg inflating: /content/kaggle/train_images_jpeg/848013707.jpg inflating: /content/kaggle/train_images_jpeg/848032997.jpg inflating: /content/kaggle/train_images_jpeg/848164169.jpg inflating: /content/kaggle/train_images_jpeg/848445335.jpg inflating: /content/kaggle/train_images_jpeg/848551484.jpg inflating: /content/kaggle/train_images_jpeg/84878474.jpg inflating: /content/kaggle/train_images_jpeg/848914861.jpg inflating: /content/kaggle/train_images_jpeg/849133698.jpg inflating: /content/kaggle/train_images_jpeg/849182498.jpg inflating: /content/kaggle/train_images_jpeg/849184144.jpg inflating: /content/kaggle/train_images_jpeg/849308241.jpg inflating: /content/kaggle/train_images_jpeg/850135313.jpg inflating: /content/kaggle/train_images_jpeg/850170128.jpg inflating: /content/kaggle/train_images_jpeg/85027801.jpg inflating: /content/kaggle/train_images_jpeg/850402862.jpg inflating: /content/kaggle/train_images_jpeg/851063850.jpg inflating: /content/kaggle/train_images_jpeg/851274376.jpg inflating: /content/kaggle/train_images_jpeg/851313778.jpg inflating: /content/kaggle/train_images_jpeg/851345669.jpg inflating: /content/kaggle/train_images_jpeg/851450770.jpg inflating: /content/kaggle/train_images_jpeg/851525462.jpg inflating: /content/kaggle/train_images_jpeg/851527428.jpg inflating: /content/kaggle/train_images_jpeg/851594010.jpg inflating: /content/kaggle/train_images_jpeg/851691285.jpg inflating: /content/kaggle/train_images_jpeg/851791464.jpg inflating: /content/kaggle/train_images_jpeg/852177343.jpg inflating: /content/kaggle/train_images_jpeg/85222011.jpg inflating: /content/kaggle/train_images_jpeg/852377476.jpg inflating: /content/kaggle/train_images_jpeg/852424690.jpg inflating: /content/kaggle/train_images_jpeg/852636262.jpg inflating: /content/kaggle/train_images_jpeg/8526431.jpg inflating: /content/kaggle/train_images_jpeg/852760236.jpg inflating: /content/kaggle/train_images_jpeg/852886751.jpg inflating: /content/kaggle/train_images_jpeg/853854962.jpg inflating: /content/kaggle/train_images_jpeg/854018271.jpg inflating: /content/kaggle/train_images_jpeg/854102052.jpg inflating: /content/kaggle/train_images_jpeg/854205384.jpg inflating: /content/kaggle/train_images_jpeg/854309880.jpg inflating: /content/kaggle/train_images_jpeg/85453943.jpg inflating: /content/kaggle/train_images_jpeg/854627770.jpg inflating: /content/kaggle/train_images_jpeg/854773586.jpg inflating: /content/kaggle/train_images_jpeg/854883118.jpg inflating: /content/kaggle/train_images_jpeg/855456135.jpg inflating: /content/kaggle/train_images_jpeg/855496041.jpg inflating: /content/kaggle/train_images_jpeg/856327781.jpg inflating: /content/kaggle/train_images_jpeg/856446971.jpg inflating: /content/kaggle/train_images_jpeg/85709703.jpg inflating: /content/kaggle/train_images_jpeg/857136897.jpg inflating: /content/kaggle/train_images_jpeg/857300948.jpg inflating: /content/kaggle/train_images_jpeg/85739324.jpg inflating: /content/kaggle/train_images_jpeg/857551546.jpg inflating: /content/kaggle/train_images_jpeg/857708502.jpg inflating: /content/kaggle/train_images_jpeg/857728426.jpg inflating: /content/kaggle/train_images_jpeg/857836383.jpg inflating: /content/kaggle/train_images_jpeg/857910285.jpg inflating: /content/kaggle/train_images_jpeg/857934902.jpg inflating: /content/kaggle/train_images_jpeg/857983591.jpg inflating: /content/kaggle/train_images_jpeg/858147094.jpg inflating: /content/kaggle/train_images_jpeg/858578370.jpg inflating: /content/kaggle/train_images_jpeg/858590964.jpg inflating: /content/kaggle/train_images_jpeg/85888563.jpg inflating: /content/kaggle/train_images_jpeg/859362925.jpg inflating: /content/kaggle/train_images_jpeg/860103938.jpg inflating: /content/kaggle/train_images_jpeg/860267524.jpg inflating: /content/kaggle/train_images_jpeg/860380969.jpg inflating: /content/kaggle/train_images_jpeg/860415279.jpg inflating: /content/kaggle/train_images_jpeg/860744310.jpg inflating: /content/kaggle/train_images_jpeg/860758884.jpg inflating: /content/kaggle/train_images_jpeg/860785504.jpg inflating: /content/kaggle/train_images_jpeg/860826658.jpg inflating: /content/kaggle/train_images_jpeg/860926146.jpg inflating: /content/kaggle/train_images_jpeg/862068300.jpg inflating: /content/kaggle/train_images_jpeg/862141742.jpg inflating: /content/kaggle/train_images_jpeg/862706097.jpg inflating: /content/kaggle/train_images_jpeg/86271785.jpg inflating: /content/kaggle/train_images_jpeg/863348541.jpg inflating: /content/kaggle/train_images_jpeg/863369540.jpg inflating: /content/kaggle/train_images_jpeg/863550761.jpg inflating: /content/kaggle/train_images_jpeg/863785025.jpg inflating: /content/kaggle/train_images_jpeg/864235203.jpg inflating: /content/kaggle/train_images_jpeg/864600398.jpg inflating: /content/kaggle/train_images_jpeg/864636811.jpg inflating: /content/kaggle/train_images_jpeg/864816883.jpg inflating: /content/kaggle/train_images_jpeg/864903108.jpg inflating: /content/kaggle/train_images_jpeg/864933566.jpg inflating: /content/kaggle/train_images_jpeg/865580599.jpg inflating: /content/kaggle/train_images_jpeg/865733342.jpg inflating: /content/kaggle/train_images_jpeg/866479018.jpg inflating: /content/kaggle/train_images_jpeg/866496354.jpg inflating: /content/kaggle/train_images_jpeg/866506259.jpg inflating: /content/kaggle/train_images_jpeg/866605488.jpg inflating: /content/kaggle/train_images_jpeg/867025399.jpg inflating: /content/kaggle/train_images_jpeg/867127390.jpg inflating: /content/kaggle/train_images_jpeg/867622395.jpg inflating: /content/kaggle/train_images_jpeg/867855885.jpg inflating: /content/kaggle/train_images_jpeg/868228069.jpg inflating: /content/kaggle/train_images_jpeg/868275841.jpg inflating: /content/kaggle/train_images_jpeg/868625768.jpg inflating: /content/kaggle/train_images_jpeg/86890165.jpg inflating: /content/kaggle/train_images_jpeg/870197588.jpg inflating: /content/kaggle/train_images_jpeg/870659549.jpg inflating: /content/kaggle/train_images_jpeg/870679029.jpg inflating: /content/kaggle/train_images_jpeg/870967605.jpg inflating: /content/kaggle/train_images_jpeg/871035730.jpg inflating: /content/kaggle/train_images_jpeg/871044756.jpg inflating: /content/kaggle/train_images_jpeg/871153188.jpg inflating: /content/kaggle/train_images_jpeg/871251409.jpg inflating: /content/kaggle/train_images_jpeg/871333676.jpg inflating: /content/kaggle/train_images_jpeg/871530899.jpg inflating: /content/kaggle/train_images_jpeg/87170987.jpg inflating: /content/kaggle/train_images_jpeg/871711168.jpg inflating: /content/kaggle/train_images_jpeg/871726672.jpg inflating: /content/kaggle/train_images_jpeg/871966628.jpg inflating: /content/kaggle/train_images_jpeg/872344634.jpg inflating: /content/kaggle/train_images_jpeg/872498616.jpg inflating: /content/kaggle/train_images_jpeg/872590456.jpg inflating: /content/kaggle/train_images_jpeg/872771572.jpg inflating: /content/kaggle/train_images_jpeg/872791534.jpg inflating: /content/kaggle/train_images_jpeg/872798260.jpg inflating: /content/kaggle/train_images_jpeg/872856215.jpg inflating: /content/kaggle/train_images_jpeg/872867609.jpg inflating: /content/kaggle/train_images_jpeg/872917593.jpg inflating: /content/kaggle/train_images_jpeg/87295869.jpg inflating: /content/kaggle/train_images_jpeg/87302244.jpg inflating: /content/kaggle/train_images_jpeg/873095310.jpg inflating: /content/kaggle/train_images_jpeg/873125819.jpg inflating: /content/kaggle/train_images_jpeg/873205488.jpg inflating: /content/kaggle/train_images_jpeg/873468650.jpg inflating: /content/kaggle/train_images_jpeg/873526870.jpg inflating: /content/kaggle/train_images_jpeg/873637313.jpg inflating: /content/kaggle/train_images_jpeg/873720819.jpg inflating: /content/kaggle/train_images_jpeg/873753078.jpg inflating: /content/kaggle/train_images_jpeg/874590064.jpg inflating: /content/kaggle/train_images_jpeg/87470037.jpg inflating: /content/kaggle/train_images_jpeg/874716908.jpg inflating: /content/kaggle/train_images_jpeg/874878736.jpg inflating: /content/kaggle/train_images_jpeg/875205703.jpg inflating: /content/kaggle/train_images_jpeg/875229192.jpg inflating: /content/kaggle/train_images_jpeg/875360512.jpg inflating: /content/kaggle/train_images_jpeg/875582353.jpg inflating: /content/kaggle/train_images_jpeg/87567596.jpg inflating: /content/kaggle/train_images_jpeg/875836279.jpg inflating: /content/kaggle/train_images_jpeg/875889242.jpg inflating: /content/kaggle/train_images_jpeg/876450779.jpg inflating: /content/kaggle/train_images_jpeg/87653700.jpg inflating: /content/kaggle/train_images_jpeg/876588489.jpg inflating: /content/kaggle/train_images_jpeg/876612201.jpg inflating: /content/kaggle/train_images_jpeg/87664918.jpg inflating: /content/kaggle/train_images_jpeg/876666484.jpg inflating: /content/kaggle/train_images_jpeg/876953624.jpg inflating: /content/kaggle/train_images_jpeg/87703299.jpg inflating: /content/kaggle/train_images_jpeg/877430309.jpg inflating: /content/kaggle/train_images_jpeg/877739255.jpg inflating: /content/kaggle/train_images_jpeg/878020784.jpg inflating: /content/kaggle/train_images_jpeg/87817214.jpg inflating: /content/kaggle/train_images_jpeg/878183173.jpg inflating: /content/kaggle/train_images_jpeg/878188830.jpg inflating: /content/kaggle/train_images_jpeg/878287964.jpg inflating: /content/kaggle/train_images_jpeg/878628068.jpg inflating: /content/kaggle/train_images_jpeg/878720080.jpg inflating: /content/kaggle/train_images_jpeg/878936941.jpg inflating: /content/kaggle/train_images_jpeg/879089403.jpg inflating: /content/kaggle/train_images_jpeg/879360102.jpg inflating: /content/kaggle/train_images_jpeg/879395678.jpg inflating: /content/kaggle/train_images_jpeg/879939600.jpg inflating: /content/kaggle/train_images_jpeg/879965131.jpg inflating: /content/kaggle/train_images_jpeg/880178968.jpg inflating: /content/kaggle/train_images_jpeg/880913529.jpg inflating: /content/kaggle/train_images_jpeg/880947570.jpg inflating: /content/kaggle/train_images_jpeg/881020012.jpg inflating: /content/kaggle/train_images_jpeg/881299929.jpg inflating: /content/kaggle/train_images_jpeg/881681381.jpg inflating: /content/kaggle/train_images_jpeg/881721336.jpg inflating: /content/kaggle/train_images_jpeg/881727330.jpg inflating: /content/kaggle/train_images_jpeg/882292084.jpg inflating: /content/kaggle/train_images_jpeg/883103193.jpg inflating: /content/kaggle/train_images_jpeg/883338295.jpg inflating: /content/kaggle/train_images_jpeg/883362689.jpg inflating: /content/kaggle/train_images_jpeg/883429890.jpg inflating: /content/kaggle/train_images_jpeg/883807735.jpg inflating: /content/kaggle/train_images_jpeg/884267453.jpg inflating: /content/kaggle/train_images_jpeg/88427493.jpg inflating: /content/kaggle/train_images_jpeg/88427642.jpg inflating: /content/kaggle/train_images_jpeg/884360889.jpg inflating: /content/kaggle/train_images_jpeg/884573629.jpg inflating: /content/kaggle/train_images_jpeg/884612746.jpg inflating: /content/kaggle/train_images_jpeg/884672332.jpg inflating: /content/kaggle/train_images_jpeg/885231391.jpg inflating: /content/kaggle/train_images_jpeg/885374983.jpg inflating: /content/kaggle/train_images_jpeg/885474029.jpg inflating: /content/kaggle/train_images_jpeg/886180513.jpg inflating: /content/kaggle/train_images_jpeg/886212263.jpg inflating: /content/kaggle/train_images_jpeg/886449535.jpg inflating: /content/kaggle/train_images_jpeg/886468951.jpg inflating: /content/kaggle/train_images_jpeg/886507028.jpg inflating: /content/kaggle/train_images_jpeg/886793145.jpg inflating: /content/kaggle/train_images_jpeg/886973933.jpg inflating: /content/kaggle/train_images_jpeg/887146964.jpg inflating: /content/kaggle/train_images_jpeg/887272418.jpg inflating: /content/kaggle/train_images_jpeg/887328137.jpg inflating: /content/kaggle/train_images_jpeg/887896841.jpg inflating: /content/kaggle/train_images_jpeg/888139689.jpg inflating: /content/kaggle/train_images_jpeg/888266861.jpg inflating: /content/kaggle/train_images_jpeg/888332390.jpg inflating: /content/kaggle/train_images_jpeg/888355689.jpg inflating: /content/kaggle/train_images_jpeg/888503637.jpg inflating: /content/kaggle/train_images_jpeg/888578440.jpg inflating: /content/kaggle/train_images_jpeg/88884986.jpg inflating: /content/kaggle/train_images_jpeg/888983519.jpg inflating: /content/kaggle/train_images_jpeg/889052134.jpg inflating: /content/kaggle/train_images_jpeg/889645201.jpg inflating: /content/kaggle/train_images_jpeg/889769958.jpg inflating: /content/kaggle/train_images_jpeg/889797204.jpg inflating: /content/kaggle/train_images_jpeg/889990340.jpg inflating: /content/kaggle/train_images_jpeg/890004895.jpg inflating: /content/kaggle/train_images_jpeg/890232152.jpg inflating: /content/kaggle/train_images_jpeg/890494248.jpg inflating: /content/kaggle/train_images_jpeg/890852512.jpg inflating: /content/kaggle/train_images_jpeg/891029839.jpg inflating: /content/kaggle/train_images_jpeg/891320496.jpg inflating: /content/kaggle/train_images_jpeg/891426683.jpg inflating: /content/kaggle/train_images_jpeg/891797832.jpg inflating: /content/kaggle/train_images_jpeg/891874541.jpg inflating: /content/kaggle/train_images_jpeg/891967804.jpg inflating: /content/kaggle/train_images_jpeg/892025744.jpg inflating: /content/kaggle/train_images_jpeg/893035.jpg inflating: /content/kaggle/train_images_jpeg/893103956.jpg inflating: /content/kaggle/train_images_jpeg/893564766.jpg inflating: /content/kaggle/train_images_jpeg/89393849.jpg inflating: /content/kaggle/train_images_jpeg/894286115.jpg inflating: /content/kaggle/train_images_jpeg/89450181.jpg inflating: /content/kaggle/train_images_jpeg/894506110.jpg inflating: /content/kaggle/train_images_jpeg/894849758.jpg inflating: /content/kaggle/train_images_jpeg/895206716.jpg inflating: /content/kaggle/train_images_jpeg/895280240.jpg inflating: /content/kaggle/train_images_jpeg/895356426.jpg inflating: /content/kaggle/train_images_jpeg/89587006.jpg inflating: /content/kaggle/train_images_jpeg/895910836.jpg inflating: /content/kaggle/train_images_jpeg/896047900.jpg inflating: /content/kaggle/train_images_jpeg/896277237.jpg inflating: /content/kaggle/train_images_jpeg/896983896.jpg inflating: /content/kaggle/train_images_jpeg/897274381.jpg inflating: /content/kaggle/train_images_jpeg/897533656.jpg inflating: /content/kaggle/train_images_jpeg/897638042.jpg inflating: /content/kaggle/train_images_jpeg/897739604.jpg inflating: /content/kaggle/train_images_jpeg/897745173.jpg inflating: /content/kaggle/train_images_jpeg/897925605.jpg inflating: /content/kaggle/train_images_jpeg/897929380.jpg inflating: /content/kaggle/train_images_jpeg/898210047.jpg inflating: /content/kaggle/train_images_jpeg/898358711.jpg inflating: /content/kaggle/train_images_jpeg/898386629.jpg inflating: /content/kaggle/train_images_jpeg/898578569.jpg inflating: /content/kaggle/train_images_jpeg/898753077.jpg inflating: /content/kaggle/train_images_jpeg/899172508.jpg inflating: /content/kaggle/train_images_jpeg/899398361.jpg inflating: /content/kaggle/train_images_jpeg/899918978.jpg inflating: /content/kaggle/train_images_jpeg/900061330.jpg inflating: /content/kaggle/train_images_jpeg/900061372.jpg inflating: /content/kaggle/train_images_jpeg/900067937.jpg inflating: /content/kaggle/train_images_jpeg/900415263.jpg inflating: /content/kaggle/train_images_jpeg/900445109.jpg inflating: /content/kaggle/train_images_jpeg/900587637.jpg inflating: /content/kaggle/train_images_jpeg/90091578.jpg inflating: /content/kaggle/train_images_jpeg/901055316.jpg inflating: /content/kaggle/train_images_jpeg/901063337.jpg inflating: /content/kaggle/train_images_jpeg/901196930.jpg inflating: /content/kaggle/train_images_jpeg/901274607.jpg inflating: /content/kaggle/train_images_jpeg/901404486.jpg inflating: /content/kaggle/train_images_jpeg/901443823.jpg inflating: /content/kaggle/train_images_jpeg/901886086.jpg inflating: /content/kaggle/train_images_jpeg/902007589.jpg inflating: /content/kaggle/train_images_jpeg/902139572.jpg inflating: /content/kaggle/train_images_jpeg/902299178.jpg inflating: /content/kaggle/train_images_jpeg/902300140.jpg inflating: /content/kaggle/train_images_jpeg/902429772.jpg inflating: /content/kaggle/train_images_jpeg/903186998.jpg inflating: /content/kaggle/train_images_jpeg/903252353.jpg inflating: /content/kaggle/train_images_jpeg/903309442.jpg inflating: /content/kaggle/train_images_jpeg/90333661.jpg inflating: /content/kaggle/train_images_jpeg/903439156.jpg inflating: /content/kaggle/train_images_jpeg/90391907.jpg inflating: /content/kaggle/train_images_jpeg/904192489.jpg inflating: /content/kaggle/train_images_jpeg/904658704.jpg inflating: /content/kaggle/train_images_jpeg/904699922.jpg inflating: /content/kaggle/train_images_jpeg/904750787.jpg inflating: /content/kaggle/train_images_jpeg/904785196.jpg inflating: /content/kaggle/train_images_jpeg/904850856.jpg inflating: /content/kaggle/train_images_jpeg/905336234.jpg inflating: /content/kaggle/train_images_jpeg/905387859.jpg inflating: /content/kaggle/train_images_jpeg/90543852.jpg inflating: /content/kaggle/train_images_jpeg/905776324.jpg inflating: /content/kaggle/train_images_jpeg/90606533.jpg inflating: /content/kaggle/train_images_jpeg/906115763.jpg inflating: /content/kaggle/train_images_jpeg/90625412.jpg inflating: /content/kaggle/train_images_jpeg/90670123.jpg inflating: /content/kaggle/train_images_jpeg/906750363.jpg inflating: /content/kaggle/train_images_jpeg/906773049.jpg inflating: /content/kaggle/train_images_jpeg/907434734.jpg inflating: /content/kaggle/train_images_jpeg/907691648.jpg inflating: /content/kaggle/train_images_jpeg/907755458.jpg inflating: /content/kaggle/train_images_jpeg/90800414.jpg inflating: /content/kaggle/train_images_jpeg/908540232.jpg inflating: /content/kaggle/train_images_jpeg/908770281.jpg inflating: /content/kaggle/train_images_jpeg/909032861.jpg inflating: /content/kaggle/train_images_jpeg/909125892.jpg inflating: /content/kaggle/train_images_jpeg/909499250.jpg inflating: /content/kaggle/train_images_jpeg/909719809.jpg inflating: /content/kaggle/train_images_jpeg/910008110.jpg inflating: /content/kaggle/train_images_jpeg/91058248.jpg inflating: /content/kaggle/train_images_jpeg/910617288.jpg inflating: /content/kaggle/train_images_jpeg/910735649.jpg inflating: /content/kaggle/train_images_jpeg/910870110.jpg inflating: /content/kaggle/train_images_jpeg/911144460.jpg inflating: /content/kaggle/train_images_jpeg/911151169.jpg inflating: /content/kaggle/train_images_jpeg/911449508.jpg inflating: /content/kaggle/train_images_jpeg/911861181.jpg inflating: /content/kaggle/train_images_jpeg/912150703.jpg inflating: /content/kaggle/train_images_jpeg/912628381.jpg inflating: /content/kaggle/train_images_jpeg/91285032.jpg inflating: /content/kaggle/train_images_jpeg/912860596.jpg inflating: /content/kaggle/train_images_jpeg/912912811.jpg inflating: /content/kaggle/train_images_jpeg/912959875.jpg inflating: /content/kaggle/train_images_jpeg/913152954.jpg inflating: /content/kaggle/train_images_jpeg/913261951.jpg inflating: /content/kaggle/train_images_jpeg/913294875.jpg inflating: /content/kaggle/train_images_jpeg/913436788.jpg inflating: /content/kaggle/train_images_jpeg/913480541.jpg inflating: /content/kaggle/train_images_jpeg/913567043.jpg inflating: /content/kaggle/train_images_jpeg/913589859.jpg inflating: /content/kaggle/train_images_jpeg/913877640.jpg inflating: /content/kaggle/train_images_jpeg/913968833.jpg inflating: /content/kaggle/train_images_jpeg/914090622.jpg inflating: /content/kaggle/train_images_jpeg/914202291.jpg inflating: /content/kaggle/train_images_jpeg/91429187.jpg inflating: /content/kaggle/train_images_jpeg/914936842.jpg inflating: /content/kaggle/train_images_jpeg/915371714.jpg inflating: /content/kaggle/train_images_jpeg/915715866.jpg inflating: /content/kaggle/train_images_jpeg/915719493.jpg inflating: /content/kaggle/train_images_jpeg/915934159.jpg inflating: /content/kaggle/train_images_jpeg/916184661.jpg inflating: /content/kaggle/train_images_jpeg/916221971.jpg inflating: /content/kaggle/train_images_jpeg/916237184.jpg inflating: /content/kaggle/train_images_jpeg/916448505.jpg inflating: /content/kaggle/train_images_jpeg/916669333.jpg inflating: /content/kaggle/train_images_jpeg/917014359.jpg inflating: /content/kaggle/train_images_jpeg/917042560.jpg inflating: /content/kaggle/train_images_jpeg/917153346.jpg inflating: /content/kaggle/train_images_jpeg/91726136.jpg inflating: /content/kaggle/train_images_jpeg/917306689.jpg inflating: /content/kaggle/train_images_jpeg/917489819.jpg inflating: /content/kaggle/train_images_jpeg/917739753.jpg inflating: /content/kaggle/train_images_jpeg/91815905.jpg inflating: /content/kaggle/train_images_jpeg/918168306.jpg inflating: /content/kaggle/train_images_jpeg/918301064.jpg inflating: /content/kaggle/train_images_jpeg/918306924.jpg inflating: /content/kaggle/train_images_jpeg/91850516.jpg inflating: /content/kaggle/train_images_jpeg/918534391.jpg inflating: /content/kaggle/train_images_jpeg/918535914.jpg inflating: /content/kaggle/train_images_jpeg/918605153.jpg inflating: /content/kaggle/train_images_jpeg/918675350.jpg inflating: /content/kaggle/train_images_jpeg/918696208.jpg inflating: /content/kaggle/train_images_jpeg/918823473.jpg inflating: /content/kaggle/train_images_jpeg/919127864.jpg inflating: /content/kaggle/train_images_jpeg/919479322.jpg inflating: /content/kaggle/train_images_jpeg/919514433.jpg inflating: /content/kaggle/train_images_jpeg/919581401.jpg inflating: /content/kaggle/train_images_jpeg/919597577.jpg inflating: /content/kaggle/train_images_jpeg/920229727.jpg inflating: /content/kaggle/train_images_jpeg/920295213.jpg inflating: /content/kaggle/train_images_jpeg/920401054.jpg inflating: /content/kaggle/train_images_jpeg/920600492.jpg inflating: /content/kaggle/train_images_jpeg/920623788.jpg inflating: /content/kaggle/train_images_jpeg/920702050.jpg inflating: /content/kaggle/train_images_jpeg/920981656.jpg inflating: /content/kaggle/train_images_jpeg/92128754.jpg inflating: /content/kaggle/train_images_jpeg/921425159.jpg inflating: /content/kaggle/train_images_jpeg/921779789.jpg inflating: /content/kaggle/train_images_jpeg/921830074.jpg inflating: /content/kaggle/train_images_jpeg/921927890.jpg inflating: /content/kaggle/train_images_jpeg/921975951.jpg inflating: /content/kaggle/train_images_jpeg/922000226.jpg inflating: /content/kaggle/train_images_jpeg/922100024.jpg inflating: /content/kaggle/train_images_jpeg/922395842.jpg inflating: /content/kaggle/train_images_jpeg/9224019.jpg inflating: /content/kaggle/train_images_jpeg/922729409.jpg inflating: /content/kaggle/train_images_jpeg/922756535.jpg inflating: /content/kaggle/train_images_jpeg/922802657.jpg inflating: /content/kaggle/train_images_jpeg/922887047.jpg inflating: /content/kaggle/train_images_jpeg/923034542.jpg inflating: /content/kaggle/train_images_jpeg/923294415.jpg inflating: /content/kaggle/train_images_jpeg/92340091.jpg inflating: /content/kaggle/train_images_jpeg/923499439.jpg inflating: /content/kaggle/train_images_jpeg/923718.jpg inflating: /content/kaggle/train_images_jpeg/923880010.jpg inflating: /content/kaggle/train_images_jpeg/923936404.jpg inflating: /content/kaggle/train_images_jpeg/923965406.jpg inflating: /content/kaggle/train_images_jpeg/923987815.jpg inflating: /content/kaggle/train_images_jpeg/924036255.jpg inflating: /content/kaggle/train_images_jpeg/924049818.jpg inflating: /content/kaggle/train_images_jpeg/92439956.jpg inflating: /content/kaggle/train_images_jpeg/925327697.jpg inflating: /content/kaggle/train_images_jpeg/925427672.jpg inflating: /content/kaggle/train_images_jpeg/9255514.jpg inflating: /content/kaggle/train_images_jpeg/925707812.jpg inflating: /content/kaggle/train_images_jpeg/925945591.jpg inflating: /content/kaggle/train_images_jpeg/926848319.jpg inflating: /content/kaggle/train_images_jpeg/927165736.jpg inflating: /content/kaggle/train_images_jpeg/927454705.jpg inflating: /content/kaggle/train_images_jpeg/927566387.jpg inflating: /content/kaggle/train_images_jpeg/927713688.jpg inflating: /content/kaggle/train_images_jpeg/927745791.jpg inflating: /content/kaggle/train_images_jpeg/929286178.jpg inflating: /content/kaggle/train_images_jpeg/929341901.jpg inflating: /content/kaggle/train_images_jpeg/929638503.jpg inflating: /content/kaggle/train_images_jpeg/929695181.jpg inflating: /content/kaggle/train_images_jpeg/930097123.jpg inflating: /content/kaggle/train_images_jpeg/9312065.jpg inflating: /content/kaggle/train_images_jpeg/931787054.jpg inflating: /content/kaggle/train_images_jpeg/931943521.jpg inflating: /content/kaggle/train_images_jpeg/93194372.jpg inflating: /content/kaggle/train_images_jpeg/932287482.jpg inflating: /content/kaggle/train_images_jpeg/932462711.jpg inflating: /content/kaggle/train_images_jpeg/932643551.jpg inflating: /content/kaggle/train_images_jpeg/932915192.jpg inflating: /content/kaggle/train_images_jpeg/932971696.jpg inflating: /content/kaggle/train_images_jpeg/932990043.jpg inflating: /content/kaggle/train_images_jpeg/933070130.jpg inflating: /content/kaggle/train_images_jpeg/933074739.jpg inflating: /content/kaggle/train_images_jpeg/933311576.jpg inflating: /content/kaggle/train_images_jpeg/933957288.jpg inflating: /content/kaggle/train_images_jpeg/934025261.jpg inflating: /content/kaggle/train_images_jpeg/934902236.jpg inflating: /content/kaggle/train_images_jpeg/935569912.jpg inflating: /content/kaggle/train_images_jpeg/935603838.jpg inflating: /content/kaggle/train_images_jpeg/935621798.jpg inflating: /content/kaggle/train_images_jpeg/935723534.jpg inflating: /content/kaggle/train_images_jpeg/936011517.jpg inflating: /content/kaggle/train_images_jpeg/936336303.jpg inflating: /content/kaggle/train_images_jpeg/936438569.jpg inflating: /content/kaggle/train_images_jpeg/93658953.jpg inflating: /content/kaggle/train_images_jpeg/936775758.jpg inflating: /content/kaggle/train_images_jpeg/936942979.jpg inflating: /content/kaggle/train_images_jpeg/937045512.jpg inflating: /content/kaggle/train_images_jpeg/937960341.jpg inflating: /content/kaggle/train_images_jpeg/938396942.jpg inflating: /content/kaggle/train_images_jpeg/938595992.jpg inflating: /content/kaggle/train_images_jpeg/938742634.jpg inflating: /content/kaggle/train_images_jpeg/938751255.jpg inflating: /content/kaggle/train_images_jpeg/938858274.jpg inflating: /content/kaggle/train_images_jpeg/938932168.jpg inflating: /content/kaggle/train_images_jpeg/939109747.jpg inflating: /content/kaggle/train_images_jpeg/939153733.jpg inflating: /content/kaggle/train_images_jpeg/939241436.jpg inflating: /content/kaggle/train_images_jpeg/939534770.jpg inflating: /content/kaggle/train_images_jpeg/93955818.jpg inflating: /content/kaggle/train_images_jpeg/939566384.jpg inflating: /content/kaggle/train_images_jpeg/93971994.jpg inflating: /content/kaggle/train_images_jpeg/939773003.jpg inflating: /content/kaggle/train_images_jpeg/939849436.jpg inflating: /content/kaggle/train_images_jpeg/939861475.jpg inflating: /content/kaggle/train_images_jpeg/93993180.jpg inflating: /content/kaggle/train_images_jpeg/940055437.jpg inflating: /content/kaggle/train_images_jpeg/940080842.jpg inflating: /content/kaggle/train_images_jpeg/940186959.jpg inflating: /content/kaggle/train_images_jpeg/94072059.jpg inflating: /content/kaggle/train_images_jpeg/940742966.jpg inflating: /content/kaggle/train_images_jpeg/940939729.jpg inflating: /content/kaggle/train_images_jpeg/940947597.jpg inflating: /content/kaggle/train_images_jpeg/941078633.jpg inflating: /content/kaggle/train_images_jpeg/941512047.jpg inflating: /content/kaggle/train_images_jpeg/94157515.jpg inflating: /content/kaggle/train_images_jpeg/94205320.jpg inflating: /content/kaggle/train_images_jpeg/942420393.jpg inflating: /content/kaggle/train_images_jpeg/942584104.jpg inflating: /content/kaggle/train_images_jpeg/942738737.jpg inflating: /content/kaggle/train_images_jpeg/942754311.jpg inflating: /content/kaggle/train_images_jpeg/942994246.jpg inflating: /content/kaggle/train_images_jpeg/943096902.jpg inflating: /content/kaggle/train_images_jpeg/943110824.jpg inflating: /content/kaggle/train_images_jpeg/943323109.jpg inflating: /content/kaggle/train_images_jpeg/943360126.jpg inflating: /content/kaggle/train_images_jpeg/943634756.jpg inflating: /content/kaggle/train_images_jpeg/943699790.jpg inflating: /content/kaggle/train_images_jpeg/943969763.jpg inflating: /content/kaggle/train_images_jpeg/944255811.jpg inflating: /content/kaggle/train_images_jpeg/944398652.jpg inflating: /content/kaggle/train_images_jpeg/944726140.jpg inflating: /content/kaggle/train_images_jpeg/944732366.jpg inflating: /content/kaggle/train_images_jpeg/944920581.jpg inflating: /content/kaggle/train_images_jpeg/94506249.jpg inflating: /content/kaggle/train_images_jpeg/945272909.jpg inflating: /content/kaggle/train_images_jpeg/94536090.jpg inflating: /content/kaggle/train_images_jpeg/9454129.jpg inflating: /content/kaggle/train_images_jpeg/945491819.jpg inflating: /content/kaggle/train_images_jpeg/945680317.jpg inflating: /content/kaggle/train_images_jpeg/94587407.jpg inflating: /content/kaggle/train_images_jpeg/945966296.jpg inflating: /content/kaggle/train_images_jpeg/946164219.jpg inflating: /content/kaggle/train_images_jpeg/947208202.jpg inflating: /content/kaggle/train_images_jpeg/947245045.jpg inflating: /content/kaggle/train_images_jpeg/947554618.jpg inflating: /content/kaggle/train_images_jpeg/947599962.jpg inflating: /content/kaggle/train_images_jpeg/948186989.jpg inflating: /content/kaggle/train_images_jpeg/948363257.jpg inflating: /content/kaggle/train_images_jpeg/948559323.jpg inflating: /content/kaggle/train_images_jpeg/948754372.jpg inflating: /content/kaggle/train_images_jpeg/94882276.jpg inflating: /content/kaggle/train_images_jpeg/948836344.jpg inflating: /content/kaggle/train_images_jpeg/948846771.jpg inflating: /content/kaggle/train_images_jpeg/949046024.jpg inflating: /content/kaggle/train_images_jpeg/94958616.jpg inflating: /content/kaggle/train_images_jpeg/949963080.jpg inflating: /content/kaggle/train_images_jpeg/949971770.jpg inflating: /content/kaggle/train_images_jpeg/950405197.jpg inflating: /content/kaggle/train_images_jpeg/950677455.jpg inflating: /content/kaggle/train_images_jpeg/950886267.jpg inflating: /content/kaggle/train_images_jpeg/951048679.jpg inflating: /content/kaggle/train_images_jpeg/95121846.jpg inflating: /content/kaggle/train_images_jpeg/951326010.jpg inflating: /content/kaggle/train_images_jpeg/951448683.jpg inflating: /content/kaggle/train_images_jpeg/9516242.jpg inflating: /content/kaggle/train_images_jpeg/951654457.jpg inflating: /content/kaggle/train_images_jpeg/951757474.jpg inflating: /content/kaggle/train_images_jpeg/951959546.jpg inflating: /content/kaggle/train_images_jpeg/952111080.jpg inflating: /content/kaggle/train_images_jpeg/952146173.jpg inflating: /content/kaggle/train_images_jpeg/952303505.jpg inflating: /content/kaggle/train_images_jpeg/952391818.jpg inflating: /content/kaggle/train_images_jpeg/952519185.jpg inflating: /content/kaggle/train_images_jpeg/952920056.jpg inflating: /content/kaggle/train_images_jpeg/953218056.jpg inflating: /content/kaggle/train_images_jpeg/95322868.jpg inflating: /content/kaggle/train_images_jpeg/953313732.jpg inflating: /content/kaggle/train_images_jpeg/953375887.jpg inflating: /content/kaggle/train_images_jpeg/953808797.jpg inflating: /content/kaggle/train_images_jpeg/953844785.jpg inflating: /content/kaggle/train_images_jpeg/953936588.jpg inflating: /content/kaggle/train_images_jpeg/954479560.jpg inflating: /content/kaggle/train_images_jpeg/954749288.jpg inflating: /content/kaggle/train_images_jpeg/954778743.jpg inflating: /content/kaggle/train_images_jpeg/9548002.jpg inflating: /content/kaggle/train_images_jpeg/955110808.jpg inflating: /content/kaggle/train_images_jpeg/955346673.jpg inflating: /content/kaggle/train_images_jpeg/95553397.jpg inflating: /content/kaggle/train_images_jpeg/955794248.jpg inflating: /content/kaggle/train_images_jpeg/956312100.jpg inflating: /content/kaggle/train_images_jpeg/956315464.jpg inflating: /content/kaggle/train_images_jpeg/956343434.jpg inflating: /content/kaggle/train_images_jpeg/956840852.jpg inflating: /content/kaggle/train_images_jpeg/957199549.jpg inflating: /content/kaggle/train_images_jpeg/957437817.jpg inflating: /content/kaggle/train_images_jpeg/957970680.jpg inflating: /content/kaggle/train_images_jpeg/958074553.jpg inflating: /content/kaggle/train_images_jpeg/958289716.jpg inflating: /content/kaggle/train_images_jpeg/958465343.jpg inflating: /content/kaggle/train_images_jpeg/958551982.jpg inflating: /content/kaggle/train_images_jpeg/958879588.jpg inflating: /content/kaggle/train_images_jpeg/959014862.jpg inflating: /content/kaggle/train_images_jpeg/959693086.jpg inflating: /content/kaggle/train_images_jpeg/959863341.jpg inflating: /content/kaggle/train_images_jpeg/96006478.jpg inflating: /content/kaggle/train_images_jpeg/960202242.jpg inflating: /content/kaggle/train_images_jpeg/960261822.jpg inflating: /content/kaggle/train_images_jpeg/960297309.jpg inflating: /content/kaggle/train_images_jpeg/96041444.jpg inflating: /content/kaggle/train_images_jpeg/960648391.jpg inflating: /content/kaggle/train_images_jpeg/960773451.jpg inflating: /content/kaggle/train_images_jpeg/961575661.jpg inflating: /content/kaggle/train_images_jpeg/961826850.jpg inflating: /content/kaggle/train_images_jpeg/961914208.jpg inflating: /content/kaggle/train_images_jpeg/961915557.jpg inflating: /content/kaggle/train_images_jpeg/962347612.jpg inflating: /content/kaggle/train_images_jpeg/96246425.jpg inflating: /content/kaggle/train_images_jpeg/962558857.jpg inflating: /content/kaggle/train_images_jpeg/963168720.jpg inflating: /content/kaggle/train_images_jpeg/963176335.jpg inflating: /content/kaggle/train_images_jpeg/963450221.jpg inflating: /content/kaggle/train_images_jpeg/963790461.jpg inflating: /content/kaggle/train_images_jpeg/964030938.jpg inflating: /content/kaggle/train_images_jpeg/964359867.jpg inflating: /content/kaggle/train_images_jpeg/964482896.jpg inflating: /content/kaggle/train_images_jpeg/964570288.jpg inflating: /content/kaggle/train_images_jpeg/964655123.jpg inflating: /content/kaggle/train_images_jpeg/965026603.jpg inflating: /content/kaggle/train_images_jpeg/965133347.jpg inflating: /content/kaggle/train_images_jpeg/965889493.jpg inflating: /content/kaggle/train_images_jpeg/965919968.jpg inflating: /content/kaggle/train_images_jpeg/966050678.jpg inflating: /content/kaggle/train_images_jpeg/966306135.jpg inflating: /content/kaggle/train_images_jpeg/966432144.jpg inflating: /content/kaggle/train_images_jpeg/966652545.jpg inflating: /content/kaggle/train_images_jpeg/966788042.jpg inflating: /content/kaggle/train_images_jpeg/967123833.jpg inflating: /content/kaggle/train_images_jpeg/967319500.jpg inflating: /content/kaggle/train_images_jpeg/967603965.jpg inflating: /content/kaggle/train_images_jpeg/967831491.jpg inflating: /content/kaggle/train_images_jpeg/967883712.jpg inflating: /content/kaggle/train_images_jpeg/968449352.jpg inflating: /content/kaggle/train_images_jpeg/968472833.jpg inflating: /content/kaggle/train_images_jpeg/968790449.jpg inflating: /content/kaggle/train_images_jpeg/968853775.jpg inflating: /content/kaggle/train_images_jpeg/968865165.jpg inflating: /content/kaggle/train_images_jpeg/969069572.jpg inflating: /content/kaggle/train_images_jpeg/96912752.jpg inflating: /content/kaggle/train_images_jpeg/969194504.jpg inflating: /content/kaggle/train_images_jpeg/969211018.jpg inflating: /content/kaggle/train_images_jpeg/969351419.jpg inflating: /content/kaggle/train_images_jpeg/969719974.jpg inflating: /content/kaggle/train_images_jpeg/969749670.jpg inflating: /content/kaggle/train_images_jpeg/969938724.jpg inflating: /content/kaggle/train_images_jpeg/970041120.jpg inflating: /content/kaggle/train_images_jpeg/970540696.jpg inflating: /content/kaggle/train_images_jpeg/970669415.jpg inflating: /content/kaggle/train_images_jpeg/970899436.jpg inflating: /content/kaggle/train_images_jpeg/970946808.jpg inflating: /content/kaggle/train_images_jpeg/971188113.jpg inflating: /content/kaggle/train_images_jpeg/971500459.jpg inflating: /content/kaggle/train_images_jpeg/971639298.jpg inflating: /content/kaggle/train_images_jpeg/971713914.jpg inflating: /content/kaggle/train_images_jpeg/972419188.jpg inflating: /content/kaggle/train_images_jpeg/972622677.jpg inflating: /content/kaggle/train_images_jpeg/972649867.jpg inflating: /content/kaggle/train_images_jpeg/972793378.jpg inflating: /content/kaggle/train_images_jpeg/972840038.jpg inflating: /content/kaggle/train_images_jpeg/972873188.jpg inflating: /content/kaggle/train_images_jpeg/97287651.jpg inflating: /content/kaggle/train_images_jpeg/972959733.jpg inflating: /content/kaggle/train_images_jpeg/973300702.jpg inflating: /content/kaggle/train_images_jpeg/973746900.jpg inflating: /content/kaggle/train_images_jpeg/97383533.jpg inflating: /content/kaggle/train_images_jpeg/974073314.jpg inflating: /content/kaggle/train_images_jpeg/974489379.jpg inflating: /content/kaggle/train_images_jpeg/974614447.jpg inflating: /content/kaggle/train_images_jpeg/975110881.jpg inflating: /content/kaggle/train_images_jpeg/975158492.jpg inflating: /content/kaggle/train_images_jpeg/975311959.jpg inflating: /content/kaggle/train_images_jpeg/975534940.jpg inflating: /content/kaggle/train_images_jpeg/975560905.jpg inflating: /content/kaggle/train_images_jpeg/976098356.jpg inflating: /content/kaggle/train_images_jpeg/976138379.jpg inflating: /content/kaggle/train_images_jpeg/976558597.jpg inflating: /content/kaggle/train_images_jpeg/976680630.jpg inflating: /content/kaggle/train_images_jpeg/976801924.jpg inflating: /content/kaggle/train_images_jpeg/976902576.jpg inflating: /content/kaggle/train_images_jpeg/976955239.jpg inflating: /content/kaggle/train_images_jpeg/976976000.jpg inflating: /content/kaggle/train_images_jpeg/977106252.jpg inflating: /content/kaggle/train_images_jpeg/977380491.jpg inflating: /content/kaggle/train_images_jpeg/977617704.jpg inflating: /content/kaggle/train_images_jpeg/977709534.jpg inflating: /content/kaggle/train_images_jpeg/977735708.jpg inflating: /content/kaggle/train_images_jpeg/978029996.jpg inflating: /content/kaggle/train_images_jpeg/978111352.jpg inflating: /content/kaggle/train_images_jpeg/978618490.jpg inflating: /content/kaggle/train_images_jpeg/978877878.jpg inflating: /content/kaggle/train_images_jpeg/979088794.jpg inflating: /content/kaggle/train_images_jpeg/979089486.jpg inflating: /content/kaggle/train_images_jpeg/979574790.jpg inflating: /content/kaggle/train_images_jpeg/979671168.jpg inflating: /content/kaggle/train_images_jpeg/979717744.jpg inflating: /content/kaggle/train_images_jpeg/979858694.jpg inflating: /content/kaggle/train_images_jpeg/979927449.jpg inflating: /content/kaggle/train_images_jpeg/980174544.jpg inflating: /content/kaggle/train_images_jpeg/980401006.jpg inflating: /content/kaggle/train_images_jpeg/980448273.jpg inflating: /content/kaggle/train_images_jpeg/980682914.jpg inflating: /content/kaggle/train_images_jpeg/980911264.jpg inflating: /content/kaggle/train_images_jpeg/981211210.jpg inflating: /content/kaggle/train_images_jpeg/981248959.jpg inflating: /content/kaggle/train_images_jpeg/98151706.jpg inflating: /content/kaggle/train_images_jpeg/981946821.jpg inflating: /content/kaggle/train_images_jpeg/981961513.jpg inflating: /content/kaggle/train_images_jpeg/982230894.jpg inflating: /content/kaggle/train_images_jpeg/982241079.jpg inflating: /content/kaggle/train_images_jpeg/982255022.jpg inflating: /content/kaggle/train_images_jpeg/982343142.jpg inflating: /content/kaggle/train_images_jpeg/982426232.jpg inflating: /content/kaggle/train_images_jpeg/982556736.jpg inflating: /content/kaggle/train_images_jpeg/982733151.jpg inflating: /content/kaggle/train_images_jpeg/982829407.jpg inflating: /content/kaggle/train_images_jpeg/982968211.jpg inflating: /content/kaggle/train_images_jpeg/983014273.jpg inflating: /content/kaggle/train_images_jpeg/983360921.jpg inflating: /content/kaggle/train_images_jpeg/983368345.jpg inflating: /content/kaggle/train_images_jpeg/984082173.jpg inflating: /content/kaggle/train_images_jpeg/984192870.jpg inflating: /content/kaggle/train_images_jpeg/984417293.jpg inflating: /content/kaggle/train_images_jpeg/984672636.jpg inflating: /content/kaggle/train_images_jpeg/984892057.jpg inflating: /content/kaggle/train_images_jpeg/985972952.jpg inflating: /content/kaggle/train_images_jpeg/986134312.jpg inflating: /content/kaggle/train_images_jpeg/986273020.jpg inflating: /content/kaggle/train_images_jpeg/986387276.jpg inflating: /content/kaggle/train_images_jpeg/986788190.jpg inflating: /content/kaggle/train_images_jpeg/986854149.jpg inflating: /content/kaggle/train_images_jpeg/986888785.jpg inflating: /content/kaggle/train_images_jpeg/986965803.jpg inflating: /content/kaggle/train_images_jpeg/986999751.jpg inflating: /content/kaggle/train_images_jpeg/987080644.jpg inflating: /content/kaggle/train_images_jpeg/987098553.jpg inflating: /content/kaggle/train_images_jpeg/987159645.jpg inflating: /content/kaggle/train_images_jpeg/987636835.jpg inflating: /content/kaggle/train_images_jpeg/988174802.jpg inflating: /content/kaggle/train_images_jpeg/988306549.jpg inflating: /content/kaggle/train_images_jpeg/988318054.jpg inflating: /content/kaggle/train_images_jpeg/989030666.jpg inflating: /content/kaggle/train_images_jpeg/989119004.jpg inflating: /content/kaggle/train_images_jpeg/989164337.jpg inflating: /content/kaggle/train_images_jpeg/989239917.jpg inflating: /content/kaggle/train_images_jpeg/990558315.jpg inflating: /content/kaggle/train_images_jpeg/99056821.jpg inflating: /content/kaggle/train_images_jpeg/990768027.jpg inflating: /content/kaggle/train_images_jpeg/990890343.jpg inflating: /content/kaggle/train_images_jpeg/991522911.jpg inflating: /content/kaggle/train_images_jpeg/991676292.jpg inflating: /content/kaggle/train_images_jpeg/991916399.jpg inflating: /content/kaggle/train_images_jpeg/99202708.jpg inflating: /content/kaggle/train_images_jpeg/992163916.jpg inflating: /content/kaggle/train_images_jpeg/992224132.jpg inflating: /content/kaggle/train_images_jpeg/992352996.jpg inflating: /content/kaggle/train_images_jpeg/992748624.jpg inflating: /content/kaggle/train_images_jpeg/992964459.jpg inflating: /content/kaggle/train_images_jpeg/993130539.jpg inflating: /content/kaggle/train_images_jpeg/993366541.jpg inflating: /content/kaggle/train_images_jpeg/993844551.jpg inflating: /content/kaggle/train_images_jpeg/99391680.jpg inflating: /content/kaggle/train_images_jpeg/993984792.jpg inflating: /content/kaggle/train_images_jpeg/994252979.jpg inflating: /content/kaggle/train_images_jpeg/994445094.jpg inflating: /content/kaggle/train_images_jpeg/994621972.jpg inflating: /content/kaggle/train_images_jpeg/994727955.jpg inflating: /content/kaggle/train_images_jpeg/995075067.jpg inflating: /content/kaggle/train_images_jpeg/995123333.jpg inflating: /content/kaggle/train_images_jpeg/995155483.jpg inflating: /content/kaggle/train_images_jpeg/995221528.jpg inflating: /content/kaggle/train_images_jpeg/99645916.jpg inflating: /content/kaggle/train_images_jpeg/996534381.jpg inflating: /content/kaggle/train_images_jpeg/996539252.jpg inflating: /content/kaggle/train_images_jpeg/996762577.jpg inflating: /content/kaggle/train_images_jpeg/996927503.jpg inflating: /content/kaggle/train_images_jpeg/996947690.jpg inflating: /content/kaggle/train_images_jpeg/996957803.jpg inflating: /content/kaggle/train_images_jpeg/997161074.jpg inflating: /content/kaggle/train_images_jpeg/997179968.jpg inflating: /content/kaggle/train_images_jpeg/997289539.jpg inflating: /content/kaggle/train_images_jpeg/997485103.jpg inflating: /content/kaggle/train_images_jpeg/997651546.jpg inflating: /content/kaggle/train_images_jpeg/997857988.jpg inflating: /content/kaggle/train_images_jpeg/997910101.jpg inflating: /content/kaggle/train_images_jpeg/997973414.jpg inflating: /content/kaggle/train_images_jpeg/998910982.jpg inflating: /content/kaggle/train_images_jpeg/999068805.jpg inflating: /content/kaggle/train_images_jpeg/999329392.jpg inflating: /content/kaggle/train_images_jpeg/999474432.jpg inflating: /content/kaggle/train_images_jpeg/999616605.jpg inflating: /content/kaggle/train_images_jpeg/999998473.jpg ! pip install - q torchsummary ! pip install - q torch == 1.7.0 ! pip install - q torchvision == 0.8.1 ! pip install - q scikit - learn == 0.23.2 ! pip install - q albumentations == 0.5.1 ! pip install - q geffnet == 1.0.0 ! pip install - q torchtoolbox == 0.1.5 \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.8MB 14.0MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 81kB 7.2MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 952kB 16.7MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 37.6MB 147kB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 6.6MB/s \u001b[?25h import os import random import time from typing import * import PIL.Image import albumentations import cv2 import numpy as np import pandas as pd import sklearn import torch import torch.nn as nn import torch.nn.functional as F import torchvision from albumentations.pytorch.transforms import ToTensorV2 import matplotlib.pyplot as plt from IPython.display import Image from sklearn.model_selection import StratifiedKFold , train_test_split from torch.optim import * from torch.utils.data import DataLoader , Dataset from torchsummary import summary from torchvision import models from tqdm import tqdm import geffnet from IPython.core.interactiveshell import InteractiveShell InteractiveShell . ast_node_interactivity = \"all\" Config class GlobalConfig : seed = 1930 num_classes = 11 class_list = [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] batch_size = 16 n_epochs = 15 tensor_size = ( 8 , 3 , 64 , 64 ) # unpack the key dict scheduler = 'CosineAnnealingWarmRestarts' scheduler_params = { 'StepLR' : { 'step_size' : 2 , 'gamma' : 0.3 , 'last_epoch' : - 1 , 'verbose' : True }, 'ReduceLROnPlateau' : { 'mode' : 'max' , 'factor' : 0.5 , 'patience' : 0 , 'threshold' : 0.0001 , 'threshold_mode' : 'rel' , 'cooldown' : 0 , 'min_lr' : 1e-5 , 'eps' : 1e-08 , 'verbose' : True }, 'CosineAnnealingWarmRestarts' : { 'T_0' : 10 , 'T_mult' : 1 , 'eta_min' : 1e-6 , 'last_epoch' : - 1 , 'verbose' : True }} # do scheduler.step after optimizer.step train_step_scheduler = False val_step_scheduler = True # optimizer optimizer = 'AdamW' optimizer_params = { 'AdamW' :{ 'lr' : 1e-3 , 'betas' :( 0.9 , 0.999 ), 'eps' : 1e-08 , 'weight_decay' : 1e-6 , 'amsgrad' : False }, 'Adam' :{ 'lr' : 1e-4 , 'betas' :( 0.9 , 0.999 ), 'eps' : 1e-08 , 'weight_decay' : 1e-6 , 'amsgrad' : False },} # criterion criterion = 'BCEWithLogitsLoss' criterion_val = 'BCEWithLogitsLoss' criterion_params = { 'BCEWithLogitsLoss' : { 'weight' : None , 'size_average' : None , 'reduce' : None , 'reduction' : 'mean' , 'pos_weight' : None }, 'CrossEntropyLoss' : { 'weight' : None , 'size_average' : None , 'ignore_index' : - 100 , 'reduce' : None , 'reduction' : 'mean' }, 'LabelSmoothingLoss' : { 'classes' : 2 , 'smoothing' : 0.05 , 'dim' : - 1 }, 'FocalCosineLoss' : { 'alpha' : 1 , 'gamma' : 2 , 'xent' : 0.1 }} image_col_name = 'image_id' class_col_name = 'label' paths = { 'train_path' : '/content/kaggle/train_images_jpeg/' , 'test_path' : '../input/siim-isic-melanoma-classification/jpeg/test' , 'csv_path' : '/content/drive/My Drive/Cassava/input/cassava-leaf-disease-classification/train.csv' , 'log_path' : './log.text' , 'save_path' : '/content/drive/My Drive/Cassava/weights/tf_effnet_b4_ns/18-Jan-V1' , 'model_weight_path_folder' : '/content/drive/My Drive/pretrained-effnet-weights' , 'image_path' : '/content/drive/My Drive/deep-learning-notes/notebooks/images' } model_name = 'tf_efficientnet_b5_ns' device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) config = GlobalConfig Utils def seed_all ( seed : int = 1930 ): print ( \"I love my Grandpa so the seed number is {} \" . format ( seed )) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) # set PYTHONHASHSEED env var at fixed value torch . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) torch . cuda . manual_seed ( seed ) # pytorch (both CPU and CUDA) np . random . seed ( seed ) # for numpy pseudo-random generator random . seed ( seed ) # set fixed value for python built-in pseudo-random generator torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = False torch . backends . cudnn . enabled = False def seed_worker ( _worker_id ): worker_seed = torch . initial_seed () % 2 ** 32 np . random . seed ( worker_seed ) random . seed ( worker_seed ) ###################################################################### seed_all ( config . seed ) device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) ###################################################################### I love my Grandpa so the seed number is 1930 def make_rand_tensors ( config ): # Create a batch size of 8 random images with 3 channels and 64x64 -> [N,C,W,H] rand_tensor = torch . ones ( config . tensor_size , dtype = torch . float ) . to ( config . device ) return rand_tensor def torchsummary_wrapper ( model , image_size : Tuple ): model_summary = summary ( model , image_size ) return model_summary def display_image ( config , page_num ): image = os . path . join ( config . paths [ 'hongnan_notes' ], page_num ) return Image ( image ) Learning the Syntax of PyTorch torch.view() reference torch.nn vs torch.nn.functional torch.nn.Relu vs torch.nn.functional.Relu using-dropout-in-pytorch-nn-dropout-vs-f-dropout Building Neural Networks Convolutional Layers import torch import torch.nn as nn import torch.optim as optim from torch.optim import lr_scheduler import numpy as np import torchvision from torchvision import datasets , models , transforms import matplotlib.pyplot as plt import time import os import copy plt . ion () # interactive mode ! unzip '/content/drive/My Drive/deep-learning-notes/notebooks/hymenoptera_data.zip' - d '/content/' Archive: /content/drive/My Drive/deep-learning-notes/notebooks/hymenoptera_data.zip creating: /content/hymenoptera_data/ creating: /content/hymenoptera_data/train/ creating: /content/hymenoptera_data/train/ants/ inflating: /content/hymenoptera_data/train/ants/0013035.jpg inflating: /content/hymenoptera_data/train/ants/1030023514_aad5c608f9.jpg inflating: /content/hymenoptera_data/train/ants/1095476100_3906d8afde.jpg inflating: /content/hymenoptera_data/train/ants/1099452230_d1949d3250.jpg inflating: /content/hymenoptera_data/train/ants/116570827_e9c126745d.jpg inflating: /content/hymenoptera_data/train/ants/1225872729_6f0856588f.jpg inflating: /content/hymenoptera_data/train/ants/1262877379_64fcada201.jpg inflating: /content/hymenoptera_data/train/ants/1269756697_0bce92cdab.jpg inflating: /content/hymenoptera_data/train/ants/1286984635_5119e80de1.jpg inflating: /content/hymenoptera_data/train/ants/132478121_2a430adea2.jpg inflating: /content/hymenoptera_data/train/ants/1360291657_dc248c5eea.jpg inflating: /content/hymenoptera_data/train/ants/1368913450_e146e2fb6d.jpg inflating: /content/hymenoptera_data/train/ants/1473187633_63ccaacea6.jpg inflating: /content/hymenoptera_data/train/ants/148715752_302c84f5a4.jpg inflating: /content/hymenoptera_data/train/ants/1489674356_09d48dde0a.jpg inflating: /content/hymenoptera_data/train/ants/149244013_c529578289.jpg inflating: /content/hymenoptera_data/train/ants/150801003_3390b73135.jpg inflating: /content/hymenoptera_data/train/ants/150801171_cd86f17ed8.jpg inflating: /content/hymenoptera_data/train/ants/154124431_65460430f2.jpg inflating: /content/hymenoptera_data/train/ants/162603798_40b51f1654.jpg inflating: /content/hymenoptera_data/train/ants/1660097129_384bf54490.jpg inflating: /content/hymenoptera_data/train/ants/167890289_dd5ba923f3.jpg inflating: /content/hymenoptera_data/train/ants/1693954099_46d4c20605.jpg inflating: /content/hymenoptera_data/train/ants/175998972.jpg inflating: /content/hymenoptera_data/train/ants/178538489_bec7649292.jpg inflating: /content/hymenoptera_data/train/ants/1804095607_0341701e1c.jpg inflating: /content/hymenoptera_data/train/ants/1808777855_2a895621d7.jpg inflating: /content/hymenoptera_data/train/ants/188552436_605cc9b36b.jpg inflating: /content/hymenoptera_data/train/ants/1917341202_d00a7f9af5.jpg inflating: /content/hymenoptera_data/train/ants/1924473702_daa9aacdbe.jpg inflating: /content/hymenoptera_data/train/ants/196057951_63bf063b92.jpg inflating: /content/hymenoptera_data/train/ants/196757565_326437f5fe.jpg inflating: /content/hymenoptera_data/train/ants/201558278_fe4caecc76.jpg inflating: /content/hymenoptera_data/train/ants/201790779_527f4c0168.jpg inflating: /content/hymenoptera_data/train/ants/2019439677_2db655d361.jpg inflating: /content/hymenoptera_data/train/ants/207947948_3ab29d7207.jpg inflating: /content/hymenoptera_data/train/ants/20935278_9190345f6b.jpg inflating: /content/hymenoptera_data/train/ants/224655713_3956f7d39a.jpg inflating: /content/hymenoptera_data/train/ants/2265824718_2c96f485da.jpg inflating: /content/hymenoptera_data/train/ants/2265825502_fff99cfd2d.jpg inflating: /content/hymenoptera_data/train/ants/226951206_d6bf946504.jpg inflating: /content/hymenoptera_data/train/ants/2278278459_6b99605e50.jpg inflating: /content/hymenoptera_data/train/ants/2288450226_a6e96e8fdf.jpg inflating: /content/hymenoptera_data/train/ants/2288481644_83ff7e4572.jpg inflating: /content/hymenoptera_data/train/ants/2292213964_ca51ce4bef.jpg inflating: /content/hymenoptera_data/train/ants/24335309_c5ea483bb8.jpg inflating: /content/hymenoptera_data/train/ants/245647475_9523dfd13e.jpg inflating: /content/hymenoptera_data/train/ants/255434217_1b2b3fe0a4.jpg inflating: /content/hymenoptera_data/train/ants/258217966_d9d90d18d3.jpg inflating: /content/hymenoptera_data/train/ants/275429470_b2d7d9290b.jpg inflating: /content/hymenoptera_data/train/ants/28847243_e79fe052cd.jpg inflating: /content/hymenoptera_data/train/ants/318052216_84dff3f98a.jpg inflating: /content/hymenoptera_data/train/ants/334167043_cbd1adaeb9.jpg inflating: /content/hymenoptera_data/train/ants/339670531_94b75ae47a.jpg inflating: /content/hymenoptera_data/train/ants/342438950_a3da61deab.jpg inflating: /content/hymenoptera_data/train/ants/36439863_0bec9f554f.jpg inflating: /content/hymenoptera_data/train/ants/374435068_7eee412ec4.jpg inflating: /content/hymenoptera_data/train/ants/382971067_0bfd33afe0.jpg inflating: /content/hymenoptera_data/train/ants/384191229_5779cf591b.jpg inflating: /content/hymenoptera_data/train/ants/386190770_672743c9a7.jpg inflating: /content/hymenoptera_data/train/ants/392382602_1b7bed32fa.jpg inflating: /content/hymenoptera_data/train/ants/403746349_71384f5b58.jpg inflating: /content/hymenoptera_data/train/ants/408393566_b5b694119b.jpg inflating: /content/hymenoptera_data/train/ants/424119020_6d57481dab.jpg inflating: /content/hymenoptera_data/train/ants/424873399_47658a91fb.jpg inflating: /content/hymenoptera_data/train/ants/450057712_771b3bfc91.jpg inflating: /content/hymenoptera_data/train/ants/45472593_bfd624f8dc.jpg inflating: /content/hymenoptera_data/train/ants/459694881_ac657d3187.jpg inflating: /content/hymenoptera_data/train/ants/460372577_f2f6a8c9fc.jpg inflating: /content/hymenoptera_data/train/ants/460874319_0a45ab4d05.jpg inflating: /content/hymenoptera_data/train/ants/466430434_4000737de9.jpg inflating: /content/hymenoptera_data/train/ants/470127037_513711fd21.jpg inflating: /content/hymenoptera_data/train/ants/474806473_ca6caab245.jpg inflating: /content/hymenoptera_data/train/ants/475961153_b8c13fd405.jpg inflating: /content/hymenoptera_data/train/ants/484293231_e53cfc0c89.jpg inflating: /content/hymenoptera_data/train/ants/49375974_e28ba6f17e.jpg inflating: /content/hymenoptera_data/train/ants/506249802_207cd979b4.jpg inflating: /content/hymenoptera_data/train/ants/506249836_717b73f540.jpg inflating: /content/hymenoptera_data/train/ants/512164029_c0a66b8498.jpg inflating: /content/hymenoptera_data/train/ants/512863248_43c8ce579b.jpg inflating: /content/hymenoptera_data/train/ants/518773929_734dbc5ff4.jpg inflating: /content/hymenoptera_data/train/ants/522163566_fec115ca66.jpg inflating: /content/hymenoptera_data/train/ants/522415432_2218f34bf8.jpg inflating: /content/hymenoptera_data/train/ants/531979952_bde12b3bc0.jpg inflating: /content/hymenoptera_data/train/ants/533848102_70a85ad6dd.jpg inflating: /content/hymenoptera_data/train/ants/535522953_308353a07c.jpg inflating: /content/hymenoptera_data/train/ants/540889389_48bb588b21.jpg inflating: /content/hymenoptera_data/train/ants/541630764_dbd285d63c.jpg inflating: /content/hymenoptera_data/train/ants/543417860_b14237f569.jpg inflating: /content/hymenoptera_data/train/ants/560966032_988f4d7bc4.jpg inflating: /content/hymenoptera_data/train/ants/5650366_e22b7e1065.jpg inflating: /content/hymenoptera_data/train/ants/6240329_72c01e663e.jpg inflating: /content/hymenoptera_data/train/ants/6240338_93729615ec.jpg inflating: /content/hymenoptera_data/train/ants/649026570_e58656104b.jpg inflating: /content/hymenoptera_data/train/ants/662541407_ff8db781e7.jpg inflating: /content/hymenoptera_data/train/ants/67270775_e9fdf77e9d.jpg inflating: /content/hymenoptera_data/train/ants/6743948_2b8c096dda.jpg inflating: /content/hymenoptera_data/train/ants/684133190_35b62c0c1d.jpg inflating: /content/hymenoptera_data/train/ants/69639610_95e0de17aa.jpg inflating: /content/hymenoptera_data/train/ants/707895295_009cf23188.jpg inflating: /content/hymenoptera_data/train/ants/7759525_1363d24e88.jpg inflating: /content/hymenoptera_data/train/ants/795000156_a9900a4a71.jpg inflating: /content/hymenoptera_data/train/ants/822537660_caf4ba5514.jpg inflating: /content/hymenoptera_data/train/ants/82852639_52b7f7f5e3.jpg inflating: /content/hymenoptera_data/train/ants/841049277_b28e58ad05.jpg inflating: /content/hymenoptera_data/train/ants/886401651_f878e888cd.jpg inflating: /content/hymenoptera_data/train/ants/892108839_f1aad4ca46.jpg inflating: /content/hymenoptera_data/train/ants/938946700_ca1c669085.jpg inflating: /content/hymenoptera_data/train/ants/957233405_25c1d1187b.jpg inflating: /content/hymenoptera_data/train/ants/9715481_b3cb4114ff.jpg inflating: /content/hymenoptera_data/train/ants/998118368_6ac1d91f81.jpg inflating: /content/hymenoptera_data/train/ants/ant photos.jpg inflating: /content/hymenoptera_data/train/ants/Ant_1.jpg inflating: /content/hymenoptera_data/train/ants/army-ants-red-picture.jpg inflating: /content/hymenoptera_data/train/ants/formica.jpeg inflating: /content/hymenoptera_data/train/ants/hormiga_co_por.jpg inflating: /content/hymenoptera_data/train/ants/imageNotFound.gif inflating: /content/hymenoptera_data/train/ants/kurokusa.jpg inflating: /content/hymenoptera_data/train/ants/MehdiabadiAnt2_600.jpg inflating: /content/hymenoptera_data/train/ants/Nepenthes_rafflesiana_ant.jpg inflating: /content/hymenoptera_data/train/ants/swiss-army-ant.jpg inflating: /content/hymenoptera_data/train/ants/termite-vs-ant.jpg inflating: /content/hymenoptera_data/train/ants/trap-jaw-ant-insect-bg.jpg inflating: /content/hymenoptera_data/train/ants/VietnameseAntMimicSpider.jpg creating: /content/hymenoptera_data/train/bees/ inflating: /content/hymenoptera_data/train/bees/1092977343_cb42b38d62.jpg inflating: /content/hymenoptera_data/train/bees/1093831624_fb5fbe2308.jpg inflating: /content/hymenoptera_data/train/bees/1097045929_1753d1c765.jpg inflating: /content/hymenoptera_data/train/bees/1232245714_f862fbe385.jpg inflating: /content/hymenoptera_data/train/bees/129236073_0985e91c7d.jpg inflating: /content/hymenoptera_data/train/bees/1295655112_7813f37d21.jpg inflating: /content/hymenoptera_data/train/bees/132511197_0b86ad0fff.jpg inflating: /content/hymenoptera_data/train/bees/132826773_dbbcb117b9.jpg inflating: /content/hymenoptera_data/train/bees/150013791_969d9a968b.jpg inflating: /content/hymenoptera_data/train/bees/1508176360_2972117c9d.jpg inflating: /content/hymenoptera_data/train/bees/154600396_53e1252e52.jpg inflating: /content/hymenoptera_data/train/bees/16838648_415acd9e3f.jpg inflating: /content/hymenoptera_data/train/bees/1691282715_0addfdf5e8.jpg inflating: /content/hymenoptera_data/train/bees/17209602_fe5a5a746f.jpg inflating: /content/hymenoptera_data/train/bees/174142798_e5ad6d76e0.jpg inflating: /content/hymenoptera_data/train/bees/1799726602_8580867f71.jpg inflating: /content/hymenoptera_data/train/bees/1807583459_4fe92b3133.jpg inflating: /content/hymenoptera_data/train/bees/196430254_46bd129ae7.jpg inflating: /content/hymenoptera_data/train/bees/196658222_3fffd79c67.jpg inflating: /content/hymenoptera_data/train/bees/198508668_97d818b6c4.jpg inflating: /content/hymenoptera_data/train/bees/2031225713_50ed499635.jpg inflating: /content/hymenoptera_data/train/bees/2037437624_2d7bce461f.jpg inflating: /content/hymenoptera_data/train/bees/2053200300_8911ef438a.jpg inflating: /content/hymenoptera_data/train/bees/205835650_e6f2614bee.jpg inflating: /content/hymenoptera_data/train/bees/208702903_42fb4d9748.jpg inflating: /content/hymenoptera_data/train/bees/21399619_3e61e5bb6f.jpg inflating: /content/hymenoptera_data/train/bees/2227611847_ec72d40403.jpg inflating: /content/hymenoptera_data/train/bees/2321139806_d73d899e66.jpg inflating: /content/hymenoptera_data/train/bees/2330918208_8074770c20.jpg inflating: /content/hymenoptera_data/train/bees/2345177635_caf07159b3.jpg inflating: /content/hymenoptera_data/train/bees/2358061370_9daabbd9ac.jpg inflating: /content/hymenoptera_data/train/bees/2364597044_3c3e3fc391.jpg inflating: /content/hymenoptera_data/train/bees/2384149906_2cd8b0b699.jpg inflating: /content/hymenoptera_data/train/bees/2397446847_04ef3cd3e1.jpg inflating: /content/hymenoptera_data/train/bees/2405441001_b06c36fa72.jpg inflating: /content/hymenoptera_data/train/bees/2445215254_51698ff797.jpg inflating: /content/hymenoptera_data/train/bees/2452236943_255bfd9e58.jpg inflating: /content/hymenoptera_data/train/bees/2467959963_a7831e9ff0.jpg inflating: /content/hymenoptera_data/train/bees/2470492904_837e97800d.jpg inflating: /content/hymenoptera_data/train/bees/2477324698_3d4b1b1cab.jpg inflating: /content/hymenoptera_data/train/bees/2477349551_e75c97cf4d.jpg inflating: /content/hymenoptera_data/train/bees/2486729079_62df0920be.jpg inflating: /content/hymenoptera_data/train/bees/2486746709_c43cec0e42.jpg inflating: /content/hymenoptera_data/train/bees/2493379287_4100e1dacc.jpg inflating: /content/hymenoptera_data/train/bees/2495722465_879acf9d85.jpg inflating: /content/hymenoptera_data/train/bees/2528444139_fa728b0f5b.jpg inflating: /content/hymenoptera_data/train/bees/2538361678_9da84b77e3.jpg inflating: /content/hymenoptera_data/train/bees/2551813042_8a070aeb2b.jpg inflating: /content/hymenoptera_data/train/bees/2580598377_a4caecdb54.jpg inflating: /content/hymenoptera_data/train/bees/2601176055_8464e6aa71.jpg inflating: /content/hymenoptera_data/train/bees/2610833167_79bf0bcae5.jpg inflating: /content/hymenoptera_data/train/bees/2610838525_fe8e3cae47.jpg inflating: /content/hymenoptera_data/train/bees/2617161745_fa3ebe85b4.jpg inflating: /content/hymenoptera_data/train/bees/2625499656_e3415e374d.jpg inflating: /content/hymenoptera_data/train/bees/2634617358_f32fd16bea.jpg inflating: /content/hymenoptera_data/train/bees/2638074627_6b3ae746a0.jpg inflating: /content/hymenoptera_data/train/bees/2645107662_b73a8595cc.jpg inflating: /content/hymenoptera_data/train/bees/2651621464_a2fa8722eb.jpg inflating: /content/hymenoptera_data/train/bees/2652877533_a564830cbf.jpg inflating: /content/hymenoptera_data/train/bees/266644509_d30bb16a1b.jpg inflating: /content/hymenoptera_data/train/bees/2683605182_9d2a0c66cf.jpg inflating: /content/hymenoptera_data/train/bees/2704348794_eb5d5178c2.jpg inflating: /content/hymenoptera_data/train/bees/2707440199_cd170bd512.jpg inflating: /content/hymenoptera_data/train/bees/2710368626_cb42882dc8.jpg inflating: /content/hymenoptera_data/train/bees/2722592222_258d473e17.jpg inflating: /content/hymenoptera_data/train/bees/2728759455_ce9bb8cd7a.jpg inflating: /content/hymenoptera_data/train/bees/2756397428_1d82a08807.jpg inflating: /content/hymenoptera_data/train/bees/2765347790_da6cf6cb40.jpg inflating: /content/hymenoptera_data/train/bees/2781170484_5d61835d63.jpg inflating: /content/hymenoptera_data/train/bees/279113587_b4843db199.jpg inflating: /content/hymenoptera_data/train/bees/2792000093_e8ae0718cf.jpg inflating: /content/hymenoptera_data/train/bees/2801728106_833798c909.jpg inflating: /content/hymenoptera_data/train/bees/2822388965_f6dca2a275.jpg inflating: /content/hymenoptera_data/train/bees/2861002136_52c7c6f708.jpg inflating: /content/hymenoptera_data/train/bees/2908916142_a7ac8b57a8.jpg inflating: /content/hymenoptera_data/train/bees/29494643_e3410f0d37.jpg inflating: /content/hymenoptera_data/train/bees/2959730355_416a18c63c.jpg inflating: /content/hymenoptera_data/train/bees/2962405283_22718d9617.jpg inflating: /content/hymenoptera_data/train/bees/3006264892_30e9cced70.jpg inflating: /content/hymenoptera_data/train/bees/3030189811_01d095b793.jpg inflating: /content/hymenoptera_data/train/bees/3030772428_8578335616.jpg inflating: /content/hymenoptera_data/train/bees/3044402684_3853071a87.jpg inflating: /content/hymenoptera_data/train/bees/3074585407_9854eb3153.jpg inflating: /content/hymenoptera_data/train/bees/3079610310_ac2d0ae7bc.jpg inflating: /content/hymenoptera_data/train/bees/3090975720_71f12e6de4.jpg inflating: /content/hymenoptera_data/train/bees/3100226504_c0d4f1e3f1.jpg inflating: /content/hymenoptera_data/train/bees/342758693_c56b89b6b6.jpg inflating: /content/hymenoptera_data/train/bees/354167719_22dca13752.jpg inflating: /content/hymenoptera_data/train/bees/359928878_b3b418c728.jpg inflating: /content/hymenoptera_data/train/bees/365759866_b15700c59b.jpg inflating: /content/hymenoptera_data/train/bees/36900412_92b81831ad.jpg inflating: /content/hymenoptera_data/train/bees/39672681_1302d204d1.jpg inflating: /content/hymenoptera_data/train/bees/39747887_42df2855ee.jpg inflating: /content/hymenoptera_data/train/bees/421515404_e87569fd8b.jpg inflating: /content/hymenoptera_data/train/bees/444532809_9e931e2279.jpg inflating: /content/hymenoptera_data/train/bees/446296270_d9e8b93ecf.jpg inflating: /content/hymenoptera_data/train/bees/452462677_7be43af8ff.jpg inflating: /content/hymenoptera_data/train/bees/452462695_40a4e5b559.jpg inflating: /content/hymenoptera_data/train/bees/457457145_5f86eb7e9c.jpg inflating: /content/hymenoptera_data/train/bees/465133211_80e0c27f60.jpg inflating: /content/hymenoptera_data/train/bees/469333327_358ba8fe8a.jpg inflating: /content/hymenoptera_data/train/bees/472288710_2abee16fa0.jpg inflating: /content/hymenoptera_data/train/bees/473618094_8ffdcab215.jpg inflating: /content/hymenoptera_data/train/bees/476347960_52edd72b06.jpg inflating: /content/hymenoptera_data/train/bees/478701318_bbd5e557b8.jpg inflating: /content/hymenoptera_data/train/bees/507288830_f46e8d4cb2.jpg inflating: /content/hymenoptera_data/train/bees/509247772_2db2d01374.jpg inflating: /content/hymenoptera_data/train/bees/513545352_fd3e7c7c5d.jpg inflating: /content/hymenoptera_data/train/bees/522104315_5d3cb2758e.jpg inflating: /content/hymenoptera_data/train/bees/537309131_532bfa59ea.jpg inflating: /content/hymenoptera_data/train/bees/586041248_3032e277a9.jpg inflating: /content/hymenoptera_data/train/bees/760526046_547e8b381f.jpg inflating: /content/hymenoptera_data/train/bees/760568592_45a52c847f.jpg inflating: /content/hymenoptera_data/train/bees/774440991_63a4aa0cbe.jpg inflating: /content/hymenoptera_data/train/bees/85112639_6e860b0469.jpg inflating: /content/hymenoptera_data/train/bees/873076652_eb098dab2d.jpg inflating: /content/hymenoptera_data/train/bees/90179376_abc234e5f4.jpg inflating: /content/hymenoptera_data/train/bees/92663402_37f379e57a.jpg inflating: /content/hymenoptera_data/train/bees/95238259_98470c5b10.jpg inflating: /content/hymenoptera_data/train/bees/969455125_58c797ef17.jpg inflating: /content/hymenoptera_data/train/bees/98391118_bdb1e80cce.jpg creating: /content/hymenoptera_data/val/ creating: /content/hymenoptera_data/val/ants/ inflating: /content/hymenoptera_data/val/ants/10308379_1b6c72e180.jpg inflating: /content/hymenoptera_data/val/ants/1053149811_f62a3410d3.jpg inflating: /content/hymenoptera_data/val/ants/1073564163_225a64f170.jpg inflating: /content/hymenoptera_data/val/ants/1119630822_cd325ea21a.jpg inflating: /content/hymenoptera_data/val/ants/1124525276_816a07c17f.jpg inflating: /content/hymenoptera_data/val/ants/11381045_b352a47d8c.jpg inflating: /content/hymenoptera_data/val/ants/119785936_dd428e40c3.jpg inflating: /content/hymenoptera_data/val/ants/1247887232_edcb61246c.jpg inflating: /content/hymenoptera_data/val/ants/1262751255_c56c042b7b.jpg inflating: /content/hymenoptera_data/val/ants/1337725712_2eb53cd742.jpg inflating: /content/hymenoptera_data/val/ants/1358854066_5ad8015f7f.jpg inflating: /content/hymenoptera_data/val/ants/1440002809_b268d9a66a.jpg inflating: /content/hymenoptera_data/val/ants/147542264_79506478c2.jpg inflating: /content/hymenoptera_data/val/ants/152286280_411648ec27.jpg inflating: /content/hymenoptera_data/val/ants/153320619_2aeb5fa0ee.jpg inflating: /content/hymenoptera_data/val/ants/153783656_85f9c3ac70.jpg inflating: /content/hymenoptera_data/val/ants/157401988_d0564a9d02.jpg inflating: /content/hymenoptera_data/val/ants/159515240_d5981e20d1.jpg inflating: /content/hymenoptera_data/val/ants/161076144_124db762d6.jpg inflating: /content/hymenoptera_data/val/ants/161292361_c16e0bf57a.jpg inflating: /content/hymenoptera_data/val/ants/170652283_ecdaff5d1a.jpg inflating: /content/hymenoptera_data/val/ants/17081114_79b9a27724.jpg inflating: /content/hymenoptera_data/val/ants/172772109_d0a8e15fb0.jpg inflating: /content/hymenoptera_data/val/ants/1743840368_b5ccda82b7.jpg inflating: /content/hymenoptera_data/val/ants/181942028_961261ef48.jpg inflating: /content/hymenoptera_data/val/ants/183260961_64ab754c97.jpg inflating: /content/hymenoptera_data/val/ants/2039585088_c6f47c592e.jpg inflating: /content/hymenoptera_data/val/ants/205398178_c395c5e460.jpg inflating: /content/hymenoptera_data/val/ants/208072188_f293096296.jpg inflating: /content/hymenoptera_data/val/ants/209615353_eeb38ba204.jpg inflating: /content/hymenoptera_data/val/ants/2104709400_8831b4fc6f.jpg inflating: /content/hymenoptera_data/val/ants/212100470_b485e7b7b9.jpg inflating: /content/hymenoptera_data/val/ants/2127908701_d49dc83c97.jpg inflating: /content/hymenoptera_data/val/ants/2191997003_379df31291.jpg inflating: /content/hymenoptera_data/val/ants/2211974567_ee4606b493.jpg inflating: /content/hymenoptera_data/val/ants/2219621907_47bc7cc6b0.jpg inflating: /content/hymenoptera_data/val/ants/2238242353_52c82441df.jpg inflating: /content/hymenoptera_data/val/ants/2255445811_dabcdf7258.jpg inflating: /content/hymenoptera_data/val/ants/239161491_86ac23b0a3.jpg inflating: /content/hymenoptera_data/val/ants/263615709_cfb28f6b8e.jpg inflating: /content/hymenoptera_data/val/ants/308196310_1db5ffa01b.jpg inflating: /content/hymenoptera_data/val/ants/319494379_648fb5a1c6.jpg inflating: /content/hymenoptera_data/val/ants/35558229_1fa4608a7a.jpg inflating: /content/hymenoptera_data/val/ants/412436937_4c2378efc2.jpg inflating: /content/hymenoptera_data/val/ants/436944325_d4925a38c7.jpg inflating: /content/hymenoptera_data/val/ants/445356866_6cb3289067.jpg inflating: /content/hymenoptera_data/val/ants/459442412_412fecf3fe.jpg inflating: /content/hymenoptera_data/val/ants/470127071_8b8ee2bd74.jpg inflating: /content/hymenoptera_data/val/ants/477437164_bc3e6e594a.jpg inflating: /content/hymenoptera_data/val/ants/488272201_c5aa281348.jpg inflating: /content/hymenoptera_data/val/ants/502717153_3e4865621a.jpg inflating: /content/hymenoptera_data/val/ants/518746016_bcc28f8b5b.jpg inflating: /content/hymenoptera_data/val/ants/540543309_ddbb193ee5.jpg inflating: /content/hymenoptera_data/val/ants/562589509_7e55469b97.jpg inflating: /content/hymenoptera_data/val/ants/57264437_a19006872f.jpg inflating: /content/hymenoptera_data/val/ants/573151833_ebbc274b77.jpg inflating: /content/hymenoptera_data/val/ants/649407494_9b6bc4949f.jpg inflating: /content/hymenoptera_data/val/ants/751649788_78dd7d16ce.jpg inflating: /content/hymenoptera_data/val/ants/768870506_8f115d3d37.jpg inflating: /content/hymenoptera_data/val/ants/800px-Meat_eater_ant_qeen_excavating_hole.jpg inflating: /content/hymenoptera_data/val/ants/8124241_36b290d372.jpg inflating: /content/hymenoptera_data/val/ants/8398478_50ef10c47a.jpg inflating: /content/hymenoptera_data/val/ants/854534770_31f6156383.jpg inflating: /content/hymenoptera_data/val/ants/892676922_4ab37dce07.jpg inflating: /content/hymenoptera_data/val/ants/94999827_36895faade.jpg inflating: /content/hymenoptera_data/val/ants/Ant-1818.jpg inflating: /content/hymenoptera_data/val/ants/ants-devouring-remains-of-large-dead-insect-on-red-tile-in-Stellenbosch-South-Africa-closeup-1-DHD.jpg inflating: /content/hymenoptera_data/val/ants/desert_ant.jpg inflating: /content/hymenoptera_data/val/ants/F.pergan.28(f).jpg inflating: /content/hymenoptera_data/val/ants/Hormiga.jpg creating: /content/hymenoptera_data/val/bees/ inflating: /content/hymenoptera_data/val/bees/1032546534_06907fe3b3.jpg inflating: /content/hymenoptera_data/val/bees/10870992_eebeeb3a12.jpg inflating: /content/hymenoptera_data/val/bees/1181173278_23c36fac71.jpg inflating: /content/hymenoptera_data/val/bees/1297972485_33266a18d9.jpg inflating: /content/hymenoptera_data/val/bees/1328423762_f7a88a8451.jpg inflating: /content/hymenoptera_data/val/bees/1355974687_1341c1face.jpg inflating: /content/hymenoptera_data/val/bees/144098310_a4176fd54d.jpg inflating: /content/hymenoptera_data/val/bees/1486120850_490388f84b.jpg inflating: /content/hymenoptera_data/val/bees/149973093_da3c446268.jpg inflating: /content/hymenoptera_data/val/bees/151594775_ee7dc17b60.jpg inflating: /content/hymenoptera_data/val/bees/151603988_2c6f7d14c7.jpg inflating: /content/hymenoptera_data/val/bees/1519368889_4270261ee3.jpg inflating: /content/hymenoptera_data/val/bees/152789693_220b003452.jpg inflating: /content/hymenoptera_data/val/bees/177677657_a38c97e572.jpg inflating: /content/hymenoptera_data/val/bees/1799729694_0c40101071.jpg inflating: /content/hymenoptera_data/val/bees/181171681_c5a1a82ded.jpg inflating: /content/hymenoptera_data/val/bees/187130242_4593a4c610.jpg inflating: /content/hymenoptera_data/val/bees/203868383_0fcbb48278.jpg inflating: /content/hymenoptera_data/val/bees/2060668999_e11edb10d0.jpg inflating: /content/hymenoptera_data/val/bees/2086294791_6f3789d8a6.jpg inflating: /content/hymenoptera_data/val/bees/2103637821_8d26ee6b90.jpg inflating: /content/hymenoptera_data/val/bees/2104135106_a65eede1de.jpg inflating: /content/hymenoptera_data/val/bees/215512424_687e1e0821.jpg inflating: /content/hymenoptera_data/val/bees/2173503984_9c6aaaa7e2.jpg inflating: /content/hymenoptera_data/val/bees/220376539_20567395d8.jpg inflating: /content/hymenoptera_data/val/bees/224841383_d050f5f510.jpg inflating: /content/hymenoptera_data/val/bees/2321144482_f3785ba7b2.jpg inflating: /content/hymenoptera_data/val/bees/238161922_55fa9a76ae.jpg inflating: /content/hymenoptera_data/val/bees/2407809945_fb525ef54d.jpg inflating: /content/hymenoptera_data/val/bees/2415414155_1916f03b42.jpg inflating: /content/hymenoptera_data/val/bees/2438480600_40a1249879.jpg inflating: /content/hymenoptera_data/val/bees/2444778727_4b781ac424.jpg inflating: /content/hymenoptera_data/val/bees/2457841282_7867f16639.jpg inflating: /content/hymenoptera_data/val/bees/2470492902_3572c90f75.jpg inflating: /content/hymenoptera_data/val/bees/2478216347_535c8fe6d7.jpg inflating: /content/hymenoptera_data/val/bees/2501530886_e20952b97d.jpg inflating: /content/hymenoptera_data/val/bees/2506114833_90a41c5267.jpg inflating: /content/hymenoptera_data/val/bees/2509402554_31821cb0b6.jpg inflating: /content/hymenoptera_data/val/bees/2525379273_dcb26a516d.jpg inflating: /content/hymenoptera_data/val/bees/26589803_5ba7000313.jpg inflating: /content/hymenoptera_data/val/bees/2668391343_45e272cd07.jpg inflating: /content/hymenoptera_data/val/bees/2670536155_c170f49cd0.jpg inflating: /content/hymenoptera_data/val/bees/2685605303_9eed79d59d.jpg inflating: /content/hymenoptera_data/val/bees/2702408468_d9ed795f4f.jpg inflating: /content/hymenoptera_data/val/bees/2709775832_85b4b50a57.jpg inflating: /content/hymenoptera_data/val/bees/2717418782_bd83307d9f.jpg inflating: /content/hymenoptera_data/val/bees/272986700_d4d4bf8c4b.jpg inflating: /content/hymenoptera_data/val/bees/2741763055_9a7bb00802.jpg inflating: /content/hymenoptera_data/val/bees/2745389517_250a397f31.jpg inflating: /content/hymenoptera_data/val/bees/2751836205_6f7b5eff30.jpg inflating: /content/hymenoptera_data/val/bees/2782079948_8d4e94a826.jpg inflating: /content/hymenoptera_data/val/bees/2809496124_5f25b5946a.jpg inflating: /content/hymenoptera_data/val/bees/2815838190_0a9889d995.jpg inflating: /content/hymenoptera_data/val/bees/2841437312_789699c740.jpg inflating: /content/hymenoptera_data/val/bees/2883093452_7e3a1eb53f.jpg inflating: /content/hymenoptera_data/val/bees/290082189_f66cb80bfc.jpg inflating: /content/hymenoptera_data/val/bees/296565463_d07a7bed96.jpg inflating: /content/hymenoptera_data/val/bees/3077452620_548c79fda0.jpg inflating: /content/hymenoptera_data/val/bees/348291597_ee836fbb1a.jpg inflating: /content/hymenoptera_data/val/bees/350436573_41f4ecb6c8.jpg inflating: /content/hymenoptera_data/val/bees/353266603_d3eac7e9a0.jpg inflating: /content/hymenoptera_data/val/bees/372228424_16da1f8884.jpg inflating: /content/hymenoptera_data/val/bees/400262091_701c00031c.jpg inflating: /content/hymenoptera_data/val/bees/416144384_961c326481.jpg inflating: /content/hymenoptera_data/val/bees/44105569_16720a960c.jpg inflating: /content/hymenoptera_data/val/bees/456097971_860949c4fc.jpg inflating: /content/hymenoptera_data/val/bees/464594019_1b24a28bb1.jpg inflating: /content/hymenoptera_data/val/bees/485743562_d8cc6b8f73.jpg inflating: /content/hymenoptera_data/val/bees/540976476_844950623f.jpg inflating: /content/hymenoptera_data/val/bees/54736755_c057723f64.jpg inflating: /content/hymenoptera_data/val/bees/57459255_752774f1b2.jpg inflating: /content/hymenoptera_data/val/bees/576452297_897023f002.jpg inflating: /content/hymenoptera_data/val/bees/586474709_ae436da045.jpg inflating: /content/hymenoptera_data/val/bees/590318879_68cf112861.jpg inflating: /content/hymenoptera_data/val/bees/59798110_2b6a3c8031.jpg inflating: /content/hymenoptera_data/val/bees/603709866_a97c7cfc72.jpg inflating: /content/hymenoptera_data/val/bees/603711658_4c8cd2201e.jpg inflating: /content/hymenoptera_data/val/bees/65038344_52a45d090d.jpg inflating: /content/hymenoptera_data/val/bees/6a00d8341c630a53ef00e553d0beb18834-800wi.jpg inflating: /content/hymenoptera_data/val/bees/72100438_73de9f17af.jpg inflating: /content/hymenoptera_data/val/bees/759745145_e8bc776ec8.jpg inflating: /content/hymenoptera_data/val/bees/936182217_c4caa5222d.jpg inflating: /content/hymenoptera_data/val/bees/abeja.jpg # Data augmentation and normalization for training # Just normalization for validation data_transforms = { 'train' : transforms . Compose ([ transforms . RandomResizedCrop ( 224 ), transforms . RandomHorizontalFlip (), transforms . ToTensor (), transforms . Normalize ([ 0.485 , 0.456 , 0.406 ], [ 0.229 , 0.224 , 0.225 ]) ]), 'val' : transforms . Compose ([ transforms . Resize ( 256 ), transforms . CenterCrop ( 224 ), transforms . ToTensor (), transforms . Normalize ([ 0.485 , 0.456 , 0.406 ], [ 0.229 , 0.224 , 0.225 ]) ]), } data_dir = 'hymenoptera_data' image_datasets = { x : datasets . ImageFolder ( os . path . join ( data_dir , x ), data_transforms [ x ]) for x in [ 'train' , 'val' ]} dataloaders = { x : torch . utils . data . DataLoader ( image_datasets [ x ], batch_size = 4 , shuffle = True , num_workers = 4 ) for x in [ 'train' , 'val' ]} dataset_sizes = { x : len ( image_datasets [ x ]) for x in [ 'train' , 'val' ]} class_names = image_datasets [ 'train' ] . classes device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) def imshow ( inp , title = None ): \"\"\"Imshow for Tensor.\"\"\" inp = inp . numpy () . transpose (( 1 , 2 , 0 )) mean = np . array ([ 0.485 , 0.456 , 0.406 ]) std = np . array ([ 0.229 , 0.224 , 0.225 ]) inp = std * inp + mean inp = np . clip ( inp , 0 , 1 ) plt . imshow ( inp ) if title is not None : plt . title ( title ) plt . pause ( 0.001 ) # pause a bit so that plots are updated # Get a batch of training data inputs , classes = next ( iter ( dataloaders [ 'train' ])) # Make a grid from batch out = torchvision . utils . make_grid ( inputs ) imshow ( out , title = [ class_names [ x ] for x in classes ]) def train_model ( model , criterion , optimizer , scheduler , num_epochs = 25 ): since = time . time () best_model_wts = copy . deepcopy ( model . state_dict ()) best_acc = 0.0 for epoch in range ( num_epochs ): print ( 'Epoch {} / {} ' . format ( epoch , num_epochs - 1 )) print ( '-' * 10 ) # Each epoch has a training and validation phase for phase in [ 'train' , 'val' ]: if phase == 'train' : model . train () # Set model to training mode else : model . eval () # Set model to evaluate mode running_loss = 0.0 running_corrects = 0 # Iterate over data. for inputs , labels in dataloaders [ phase ]: inputs = inputs . to ( device ) labels = labels . to ( device ) # zero the parameter gradients optimizer . zero_grad () # forward # track history if only in train with torch . set_grad_enabled ( phase == 'train' ): outputs = model ( inputs ) _ , preds = torch . max ( outputs , 1 ) loss = criterion ( outputs , labels ) # backward + optimize only if in training phase if phase == 'train' : loss . backward () optimizer . step () # statistics running_loss += loss . item () * inputs . size ( 0 ) running_corrects += torch . sum ( preds == labels . data ) if phase == 'train' : scheduler . step () epoch_loss = running_loss / dataset_sizes [ phase ] epoch_acc = running_corrects . double () / dataset_sizes [ phase ] print ( ' {} Loss: {:.4f} Acc: {:.4f} ' . format ( phase , epoch_loss , epoch_acc )) # deep copy the model if phase == 'val' and epoch_acc > best_acc : best_acc = epoch_acc best_model_wts = copy . deepcopy ( model . state_dict ()) print () time_elapsed = time . time () - since print ( 'Training complete in {:.0f} m {:.0f} s' . format ( time_elapsed // 60 , time_elapsed % 60 )) print ( 'Best val Acc: {:4f} ' . format ( best_acc )) # load best model weights model . load_state_dict ( best_model_wts ) return model model_ft = models . resnet18 ( pretrained = True ) num_ftrs = model_ft . fc . in_features # Here the size of each output sample is set to 2. # Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)). model_ft . fc = nn . Linear ( num_ftrs , 2 ) model_ft = model_ft . to ( device ) criterion = nn . CrossEntropyLoss () # Observe that all parameters are being optimized optimizer_ft = optim . SGD ( model_ft . parameters (), lr = 0.001 , momentum = 0.9 ) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler . StepLR ( optimizer_ft , step_size = 7 , gamma = 0.1 ) model_ft = train_model ( model_ft , criterion , optimizer_ft , exp_lr_scheduler , num_epochs = 1 ) Epoch 0/0 ---------- train Loss: 0.4348 Acc: 0.7951 val Loss: 0.1981 Acc: 0.9085 Training complete in 0m 3s Best val Acc: 0.908497 Hooks - The amazing trick you should know the-one-pytorch-trick-which-you-should-know import torch from torchvision.models import resnet34 device = torch . device ( 'cuda' ) if torch . cuda . is_available () else torch . device ( 'cpu' ) model = resnet34 ( pretrained = True ) model = model . to ( device ) class SaveOutput : def __init__ ( self ): self . outputs = [] def __call__ ( self , module , module_in , module_out ): self . outputs . append ( module_out ) def clear ( self ): self . outputs = [] Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth HBox(children=(FloatProgress(value=0.0, max=87306240.0), HTML(value=''))) save_output = SaveOutput () hook_handles = [] for layer in model . modules (): if isinstance ( layer , torch . nn . modules . conv . Conv2d ): handle = layer . register_forward_hook ( save_output ) hook_handles . append ( handle ) image = PIL . Image . open ( os . path . join ( config . paths [ 'image_path' ], 'cat.jpg' )) transform = transforms . Compose ([ transforms . Resize (( 224 , 224 )), transforms . ToTensor (), transforms . Normalize ([ 0.485 , 0.456 , 0.406 ], [ 0.229 , 0.224 , 0.225 ])]) X = transform ( image ) . unsqueeze ( dim = 0 ) . to ( device ) out = model ( X ) len ( save_output . outputs ) 36 save_output . outputs [tensor([[[[-8.4063e-02, -3.1156e-01, -3.7922e-01, ..., -3.1241e-01, -3.1991e-01, -7.8703e-02], [-6.6684e-01, -1.0324e+00, -1.1204e+00, ..., -1.5653e+00, -1.5471e+00, -6.4014e-01], [-3.4449e-01, -6.4493e-01, -7.2495e-01, ..., -1.2277e+00, -1.2087e+00, -4.5994e-01], ..., [ 2.1659e-02, -4.6106e-02, -2.1702e-01, ..., -1.0963e-01, -1.2138e-01, -1.5617e-01], [-3.2920e-02, -1.3041e-01, -3.1823e-01, ..., -2.6584e-02, -3.2380e-02, -1.1487e-01], [ 4.4561e-01, -2.9023e-01, -1.0615e+00, ..., 4.9755e-01, 4.4903e-01, 2.2949e-01]], [[-4.6110e-02, -3.0740e-02, -1.0826e-01, ..., -7.0238e-02, -9.2460e-02, -2.0240e+00], [-3.5144e-02, -1.7679e-02, -4.9731e-02, ..., -1.2231e-02, -9.3168e-03, -2.8310e+00], [-7.0530e-02, -3.8022e-02, -4.4606e-02, ..., -4.5727e-03, 2.5627e-02, -2.7854e+00], ..., [-7.3700e-01, 6.4940e-01, -5.2753e-01, ..., 1.7030e-02, -3.7055e-02, -2.0967e-02], [-7.4464e-01, 6.5585e-01, -5.2193e-01, ..., 4.4152e-02, -8.7973e-03, 4.6403e-02], [-6.1189e-01, 5.5873e-01, -4.2375e-01, ..., 6.6532e-02, 3.7737e-02, 6.0204e-02]], [[-2.3457e-01, -1.2645e-01, -1.7406e-01, ..., -4.1817e-01, -4.3566e-01, -1.6011e+00], [-6.6614e-02, -2.3226e-02, -6.2236e-02, ..., 2.1789e-02, 3.5555e-02, 2.7135e-01], [-4.7556e-02, -7.3814e-02, -4.9432e-02, ..., -5.7698e-02, -7.4298e-02, -6.5381e-02], ..., [ 3.9153e-03, -3.1434e-02, -2.9721e-02, ..., -1.0715e-01, -6.9462e-02, -6.0394e-02], [ 1.6745e-02, -3.0932e-02, -2.6025e-02, ..., -6.5645e-02, -1.0768e-01, -6.5954e-02], [ 1.7977e-01, 1.5711e-01, -9.2721e-02, ..., -1.2713e-01, -1.2850e-01, -3.8657e-02]], ..., [[ 8.5574e-02, 1.1745e-01, 1.5548e-01, ..., 4.0933e-01, 4.1436e-01, 9.9362e-01], [-2.6615e-01, 4.7584e-02, 6.8852e-02, ..., 1.1224e-01, 1.0466e-01, 5.7341e-02], [-2.5215e-01, 2.7188e-02, 8.2145e-02, ..., 1.0604e-01, 1.0107e-01, 2.3964e-02], ..., [-1.5416e-01, 5.6333e-02, -1.2265e-01, ..., 8.9357e-02, 7.2297e-02, 4.5511e-02], [-1.6092e-01, 5.2559e-02, -1.2424e-01, ..., 6.9515e-02, 1.0962e-01, 5.3230e-02], [ 7.6358e-02, 5.4660e-02, -4.3079e-01, ..., -4.0607e-03, 1.3518e-02, -7.2620e-02]], [[ 3.1630e-01, 8.0494e-01, 1.0661e+00, ..., 2.1891e+00, 2.1899e+00, 1.3774e+00], [ 6.3669e-01, 1.2647e+00, 1.4631e+00, ..., 3.0832e+00, 3.0457e+00, 1.9488e+00], [ 2.7135e-01, 6.2898e-01, 6.9927e-01, ..., 2.0648e+00, 2.0256e+00, 1.1266e+00], ..., [-9.1054e-02, -3.8858e-02, 3.3645e-01, ..., -1.9231e-01, -1.5554e-01, -1.0143e-01], [-8.0800e-02, -1.7947e-02, 3.7280e-01, ..., -2.6777e-01, -2.3381e-01, -1.4636e-01], [-2.4702e-01, -5.7148e-02, 5.2341e-01, ..., -1.4570e-01, -1.0016e-01, -4.2076e-02]], [[ 1.8776e-07, 2.7738e-07, 3.4406e-07, ..., 5.4105e-07, 5.3934e-07, 3.5630e-07], [ 2.7450e-07, 4.1102e-07, 5.1028e-07, ..., 8.2784e-07, 8.2309e-07, 5.3809e-07], [ 2.9428e-07, 4.4021e-07, 5.3925e-07, ..., 8.8300e-07, 8.7416e-07, 5.5885e-07], ..., [-2.8161e-08, 6.2136e-08, 1.4658e-07, ..., 2.8521e-07, 2.8608e-07, 1.9511e-07], [-3.0600e-08, 5.6445e-08, 1.3730e-07, ..., 2.6854e-07, 2.7299e-07, 1.8746e-07], [-1.4595e-08, 4.4339e-08, 9.0041e-08, ..., 1.8611e-07, 1.8967e-07, 1.3514e-07]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-7.1307e-01, -8.3134e-01, -8.4162e-01, ..., -8.3447e-01, -8.8322e-01, -8.0581e-01], [-8.5292e-01, -7.7469e-01, -8.0102e-01, ..., -7.7430e-01, -7.3565e-01, -5.8939e-01], [-8.3598e-01, -8.0091e-01, -9.0385e-01, ..., -8.0386e-01, -8.3582e-01, -7.0583e-01], ..., [-1.0630e+00, -1.0327e+00, -9.0952e-01, ..., -8.5371e-01, -9.6862e-01, -1.0368e+00], [-1.1039e+00, -1.0192e+00, -8.9569e-01, ..., -8.6189e-01, -1.0371e+00, -1.1618e+00], [-8.5768e-01, -9.8439e-01, -8.9813e-01, ..., -1.0932e+00, -1.2086e+00, -1.2490e+00]], [[-3.9721e-02, 1.8612e-02, -5.6467e-02, ..., -7.6485e-01, -9.7524e-01, -3.5409e-01], [ 3.4262e-01, 3.3146e-01, 1.9839e-01, ..., 1.2355e-01, 9.3290e-02, 9.5830e-02], [ 1.3413e-01, 1.1606e-01, 2.6072e-02, ..., -6.3812e-02, -5.7438e-02, -2.1188e-03], ..., [-1.8598e-02, -1.2766e-01, -2.4095e-02, ..., 1.4271e-01, 1.1860e-01, -4.7149e-02], [-9.9174e-03, -1.2616e-01, 4.7130e-03, ..., 1.3165e-01, -4.2112e-02, -1.3193e-01], [-2.8945e-01, -5.4195e-02, 3.9133e-02, ..., -2.2187e+00, -2.4321e+00, -1.8812e+00]], [[-6.8958e-01, -2.9590e-01, -3.4411e-01, ..., -2.2276e-01, -3.4415e-01, -1.8591e+00], [-7.0155e-01, -4.5380e-01, -5.7219e-01, ..., -4.8057e-01, -6.6103e-01, -1.7357e+00], [-4.6282e-01, -3.0516e-01, -4.1870e-01, ..., -3.9162e-01, -6.0281e-01, -1.7183e+00], ..., [-1.5330e+00, -5.3029e-01, -3.5363e-01, ..., -1.6527e-01, -5.8343e-01, -2.1110e+00], [-1.5865e+00, -5.5779e-01, -2.7971e-01, ..., -4.0614e-01, -9.4208e-01, -2.4309e+00], [-1.5805e+00, -7.1949e-01, -3.7832e-01, ..., -6.8876e-01, -1.1460e+00, -2.1012e+00]], ..., [[-9.6639e-01, -1.0328e+00, -1.0597e+00, ..., -1.1950e+00, -1.2843e+00, -1.2610e+00], [-1.1809e+00, -1.1479e+00, -1.2401e+00, ..., -1.3782e+00, -1.3827e+00, -1.0675e+00], [-1.1952e+00, -1.2641e+00, -1.3910e+00, ..., -1.3886e+00, -1.3968e+00, -1.0293e+00], ..., [-2.8220e+00, -1.7530e+00, -1.6388e+00, ..., -1.2392e+00, -1.2937e+00, -1.1474e+00], [-2.8713e+00, -1.6669e+00, -1.5304e+00, ..., -1.2188e+00, -1.3438e+00, -1.1898e+00], [-2.7238e+00, -1.5046e+00, -1.3087e+00, ..., -9.7235e-01, -1.0158e+00, -9.8973e-01]], [[-2.5968e+00, -1.6794e+00, -1.4204e+00, ..., -1.1400e+00, -1.2257e+00, -1.2097e+00], [-2.6432e+00, -1.9258e+00, -1.7191e+00, ..., -1.3148e+00, -1.1469e+00, -1.2445e+00], [-2.3157e+00, -2.0002e+00, -2.0134e+00, ..., -1.4132e+00, -1.2471e+00, -1.5705e+00], ..., [-2.5916e+00, -3.0395e+00, -1.8192e+00, ..., -1.3918e+00, -1.3557e+00, -2.4565e+00], [-2.6827e+00, -3.0189e+00, -1.6226e+00, ..., -1.2810e+00, -1.2225e+00, -2.5779e+00], [-2.3964e+00, -2.7206e+00, -1.5952e+00, ..., -1.3886e+00, -1.3188e+00, -2.3821e+00]], [[ 9.3492e-01, 2.9573e-01, 2.0938e-01, ..., 1.5254e-01, 9.0258e-03, -7.4639e-01], [ 6.4624e-01, -9.7434e-02, -1.3152e-01, ..., 1.3951e-01, -1.5915e-01, -1.7132e+00], [ 5.5572e-01, -1.0975e-01, -1.4220e-01, ..., -4.1307e-02, -3.4354e-01, -1.7407e+00], ..., [-9.0136e-01, -8.2252e-02, -6.0170e-02, ..., 7.6841e-02, 1.1324e-01, -1.2357e+00], [-7.7420e-01, -2.0381e-02, -8.0946e-02, ..., 8.4156e-02, 1.3501e-01, -1.3044e+00], [-3.3190e-01, 8.8170e-02, -1.7382e-02, ..., -3.6395e-01, -4.6098e-01, -9.1564e-01]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-0.0067, 0.0934, 0.2068, ..., 0.2558, 0.2704, 0.1711], [-0.0525, -0.0926, 0.0183, ..., 0.2465, 0.2182, 0.1562], [ 0.0718, 0.0516, 0.0838, ..., 0.3026, 0.2200, 0.1786], ..., [ 0.0680, 0.0353, 0.0985, ..., 0.0679, -0.0684, -0.0239], [ 0.1705, 0.0419, 0.0697, ..., -0.0153, -0.0345, 0.0361], [ 0.1610, -0.1131, -0.0392, ..., 0.0911, 0.0280, 0.0677]], [[ 0.1090, 0.0759, 0.1343, ..., 0.0514, 0.0668, 0.0639], [ 0.1767, 0.1108, 0.1808, ..., 0.0470, 0.0197, 0.0140], [ 0.2040, 0.1734, 0.2154, ..., 0.0737, 0.0707, 0.0720], ..., [ 0.2990, 0.3060, 0.3207, ..., 0.3023, 0.2745, 0.1356], [ 0.2607, 0.2654, 0.2734, ..., 0.3501, 0.3430, 0.2238], [ 0.1456, 0.1358, 0.1019, ..., 0.1606, 0.1911, 0.1065]], [[ 0.1066, 0.0436, 0.0515, ..., 0.1293, 0.0921, -0.0538], [-0.2412, -0.3833, -0.2128, ..., -0.2032, -0.2209, -0.0512], [-0.2085, -0.1475, -0.0593, ..., -0.1508, -0.2108, -0.0181], ..., [-0.0955, -0.1585, -0.1726, ..., -0.1232, -0.0873, -0.0838], [-0.2223, -0.3166, -0.2828, ..., -0.2100, -0.1755, -0.1166], [-0.2328, -0.0936, -0.1716, ..., -0.1858, -0.1902, -0.0887]], ..., [[ 0.0435, -0.0346, -0.0193, ..., -0.0864, -0.0844, -0.0661], [-0.0094, -0.0602, 0.0022, ..., -0.1411, -0.1402, -0.0684], [ 0.0354, 0.0333, 0.0703, ..., -0.1200, -0.1259, -0.0280], ..., [ 0.0172, 0.0704, 0.0566, ..., 0.1495, 0.1632, 0.0931], [-0.0317, 0.0102, -0.0065, ..., 0.1540, 0.1796, 0.1241], [-0.0509, -0.0178, -0.0326, ..., 0.0413, 0.0676, 0.0497]], [[-0.3162, -0.2310, -0.1908, ..., -0.2563, -0.2508, -0.1339], [-0.2608, -0.2658, -0.3400, ..., -0.3716, -0.3720, -0.0844], [-0.1699, -0.3054, -0.3627, ..., -0.4232, -0.3379, -0.0883], ..., [-0.1122, -0.3011, -0.3755, ..., -0.0965, -0.1681, 0.0880], [-0.2009, -0.3449, -0.3103, ..., -0.1556, -0.2994, 0.1678], [-0.0541, -0.2684, -0.1538, ..., -0.0941, -0.1122, 0.0796]], [[ 0.2191, 0.2895, 0.3184, ..., 0.3778, 0.4290, 0.3253], [ 0.3126, 0.3261, 0.3293, ..., 0.4939, 0.6433, 0.4783], [ 0.2731, 0.2239, 0.2208, ..., 0.4430, 0.5743, 0.4407], ..., [ 0.1634, 0.1989, 0.3175, ..., 0.1045, 0.0940, 0.1423], [ 0.1457, 0.1518, 0.2690, ..., 0.1048, 0.0339, 0.0605], [ 0.0594, 0.0695, 0.1093, ..., 0.0717, 0.0124, 0.0449]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[ 0.1288, -0.2503, -0.1333, ..., 0.3151, 0.4780, -0.0678], [-0.0787, -0.1792, 0.0485, ..., 0.4072, 0.4981, 0.3600], [-0.1732, -0.2616, 0.1061, ..., 0.4136, 0.4425, 0.3536], ..., [ 0.3060, 0.4366, 0.2526, ..., 0.0864, 0.3143, 0.0586], [ 0.3617, 0.2622, 0.2624, ..., 0.1249, 0.4111, -0.2825], [-0.4700, -0.8507, -0.8230, ..., -1.6068, -1.5898, -1.3000]], [[ 0.1018, -0.3544, -0.3491, ..., -0.4679, -0.7683, -1.1598], [ 0.3575, -0.4819, -0.7272, ..., -0.7274, -1.1150, -1.3365], [ 0.3272, -0.4256, -0.6880, ..., -0.7192, -1.0137, -1.2675], ..., [ 0.1819, -1.3496, -1.1823, ..., -0.5881, -0.5162, -0.5722], [ 0.1448, -1.2669, -0.9025, ..., -0.6687, -0.3966, -0.2392], [ 0.2109, -0.8323, -0.4376, ..., -0.2245, 0.0652, -0.1672]], [[-2.3957, -1.9007, -1.9473, ..., -1.6839, -1.8994, -2.1858], [-1.2779, -1.4584, -1.7060, ..., -1.2947, -1.5838, -2.1106], [-1.2658, -1.6701, -1.8100, ..., -1.5734, -1.5880, -2.0974], ..., [-1.2345, -1.9685, -1.6607, ..., -1.5137, -1.6177, -1.7467], [-1.2292, -1.9627, -1.5617, ..., -1.7320, -1.6819, -1.7406], [-1.1952, -1.7024, -1.2822, ..., -1.4485, -1.3326, -1.3642]], ..., [[-0.1999, 0.7422, 0.7286, ..., 0.6396, 0.4921, 0.5735], [-1.8417, -0.0774, 0.0263, ..., 0.1794, 0.2562, 0.3815], [-1.6356, 0.1033, 0.0197, ..., 0.1362, 0.1361, -0.0085], ..., [-0.2457, 0.3278, 0.3608, ..., 0.0249, -0.0697, 0.5960], [-0.1247, 0.2252, 0.2401, ..., -0.4984, -0.5210, 0.4292], [-0.0336, 0.2188, 0.0487, ..., 0.5139, 0.6008, 1.5812]], [[-0.8325, -0.2705, -0.1320, ..., -0.0169, 0.0082, 0.7680], [-0.4834, -0.6586, -0.3605, ..., -0.2657, -0.3053, -0.7996], [-0.5342, -0.6958, -0.3008, ..., 0.2130, 0.0545, -0.5790], ..., [-0.4812, -0.5852, -0.1952, ..., -0.3104, -0.3786, -0.7488], [-0.4629, -0.5718, -0.3029, ..., -0.2068, -0.0184, -0.5921], [-0.5736, -0.6432, -0.3380, ..., -0.2731, -0.3399, -0.5874]], [[ 0.6537, 0.2888, 0.3033, ..., 1.1874, 1.1648, 1.3152], [ 0.9450, 0.6366, 0.6243, ..., 1.1310, 1.0945, 1.3916], [ 1.1746, 0.9364, 0.9100, ..., 1.3841, 1.2866, 1.4843], ..., [ 1.2870, 1.4445, 1.4908, ..., 0.4462, 0.4064, 0.9422], [ 1.2159, 1.3198, 1.4482, ..., 1.2616, 1.2065, 1.4373], [-0.4371, -0.9823, -0.9968, ..., 0.9042, 0.7254, 1.2273]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-0.0690, 0.0205, 0.0238, ..., 0.0504, 0.0489, 0.0620], [-0.0085, -0.2284, -0.2074, ..., -0.0862, -0.2175, -0.0029], [ 0.1603, 0.0254, -0.0896, ..., -0.1621, -0.2845, 0.0653], ..., [-0.0922, -0.0500, -0.0803, ..., 0.1579, -0.0258, 0.1260], [ 0.1160, -0.0575, -0.1284, ..., -0.1459, 0.0544, 0.1429], [ 0.1766, -0.3412, -0.2090, ..., -0.2005, -0.1526, -0.0480]], [[ 0.0569, -0.0271, 0.0484, ..., 0.0785, 0.0668, 0.0726], [ 0.1256, -0.0083, 0.1261, ..., 0.1567, 0.1875, 0.0977], [ 0.0912, -0.0189, 0.1105, ..., 0.1244, 0.1274, 0.0368], ..., [ 0.0576, 0.0070, 0.0357, ..., -0.1868, -0.2296, -0.1509], [ 0.1553, 0.0701, 0.0321, ..., -0.2530, -0.2889, -0.2127], [ 0.0855, 0.0397, -0.0359, ..., -0.1913, -0.2375, -0.1456]], [[ 0.1201, 0.0909, 0.0267, ..., 0.0299, 0.1325, 0.0393], [-0.0334, 0.0592, 0.0434, ..., 0.0855, 0.1506, 0.1683], [-0.1773, -0.0331, 0.0200, ..., 0.0043, 0.0764, 0.0877], ..., [-0.0717, 0.0951, 0.0525, ..., -0.0263, 0.0365, 0.0329], [-0.0570, 0.0173, -0.0354, ..., -0.0474, -0.0211, 0.0258], [-0.1326, 0.0810, -0.0064, ..., -0.0219, -0.0343, 0.0497]], ..., [[ 0.0337, 0.0411, 0.0187, ..., -0.0363, -0.0619, -0.0256], [-0.0246, 0.0176, 0.0450, ..., -0.0725, -0.1047, -0.0112], [-0.0102, 0.0386, 0.0480, ..., -0.0989, -0.1209, 0.0060], ..., [ 0.0540, 0.0942, 0.0474, ..., 0.0919, 0.0928, 0.1310], [ 0.0908, 0.1295, 0.0543, ..., 0.1005, 0.1049, 0.1038], [ 0.0257, 0.0705, 0.0354, ..., 0.0861, 0.0639, 0.0800]], [[-0.0520, -0.1138, -0.1880, ..., -0.2184, -0.2333, -0.2963], [-0.0347, 0.2291, -0.0918, ..., 0.0682, 0.1796, -0.1232], [-0.0566, 0.0762, -0.2317, ..., -0.1599, 0.0066, -0.0630], ..., [-0.1647, 0.0511, -0.1918, ..., -0.0743, -0.0133, -0.0464], [-0.2029, 0.0046, -0.0944, ..., 0.0204, -0.0767, -0.0127], [ 0.0343, -0.0258, -0.0422, ..., -0.0718, -0.1464, 0.0529]], [[ 0.0959, -0.1765, -0.1379, ..., -0.2142, -0.2033, 0.0204], [ 0.2175, -0.1815, -0.1862, ..., -0.2911, -0.1999, -0.0215], [ 0.3199, -0.1060, -0.1423, ..., -0.2524, -0.1460, 0.0433], ..., [ 0.2595, -0.0375, -0.0292, ..., 0.1552, 0.2469, 0.2323], [ 0.2947, 0.0060, -0.0351, ..., 0.3238, 0.4278, 0.3459], [ 0.2347, 0.1015, 0.0621, ..., 0.3857, 0.4319, 0.3006]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-0.7312, -1.0573, -0.8696, ..., -0.9803, -1.2274, -0.4400], [-0.4258, -1.6297, -1.3173, ..., -1.3742, -1.6037, -0.2364], [-0.5914, -1.0902, -1.3082, ..., -1.8181, -1.7968, -0.3723], ..., [-0.0704, -1.8710, -1.3881, ..., -1.1977, -1.1060, -0.3352], [ 0.0396, -2.2010, -1.4669, ..., -1.7222, -0.9263, -0.2770], [ 0.2731, -1.0796, -0.5575, ..., -0.3376, -0.2581, -0.1647]], [[ 0.7737, 0.8047, 0.0747, ..., 0.1028, 0.4023, -0.9467], [ 0.6843, 1.3533, 0.0049, ..., 0.0604, 0.2824, -0.6501], [ 0.3333, 1.1694, 0.5182, ..., -0.0359, 0.2724, -0.2666], ..., [ 1.1152, 1.0655, -0.3863, ..., -0.1095, 0.0656, 0.4892], [ 1.3597, 1.0931, -0.1453, ..., -0.5847, -0.5435, 0.5415], [ 1.5063, 0.8190, -0.0748, ..., -0.2666, 0.0716, 0.4723]], [[-0.0421, 0.1598, 0.3452, ..., 0.6064, 0.5386, 0.9047], [ 0.0706, 0.1349, 0.2835, ..., 0.7040, 0.5819, 1.0330], [ 0.1681, 0.2871, 0.3497, ..., 0.8788, 0.6498, 0.8639], ..., [ 0.9277, 1.1339, 0.8439, ..., 0.2173, -0.0893, 0.3682], [ 1.2604, 1.4140, 1.0508, ..., 0.1766, -0.1631, 0.3723], [ 0.8026, 0.9001, 0.7016, ..., 0.1117, -0.3394, 0.0032]], ..., [[ 0.3208, -0.5787, -0.2745, ..., -0.4792, -0.7870, -1.1380], [ 0.1350, -0.8132, -0.8228, ..., -0.6536, -0.6146, -1.3862], [ 0.1851, -0.2499, -0.5231, ..., -1.0034, -0.4775, -1.1100], ..., [ 0.3556, -1.2090, -1.0370, ..., -0.5365, 0.0756, -0.8619], [ 0.4460, -0.9401, -0.7835, ..., -0.5803, 0.0291, -0.5543], [-0.0055, -1.0113, -0.8210, ..., -0.3147, -0.1591, -0.7711]], [[ 1.0562, 0.7463, 1.1578, ..., 0.7318, 0.2068, 0.3523], [ 0.5896, 0.3341, 1.3005, ..., 0.6428, -0.1286, -0.1280], [ 0.6546, 0.4339, 1.3679, ..., 1.0451, 0.3791, -0.1241], ..., [ 0.3876, 1.0999, 1.5070, ..., 0.8180, 0.7997, -0.3036], [ 0.5483, 1.5749, 1.3768, ..., 1.0305, 0.9950, -0.3318], [-0.3145, 0.1694, 0.0642, ..., 0.0511, -0.0087, -0.5342]], [[ 0.4556, 0.0650, 0.5178, ..., 0.5446, 0.9076, -1.0967], [-0.2459, -0.5475, -0.1135, ..., 0.3136, 0.9088, -1.6499], [-0.2022, -0.3190, -0.1806, ..., 0.0904, 0.7406, -1.3833], ..., [-0.8970, 0.2649, -0.0789, ..., -0.0769, 1.1681, -2.2012], [-1.2325, 0.4407, -0.1711, ..., -0.0385, 1.1555, -2.5085], [-0.5395, 0.5949, 0.0827, ..., 0.2193, 0.9333, -1.8239]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-3.7964e-01, -4.0134e-01, -1.7608e-01, ..., -2.5551e-01, -2.3841e-01, -2.2295e-01], [ 3.9957e-02, -1.3263e-01, 8.1712e-02, ..., 2.3840e-01, 1.3429e-01, 7.8315e-02], [ 3.1333e-03, -1.0131e-02, -6.6429e-02, ..., -2.0292e-01, -2.8177e-01, -5.3561e-02], ..., [-2.1268e-01, 1.5780e-01, 6.7509e-02, ..., 1.0054e-01, -1.3563e-01, -2.1604e-01], [ 1.4755e-02, 8.2588e-02, -1.2901e-01, ..., -3.6963e-01, -8.8912e-02, -3.1725e-02], [-1.5123e-02, -4.9199e-01, -3.9780e-01, ..., -4.1590e-01, -4.3884e-01, -4.5005e-01]], [[-2.4282e-02, -8.2193e-02, -8.2178e-02, ..., -1.3298e-01, 5.8532e-02, -7.8329e-02], [-3.2592e-03, -1.6045e-01, -1.6002e-01, ..., -1.8618e-01, 2.2344e-03, -2.6714e-01], [ 1.4457e-02, -7.3774e-02, -1.1208e-01, ..., -1.1209e-01, 3.8288e-02, -2.6211e-01], ..., [ 2.3041e-01, 1.1639e-01, 2.1283e-02, ..., 1.4279e-02, 3.2593e-02, -1.5568e-01], [ 2.3745e-01, 7.5388e-02, -6.5470e-02, ..., 1.2928e-01, 1.2930e-01, -9.1972e-02], [ 7.6496e-02, -1.1406e-01, -2.1356e-01, ..., -8.5138e-02, -8.2041e-02, -1.7543e-01]], [[ 3.8545e-02, 1.7309e-02, -1.7679e-01, ..., -7.5756e-02, -6.3311e-02, -8.1885e-02], [ 2.0244e-01, 2.7231e-01, 3.9004e-02, ..., 1.4688e-02, -4.5041e-02, 5.9030e-02], [ 4.0409e-02, 2.9233e-01, 1.1852e-01, ..., -3.0633e-02, 4.2346e-02, 1.1974e-01], ..., [ 1.0777e-01, 1.0976e-01, 5.1145e-02, ..., 8.7173e-02, 6.6064e-02, 1.9359e-01], [ 8.7519e-02, -1.2386e-02, 7.9483e-02, ..., 3.3585e-02, 1.0019e-01, 3.8054e-01], [ 1.6395e-01, 3.0325e-01, 8.2967e-02, ..., 1.3820e-01, 3.1377e-01, 4.5450e-01]], ..., [[ 1.3808e-01, 2.0525e-01, 1.7885e-01, ..., 2.8482e-01, 2.8515e-01, 1.2578e-01], [ 9.4201e-04, 1.4381e-01, 1.7652e-01, ..., 2.2403e-01, 2.0144e-01, 1.4126e-01], [-1.8357e-02, 7.3374e-02, 1.4470e-01, ..., 1.3975e-01, 1.9496e-01, 1.3552e-01], ..., [ 1.2864e-01, 1.7703e-02, 7.7141e-04, ..., 1.4196e-01, 1.6583e-01, 7.5212e-02], [ 8.7795e-02, 1.2203e-01, 4.5601e-02, ..., 4.8339e-02, 1.1783e-01, 3.1923e-02], [ 8.8624e-02, 2.0204e-01, 1.8055e-01, ..., 9.0144e-02, 1.2010e-01, 7.4829e-02]], [[-1.2925e-01, -2.7588e-02, -4.4342e-02, ..., 8.2830e-02, -1.7703e-02, -6.3619e-02], [-1.4586e-01, -9.6416e-03, 5.4718e-02, ..., -3.5866e-02, -1.2122e-01, -2.9893e-02], [-3.0947e-01, -8.5613e-02, -1.7647e-02, ..., -2.0618e-01, -4.2163e-02, -4.4049e-04], ..., [-2.7113e-01, -1.3855e-01, -2.0130e-01, ..., -2.0272e-01, -1.9512e-01, -1.2623e-01], [-2.4112e-01, -1.5066e-01, -1.6733e-01, ..., -1.9722e-01, -3.0106e-01, -1.2402e-01], [-2.4192e-01, -1.7451e-01, 2.7594e-02, ..., -1.0973e-02, -1.1463e-01, -6.7437e-02]], [[ 2.1856e-01, 1.2767e-01, 8.7509e-02, ..., 1.7413e-01, 1.3547e-01, 2.4634e-01], [ 3.9917e-01, 2.7012e-01, 1.7836e-01, ..., 3.1700e-01, 3.3307e-01, 3.6791e-01], [ 1.9946e-01, 9.8512e-02, -6.8485e-03, ..., 1.6686e-01, 2.0509e-01, 2.7482e-01], ..., [ 3.2994e-01, 2.2487e-01, 1.6483e-01, ..., 1.2141e-01, 1.8807e-01, 2.3022e-01], [ 3.5468e-01, 3.0923e-01, 1.8328e-01, ..., 1.1140e-01, 1.1644e-01, 1.4330e-01], [ 2.4101e-01, 2.4712e-01, 1.5476e-01, ..., 1.7904e-01, 1.7466e-01, 1.4160e-01]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-9.0091e-01, 6.3870e-01, -3.0263e-01, ..., -6.5964e-01, 7.8636e-01, -1.0163e+00], [-1.1825e+00, 6.3744e-01, -3.5650e-01, ..., -8.8556e-02, 3.4580e-01, -1.0833e-01], [-1.1111e+00, 3.4459e-01, -7.5116e-01, ..., -1.6278e-01, -2.8018e-01, 2.0702e-01], ..., [ 7.5050e-01, -9.8482e-01, 4.0760e-01, ..., 6.2197e-01, 1.2554e-01, -6.7755e-01], [-1.5456e+00, 1.3781e-01, 6.0317e-01, ..., 1.3621e+00, -1.3546e-01, 1.2409e-01], [-1.1613e+00, -3.4615e-01, 1.3554e-01, ..., 4.6713e-02, -1.0961e-01, 5.2183e-01]], [[ 6.4298e-01, 2.1897e-01, 1.5417e-01, ..., 6.1239e-01, 6.5252e-01, 1.9946e-01], [-2.1554e-01, -4.5524e-01, 3.1487e-01, ..., 1.2128e+00, 1.7485e+00, 1.2734e-02], [-4.8627e-02, -8.1735e-02, -1.5425e-01, ..., -3.5698e-02, 9.1563e-01, 1.5359e+00], ..., [-8.6189e-01, -1.1113e-01, -1.0548e-01, ..., -2.2268e-01, 2.1504e-02, -4.1924e-01], [-7.7746e-01, -6.7780e-01, 6.5219e-02, ..., -2.7417e-01, -1.9657e-01, 3.0158e-02], [-4.2939e-01, -1.1187e-01, 8.3300e-01, ..., -3.0947e-01, 1.2986e-02, -7.6760e-02]], [[-2.2114e-01, -3.9811e-01, -4.3892e-01, ..., -4.2268e-01, -6.8673e-01, -4.9937e-01], [-7.4385e-02, 1.0765e-01, -6.6805e-01, ..., -2.9073e-01, -7.2485e-01, 3.2642e-01], [ 1.7151e-01, -9.0508e-02, -1.4112e-01, ..., 2.7432e-01, 3.4726e-02, 8.5700e-02], ..., [ 4.4768e-02, 7.3492e-01, 1.7399e-01, ..., 1.5529e-01, -5.5666e-02, -1.0640e+00], [ 5.0144e-01, 4.9841e-01, 6.9592e-04, ..., -2.5502e-01, -5.8249e-01, -4.9665e-03], [ 3.5961e-01, -6.5753e-01, -2.9218e-01, ..., 7.3599e-03, -4.0838e-01, 2.6860e-02]], ..., [[ 7.9693e-01, 1.1981e+00, -2.6719e-02, ..., -1.4044e-01, 9.2718e-01, -5.7674e-01], [ 8.0784e-01, -1.5681e+00, 1.8397e-01, ..., 8.7765e-01, -6.4868e-01, 6.9011e-01], [-2.4827e-02, 1.6731e-01, -1.2616e-01, ..., -3.6915e-01, 2.0152e-01, -3.9128e-01], ..., [-1.1125e+00, 4.7629e-01, 8.7911e-01, ..., -9.4275e-04, -9.2995e-01, 2.9742e-01], [ 5.7873e-01, 6.8500e-01, -5.4961e-01, ..., -3.0391e-01, 5.8966e-01, -1.5808e-01], [-5.0518e-01, 5.9546e-01, -4.2340e-01, ..., -7.0296e-02, 1.5118e-01, 5.6469e-01]], [[-1.1372e-01, 1.9317e-01, -2.0616e-01, ..., -4.5876e-02, 2.9818e-02, -1.4028e-01], [ 2.7155e-02, 4.5951e-01, 1.8610e-01, ..., 4.6667e-01, -3.9038e-01, 1.1254e+00], [ 2.1434e-01, 5.7286e-02, -2.0534e-01, ..., -2.1218e-01, 3.6298e-02, 5.4128e-01], ..., [-1.6878e+00, -1.5219e+00, 8.6590e-02, ..., 5.5942e-01, 3.5859e-01, 1.3370e+00], [-1.2910e+00, 2.3102e-01, 6.7816e-01, ..., -2.6203e-01, -2.4199e-01, -1.0794e-01], [-4.2782e-02, 7.7749e-01, 9.3433e-01, ..., 9.2322e-01, 1.2178e+00, 1.8176e+00]], [[-7.9935e-01, -1.2774e+00, -4.3572e-01, ..., -7.2118e-02, -2.2340e-01, -4.4651e-01], [-1.7296e+00, -2.1770e+00, -8.5848e-01, ..., -6.6132e-01, -3.8605e-01, -7.1216e-01], [-1.2839e+00, -2.0997e+00, -1.4358e+00, ..., -1.1509e+00, -7.3437e-01, -1.2747e+00], ..., [-1.7467e+00, -2.7322e+00, -1.0077e+00, ..., -9.5499e-01, -6.3091e-01, -1.0631e+00], [-2.0026e+00, -1.7775e+00, -9.2295e-01, ..., -6.0964e-01, -4.7979e-01, -5.8898e-01], [-1.4926e+00, -1.1507e+00, -3.1636e-01, ..., -1.2428e-01, -4.7847e-01, -4.4640e-01]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[ 9.5913e-02, -1.6541e-01, -5.6267e-02, ..., -1.8978e-01, -1.0216e-02, -2.0282e-02], [-7.9051e-02, -1.5625e-01, -9.1682e-02, ..., -1.2345e-01, -5.0356e-02, 6.8900e-02], [-1.3790e-01, -1.4030e-01, -7.8608e-02, ..., -1.1149e-01, -3.1122e-01, -2.4360e-01], ..., [-2.0402e-01, -2.7970e-01, -5.8880e-02, ..., -1.2766e-01, -1.5945e-01, -1.4166e-04], [ 1.0777e-01, -5.8746e-02, -3.4232e-01, ..., -6.7850e-02, -1.2919e-01, 2.3733e-02], [-1.5006e-01, 1.8371e-01, -2.5554e-01, ..., -2.5197e-01, -8.3513e-03, 7.0744e-02]], [[-1.1158e-02, -2.5456e-02, -3.6755e-02, ..., 6.3668e-02, -1.7311e-01, 1.1011e-01], [-6.5492e-02, 9.1241e-02, -5.5219e-02, ..., 3.3896e-02, -8.6943e-02, 2.9966e-02], [-2.4388e-02, -2.0955e-02, -1.2032e-02, ..., -4.6190e-03, 1.3383e-02, 5.6576e-02], ..., [ 6.9491e-03, 1.1501e-01, -1.6926e-04, ..., 3.8924e-02, -1.5815e-02, 3.5392e-02], [-7.1771e-02, -1.1710e-01, 1.3267e-02, ..., -1.1833e-01, 4.6548e-02, 3.4670e-02], [-2.0510e-02, 4.5852e-02, -9.4286e-03, ..., 7.4114e-02, 2.8800e-02, -4.0978e-02]], [[ 2.8442e-01, 1.8822e-01, 2.3620e-01, ..., 1.2417e-01, 2.6348e-01, 3.2023e-01], [ 3.0938e-02, 7.4200e-02, 2.4235e-01, ..., 1.3457e-01, 1.1945e-01, 9.4352e-02], [ 1.0847e-01, 1.3481e-01, -2.0223e-02, ..., 1.2688e-01, 1.3234e-01, 2.9863e-02], ..., [-9.2610e-02, 2.1935e-02, -1.5643e-01, ..., -4.6844e-02, -1.0332e-01, 8.3054e-02], [ 3.7796e-02, 2.5773e-01, 1.3152e-02, ..., -6.5266e-02, -2.9678e-02, -1.7658e-01], [ 3.2729e-01, 2.8362e-01, 1.0696e-01, ..., 2.2743e-01, 1.2019e-02, 1.5634e-01]], ..., [[-3.0847e-01, -2.5876e-01, -3.6083e-01, ..., -2.5877e-01, -3.1287e-01, -3.7233e-01], [-1.3438e-01, -1.4756e-01, -2.5999e-01, ..., -2.9874e-01, -2.4534e-01, -1.3022e-01], [ 7.5756e-02, -9.8625e-02, -1.1781e-01, ..., -6.3175e-02, -1.2338e-01, 8.6862e-02], ..., [-1.4622e-01, -3.0403e-01, -1.7429e-01, ..., -1.5258e-01, -1.9483e-01, -1.2083e-04], [ 5.3101e-02, -1.3620e-01, -1.3141e-01, ..., 2.1619e-01, -8.5160e-02, -4.4242e-02], [-7.8481e-02, 8.2884e-02, 1.2997e-02, ..., 7.7851e-02, 1.4876e-01, 5.9686e-02]], [[-4.4822e-01, -4.4531e-01, -5.4460e-01, ..., -4.2265e-01, -5.7580e-01, -3.2392e-01], [-4.1808e-01, -4.3049e-01, -5.1591e-01, ..., -4.7325e-01, -4.2901e-01, -3.3930e-01], [-3.9814e-01, -3.4293e-01, -4.2360e-01, ..., -5.9320e-01, -4.5428e-01, -3.0691e-01], ..., [-5.2022e-01, -4.3604e-01, -2.8243e-01, ..., -3.7317e-01, -3.8210e-01, -5.0886e-01], [-7.2720e-01, -4.8945e-01, -1.4572e-01, ..., -4.5263e-01, -4.7482e-01, -6.6300e-01], [-5.8000e-01, -3.1295e-01, -3.2944e-01, ..., -4.0389e-01, -4.3888e-01, -3.2057e-01]], [[ 9.7382e-02, 9.4367e-02, 7.4948e-03, ..., 2.4979e-01, 3.8360e-01, 2.1761e-01], [-2.4950e-01, -8.9020e-02, -1.3273e-01, ..., 8.9120e-02, 2.7409e-01, -1.7473e-01], [-5.4640e-02, 1.3543e-01, -3.0125e-02, ..., 1.0683e-01, 1.2142e-01, 2.9367e-01], ..., [ 6.0568e-01, -6.5119e-02, -8.1571e-02, ..., 6.2436e-02, -4.4642e-02, 8.2873e-02], [ 5.8876e-02, -8.7217e-02, 7.7449e-02, ..., -1.0470e-01, -2.2360e-02, -7.8556e-03], [ 7.5289e-03, 1.0554e-01, 5.1145e-02, ..., 1.0968e-01, 1.1641e-01, -6.0809e-02]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-4.2273e-01, -2.0987e-01, -1.8386e-01, ..., -1.6742e-01, -1.4549e-01, -2.3816e-01], [ 5.2229e-02, -2.1747e-01, 1.6554e-03, ..., 9.4766e-02, 2.8188e-03, 2.2632e-02], [-2.1932e-01, -4.8531e-02, 1.7961e-02, ..., -1.8766e-01, -1.5018e-01, -2.3167e-01], ..., [-2.6574e-01, -2.2816e-01, -1.6534e-02, ..., -1.1980e-01, -9.9002e-02, -1.0444e-01], [-1.4383e-01, -4.1912e-02, 1.2204e-01, ..., 1.0591e-02, -7.6314e-03, -3.2738e-01], [-1.8492e-01, -6.7604e-02, -2.1401e-02, ..., -2.0158e-01, -1.5867e-01, -5.3975e-02]], [[ 1.3357e+00, 1.1842e+00, 1.3085e+00, ..., 8.3036e-01, 8.1247e-01, 1.1123e+00], [ 9.6221e-01, 9.3270e-01, 1.3355e+00, ..., 6.1226e-01, 2.3988e-01, 4.3982e-01], [ 8.8059e-01, 7.4175e-01, 8.8665e-01, ..., 8.1106e-01, 4.9318e-01, 7.6294e-01], ..., [ 1.9570e-01, -8.9729e-02, -1.5378e-01, ..., 1.0819e+00, 9.9820e-01, 1.2328e+00], [-1.6463e-01, -1.0758e-01, -1.3862e-01, ..., 1.1708e+00, 9.6169e-01, 1.4559e+00], [-2.3193e-01, -2.6281e-01, -1.7272e-01, ..., 1.1984e+00, 1.3402e+00, 1.5838e+00]], [[-2.9064e-01, -3.3235e-01, -3.0300e-01, ..., -1.4648e-01, -1.0491e-01, -8.5999e-02], [ 3.0795e-01, -3.6245e-01, 1.7980e-01, ..., 2.4748e-01, 8.4595e-02, 8.1217e-02], [-3.0408e-02, -2.1951e-02, -1.8527e-02, ..., -2.8260e-03, -4.4726e-02, -4.8306e-02], ..., [ 3.6590e-02, -2.2649e-01, -5.1185e-02, ..., -3.1439e-02, -4.5245e-01, -2.1810e-02], [ 1.1245e-01, -3.2933e-01, 8.5335e-03, ..., 4.0620e-02, 1.7344e-01, -3.1027e-01], [ 2.3234e-01, 2.2664e-01, -7.7205e-02, ..., -2.2093e-01, -4.8593e-02, 2.4661e-02]], ..., [[-9.3261e-01, -7.6260e-01, -7.3332e-01, ..., -7.7190e-01, -6.0499e-01, -5.9985e-01], [-8.5653e-01, -2.1887e-01, -5.4355e-01, ..., -3.8094e-01, -5.8377e-01, -5.0749e-01], [-4.9705e-01, -1.9928e-01, -4.5073e-01, ..., 1.2020e-03, -1.8645e-01, -1.2595e-01], ..., [-6.3847e-01, -3.3980e-01, -5.2701e-01, ..., -3.0330e-01, -2.8109e-01, -1.6475e-01], [-6.8109e-01, -5.0743e-01, -5.2533e-01, ..., -5.2017e-01, -6.2944e-01, -6.2879e-01], [-2.7850e-01, -6.1580e-01, -5.2393e-01, ..., -5.9199e-01, -6.9086e-01, -6.9969e-01]], [[ 2.5167e-01, 4.3919e-02, 1.0668e-02, ..., -7.8221e-02, 2.1594e-01, 1.3642e-01], [ 2.4208e-01, 2.5012e-01, 2.5683e-01, ..., 2.6633e-01, -5.7677e-02, 3.9456e-01], [ 3.7880e-01, 1.2930e-01, -3.5076e-02, ..., 2.1143e-01, 1.0497e-01, 4.1619e-01], ..., [ 3.1514e-01, 2.0994e-01, 1.2845e-01, ..., 2.5211e-01, 9.9327e-02, 1.8318e-01], [ 3.1479e-01, 1.2375e-01, 1.1239e-01, ..., 2.0128e-01, 1.0337e-01, 3.0883e-01], [ 3.8773e-01, 1.0302e-01, 2.2351e-01, ..., -7.8994e-02, 6.5807e-03, -1.0335e-01]], [[ 1.2423e-03, -1.0223e-01, 9.5612e-02, ..., -9.4084e-02, -4.2660e-02, 7.5494e-02], [-5.1509e-01, -5.2124e-02, -2.5991e-01, ..., -3.1778e-01, -2.3171e-02, -2.8931e-01], [-2.6891e-01, -7.5821e-02, -2.3992e-01, ..., 1.3733e-01, 2.6749e-02, -9.0869e-02], ..., [-1.9027e-01, -1.9019e-01, -1.7996e-01, ..., -1.0913e-01, 1.2905e-01, -9.7400e-02], [-4.8791e-01, -2.2153e-01, -1.8444e-01, ..., -1.9465e-01, -2.4405e-01, 5.0802e-02], [-4.1235e-01, -4.6859e-01, -3.2017e-01, ..., -2.3232e-01, -1.8305e-01, -4.1211e-01]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[ 0.0551, -0.5440, -0.6146, ..., 0.1801, -0.3121, 0.2243], [ 0.2878, -0.2937, 0.0180, ..., -0.3412, -0.9847, 0.3291], [-0.2521, -0.2032, 0.6301, ..., -0.9992, -1.3043, 0.1372], ..., [-0.4388, -2.0501, -0.9857, ..., -1.4562, -1.1338, -0.2438], [-0.6619, -1.0800, -0.6420, ..., -0.9531, -0.9155, -0.5830], [-0.9757, -0.4903, -0.7896, ..., -0.0207, 0.0709, -0.3874]], [[-0.5626, -0.8982, -0.6051, ..., -0.4369, -0.3069, -0.0818], [-0.8539, -1.2124, -1.1421, ..., -1.0073, -1.0924, -0.4628], [-0.6097, -0.7027, -0.6621, ..., -1.2089, -1.3066, -0.7056], ..., [-0.5803, -0.5633, -0.2863, ..., -0.4478, -0.9250, -0.5883], [-0.6545, -0.8446, -0.5428, ..., -0.3727, -0.2968, 0.0574], [-0.6146, -0.6378, -0.3994, ..., -0.5582, -0.4681, -0.0292]], [[ 0.5193, 0.6878, 0.3467, ..., 0.2677, 0.1245, 0.1870], [ 0.1295, -0.1289, -0.1073, ..., 0.2351, -0.0065, 0.0047], [-0.1027, -0.4202, -0.5724, ..., -0.3395, -0.0770, 0.0321], ..., [-0.0276, 0.5198, 0.3101, ..., -0.0947, -0.1507, -0.2880], [-0.3556, -0.0265, -0.3138, ..., 0.0869, 0.0704, 0.0083], [ 0.1238, 0.6825, 0.2532, ..., -1.6974, -1.8882, -1.3618]], ..., [[ 0.8641, 0.6122, 0.5621, ..., 0.4198, 0.8420, 0.1588], [ 0.6023, 0.1794, 0.3502, ..., -0.0430, 0.5074, -0.0994], [ 0.1859, 0.1100, 0.5256, ..., 0.5270, 1.0384, -0.0507], ..., [ 0.1272, 0.4405, 0.4913, ..., 0.1201, -0.0852, -0.2832], [-0.0867, 0.4949, 0.4213, ..., 0.3035, 0.3179, -0.0499], [ 0.1581, 0.7226, 0.5054, ..., 0.4537, 0.4473, 0.4062]], [[-0.8049, -1.0994, -1.1123, ..., -0.5673, -0.4921, -0.0453], [-0.5760, -0.7352, -0.4998, ..., -0.4954, -0.2212, 0.0798], [-0.2649, 0.0948, -0.3051, ..., -0.3232, -0.4791, -0.2479], ..., [-0.2960, -1.0196, -0.1207, ..., -0.2136, -0.4638, 0.2480], [ 0.3223, -0.2110, -0.0877, ..., -0.2047, -0.7102, 0.3825], [-0.4926, -0.8470, -0.7704, ..., 0.5240, 0.2731, 0.9488]], [[ 0.7633, 0.5597, 0.6460, ..., 0.8756, 0.7951, 0.4977], [ 1.0543, 0.6982, 1.0793, ..., 1.1163, 1.2171, 0.3768], [ 0.6776, 0.4172, 0.4710, ..., 1.1850, 1.4658, 0.7831], ..., [ 0.8799, 0.0338, 0.0235, ..., 1.1350, 1.4108, 0.9740], [ 0.4392, -0.0140, 0.3868, ..., 0.5046, 0.7000, 0.4616], [ 0.2597, 0.1772, 0.4445, ..., 0.7297, 0.6433, 0.2840]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[ 5.0589e-02, 3.0239e-02, -6.5652e-02, ..., 2.9136e-02, 2.0472e-02, 3.3116e-02], [-4.3490e-02, -9.9013e-02, -1.4477e-01, ..., -1.2799e-01, -1.2968e-01, -1.1374e-01], [-1.7633e-02, 8.8080e-02, 9.0812e-02, ..., 8.1622e-02, 4.6130e-02, 9.6210e-03], ..., [-3.2275e-02, -1.1952e-01, -8.8798e-02, ..., -7.3871e-02, 7.0270e-02, 9.4732e-02], [-3.6789e-04, -1.6536e-01, -1.2563e-01, ..., -3.9373e-03, -1.1487e-04, 9.4181e-02], [-3.9141e-02, -6.5069e-02, -7.1126e-03, ..., -1.3528e-03, 2.9227e-02, 7.1437e-02]], [[ 4.9216e-01, 3.2340e-01, 3.7978e-01, ..., 8.5515e-02, 2.5145e-01, 2.8095e-01], [ 1.8159e-01, -1.3266e-01, 1.4115e-01, ..., 1.5686e-02, -9.7150e-03, -4.6657e-02], [ 9.7674e-02, -1.9594e-01, -9.3590e-02, ..., 1.4430e-01, 3.0088e-01, 1.6222e-01], ..., [ 2.1367e-01, 1.3881e-01, 1.8991e-02, ..., -3.0949e-02, 3.1051e-01, 1.7396e-01], [ 1.1380e-01, -7.6312e-02, -1.8014e-02, ..., 2.8777e-02, 2.3656e-01, 7.9741e-02], [ 2.0087e-01, 1.1959e-01, 1.2988e-01, ..., -1.5768e-01, -9.1760e-02, -1.4441e-01]], [[-1.3740e-01, -1.9611e-01, -2.0091e-01, ..., -1.5659e-01, -1.2471e-01, -1.8021e-01], [-1.7893e-01, -2.0166e-01, -1.0795e-01, ..., 5.2685e-02, -6.0237e-02, -8.4450e-02], [-1.2058e-01, -6.8945e-04, -3.3964e-03, ..., 9.2430e-02, -1.5144e-02, 1.0227e-02], ..., [ 1.2023e-01, 9.7808e-02, 6.1309e-02, ..., -1.4919e-01, -8.5181e-02, 1.0121e-01], [ 1.0048e-01, -7.8018e-03, -9.2056e-02, ..., -1.8924e-01, -2.1661e-01, -7.0055e-02], [ 8.6329e-02, -5.4492e-02, -1.6985e-01, ..., -2.0948e-01, -1.6376e-01, -2.4448e-02]], ..., [[-2.8329e-01, -3.6258e-01, -3.0192e-01, ..., -9.3842e-02, -1.5187e-01, -2.5100e-02], [-1.6744e-01, -1.9852e-01, -2.5101e-01, ..., 1.1434e-01, 2.3056e-03, 9.7841e-03], [-2.3171e-02, -5.7884e-02, -1.3461e-01, ..., -2.6371e-02, -1.9229e-01, -2.0811e-01], ..., [-6.9217e-02, -2.0768e-01, -2.8877e-02, ..., -5.8385e-02, 2.6056e-02, 4.0350e-02], [-8.4423e-02, -2.5802e-01, -2.9243e-02, ..., -1.7626e-01, -2.2544e-01, -1.3741e-01], [-1.7964e-02, -1.3211e-02, 1.2998e-01, ..., 4.6026e-02, 4.3756e-02, 2.3945e-02]], [[-3.3565e-01, -5.4160e-01, -6.0805e-01, ..., -3.8045e-01, -4.3757e-01, -3.0454e-01], [-1.4515e-01, -3.4559e-01, -4.1628e-01, ..., -3.6153e-01, -3.3174e-01, -9.0514e-02], [-1.2495e-01, -3.9581e-01, -3.5128e-01, ..., -2.6482e-01, -2.4520e-01, -1.8402e-01], ..., [-2.2884e-01, -6.1938e-01, -5.5678e-01, ..., -6.8449e-01, -5.8617e-01, -2.8077e-01], [-3.3351e-01, -5.2314e-01, -4.2504e-01, ..., -5.2789e-01, -6.0944e-01, -3.6275e-01], [-1.5605e-01, -3.6710e-01, -3.8210e-01, ..., -4.0686e-01, -4.1307e-01, -1.7890e-01]], [[-2.1081e-01, -2.9553e-01, -3.2521e-01, ..., -2.0666e-01, -1.2164e-01, -1.2524e-01], [-2.4101e-01, -2.0451e-01, -1.5815e-01, ..., -3.4341e-02, 4.8311e-02, 2.4075e-02], [-2.1997e-01, -2.2960e-01, -2.4427e-01, ..., 2.7745e-02, 6.6867e-02, 4.3925e-02], ..., [-7.6800e-03, -9.8346e-02, -2.0452e-01, ..., -1.2589e-01, 1.1875e-01, 7.0566e-02], [-8.1622e-02, -1.5807e-01, -2.0304e-01, ..., -1.0088e-01, 1.0701e-02, -1.2372e-02], [-9.3799e-02, -1.8981e-01, -2.0816e-01, ..., -2.0537e-01, -1.6077e-01, -1.8046e-01]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-0.1642, 0.6111, 0.6719, ..., 0.3935, 0.4093, 0.1318], [-0.2679, 0.4372, 0.4897, ..., 0.5854, 0.5598, 0.4889], [-0.0614, 0.0641, -0.0846, ..., 0.9225, 0.8031, 0.8296], ..., [-0.0864, -0.0523, -0.1517, ..., -0.3154, -0.4248, 0.7720], [-0.0210, -0.0107, -0.4515, ..., -0.3691, -0.4755, -0.3588], [ 0.2661, 0.2514, 0.4845, ..., 0.4435, 0.6367, -0.1002]], [[-0.2612, -0.2832, -0.6690, ..., -0.6152, -0.5100, -0.5664], [-0.5795, -0.6801, -1.0398, ..., -1.1175, -1.3070, -0.9341], [-0.4090, -0.7280, -0.6889, ..., -0.6297, -1.0082, -0.7394], ..., [ 0.2380, -0.9826, -0.7688, ..., -0.3479, -0.6661, -0.5269], [-0.8029, -1.2784, -1.1533, ..., -0.5312, -1.0361, -0.8832], [-0.6589, -0.5647, -0.3576, ..., -0.0677, -0.3550, -0.3389]], [[ 0.0297, -0.4572, -0.3457, ..., 0.0052, 0.3190, 0.2809], [-0.2332, -0.5157, -0.3640, ..., -0.4438, -0.3500, -0.5304], [-0.4940, -0.6755, -0.6056, ..., -0.2920, -0.5922, -0.1759], ..., [ 0.5285, 0.7424, -0.5153, ..., -0.4805, -0.3844, 0.4224], [ 0.3556, -0.6536, -0.6248, ..., 0.0121, 0.2211, 0.0086], [ 0.4613, -0.0046, -0.2656, ..., -0.1414, -0.0830, -0.6322]], ..., [[ 0.1395, -0.0938, -0.0249, ..., -0.2357, -0.2905, -0.4863], [-0.0598, 0.2485, 0.3585, ..., -0.4103, -0.3441, -0.6461], [-0.1429, 0.5415, 0.5160, ..., -0.2196, -0.0928, -0.3425], ..., [ 1.1019, -0.0367, -0.5643, ..., 0.0514, 0.1681, 0.1267], [ 0.9417, -0.0935, -0.6389, ..., -0.0209, 0.0145, 0.0313], [ 0.5574, 0.2148, -0.5317, ..., -0.3683, -0.4497, -0.1123]], [[ 0.0148, 0.0874, -0.7874, ..., 0.4710, 0.6335, -0.4608], [ 0.2926, 0.2670, -0.9552, ..., 0.2511, 0.6559, -0.3782], [ 0.5292, -0.1217, -0.6817, ..., -0.1196, 0.1346, -0.6021], ..., [ 0.2037, -0.5333, -0.2021, ..., 0.2152, 0.5675, -1.0983], [ 0.4190, -0.6143, 0.7062, ..., 0.4490, 0.7827, -0.9449], [ 0.1773, -0.5181, 1.0329, ..., 0.5269, 0.5063, -0.6873]], [[ 0.0336, -0.1246, -0.1963, ..., -0.3679, 0.0404, -0.4178], [-0.3616, -0.3861, -0.0552, ..., -0.0681, -0.0149, -0.3101], [-0.2323, -0.6257, -0.3293, ..., 0.0823, -0.1018, -0.2396], ..., [ 0.2612, -0.2018, -0.5177, ..., -0.7051, -0.2484, -0.2144], [ 0.2345, -0.1775, -0.4967, ..., -0.4719, -0.0954, -0.0237], [-1.0177, -0.5352, -0.4000, ..., -0.2615, -0.4057, -0.2122]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[ 1.3626e-03, -2.4663e-02, 4.6051e-02, ..., -9.1565e-03, 2.6631e-03, -2.3246e-02], [-1.3125e-01, -5.8948e-02, 7.5310e-02, ..., -4.9859e-02, -2.3506e-02, -4.2598e-02], [ 6.0517e-03, 7.9837e-02, 1.3708e-01, ..., -6.7412e-02, -1.6596e-01, -1.9063e-01], ..., [-1.0412e-01, 1.0787e-01, 1.9226e-02, ..., -1.7789e-01, -1.4185e-01, -1.1365e-01], [ 1.9537e-01, -8.7989e-02, -1.1354e-01, ..., -8.3751e-02, -8.7740e-02, -7.4591e-02], [ 1.0447e-01, -5.5854e-02, -5.7268e-02, ..., -5.2313e-02, 8.7574e-02, 4.5343e-02]], [[-4.4239e-02, -2.1504e-02, 4.3372e-03, ..., -1.2835e-02, -7.4009e-02, -4.2044e-02], [-5.3044e-02, -8.7785e-02, -1.2914e-01, ..., -4.7358e-02, -1.3473e-01, -6.2772e-02], [-9.3551e-02, -1.3260e-01, -1.3236e-01, ..., 1.3536e-01, 1.0653e-01, 1.2801e-01], ..., [-1.8869e-03, -6.6526e-02, -1.0954e-02, ..., 5.1577e-02, 2.0316e-01, 1.2561e-01], [-2.7441e-02, -9.5117e-02, 1.7368e-02, ..., 1.2589e-01, 2.3397e-01, 1.5138e-01], [-5.1650e-02, -5.0134e-02, 5.3820e-02, ..., 1.3207e-02, -7.1995e-04, -9.8985e-02]], [[-1.3365e-02, -4.0225e-02, 1.1895e-02, ..., -4.4649e-02, -2.2931e-02, -4.4239e-02], [ 8.7024e-03, -7.6718e-03, 5.0184e-02, ..., 7.8267e-03, -2.9017e-02, -1.0779e-02], [ 6.0618e-02, 1.2400e-02, 1.8309e-02, ..., -2.1703e-02, -4.2587e-02, -8.2061e-03], ..., [-4.6708e-03, -4.0856e-02, -1.2657e-01, ..., -7.2252e-02, 1.7713e-02, 5.6455e-02], [ 1.2032e-01, 8.5076e-03, -4.6789e-02, ..., 7.3983e-03, -6.1203e-02, 9.5531e-04], [ 9.6192e-02, 3.0080e-02, -2.5663e-02, ..., -2.1131e-02, -2.0930e-02, 5.3250e-03]], ..., [[ 8.0602e-02, 2.0411e-02, 6.5288e-02, ..., 6.6563e-02, 1.1606e-02, 2.7365e-02], [ 7.2081e-02, 8.9982e-02, 1.4145e-01, ..., 4.2192e-02, 5.2420e-02, 1.0945e-01], [ 5.4702e-02, 8.3627e-02, 3.0349e-02, ..., 9.6957e-02, 1.3886e-01, 1.6291e-01], ..., [ 1.2677e-01, -7.0860e-02, 4.2757e-02, ..., 5.4127e-02, 1.3381e-01, 1.5856e-01], [-6.8892e-02, -1.3785e-01, 6.4451e-02, ..., 9.7933e-02, 9.6460e-02, 1.6184e-01], [ 8.2498e-03, -7.8099e-04, 5.8352e-02, ..., 4.5872e-03, 3.9126e-02, 1.2256e-01]], [[-1.6818e-01, -1.2923e-01, -1.6857e-01, ..., -5.4568e-02, -1.2155e-01, -1.6396e-01], [-4.4197e-01, -3.9939e-01, -3.0251e-01, ..., -2.8055e-01, -3.1912e-01, -2.5067e-01], [-3.9235e-01, -3.4749e-01, -2.4425e-01, ..., -3.1263e-01, -3.2038e-01, -2.7487e-01], ..., [-5.7203e-01, -3.7472e-01, -2.5183e-01, ..., -3.1058e-01, -3.8692e-01, -3.0409e-01], [-7.4071e-01, -4.7059e-01, -2.6943e-01, ..., -2.5183e-01, -3.5373e-01, -2.9527e-01], [-5.1734e-01, -3.9472e-01, -2.5330e-01, ..., -4.2560e-01, -3.7086e-01, -3.3770e-01]], [[-6.9101e-03, -5.5174e-02, -5.2258e-02, ..., -5.5020e-02, -3.8445e-02, -1.0077e-01], [-5.0044e-02, -3.1171e-02, -2.5651e-02, ..., -5.5442e-02, -3.2126e-02, -1.1928e-01], [ 7.3592e-02, 8.5570e-02, 4.4706e-02, ..., -2.2834e-02, 5.2341e-02, -3.3763e-03], ..., [ 1.5344e-01, -1.2464e-01, -7.3227e-02, ..., -9.2346e-02, 1.2123e-01, 8.5718e-02], [ 6.0997e-02, -1.2332e-01, -1.0832e-02, ..., -7.4599e-02, -7.8659e-02, -1.0594e-01], [-4.7824e-02, -7.0945e-02, -1.7477e-02, ..., -8.5501e-02, -1.0939e-01, -1.5686e-01]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-4.2873e-01, -4.6934e-01, -4.1325e-02, ..., -3.2636e-02, 7.4030e-02, 5.8232e-03], [-5.5538e-01, -1.0775e+00, -5.9274e-01, ..., -9.1038e-01, -3.8420e-01, -3.6700e-01], [-4.9508e-01, -1.0233e+00, -2.1957e-01, ..., -5.9944e-01, -3.6409e-01, -1.1305e-01], ..., [-6.3640e-01, -2.6598e-01, -2.8498e-01, ..., -3.0316e-01, -3.0609e-01, -9.3499e-01], [ 1.0858e-01, 9.1596e-02, -3.1969e-01, ..., -4.5866e-01, -2.9367e-01, -9.2299e-01], [ 6.3381e-01, 1.1834e-01, -1.6743e-01, ..., -1.6575e-01, -1.6571e-01, -2.6300e-01]], [[ 4.0905e-03, -4.3001e-01, -5.7072e-01, ..., 8.1292e-04, -6.4482e-01, -3.6637e-01], [-3.0681e-01, 1.5747e-02, -2.6038e-01, ..., 3.0524e-01, -2.2313e-01, 4.0327e-02], [-5.6381e-01, 1.9786e-03, -1.3212e-01, ..., 6.2308e-02, -5.2619e-01, -1.3217e-01], ..., [-7.0862e-01, -9.1414e-01, -2.5121e-01, ..., -4.9914e-01, -7.5175e-01, -1.7616e-01], [-2.1418e-01, -4.5349e-01, -3.8520e-01, ..., -3.2243e-01, -4.9149e-01, -3.4339e-01], [ 9.3637e-02, -8.7578e-02, -2.4526e-01, ..., -1.3802e-01, -3.6111e-01, -5.0108e-01]], [[ 7.3805e-01, 1.4611e-01, -2.7105e-01, ..., -1.2570e-01, -2.0609e-01, -7.4954e-02], [ 8.2128e-01, 3.0414e-01, -2.0269e-01, ..., 1.8832e-01, 3.5639e-01, 8.4530e-01], [ 5.3060e-01, -1.9782e-01, -3.9589e-01, ..., -3.7223e-01, -3.4382e-01, -8.0846e-02], ..., [-5.9906e-01, -6.1805e-01, -3.0498e-01, ..., -8.5905e-01, -1.4049e+00, -7.6847e-01], [-3.7451e-01, -3.8290e-01, -7.8562e-02, ..., -2.9276e-02, -6.0355e-01, -3.1064e-01], [ 8.4362e-01, 5.4807e-01, 2.0263e-01, ..., 3.1692e-01, 1.3642e-01, 6.8857e-01]], ..., [[ 5.1337e-01, 3.9596e-01, 2.2522e-01, ..., 4.1894e-01, 1.0254e-01, -4.0375e-01], [-5.3977e-03, -3.4549e-01, -5.9793e-01, ..., -1.7841e-01, -3.0228e-01, -8.4002e-01], [-2.3763e-01, -6.0266e-01, -3.2253e-01, ..., -1.2647e-01, -2.9172e-01, -1.0146e+00], ..., [-8.1082e-02, -4.2886e-01, 3.3912e-01, ..., -7.1773e-01, -3.2211e-01, -4.3716e-01], [ 3.6547e-02, 6.8812e-01, -3.2904e-01, ..., -1.8101e-01, -4.1893e-01, -7.3784e-01], [-3.4927e-01, 1.3177e+00, 2.0435e-01, ..., -1.9723e-02, -6.6064e-02, -2.3001e-01]], [[ 6.9231e-01, -1.0600e-01, -2.5855e-01, ..., -4.6072e-01, 9.7681e-01, -7.9610e-01], [-5.8889e-01, -1.1106e+00, -3.1060e-02, ..., -3.3787e-01, 5.3565e-01, -5.3625e-01], [-5.6028e-01, -7.4135e-01, -1.4226e-01, ..., 4.8697e-02, 1.7423e-01, -7.5924e-01], ..., [-5.8227e-01, -3.0304e-01, -1.0266e+00, ..., -9.6372e-01, -5.2071e-01, -5.8520e-01], [-3.2737e-01, -5.4993e-01, -1.3333e+00, ..., -5.4997e-01, -2.9026e-01, -8.1955e-01], [ 5.2538e-02, 5.0382e-01, -6.4723e-01, ..., -3.1758e-01, -2.8657e-01, -6.7551e-01]], [[-9.3298e-01, -8.5845e-01, -1.0554e+00, ..., -8.7959e-01, -7.9436e-01, -1.1912e+00], [-1.0732e+00, -9.6579e-01, -8.0460e-01, ..., -1.0780e+00, -5.7669e-01, -1.0331e+00], [-1.1648e+00, -9.7872e-01, -5.6461e-01, ..., -9.1402e-01, -4.6951e-01, -8.8273e-01], ..., [-1.5256e+00, -1.6750e-01, 2.1887e-01, ..., -9.9437e-01, -7.7857e-01, -1.0885e+00], [-1.3656e+00, 2.4792e-01, -5.4816e-01, ..., -1.2917e+00, -1.4464e+00, -1.6200e+00], [-7.4554e-01, 7.8284e-01, -6.8172e-02, ..., -4.8101e-01, -8.3025e-01, -8.2073e-01]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[ 0.0988, 0.1280, 0.0348, ..., 0.0244, 0.0435, 0.0492], [-0.0232, 0.0532, 0.0509, ..., -0.1039, -0.0123, 0.0517], [-0.0195, -0.0130, 0.0014, ..., 0.0052, 0.1269, 0.0927], ..., [ 0.0612, -0.0726, -0.0816, ..., -0.0858, 0.0101, 0.0488], [ 0.0246, -0.0265, -0.0734, ..., -0.0315, 0.0121, 0.0841], [-0.0066, -0.0592, -0.0653, ..., 0.0848, 0.0086, 0.0533]], [[ 0.0054, 0.0172, 0.0109, ..., 0.0438, -0.0023, 0.0667], [ 0.0028, 0.0551, -0.0138, ..., 0.0370, -0.0277, 0.0705], [-0.0194, 0.0452, -0.0279, ..., 0.0167, -0.0203, 0.0328], ..., [ 0.0789, -0.0072, -0.0014, ..., 0.0319, -0.0005, 0.0071], [ 0.0310, -0.0774, 0.0336, ..., -0.0352, -0.0523, 0.0280], [ 0.0317, -0.0733, 0.0212, ..., -0.0258, -0.0484, 0.0304]], [[ 0.0447, 0.0728, 0.0419, ..., 0.0372, 0.0194, 0.0387], [ 0.0556, 0.0240, 0.0387, ..., 0.0497, 0.0876, 0.0938], [-0.0082, 0.0247, 0.0094, ..., -0.0385, 0.0543, 0.0157], ..., [-0.0077, -0.0965, -0.0529, ..., 0.0185, 0.0220, 0.0010], [ 0.0037, -0.1304, -0.0403, ..., 0.0028, -0.0538, 0.0186], [ 0.0494, 0.0494, -0.0566, ..., -0.0360, 0.0525, 0.0500]], ..., [[ 0.1733, 0.1157, 0.0382, ..., 0.0869, 0.1794, 0.1358], [-0.0386, -0.0362, 0.0066, ..., 0.0206, -0.0064, -0.0312], [ 0.0028, -0.0682, -0.0012, ..., -0.0308, -0.0326, 0.0439], ..., [ 0.1916, 0.0499, -0.0054, ..., 0.0241, 0.0702, 0.0216], [ 0.1830, 0.0439, 0.0128, ..., 0.0936, 0.0854, 0.0686], [ 0.1746, 0.0807, -0.0055, ..., -0.0044, 0.0453, 0.0411]], [[-0.0622, -0.0869, -0.0834, ..., -0.0582, -0.0993, -0.1090], [-0.0826, -0.2253, -0.1340, ..., -0.1373, -0.1944, -0.1553], [-0.1094, -0.1520, -0.1378, ..., -0.1157, -0.2341, -0.2295], ..., [-0.2469, -0.1926, 0.0333, ..., -0.1980, -0.3922, -0.2547], [-0.0622, -0.0641, 0.0081, ..., -0.1415, -0.1819, -0.0514], [-0.1384, -0.0867, -0.0205, ..., -0.1600, -0.1566, -0.0787]], [[ 0.1696, 0.0830, 0.0843, ..., 0.0816, 0.1086, 0.0569], [ 0.0366, 0.1129, 0.0805, ..., 0.0165, 0.0319, -0.0460], [-0.0262, 0.0282, 0.0309, ..., 0.0037, 0.0894, 0.0436], ..., [ 0.1452, 0.0994, 0.0504, ..., 0.0656, 0.2144, 0.1010], [ 0.1647, 0.0498, 0.0299, ..., -0.0269, 0.1108, 0.0720], [ 0.1522, 0.0643, -0.0155, ..., 0.0290, 0.0490, 0.0259]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[ 4.1243e-01, 3.6304e-01, 8.8107e-01, ..., 6.0127e-01, 6.5288e-01, 9.4326e-01], [ 6.0697e-01, 6.4844e-01, 8.7361e-01, ..., 6.2816e-01, 6.8928e-01, 5.3948e-01], [-1.1460e-03, 4.0684e-01, 6.6968e-01, ..., 2.4982e-01, 3.0477e-01, 4.4353e-01], ..., [ 1.0008e+00, 3.6334e-01, 3.7838e-01, ..., 6.2974e-02, 1.3335e-01, 7.1240e-01], [ 9.4002e-01, 4.6166e-01, 3.9414e-01, ..., 1.1289e-01, 2.4865e-01, 1.6548e+00], [ 4.5392e-01, 3.8610e-01, 2.1027e-01, ..., 3.0638e-01, 4.6619e-01, 7.9911e-01]], [[-4.5250e-01, -2.0844e-01, -2.4178e-01, ..., -2.8272e-01, -3.9787e-01, -8.4349e-01], [-8.0248e-01, -2.7057e-01, -4.7645e-01, ..., -3.7947e-01, -1.8241e-01, 6.5178e-02], [-5.1991e-01, -6.0929e-01, -3.5325e-01, ..., -5.2016e-01, -2.4186e-01, -6.5532e-01], ..., [-5.9643e-01, 5.7050e-01, -8.3123e-01, ..., -4.4619e-01, -2.6789e-01, -5.2685e-01], [ 1.7776e-01, 1.0316e-01, -6.1159e-01, ..., -8.4718e-01, -9.2070e-02, -6.6023e-01], [-4.6888e-01, -3.4122e-02, -8.5488e-02, ..., -1.0417e+00, -1.8465e-02, -4.5028e-01]], [[ 1.3665e+00, 6.7283e-01, 8.2696e-01, ..., 1.1734e+00, 5.8562e-01, 1.1521e+00], [-3.5310e-01, -3.1397e-01, 1.0160e-01, ..., 2.0190e-01, -1.2062e-01, 1.1574e-02], [ 1.8670e-01, -7.1362e-01, -2.7392e-01, ..., -4.8404e-01, -5.5818e-01, -1.8868e-01], ..., [ 5.7915e-01, 4.0021e-01, 1.3172e-01, ..., -2.4053e-01, 3.9218e-02, 4.3768e-01], [ 4.3819e-01, -4.0679e-01, -2.1769e-03, ..., -8.8953e-02, 8.1186e-02, 3.6979e-01], [ 1.4249e+00, -2.3837e-02, -1.5262e-01, ..., 5.2375e-01, 2.9434e-01, 2.4603e-01]], ..., [[ 3.8155e-02, -1.6977e-01, -2.0032e-01, ..., -5.6964e-01, -2.9177e-01, -9.4349e-02], [ 2.4736e-01, 1.3977e-01, -1.2234e-01, ..., -4.4522e-01, 6.4455e-02, 3.9772e-01], [-6.1862e-01, -2.3263e-01, 2.2372e-01, ..., -1.3316e-01, 1.3979e-01, 4.3874e-01], ..., [-4.4341e-02, 1.2667e-01, -1.4757e-01, ..., -4.7259e-02, -8.4698e-02, -3.8401e-01], [ 5.8723e-02, -4.6699e-01, 2.9764e-01, ..., 1.5851e-02, 2.0578e-01, -2.7948e-01], [ 3.0847e-01, -7.3721e-01, -8.6126e-01, ..., -1.1998e+00, -4.6212e-01, 1.3137e+00]], [[ 8.6156e-02, -2.0926e-01, -7.1230e-01, ..., -1.3362e-01, 3.1789e-01, 3.6001e-01], [-1.2724e-01, -1.2035e-01, -6.0503e-01, ..., -1.7572e-01, -2.1856e-01, -1.0504e-01], [ 6.0878e-02, 2.0468e-01, 6.8006e-02, ..., 7.5139e-01, 2.5093e-02, -1.5781e-01], ..., [ 8.0393e-01, 6.9006e-02, 1.1580e+00, ..., -6.4039e-01, -7.3785e-02, 6.7414e-01], [ 8.7700e-01, -4.3417e-01, -2.9335e-01, ..., -1.4228e-01, 2.3888e-01, 7.7920e-01], [ 5.2887e-01, -3.4151e-01, -3.1082e-01, ..., 2.4944e-01, -8.7037e-02, 7.1645e-01]], [[ 1.2785e-01, -2.4757e-01, 4.0018e-01, ..., -7.1277e-01, -4.4163e-01, -3.2978e-01], [-3.8121e-01, -4.8290e-01, -4.8577e-01, ..., -1.7976e+00, -1.0235e+00, -8.7740e-01], [-4.0046e-01, -3.5255e-01, 2.1916e-02, ..., -1.6129e+00, -1.1431e+00, -9.3650e-01], ..., [-1.4788e+00, -6.9077e-01, -4.5148e-01, ..., -8.8217e-01, -9.8117e-01, -1.0065e+00], [-1.0895e+00, -9.9224e-01, -6.8925e-01, ..., -5.7167e-01, -6.9166e-01, -1.7308e+00], [-1.4956e+00, -9.4706e-01, -8.6554e-01, ..., -1.4528e+00, -7.2765e-01, -1.3770e-01]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-0.1236, -0.0602, -0.1390, ..., -0.4963, -0.4020, -0.1198], [-0.0350, -0.0439, -0.2125, ..., -0.0424, 0.0160, 0.1509], [-0.2220, -0.1353, -0.3867, ..., -0.1576, -0.0601, -0.0723], ..., [-0.1874, -0.1648, -0.3391, ..., 0.0121, -0.2412, -0.3454], [-0.1325, -0.4435, -0.3661, ..., -0.3154, -0.1932, -0.2613], [-0.3495, -0.1867, -0.0766, ..., -0.2538, -0.1465, -0.1136]], [[-0.3884, -0.2835, -0.3478, ..., -0.3397, -0.6106, -0.5631], [-0.2955, -0.1269, 0.0024, ..., -0.0864, -0.4283, -0.4903], [-0.0700, -0.0156, -0.0923, ..., 0.1401, -0.0755, -0.4042], ..., [ 0.1401, -0.3010, -0.3625, ..., -0.2610, -0.4198, -0.2596], [-0.1279, -0.3806, -0.5162, ..., -0.3461, -0.4323, -0.2634], [-0.2910, -0.3711, -0.3404, ..., -0.1880, -0.1061, -0.1619]], [[-0.2050, -0.5653, -0.7059, ..., -0.5888, -0.4452, -0.2084], [-0.3620, -0.8297, -0.5641, ..., -0.7035, -0.6533, -0.4250], [-0.2378, -0.5462, 0.0864, ..., -0.2322, -0.3490, -0.2139], ..., [-0.6504, -0.7616, 0.0459, ..., 0.2952, 0.3382, 0.0308], [-0.6238, -0.5782, 0.0213, ..., 0.0991, -0.1207, 0.2721], [-0.2482, -0.3077, 0.1315, ..., -0.0717, -0.3099, 0.0063]], ..., [[ 0.0926, -0.0044, -0.0968, ..., 0.0761, -0.0428, 0.1569], [ 0.1673, 0.0863, -0.1966, ..., 0.2720, 0.1123, 0.2954], [ 0.1487, 0.1765, 0.2782, ..., 0.0290, -0.0423, 0.4044], ..., [ 0.1914, -0.1436, -0.2107, ..., -0.1952, 0.0748, -0.0246], [-0.2369, 0.0786, -0.0242, ..., -0.1436, -0.2877, 0.1694], [-0.0514, 0.2114, 0.3048, ..., 0.1482, -0.3507, 0.4003]], [[-0.1723, 0.0760, 0.0721, ..., 0.0341, 0.1266, -0.2085], [-0.2347, -0.3643, 0.0663, ..., -0.3313, -0.1482, -0.2671], [-0.3051, -0.3792, 0.2800, ..., -0.1511, -0.1272, -0.2918], ..., [ 0.0721, 0.0340, 0.2539, ..., 0.4027, 0.0683, -0.3045], [ 0.0758, -0.1852, -0.1356, ..., 0.1785, 0.0511, -0.0705], [-0.2175, -0.0551, -0.2782, ..., 0.0408, -0.0052, -0.0972]], [[ 0.2038, 0.0062, 0.1736, ..., 0.1367, 0.2460, 0.1823], [ 0.1573, -0.2326, 0.0208, ..., -0.3015, 0.0737, 0.1335], [ 0.1387, 0.0569, 0.3607, ..., -0.0596, 0.1768, 0.2967], ..., [ 0.7866, 0.5105, 0.4362, ..., 0.0980, 0.3943, 0.5658], [ 0.5503, 0.0597, 0.1589, ..., -0.0902, 0.3756, 0.5093], [ 0.1137, -0.0809, -0.0159, ..., 0.1156, 0.3735, 0.4709]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-3.2580e-02, 4.6716e-03, -6.6709e-02, ..., -4.7522e-03, -2.2099e-03, 9.0466e-02], [-1.6302e-01, -2.9018e-02, -2.1720e-01, ..., -4.3875e-02, 1.0886e-02, 2.8372e-02], [-2.5479e-01, -1.4047e-01, -1.7275e-01, ..., 8.0531e-02, 1.9334e-01, 1.5276e-02], ..., [-2.1139e-01, -1.4825e-01, -4.9402e-03, ..., 9.2946e-02, -1.0612e-01, -1.7040e-01], [-2.0970e-01, -1.6195e-01, -1.7520e-01, ..., -6.7448e-02, 4.9452e-02, -1.1551e-01], [-2.7232e-01, -9.8204e-02, -8.8646e-02, ..., 8.4115e-03, 1.1206e-01, -1.0140e-02]], [[ 4.8812e-02, -6.0470e-02, -5.8316e-02, ..., -8.2867e-02, -9.3440e-02, -7.1303e-02], [-1.4589e-01, -1.4611e-01, -1.8509e-01, ..., 7.8155e-02, -5.8147e-03, -1.0605e-01], [-7.0111e-02, -2.2644e-01, -8.3414e-02, ..., 2.0901e-01, 8.1771e-02, -1.6470e-02], ..., [ 1.8304e-01, -8.1817e-03, 6.9792e-02, ..., -1.0695e-01, -1.7610e-01, -1.3897e-01], [ 2.4655e-01, -1.4628e-01, -2.1027e-01, ..., -2.4190e-01, 8.1670e-03, 2.2009e-01], [ 1.0761e-02, -2.2890e-01, -2.6206e-01, ..., -1.8678e-01, 5.0541e-02, 1.7424e-01]], [[-6.4699e-01, -6.5046e-01, -6.4946e-01, ..., -6.3103e-01, -5.8418e-01, -6.0015e-01], [-3.5077e-01, -2.5295e-01, -3.1632e-01, ..., -2.6033e-01, -3.3463e-01, -4.6449e-01], [-3.1468e-01, -3.2959e-01, -2.4537e-01, ..., -3.4004e-01, -2.1492e-01, -1.9570e-01], ..., [-5.4013e-01, -2.5121e-01, -1.6403e-01, ..., -1.5129e-02, -2.1977e-01, -2.5057e-01], [-3.8462e-01, -2.4791e-01, -2.3765e-01, ..., -2.2953e-01, -4.0156e-01, -2.9487e-01], [-8.4561e-01, -1.3793e-01, -1.1995e-01, ..., -3.2368e-01, -1.9963e-01, -2.4796e-01]], ..., [[ 5.7024e-01, 6.6056e-01, 9.5901e-02, ..., -2.4350e-02, 2.7821e-01, 1.2417e-01], [-7.0457e-02, 3.7125e-02, -8.5546e-02, ..., -7.4320e-02, -1.3726e-01, 6.1165e-03], [-9.4908e-02, -4.6271e-02, 2.8854e-02, ..., -4.7003e-02, 1.2840e-01, -2.9733e-02], ..., [-1.0554e-01, -6.0221e-02, -1.7736e-01, ..., 7.9574e-03, -2.8799e-01, -2.2224e-01], [-5.7899e-02, -1.0701e-01, -1.7920e-01, ..., -1.4876e-01, -2.7093e-01, -7.9749e-02], [-4.4996e-02, -2.9580e-01, 2.2398e-01, ..., -2.4354e-01, -2.1907e-01, -8.4004e-02]], [[ 3.4024e-01, -1.5116e-01, -7.1770e-02, ..., -1.6114e-01, 9.6471e-02, 3.4851e-01], [-3.5924e-01, -1.8450e-01, 9.4673e-02, ..., -3.2239e-01, -3.4507e-01, -7.4684e-02], [-2.6030e-01, -2.0770e-01, -3.4150e-01, ..., -2.8970e-01, 2.1401e-01, -9.7496e-02], ..., [ 1.9954e-01, -2.0608e-01, -1.2124e-01, ..., 1.2212e-01, 9.7532e-02, -3.5990e-01], [ 4.1975e-01, -2.3223e-01, -2.0307e-01, ..., -3.8297e-01, -3.5574e-01, -6.8504e-02], [ 1.8114e-04, -2.0356e-01, 1.0351e-01, ..., -2.3819e-01, -1.4674e-01, -1.1833e-01]], [[ 3.0758e-01, 2.7873e-01, 5.4953e-02, ..., 1.4411e-01, 1.1629e-01, 1.0042e-01], [ 1.7667e-01, -1.8428e-01, 5.3541e-02, ..., 1.0744e-01, 3.9272e-02, 1.7413e-01], [ 2.0926e-03, 1.1734e-01, 4.6211e-02, ..., -2.7263e-01, 5.6385e-03, -6.9367e-02], ..., [ 2.7898e-01, 9.0060e-02, 1.0426e-01, ..., -5.5958e-02, -9.9089e-02, 1.5218e-01], [ 7.2136e-02, -2.8248e-02, 5.7768e-02, ..., -1.7842e-01, -4.6636e-02, 3.7414e-01], [ 3.3351e-01, -1.6085e-01, 6.2398e-02, ..., -2.3159e-02, 2.2459e-01, 1.4017e-01]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[ 0.8268, 0.4441, 0.3694, ..., 0.4115, 0.7633, 0.8978], [-0.0194, -0.7248, -0.6000, ..., -0.7206, -0.5846, -0.1153], [ 0.1886, -0.4891, -0.5042, ..., -0.5467, -0.4565, -0.1056], ..., [ 1.0802, 0.5223, 0.2821, ..., -0.1917, -0.3600, -0.3545], [ 1.0232, 0.4767, 0.1597, ..., -0.3930, -0.4350, -0.0377], [ 1.0075, 0.7341, 0.2484, ..., 0.2522, -0.0150, 0.2700]], [[-0.5701, -0.7030, -0.7707, ..., -0.6556, -0.8253, -0.5767], [-0.5188, -0.8589, -0.7814, ..., -0.9747, -1.1042, -0.6838], [-0.5817, -0.6517, -0.4354, ..., -0.3231, -0.6983, -0.5580], ..., [-0.5730, -0.7506, -0.6265, ..., -0.5238, -0.5434, -0.4181], [-0.3847, -0.6073, -0.8639, ..., -0.4829, -0.4861, 0.0185], [-0.4507, -0.4574, -0.5627, ..., -0.2748, -0.5572, -0.3460]], [[ 0.1596, 0.0046, -0.2229, ..., 0.2172, 0.4685, 0.4965], [ 0.2181, 0.2517, 0.3000, ..., -0.0341, -0.2611, -0.2299], [ 0.4062, 0.3518, 0.4570, ..., 0.0234, -0.1292, -0.0786], ..., [-0.3141, -0.1117, 0.1558, ..., 0.0288, -0.2321, -0.1655], [ 0.0597, -0.0387, 0.2186, ..., 0.3252, 0.3253, 0.2932], [ 0.0218, -0.1647, -0.0439, ..., 0.1506, 0.2616, 0.2033]], ..., [[-0.3023, -0.5082, -0.4832, ..., -0.8931, -0.7944, -0.5214], [-0.4516, -0.2861, -0.5033, ..., -0.8971, -1.2599, -0.8366], [-0.5091, -0.2623, -0.6127, ..., -0.9415, -1.1568, -0.9192], ..., [-0.8310, -1.0234, -1.1779, ..., -0.3845, -0.4947, -0.1517], [-0.9886, -0.7029, -0.4522, ..., -0.5521, -0.8839, -0.6843], [-0.3870, -0.2183, -0.0513, ..., -0.3436, -0.6146, -0.6278]], [[ 0.0665, 0.0452, -0.1549, ..., 0.1974, -0.2540, -0.0679], [-0.3426, 0.0577, -0.8722, ..., -0.5749, -0.8167, -0.1882], [-0.7388, 0.0540, -0.1547, ..., -0.7001, -1.1010, -0.0070], ..., [-0.4458, -0.0720, -0.0663, ..., -0.1730, -0.5392, -0.0321], [ 0.0398, 0.3934, -0.1953, ..., -0.4248, -0.6574, -0.3950], [-0.1572, 0.5568, 0.0729, ..., -0.4862, -0.5172, -0.2908]], [[-0.9220, -1.2343, -0.1687, ..., -0.2934, -0.1706, -0.1749], [-0.9835, -1.1848, -0.7574, ..., -0.2448, -0.1289, 0.0472], [-1.0677, -1.1468, -0.6967, ..., -0.5125, -0.0795, 0.3138], ..., [-0.1524, -0.4937, -0.6800, ..., 0.0790, -0.0247, -0.2594], [-0.4430, -0.3362, -0.4801, ..., -0.0096, -0.7177, -0.2634], [-0.3126, -0.4949, -0.1230, ..., -0.0975, -0.8262, -0.2739]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[ 0.0891, -0.0343, -0.0670, ..., 0.0482, 0.0205, 0.0690], [ 0.0079, -0.0992, -0.1780, ..., -0.0172, 0.0695, 0.0783], [-0.1336, -0.1911, -0.1928, ..., -0.0254, -0.0152, -0.0049], ..., [-0.0549, -0.0683, -0.1709, ..., -0.0291, -0.0146, -0.0118], [-0.0996, -0.1211, -0.2715, ..., 0.0197, -0.0492, -0.0516], [-0.0667, -0.0486, -0.1591, ..., -0.0957, -0.0715, -0.1130]], [[-0.0414, -0.0187, -0.0580, ..., -0.0443, -0.0396, -0.0592], [-0.0774, -0.0847, -0.1103, ..., -0.0007, 0.0389, 0.0294], [-0.0698, -0.0738, -0.0786, ..., -0.0846, -0.1175, -0.1000], ..., [-0.0121, -0.1405, -0.0540, ..., -0.1062, -0.1952, -0.1241], [-0.0941, -0.1619, -0.0185, ..., -0.2264, -0.1543, -0.0991], [-0.0718, -0.0770, 0.0399, ..., -0.1147, 0.0332, 0.0185]], [[-0.0967, -0.1833, -0.1092, ..., -0.2087, -0.1302, -0.0594], [-0.1555, -0.2623, -0.1557, ..., -0.2220, -0.2110, -0.1157], [-0.1416, -0.1988, -0.1886, ..., -0.1003, -0.1234, -0.0743], ..., [-0.1254, -0.2000, -0.1661, ..., -0.0280, -0.0018, -0.0108], [-0.1081, -0.1427, -0.1374, ..., -0.1261, -0.1929, -0.0767], [-0.0184, -0.0225, -0.0251, ..., -0.0505, -0.0375, 0.0290]], ..., [[-0.2503, -0.2906, -0.2482, ..., -0.0599, -0.1782, -0.1961], [-0.2494, -0.3674, -0.2781, ..., -0.1314, -0.1071, -0.1463], [-0.0903, -0.1135, -0.0733, ..., -0.0469, 0.0529, -0.0597], ..., [-0.1460, -0.1257, -0.0230, ..., 0.0024, 0.0556, -0.0626], [-0.1900, -0.0814, -0.0467, ..., -0.0095, -0.0219, -0.0222], [-0.1668, -0.0776, -0.0633, ..., -0.0105, -0.1150, -0.0501]], [[-0.0462, -0.0058, -0.0168, ..., -0.0012, -0.0573, 0.0469], [-0.1890, -0.1472, -0.0599, ..., -0.1150, -0.0626, 0.0218], [-0.0405, -0.0680, 0.0672, ..., -0.0256, 0.0763, 0.0875], ..., [ 0.0059, -0.0371, -0.0750, ..., 0.0666, 0.0448, -0.0269], [ 0.0553, -0.0393, -0.1183, ..., 0.0850, 0.0264, -0.0066], [ 0.0347, -0.0798, -0.1994, ..., 0.0171, -0.0586, -0.0245]], [[-0.0178, -0.0318, -0.0838, ..., 0.0773, 0.0054, -0.0698], [-0.0545, -0.0315, -0.0842, ..., 0.0640, -0.0761, -0.1300], [-0.0559, -0.0413, -0.0776, ..., -0.1542, -0.1934, -0.1077], ..., [ 0.0022, 0.1460, 0.1342, ..., -0.1196, -0.1491, -0.1003], [ 0.0401, 0.1450, 0.1216, ..., -0.0765, -0.1312, -0.1566], [ 0.0010, 0.0654, 0.0558, ..., -0.0464, -0.0703, -0.1186]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-0.2521, -0.8738, -0.6532, ..., 0.0576, -0.2081, -0.3561], [-0.3011, -0.5276, -0.6430, ..., -0.1986, -0.4507, -0.3705], [-0.4933, -0.8762, -1.2042, ..., 0.0192, -0.5263, -0.4188], ..., [-0.4822, -0.5530, -1.0430, ..., -0.2959, -0.1074, -0.3416], [-0.5835, -0.6165, -0.9733, ..., -1.1879, -0.3531, -0.4250], [-0.6776, -0.6156, -0.5577, ..., -0.5834, -0.3649, -0.2203]], [[ 0.0177, -0.1427, -0.3212, ..., -0.2198, -0.2059, 0.0977], [-0.2004, -0.5116, -0.3690, ..., -0.4446, -0.8544, -0.5728], [-0.2213, -0.3730, -0.3643, ..., -0.2383, -0.4089, -0.3251], ..., [-0.1507, -1.1372, -0.9870, ..., -0.6122, -0.4332, -0.1330], [-0.3038, -0.9152, -0.8302, ..., -0.3705, -0.4816, -0.3446], [-0.4832, -0.5504, -0.2201, ..., -0.2456, -0.3247, -0.3117]], [[-0.0672, -0.1554, 0.0552, ..., -0.2345, -0.7497, -0.6175], [ 0.2831, 0.0600, -0.0768, ..., -0.3010, -0.7343, -0.7601], [-0.1145, -0.3830, -0.6809, ..., -0.5276, -0.5555, -0.6247], ..., [-0.2057, -0.7569, -0.5689, ..., -0.6663, -0.5145, 0.0535], [-0.6855, -0.7507, -0.1757, ..., -0.1187, -0.5039, 0.0934], [-0.4590, -0.5180, -0.2126, ..., -0.4002, -0.3137, -0.5877]], ..., [[-0.5173, -0.7319, -0.5628, ..., -0.7071, -0.6873, -0.4385], [-0.7093, -1.0123, -0.5945, ..., -0.5060, -0.6159, -0.7295], [-0.5527, -0.6056, -0.2464, ..., -0.7749, -0.8456, -0.8498], ..., [-0.9676, -1.5211, -1.0507, ..., -0.5427, -0.3854, -0.7520], [-0.9118, -1.0320, -0.8958, ..., -0.7115, -0.5381, -0.4616], [-0.5716, -0.8103, -0.7555, ..., -0.4182, -0.5253, -0.5415]], [[-0.5313, -0.3614, -0.5246, ..., -0.6295, -0.5290, -0.4729], [-0.2976, -0.3723, -0.5166, ..., -0.8591, -0.6088, -0.5045], [-0.7066, -0.2102, -0.0457, ..., -0.8686, -0.8044, -0.7096], ..., [-0.9559, -0.4253, -0.2284, ..., -0.5182, -0.3698, -0.6497], [-1.1067, -0.8729, -0.9031, ..., -0.4912, -0.2269, -0.6201], [-0.8432, -0.5654, -0.6099, ..., -0.4652, -0.3318, -0.4693]], [[ 0.1028, 0.1567, 0.1607, ..., 0.8193, 0.1068, 0.1878], [ 0.0629, 0.0170, 0.6012, ..., 0.4266, 0.1424, 0.2063], [ 0.1214, 0.1671, 1.2869, ..., 0.3030, 0.1830, 0.3461], ..., [-0.1943, 0.3957, -0.0051, ..., 0.3658, 0.8822, 0.6292], [ 0.2586, 0.7748, -0.0087, ..., 0.5417, 0.2212, -0.0846], [ 0.5294, 0.7278, 0.1215, ..., 0.1824, -0.2166, -0.2289]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[ 5.9690e-02, 8.1791e-02, 8.8460e-02, ..., 1.2052e-02, -2.9440e-02, 4.1868e-02], [ 2.0751e-01, 1.9978e-01, 3.1553e-01, ..., 5.5059e-02, 7.1169e-02, 7.4423e-02], [ 9.4788e-02, 9.9758e-02, 5.5432e-02, ..., -2.8651e-02, -9.6179e-02, -2.8936e-02], ..., [-5.7275e-02, -6.1507e-02, -2.1981e-02, ..., 5.6889e-02, 4.3233e-02, 8.5275e-02], [-7.9141e-02, -2.5249e-02, -6.8972e-02, ..., 3.9908e-02, -6.9416e-02, 2.6843e-03], [ 2.1747e-02, 1.2513e-01, 7.1213e-02, ..., -2.8586e-02, -1.3100e-02, -2.4738e-02]], [[-2.7763e-02, -1.1040e-01, -5.0566e-02, ..., -7.8674e-02, -3.8930e-02, 2.7603e-02], [-4.1623e-02, -9.8588e-02, -4.6023e-02, ..., -5.2057e-02, -9.7615e-02, -7.2089e-02], [-5.5634e-02, -9.4488e-02, -5.6073e-02, ..., -2.9765e-02, -1.4687e-01, -1.0436e-01], ..., [ 3.8538e-02, -3.3256e-02, -2.8461e-02, ..., -8.6775e-03, -6.4014e-02, -6.6953e-02], [-1.2763e-02, -9.8569e-02, -8.2613e-02, ..., -2.2940e-02, -7.3775e-02, -8.6286e-02], [-8.1326e-02, -1.1877e-01, -8.2915e-02, ..., -2.1756e-02, -5.5721e-02, -5.7234e-02]], [[ 5.3683e-02, -7.5098e-02, -1.4165e-01, ..., -1.2710e-01, -3.9675e-02, -2.3752e-03], [-9.5284e-02, -1.8529e-01, -1.0618e-01, ..., -9.2752e-02, -9.2028e-03, -2.8579e-02], [ 1.0723e-02, -2.3845e-02, 5.8893e-02, ..., -8.4111e-02, -1.0929e-02, -3.8908e-02], ..., [-1.2620e-01, -1.2239e-01, -6.0111e-02, ..., 1.9814e-02, -7.4255e-02, -1.9455e-02], [-1.8121e-01, -1.2122e-01, 3.9839e-02, ..., -8.2767e-02, -1.8340e-01, -1.2343e-01], [-9.4623e-02, -2.1298e-02, 9.8767e-02, ..., -1.3415e-01, -1.4433e-01, -9.5447e-02]], ..., [[-1.4666e-01, -2.1602e-01, -9.7738e-02, ..., -6.1896e-02, -5.6454e-02, -5.9906e-02], [ 1.9899e-02, 5.1323e-03, 1.3906e-02, ..., 1.5137e-02, 8.3006e-02, 3.2444e-02], [-3.5191e-02, -5.4830e-02, -2.9264e-04, ..., -5.2180e-02, -3.4717e-02, -2.7403e-02], ..., [ 2.9712e-02, 1.5094e-02, -1.5655e-02, ..., 1.8559e-02, -1.6583e-02, -1.3607e-02], [ 1.0858e-02, -7.2396e-02, -3.3979e-02, ..., -1.7926e-02, -4.6190e-02, -5.3141e-02], [-3.7275e-02, -9.7014e-02, 1.0602e-02, ..., -5.7990e-02, -2.2373e-02, -1.8351e-02]], [[ 2.2787e-03, 7.1163e-02, 1.6819e-02, ..., 9.2681e-02, 1.1505e-01, 2.3904e-02], [ 4.7862e-02, 1.1637e-01, 9.4310e-02, ..., 1.1634e-01, 1.8279e-01, 5.3646e-02], [-1.1675e-01, -7.2314e-02, 3.4619e-02, ..., -6.8577e-03, 2.9380e-02, -2.8520e-02], ..., [ 2.4590e-02, 4.2514e-02, 6.2918e-02, ..., -2.6829e-02, -1.1360e-02, -9.5783e-02], [-3.6852e-02, -6.3597e-02, -6.8385e-02, ..., -7.8031e-02, -2.6182e-02, -3.7326e-02], [-2.0531e-02, -4.0638e-02, -1.0545e-01, ..., -3.3283e-02, -1.7214e-02, -8.7905e-03]], [[ 1.4262e-01, 1.8939e-01, 4.6325e-02, ..., 5.6332e-03, -1.7036e-02, 8.5665e-02], [ 8.7077e-02, 1.5266e-01, 1.1495e-02, ..., 2.6765e-02, -7.1896e-02, 1.6017e-02], [ 2.0477e-02, 3.8997e-02, -5.1062e-02, ..., -3.8573e-02, -1.3501e-01, -4.8769e-02], ..., [ 6.8783e-02, 8.6703e-02, 3.7755e-02, ..., -1.4440e-02, -4.4981e-02, 4.6754e-02], [ 1.5211e-01, 1.4666e-01, 1.0651e-01, ..., 6.6362e-03, -2.3379e-02, 2.2118e-02], [ 1.5475e-01, 1.1244e-01, 5.5967e-02, ..., 2.7780e-02, 5.5084e-02, 3.2660e-02]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[ 0.4845, 0.0795, -0.2955, ..., -0.7637, -0.7079, -0.2085], [ 0.0782, -0.6192, -0.6169, ..., -0.5698, -0.6532, -0.1965], [-0.1840, -0.7758, -0.5835, ..., 0.4777, -0.3596, -0.5760], ..., [-0.4258, -0.6596, -0.4395, ..., -1.2278, -1.2216, -0.2326], [-0.7650, -0.4104, -0.0632, ..., -1.1424, -1.1432, -0.1594], [-0.4026, 0.0630, 0.2351, ..., -0.2858, -0.3669, 0.2083]], [[ 0.6445, 0.0482, 0.4370, ..., -0.4058, 0.3200, 0.3312], [-0.1693, -0.6156, -0.5838, ..., -0.3975, 0.0058, 0.0147], [ 0.0864, -0.4110, -0.5953, ..., -0.2295, -0.2403, -0.0342], ..., [ 0.1388, -0.2530, 0.2279, ..., -0.3627, -0.2018, 0.1189], [ 0.4692, -0.2895, 0.3101, ..., -0.4736, -0.0019, 0.2250], [ 0.3214, -0.2667, 0.1720, ..., 0.0590, 0.4691, 0.3283]], [[-0.1366, -0.6080, -0.5196, ..., 0.2603, -0.2752, -0.2446], [-0.2837, -1.1259, -0.9564, ..., 0.0706, -0.4083, -0.1435], [-0.0544, -0.5661, -0.5452, ..., -0.0945, -0.1330, -0.0922], ..., [-0.0720, -0.4958, -0.2366, ..., 0.2443, -0.0310, 0.0820], [-0.0304, -0.4755, -0.2775, ..., 0.1328, -0.2953, -0.1548], [ 0.0987, -0.4032, -0.2415, ..., -0.2818, -0.3963, -0.1066]], ..., [[ 0.3596, 0.2900, 0.1245, ..., 0.1821, 0.4045, 0.2309], [ 0.0965, -0.0323, -0.2414, ..., 0.0500, 0.1201, 0.0346], [-0.0176, 0.1297, -0.1019, ..., -0.3010, -0.2776, -0.0885], ..., [-0.0956, -0.2160, 0.2131, ..., 0.3172, 0.2728, 0.4095], [-0.0683, 0.0664, 0.2848, ..., 0.4504, 0.4178, 0.0137], [ 0.1541, 0.3728, 0.4992, ..., 0.1085, 0.6247, 0.3332]], [[-0.1562, -0.6261, -0.3393, ..., -0.0509, 0.1059, -0.1578], [-0.3468, -0.7332, -0.9827, ..., 0.0946, -0.1183, -0.4941], [-0.3986, -0.8786, -1.0388, ..., -0.1285, 0.0028, -0.2330], ..., [-0.5062, -0.3236, 0.2055, ..., -0.7337, -0.4309, 0.0667], [-0.6180, -0.3479, -0.0436, ..., -0.7868, -0.4855, 0.0638], [-0.3072, -0.2761, -0.2037, ..., 0.3174, 0.1394, 0.1267]], [[-0.2980, -0.6261, -0.3754, ..., -0.8803, -0.5116, -0.2325], [-0.2628, -0.9277, -0.9636, ..., -0.4362, -0.4065, -0.3207], [-0.2065, -0.8519, -1.0033, ..., -0.6636, -0.4973, -0.1509], ..., [-0.4886, -1.0695, -0.6502, ..., -0.6194, -0.6536, 0.0325], [-0.5144, -0.6333, -0.1508, ..., -0.9391, -1.1019, -0.7397], [-0.0694, -0.0629, 0.0898, ..., -0.4796, -0.1971, -0.3615]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-1.6082e-01, -8.7399e-02, -1.3319e-03, ..., -1.1923e-01, -1.4567e-01, -1.0296e-01], [-3.6143e-01, -5.2402e-01, -3.1734e-01, ..., -3.7360e-02, -1.4727e-01, -2.1146e-01], [-1.2748e-01, -2.0289e-01, -1.7263e-01, ..., 3.2821e-02, 2.3899e-02, -1.7116e-02], ..., [-5.4805e-02, -8.7460e-02, -7.2353e-02, ..., -8.5676e-02, -4.8657e-02, 4.4028e-02], [-7.9688e-02, -1.0489e-01, -9.2847e-02, ..., -2.2595e-02, -1.0795e-01, -7.0397e-02], [-9.7414e-02, -1.1942e-01, -9.3385e-02, ..., -6.3040e-02, -1.0286e-02, -2.8162e-02]], [[-8.3590e-02, -1.1097e-01, -7.6247e-02, ..., -8.4737e-03, 5.6614e-05, -2.6328e-02], [-3.3130e-02, -2.6316e-02, -6.1830e-02, ..., -3.6645e-02, 1.5747e-02, 1.4232e-02], [-4.1686e-02, -7.8000e-02, -7.1547e-02, ..., -3.7017e-02, -2.4838e-03, -2.6782e-02], ..., [-1.2196e-02, -2.3432e-02, -2.0606e-03, ..., -8.6377e-02, -1.2465e-01, -9.6318e-02], [ 7.4525e-03, -1.4041e-02, -4.3532e-02, ..., -6.8818e-02, -6.5830e-02, -7.8353e-02], [ 1.1506e-02, 9.2946e-03, -2.4731e-02, ..., -3.2439e-02, 2.1590e-03, 3.9627e-02]], [[-3.9879e-01, -3.8665e-01, -1.3869e-01, ..., -1.1499e-01, -1.2475e-01, -1.3711e-01], [-6.7526e-01, -7.6506e-01, -2.8433e-01, ..., -1.8596e-01, -2.0662e-01, -2.4935e-01], [-2.1169e-01, -1.6909e-01, 5.4648e-02, ..., -1.5494e-02, 1.9123e-02, -2.2116e-02], ..., [-3.6328e-02, -1.6013e-03, -3.8489e-02, ..., -1.0460e-01, -1.5847e-01, -1.7731e-01], [-6.4490e-02, -7.4539e-02, -1.0174e-01, ..., -1.0484e-01, -1.5456e-01, -1.5496e-01], [-7.7448e-02, -1.5199e-01, -2.2827e-01, ..., -9.5927e-02, -1.1893e-01, -9.5688e-02]], ..., [[-6.4548e-02, -8.8192e-02, -4.6773e-02, ..., -6.9743e-03, 2.2594e-03, -3.4030e-02], [ 1.3396e-01, 1.1073e-01, 3.6814e-02, ..., 4.9052e-03, 4.7609e-02, 2.7674e-02], [ 1.5980e-02, 5.0351e-03, 9.3483e-03, ..., 1.8490e-02, 4.5325e-02, -4.3205e-04], ..., [-5.8525e-03, 4.7031e-02, 4.5665e-02, ..., 4.1247e-02, 4.7514e-02, -2.1914e-02], [ 2.8747e-04, 4.0925e-02, 7.1120e-02, ..., 1.4343e-02, 3.0187e-02, -1.0466e-03], [-1.6504e-02, -8.5669e-03, 3.8790e-02, ..., -3.4095e-02, -1.7662e-02, -1.4444e-02]], [[-3.3402e-01, -2.9618e-01, -9.9874e-02, ..., -8.1035e-02, -1.4801e-01, -1.0487e-01], [-3.1593e-01, -2.6937e-01, -7.6912e-02, ..., -5.0427e-02, -9.1080e-02, -7.3436e-02], [-1.6077e-01, -6.5540e-02, 4.2919e-02, ..., -4.2545e-02, -3.7607e-02, -1.2003e-02], ..., [-3.7154e-03, 3.9980e-02, 1.2839e-02, ..., -1.6639e-02, -3.1090e-02, -6.4658e-02], [-2.3084e-02, 2.3174e-02, -1.3319e-02, ..., -1.1787e-02, 9.9478e-03, 2.9969e-02], [-1.9074e-02, -1.7670e-02, -7.8655e-02, ..., -2.6532e-03, -2.2898e-02, 1.5542e-02]], [[ 1.7509e-01, -2.9027e-05, -3.3672e-02, ..., 3.0066e-02, 2.1173e-02, -7.9550e-03], [-1.0897e-01, -3.1960e-01, -1.1253e-01, ..., -2.4740e-02, -9.3286e-02, -1.2415e-01], [-5.8501e-03, -1.0101e-01, 1.9465e-02, ..., -3.2247e-02, -2.0805e-02, -4.5185e-03], ..., [ 7.1553e-03, 3.4583e-02, 6.0450e-02, ..., -1.8143e-02, -8.8677e-03, -5.4216e-02], [ 3.3659e-02, 6.5209e-02, 9.3663e-02, ..., 4.1290e-02, -5.5905e-03, -5.6606e-02], [ 2.9368e-02, 3.4554e-02, 1.7176e-02, ..., 2.8348e-02, -2.7019e-02, -6.9664e-02]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[ 0.5098, 0.1736, 0.3925, ..., 0.3039, 0.4451, 0.3184], [-0.0598, -0.2861, -0.2505, ..., 0.1970, 0.1205, -0.0464], [ 0.4337, -0.0479, -0.1914, ..., 0.3584, 0.1753, -0.0405], ..., [-0.2482, -0.4840, -0.1365, ..., -0.3502, -0.2638, -0.1277], [ 0.0931, -0.1315, -0.0144, ..., -0.0708, -0.1416, 0.2576], [ 0.5693, 0.0493, -0.1726, ..., -0.0663, -0.1311, 0.2075]], [[ 0.0800, 0.3527, 0.2951, ..., -0.2240, 0.0408, 0.1746], [ 0.1391, 0.1942, 0.1127, ..., 0.1342, 0.2865, 0.1706], [ 0.1639, -0.0116, -0.0705, ..., -0.1466, -0.0055, 0.0638], ..., [ 0.1223, -0.3129, -0.2357, ..., 0.6392, 0.5878, 0.5443], [-0.0431, -0.3688, -0.2213, ..., -0.0247, -0.2139, 0.1065], [ 0.3570, 0.1945, 0.1925, ..., -0.0640, 0.1018, 0.0997]], [[ 0.6032, 0.0028, -0.0716, ..., -0.1254, 0.0954, 0.3884], [-0.1856, -0.4956, -0.4756, ..., -0.5205, -0.6469, -0.2358], [-0.1939, -0.6479, -0.3986, ..., -0.5717, -0.6354, -0.2821], ..., [-0.2615, -0.3921, 0.1904, ..., -0.4963, -0.5437, -0.2692], [-0.1927, -0.5685, -0.1972, ..., -0.2620, -0.4574, 0.1973], [ 0.1334, -0.2671, -0.1990, ..., -0.0171, -0.2605, 0.1129]], ..., [[-0.5754, -0.4448, -0.3716, ..., -0.5752, -0.8019, -0.6547], [-0.5028, -0.7751, -0.7857, ..., -0.4929, -0.5696, -0.3475], [-0.2166, -0.4623, -0.5127, ..., -0.5050, -0.4840, -0.1737], ..., [-0.9109, -1.7282, -1.1727, ..., -0.5710, -0.4864, -0.2089], [-0.9949, -1.4755, -1.4009, ..., -0.8807, -1.0772, -0.4198], [-0.5473, -0.7397, -0.7279, ..., -0.9085, -1.1764, -0.5110]], [[ 0.4664, -0.5403, -0.2714, ..., 0.2430, 0.3114, -0.2147], [ 0.0738, -0.6117, -0.7808, ..., 0.1921, -0.3795, -0.4145], [-0.0733, -0.6342, -0.9034, ..., 0.4000, -0.7577, -0.3841], ..., [-0.4114, -1.1774, -0.6825, ..., -0.4739, -0.1066, -0.1945], [-0.1428, -0.7037, -0.5981, ..., -0.6284, -0.3815, -0.3180], [ 0.0949, -0.1319, -0.2956, ..., -0.1477, -0.3470, -0.5407]], [[-0.4962, -0.0778, -0.5292, ..., -0.0404, 0.0530, -0.1162], [-0.0377, -0.0302, -0.4661, ..., -0.5547, -0.2383, 0.1106], [-0.1918, 0.2446, 0.0900, ..., -0.6465, -0.5852, 0.0785], ..., [ 0.1349, -0.1471, -0.2929, ..., -0.5733, -0.7522, 0.1922], [-0.2976, -0.0762, -0.4357, ..., -0.1059, -0.0564, 0.0568], [-0.2566, -0.1907, -0.3015, ..., -0.0227, -0.2228, -0.0242]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-0.0510, -0.0698, 0.0040, ..., -0.0179, 0.0021, -0.0078], [-0.1222, -0.0798, -0.0631, ..., 0.0161, -0.0895, -0.1311], [-0.0843, -0.1224, -0.1109, ..., -0.0426, -0.2030, -0.1647], ..., [-0.0870, -0.0207, -0.0474, ..., -0.1326, -0.0437, -0.0309], [-0.1607, -0.1051, -0.0448, ..., 0.0469, 0.0465, -0.0133], [-0.1061, -0.1567, -0.0659, ..., -0.0701, -0.0325, 0.0012]], [[-0.1245, -0.1477, -0.0338, ..., -0.0827, -0.0504, -0.1058], [ 0.1969, 0.0248, 0.0013, ..., -0.0502, 0.0326, -0.0411], [ 0.0264, -0.0859, -0.0659, ..., -0.1118, -0.0909, -0.1030], ..., [-0.1272, -0.0633, 0.0459, ..., -0.0523, -0.0801, -0.0876], [-0.1067, -0.0668, 0.0022, ..., -0.1192, -0.1293, -0.1347], [-0.0381, -0.0477, -0.0597, ..., -0.0926, -0.1077, -0.0823]], [[-0.0635, -0.1400, -0.1012, ..., -0.0068, 0.0314, 0.0102], [-0.0933, -0.1811, -0.1031, ..., -0.0319, 0.0461, 0.0047], [-0.0964, -0.1068, 0.0097, ..., -0.0365, -0.0181, -0.0691], ..., [-0.0430, -0.0466, -0.0025, ..., 0.0205, -0.0102, -0.0511], [-0.0682, -0.0683, -0.0129, ..., -0.0089, -0.0188, -0.0494], [-0.0579, -0.0713, -0.0010, ..., -0.0311, -0.0175, -0.0290]], ..., [[-0.0800, -0.0812, -0.0242, ..., -0.0869, -0.0772, -0.0011], [-0.0381, -0.0167, -0.0238, ..., -0.0680, -0.0347, -0.0412], [-0.0834, -0.0793, -0.1422, ..., -0.0534, -0.0114, -0.0406], ..., [-0.0159, 0.0017, -0.0306, ..., -0.0497, -0.0632, -0.0877], [-0.0268, 0.0149, -0.0126, ..., -0.0441, 0.0213, 0.0090], [ 0.0006, -0.0310, -0.0287, ..., -0.0085, -0.0128, -0.0178]], [[-0.0556, -0.0731, -0.0343, ..., -0.0642, -0.0661, -0.0520], [ 0.0388, 0.0129, 0.0487, ..., 0.0453, 0.0838, 0.0257], [-0.0910, -0.0915, -0.0123, ..., 0.0324, -0.0015, -0.0794], ..., [ 0.0483, -0.0290, -0.0291, ..., -0.0449, -0.0434, -0.0987], [ 0.0601, -0.0490, -0.0457, ..., 0.0019, 0.0606, 0.0470], [ 0.0335, -0.0144, -0.0024, ..., 0.0107, 0.0047, 0.0004]], [[-0.0654, -0.1429, -0.0705, ..., -0.0970, -0.0802, -0.1169], [ 0.0387, -0.0708, -0.0757, ..., -0.0317, -0.0100, -0.0739], [-0.0517, -0.1478, -0.1113, ..., -0.0170, -0.0384, -0.0802], ..., [-0.0656, -0.1084, -0.0880, ..., -0.0044, -0.0288, -0.0500], [-0.0497, -0.0753, -0.0420, ..., -0.0368, -0.0288, -0.0468], [-0.0154, -0.0363, -0.0092, ..., -0.0728, -0.0481, -0.0592]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[ 0.9101, 1.0766, 0.8197, ..., 0.3239, 0.6147, 0.6619], [ 0.3469, -0.0430, 0.0586, ..., 0.0741, -0.0061, 0.2298], [ 0.4233, 0.2378, -0.1834, ..., -0.3338, -0.4337, -0.0405], ..., [-0.2806, -0.7089, -0.6877, ..., -0.2932, -0.1125, 0.2156], [-0.1780, -0.5839, -0.3999, ..., -0.3161, -0.3349, -0.1509], [ 0.0418, -0.2564, -0.0946, ..., -0.1442, -0.0363, -0.1216]], [[ 0.8149, 0.4860, 0.3185, ..., 0.1834, 0.6390, 0.6432], [ 0.7706, 0.3752, 0.3433, ..., 0.0663, 0.5357, 0.4820], [ 0.1882, -0.2527, -0.2498, ..., -0.0375, -0.0892, -0.1041], ..., [ 0.2684, -0.4433, -0.2683, ..., -0.0835, -0.0633, -0.0604], [ 0.2136, -0.3107, -0.3213, ..., -0.1884, -0.2071, -0.0392], [ 0.2493, -0.0326, -0.0568, ..., -0.1346, -0.4392, -0.2407]], [[-0.7696, -0.7801, -0.3502, ..., -0.4878, -0.4838, -0.3692], [-0.8561, -0.7158, -0.3122, ..., -0.6538, -0.7723, -0.5182], [-0.4039, -0.3073, -0.2708, ..., -0.7497, -0.6898, -0.4187], ..., [-0.1388, -0.7416, -0.4934, ..., -0.4124, -0.5719, -0.9950], [ 0.0069, -0.7505, -0.6551, ..., -0.8837, -0.7356, -0.9177], [-0.1235, -0.8004, -0.8288, ..., -0.6875, -0.8000, -0.8135]], ..., [[ 0.5193, 0.0885, -0.1648, ..., -0.2513, -0.0280, 0.2250], [-0.1095, -0.4824, -0.6343, ..., -0.4968, -0.4093, -0.1937], [-0.1244, -0.5217, -0.5575, ..., -0.3860, -0.2766, -0.0532], ..., [-0.0555, -0.4405, -0.6392, ..., -0.7580, -0.7131, -0.6700], [ 0.0155, -0.8437, -0.9014, ..., -1.1372, -0.8675, -0.2615], [ 0.1376, -0.5330, -0.7336, ..., -0.1720, -0.0350, 0.0740]], [[-0.4117, -0.6053, -0.9020, ..., -0.6382, -0.6521, -0.1293], [-0.9844, -1.0386, -0.9518, ..., -1.1631, -1.2422, -0.5029], [-0.4233, -0.5345, -0.6717, ..., -0.6881, -1.0350, -0.4479], ..., [-0.0976, -0.1517, -0.1754, ..., 0.2021, -0.5755, -0.2413], [ 0.0702, -0.4755, -0.4894, ..., 0.3783, -0.3654, 0.0330], [-0.0632, -0.3983, -0.3590, ..., -0.3914, -0.5590, 0.2499]], [[-0.3513, -0.2525, -0.2272, ..., -0.4691, -0.4100, -0.0667], [-0.0315, -0.2985, -0.1417, ..., -0.8414, -0.7026, -0.2416], [ 0.0771, -0.1426, 0.0438, ..., -0.4894, -0.7221, -0.3022], ..., [-0.5155, -0.8266, -0.2681, ..., -0.1801, -0.8270, -0.3935], [-0.4917, -0.6305, 0.0745, ..., -0.6882, -1.1467, -0.6796], [-0.0945, -0.0862, 0.2665, ..., -0.4855, -0.6020, -0.2635]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[ 6.4430e-02, -1.6318e-01, -2.3215e-01, ..., -1.9166e-01, -1.6584e-01, -1.0940e-01], [-1.5872e-01, -3.8746e-01, -3.0614e-01, ..., -3.7735e-01, -3.5435e-01, -2.8609e-01], [-2.0114e-02, -1.1159e-01, -3.7295e-02, ..., -6.2077e-02, -1.4846e-01, -1.0356e-01], ..., [-1.1273e-01, -1.3277e-01, -9.7977e-02, ..., -7.3987e-02, -5.9154e-02, -3.6332e-02], [-1.3821e-01, -2.3602e-01, -1.7948e-01, ..., -5.4057e-02, -8.1132e-02, -8.4313e-02], [-8.6350e-02, -1.5658e-01, -9.1837e-02, ..., -1.3030e-01, -1.0429e-01, -6.4923e-02]], [[ 8.9676e-02, -6.1121e-02, -1.4292e-01, ..., -1.2495e-01, 4.0534e-02, 6.2829e-02], [-4.9366e-02, -2.0399e-01, -2.4697e-01, ..., -2.5667e-01, -1.6171e-01, -1.1283e-01], [ 2.2497e-02, -3.9560e-02, -8.7142e-02, ..., -1.9836e-01, -1.4036e-01, -1.1564e-01], ..., [-1.5411e-01, -1.2683e-01, -2.1520e-02, ..., -1.2022e-01, -1.8644e-01, -1.1395e-01], [-1.1874e-01, -1.5701e-01, -6.2235e-02, ..., -7.0218e-02, -1.0617e-01, -8.0756e-02], [-7.2681e-02, -1.4645e-01, -9.7536e-02, ..., -6.5988e-02, 2.5555e-03, -6.0063e-03]], [[ 1.8542e-01, 1.7175e-01, 1.1776e-01, ..., 4.2249e-02, 1.5117e-01, 1.2253e-01], [ 7.3441e-01, 8.1271e-01, 5.0550e-01, ..., 2.7646e-01, 5.0381e-01, 4.1670e-01], [ 1.3243e-01, 7.8072e-02, 4.7040e-02, ..., 8.7921e-02, 3.5772e-02, -1.3040e-03], ..., [-3.9274e-02, 1.2257e-02, 1.1344e-02, ..., 4.4609e-02, 2.8281e-02, 6.8394e-03], [-5.1679e-02, -1.6965e-02, -4.5877e-02, ..., -5.1035e-02, -5.9237e-04, 3.0533e-02], [ 6.0087e-02, 3.7373e-02, -1.3914e-02, ..., 7.0179e-03, 8.6234e-02, 1.1765e-01]], ..., [[ 6.0558e-03, 1.5564e-01, 5.5820e-02, ..., -4.4163e-02, -3.2737e-03, 8.2676e-02], [-2.2683e-01, -1.2114e-01, -9.8398e-02, ..., -1.3539e-01, -1.0991e-01, -1.1122e-01], [-1.3119e-01, -4.0930e-03, -4.1464e-02, ..., -1.4224e-02, -8.3651e-02, -1.7913e-01], ..., [-4.0930e-02, 1.0420e-02, -3.7359e-02, ..., -5.6199e-02, -6.6513e-02, -8.7126e-02], [-9.0039e-02, 3.0807e-02, -1.4512e-04, ..., -1.1594e-01, -3.8810e-02, -4.3311e-02], [-8.2347e-02, -1.1363e-02, -3.4658e-03, ..., -5.7998e-02, -4.6701e-03, -7.7030e-03]], [[ 5.5289e-02, -1.5680e-03, -1.1800e-01, ..., -2.2549e-01, -4.5773e-02, -2.2674e-02], [-2.0767e-01, -1.3274e-01, -1.2607e-01, ..., -1.9728e-01, -1.5975e-01, -1.3349e-01], [-1.4214e-01, -3.7481e-02, -3.9965e-02, ..., -4.2012e-02, -6.0136e-02, -1.1219e-01], ..., [-1.4361e-02, -8.7165e-02, -4.7337e-02, ..., -1.0478e-01, -6.8615e-02, -9.3372e-02], [-8.2169e-02, -1.5698e-01, -1.4019e-01, ..., -1.2017e-01, -3.3156e-03, -1.3669e-02], [-9.5183e-02, -1.0866e-01, -8.0570e-02, ..., -5.5362e-02, -3.9940e-02, -2.6436e-02]], [[ 5.6359e-02, 1.1743e-01, 3.2198e-02, ..., -3.8483e-03, 8.7708e-02, 6.4603e-02], [-1.1675e-01, -6.1985e-02, -5.4091e-02, ..., -4.9418e-02, -1.4927e-02, -4.6136e-02], [-3.6973e-02, 4.3101e-02, 5.4578e-03, ..., 2.4467e-02, 2.7021e-02, -4.8823e-03], ..., [ 5.6195e-03, -2.5033e-03, -1.4855e-02, ..., 2.3181e-02, -2.8707e-03, -1.0791e-02], [-1.2482e-02, 5.2544e-04, -1.2498e-02, ..., 1.4234e-02, 4.9095e-02, 4.3880e-03], [-1.7698e-02, 2.2928e-03, -7.0659e-03, ..., -3.8163e-03, 3.9203e-02, -2.6743e-03]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[ 1.7339e-01, -9.4808e-02, -6.1271e-01, ..., 1.4380e-01, -2.0960e-01, -3.2730e-01], [-6.3560e-02, -2.1000e-01, -6.8458e-01, ..., -9.9222e-01, 2.9317e-01, -5.5872e-02], [-1.5248e-01, -4.8554e-01, -4.8676e-01, ..., 6.1018e-02, 6.5937e-01, -1.3268e-01], ..., [-4.8350e-01, -8.6408e-01, -6.1364e-01, ..., -9.9484e-01, -7.1179e-01, -1.4440e-01], [-1.7778e-01, -1.4797e-01, -4.1957e-01, ..., -8.4083e-01, -6.0669e-01, -5.0201e-02], [-5.5214e-01, -4.5085e-01, -2.3836e-01, ..., -5.5610e-01, -8.8097e-01, -4.0863e-01]], [[-4.3246e-03, 7.2340e-02, -4.0673e-01, ..., 4.4244e-02, -1.3516e-01, -2.7559e-02], [-6.7222e-02, -1.0078e-01, -1.2998e-01, ..., -2.3212e-01, -4.1464e-01, -4.0327e-01], [-1.5512e-01, 7.9317e-02, 1.5356e-01, ..., -5.7165e-01, -4.5463e-01, -5.9033e-01], ..., [-1.8650e-01, -1.3346e-01, -2.2597e-02, ..., -9.4229e-02, -3.2536e-01, -3.4481e-01], [-6.8029e-01, -1.1684e-03, 9.0839e-02, ..., 2.6727e-01, 3.1889e-01, -2.6609e-02], [-7.7352e-01, -3.5479e-01, -5.5215e-01, ..., -4.0914e-01, 1.0151e-01, -4.4220e-01]], [[ 1.3809e+00, 7.1808e-02, -4.7408e-02, ..., -3.0978e-01, 2.6252e-01, 5.1537e-01], [ 2.0916e-01, 5.9198e-02, 3.5689e-01, ..., 2.4617e-01, -4.3292e-01, -3.2945e-01], [-9.1277e-03, 1.2988e-01, -3.6940e-01, ..., -1.1850e-01, 1.5383e-01, -1.4952e-02], ..., [-2.0953e-01, -1.1567e-01, -1.5326e-01, ..., -4.2803e-01, -4.6014e-01, -1.9415e-01], [-1.4530e-01, -1.3514e-01, -2.9253e-01, ..., -1.2311e-01, -1.1232e-01, -2.9425e-01], [ 1.9769e-01, 2.3179e-01, -2.5285e-01, ..., -3.0741e-01, -2.1593e-01, 2.9725e-02]], ..., [[ 2.5535e-01, 7.2656e-03, -1.2095e-01, ..., 3.7620e-02, 1.8716e-03, 2.3830e-01], [ 1.2931e-01, -1.1446e-01, -6.7117e-01, ..., -3.7731e-01, -1.8579e-01, 2.5818e-01], [ 1.0917e-02, -2.3147e-01, -6.8273e-01, ..., -1.5688e+00, -3.1911e-01, 1.9214e-01], ..., [ 4.9276e-01, 2.3696e-01, -1.7069e-01, ..., -2.3560e-02, -4.2982e-01, 4.3347e-02], [-1.5764e-01, 1.2234e-01, -2.6147e-01, ..., 4.9637e-02, -3.0250e-01, -9.2175e-02], [-4.0238e-01, -2.6120e-01, -4.6877e-01, ..., -8.3150e-01, -2.5967e-01, -6.1447e-01]], [[ 5.0277e-01, -2.5111e-01, -2.9781e-02, ..., 1.2346e-01, 1.2271e-01, -5.3489e-02], [ 5.4571e-01, -2.0467e-01, -5.6036e-01, ..., -6.4822e-01, -2.9836e-01, -3.3093e-01], [ 1.5907e-01, -3.0992e-01, -7.3655e-01, ..., 1.6752e-01, -2.4167e-01, -5.0056e-02], ..., [ 6.1304e-01, 1.5488e-01, -1.9809e-01, ..., 1.3724e-01, -1.7476e-01, -2.0033e-01], [ 1.4995e-01, 3.0548e-01, -1.9005e-01, ..., 1.7044e-01, -6.0262e-01, 1.1476e-02], [-1.3003e-01, -7.5153e-01, -2.9897e-01, ..., -4.0668e-01, -7.2825e-01, 6.4905e-02]], [[ 1.1347e-01, 4.4911e-02, -7.4415e-02, ..., -3.1216e-01, -2.3538e-01, 1.1548e-01], [-2.2218e-02, -2.2825e-01, -6.4981e-02, ..., 5.8519e-02, 1.0134e-01, -1.1251e-01], [-1.0316e-02, 4.1192e-01, -4.0713e-01, ..., -2.2387e-02, -8.5106e-02, -2.5153e-01], ..., [-3.3986e-01, -4.1138e-01, 3.5781e-01, ..., 2.1808e-01, -2.5594e-01, 1.1899e-01], [ 5.5571e-02, -1.4490e-01, -2.9625e-01, ..., 8.0132e-02, 1.0265e-01, 2.1682e-01], [ 6.5714e-02, -1.2890e-01, 1.6180e-01, ..., 1.4383e-01, 3.4399e-01, 6.4371e-01]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[ 0.0960, 0.0307, -0.0699, ..., 0.0172, 0.0034, -0.0184], [-0.1651, -0.2210, -0.1714, ..., -0.2050, -0.0549, -0.1189], [ 0.0070, 0.0068, 0.1551, ..., 0.0701, 0.0633, 0.0086], ..., [-0.0674, -0.0241, 0.0063, ..., 0.0346, 0.0357, 0.0131], [-0.1675, -0.0529, 0.1768, ..., 0.1033, 0.0940, 0.0378], [-0.1387, -0.1719, -0.0859, ..., 0.0721, 0.1203, 0.1045]], [[-0.0699, 0.0097, 0.0019, ..., -0.1541, -0.2316, -0.1260], [-0.0322, -0.0661, 0.1079, ..., -0.0795, -0.3717, -0.2026], [ 0.0063, 0.0893, 0.2031, ..., 0.1351, -0.1136, -0.0444], ..., [ 0.1117, 0.0801, 0.0366, ..., -0.0718, 0.0056, -0.0384], [ 0.0280, 0.0044, -0.0808, ..., -0.1459, -0.0720, -0.0570], [ 0.0297, 0.0227, 0.0394, ..., 0.1448, 0.1450, -0.0226]], [[-0.0899, -0.0195, -0.0080, ..., -0.0544, -0.1548, -0.0921], [-0.0682, 0.0799, 0.0049, ..., -0.1934, -0.2153, -0.0923], [-0.1031, -0.0454, -0.0174, ..., -0.0893, -0.1499, -0.0551], ..., [-0.0286, -0.0889, -0.1280, ..., -0.1297, -0.0631, -0.0738], [-0.1076, -0.1124, -0.1168, ..., -0.2121, -0.0860, -0.1526], [-0.1690, -0.0428, -0.0780, ..., -0.1965, -0.1253, -0.1400]], ..., [[-0.1522, -0.1636, -0.1202, ..., -0.0121, -0.0746, -0.1115], [-0.0114, -0.1939, -0.1007, ..., -0.0196, 0.0041, -0.0400], [-0.0623, -0.1104, -0.0841, ..., -0.0666, 0.0758, 0.0979], ..., [-0.0540, -0.1088, -0.0957, ..., -0.1641, -0.0596, -0.0542], [-0.1630, -0.1650, -0.1441, ..., -0.1307, -0.1170, -0.0429], [-0.0806, -0.1046, -0.0604, ..., -0.0512, 0.1480, 0.0605]], [[-0.0534, -0.0888, -0.1538, ..., -0.1150, -0.0616, -0.1154], [-0.0053, -0.2251, -0.1865, ..., -0.2248, -0.1754, -0.0764], [ 0.0032, -0.1462, -0.0746, ..., 0.0086, -0.0652, 0.0277], ..., [-0.0308, 0.0306, 0.0580, ..., -0.0542, -0.0445, -0.0155], [-0.0855, -0.0960, -0.0220, ..., -0.1566, -0.1331, 0.0572], [-0.1060, -0.0912, 0.0406, ..., -0.0249, -0.0581, 0.0064]], [[ 0.1796, 0.2143, 0.0647, ..., -0.0623, -0.0173, 0.0643], [-0.0477, -0.0548, -0.0255, ..., -0.2377, -0.2283, -0.0758], [ 0.0988, 0.1510, 0.1109, ..., 0.0127, -0.0725, -0.0160], ..., [-0.0108, -0.0191, -0.0287, ..., -0.0878, -0.1053, -0.0423], [-0.0221, 0.0222, 0.0112, ..., -0.0241, -0.0022, 0.0472], [ 0.0500, 0.1050, 0.0466, ..., 0.0299, 0.0499, 0.1599]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-0.4231, -0.0231, -0.0056, ..., -0.0265, -0.1730, -0.1304], [-0.1795, 0.0069, -0.0698, ..., -0.0588, 0.1447, 0.0358], [-0.0325, 0.0794, 0.0769, ..., 0.1704, 0.1836, -0.0286], ..., [-0.0966, -0.0066, 0.0286, ..., -0.0133, 0.0371, -0.1012], [ 0.0106, 0.0447, 0.0192, ..., 0.0716, 0.0035, -0.0776], [-0.0816, 0.0074, 0.0128, ..., -0.0761, -0.0472, 0.1071]], [[-0.1681, -0.0969, 0.0657, ..., -0.0468, -0.1331, -0.3370], [-0.1509, -0.0407, 0.2268, ..., 0.1119, -0.0907, -0.1011], [-0.0282, 0.0365, 0.0505, ..., 0.1827, 0.0482, -0.1284], ..., [-0.0058, 0.0085, -0.0955, ..., -0.1342, -0.1231, 0.0110], [ 0.1221, -0.1235, -0.1002, ..., -0.0979, -0.1216, -0.1265], [-0.0721, -0.1120, 0.0545, ..., 0.3044, -0.0122, -0.0502]], [[ 0.0532, -0.1504, -0.0791, ..., -0.0050, 0.0386, -0.0034], [-0.0949, 0.0850, -0.1386, ..., -0.1174, -0.0495, -0.0722], [-0.0897, -0.0400, -0.1848, ..., -0.0402, -0.0539, 0.0251], ..., [-0.0268, -0.1440, -0.1328, ..., -0.1762, -0.1322, -0.0544], [-0.2909, -0.1445, -0.0061, ..., -0.2415, -0.0795, -0.0177], [-0.3183, -0.1925, -0.2452, ..., -0.4863, -0.0652, -0.0086]], ..., [[-0.2409, -0.1967, -0.0914, ..., -0.1281, -0.2115, -0.2216], [-0.1904, -0.0376, -0.1786, ..., 0.0372, 0.1565, 0.0133], [-0.1434, -0.0010, -0.0514, ..., -0.3211, 0.0617, -0.1025], ..., [-0.1048, 0.0027, -0.0717, ..., -0.2067, -0.0891, -0.1054], [-0.2565, -0.0988, -0.1175, ..., -0.1354, -0.0973, -0.1034], [-0.2623, -0.0985, -0.0262, ..., -0.1220, -0.0202, 0.0135]], [[-0.2197, -0.0848, 0.0301, ..., -0.2480, -0.1971, 0.0863], [ 0.0555, 0.0098, -0.1250, ..., 0.0984, -0.0698, 0.0870], [-0.0286, -0.0368, 0.1240, ..., -0.1178, -0.1522, -0.0569], ..., [-0.0732, -0.0158, -0.0738, ..., 0.0430, -0.0608, -0.0357], [-0.2239, -0.0886, -0.0522, ..., 0.0225, -0.2635, -0.1263], [ 0.0212, -0.1778, -0.0208, ..., -0.1523, -0.1107, -0.1440]], [[ 0.0253, 0.0283, 0.0140, ..., 0.0084, -0.0078, 0.1810], [ 0.0963, 0.0438, -0.1671, ..., -0.1635, -0.2097, -0.0558], [ 0.0865, 0.0705, -0.1457, ..., -0.1799, -0.1054, -0.0024], ..., [ 0.1376, 0.0282, 0.0467, ..., 0.0566, 0.0660, -0.0354], [-0.1233, 0.0141, 0.0358, ..., -0.0034, -0.0085, -0.0285], [ 0.0108, 0.0045, 0.1608, ..., -0.0417, -0.0297, 0.0972]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-0.4958, -1.6956, -0.6618, ..., -0.9556, -1.6244, -0.7040], [-1.6635, -2.5068, -1.4933, ..., -1.6722, -2.4164, -1.5556], [-0.7953, -0.8299, -0.3875, ..., -0.9296, -1.3441, -0.9987], ..., [-1.8495, -2.9274, -1.7244, ..., -0.8433, -0.9403, -0.4768], [-1.1743, -2.8202, -2.3558, ..., -2.1676, -2.2784, -1.2847], [-0.6261, -1.4864, -0.9453, ..., -1.0875, -1.3397, -0.8367]], [[ 0.2355, -0.8715, -1.2119, ..., -1.2575, -0.6488, 0.0329], [-0.3182, -1.5435, -1.9886, ..., -2.0758, -1.5624, -0.3341], [-0.3772, -1.9734, -2.4813, ..., -2.1128, -1.6591, -0.9725], ..., [-0.7824, -1.4839, -1.3397, ..., -1.5276, -2.2641, -1.6701], [-1.3119, -2.0934, -1.6868, ..., -1.9044, -2.1447, -1.2452], [-0.6846, -1.6288, -1.7022, ..., -0.8273, -0.4320, 0.1732]], [[ 0.0166, -1.7161, -2.3600, ..., -3.1715, -2.6389, -1.7020], [-1.2275, -2.8275, -3.6183, ..., -4.1244, -3.2650, -1.9257], [-0.1512, -0.1212, -0.3152, ..., -1.9853, -2.1037, -1.3129], ..., [-0.4808, -0.8547, -0.7786, ..., -0.8631, -1.9802, -1.4388], [-0.3164, -1.0302, -0.7526, ..., -1.7501, -2.7765, -1.1548], [-0.8256, -1.4198, -1.1582, ..., -1.0846, -0.4300, 0.4089]], ..., [[-1.2897, -1.8389, -0.8400, ..., -0.6759, -0.5749, -0.1012], [-1.9064, -3.2975, -2.6617, ..., -2.9298, -2.4174, -1.3648], [-1.6253, -2.6109, -2.5476, ..., -3.3257, -3.4099, -2.1412], ..., [-1.2108, -2.3047, -1.9960, ..., -1.4551, -1.7386, -0.9149], [-1.3869, -2.4565, -2.1475, ..., -1.0893, -1.9669, -1.2323], [-0.6772, -1.1768, -1.0358, ..., -0.3274, -1.3807, -0.8585]], [[-1.8343, -0.8251, -1.0899, ..., -1.2018, -1.3029, -0.1550], [-0.5182, 0.0390, 0.0699, ..., -1.9062, -1.7981, 0.4875], [-0.8099, -0.7638, -1.8919, ..., -1.0373, -2.1952, -0.7600], ..., [-1.0898, -1.5434, -1.1102, ..., -2.0837, -2.9177, -0.6845], [-0.8162, -1.4854, -1.4141, ..., -2.6396, -3.5088, -0.8067], [-1.2074, -1.2848, -0.3816, ..., -0.5490, -2.6263, -0.3691]], [[-1.4092, -0.9987, -0.0146, ..., -0.7766, -2.0979, -1.7557], [-2.2071, -2.9781, -1.7933, ..., -2.5191, -2.7386, -1.6995], [-1.3005, -1.7724, -1.4228, ..., -1.1738, -1.2907, -1.2182], ..., [-0.3932, -1.2425, -2.0961, ..., -2.0225, -1.9730, -1.6157], [-1.0402, -1.3846, -1.5287, ..., -1.3333, -2.1565, -2.2994], [-1.1169, -1.1005, -0.9754, ..., -0.4825, -1.5659, -1.9117]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-0.0620, -0.0237, -0.0037, ..., 0.0130, -0.0199, -0.0416], [-0.1411, -0.1366, -0.0297, ..., -0.0025, -0.0703, -0.0618], [-0.0425, 0.0070, 0.0373, ..., 0.0118, -0.0123, -0.0235], ..., [-0.0277, -0.0095, -0.0008, ..., 0.0151, 0.0450, 0.0469], [-0.0916, -0.0925, -0.0732, ..., -0.1455, -0.1627, -0.0588], [-0.0897, -0.0521, -0.0247, ..., -0.0691, -0.1293, -0.0736]], [[-0.0568, -0.0628, -0.0028, ..., 0.0254, 0.0397, 0.0392], [-0.0439, -0.0591, 0.0522, ..., 0.0254, 0.0312, 0.0318], [-0.0353, -0.0663, -0.0202, ..., 0.0170, 0.0434, 0.0318], ..., [ 0.0143, 0.0021, 0.0039, ..., 0.0114, -0.0168, -0.0155], [ 0.0041, -0.0421, -0.0107, ..., 0.0293, 0.0091, 0.0105], [ 0.0061, -0.0431, 0.0033, ..., 0.0891, 0.1117, 0.0562]], [[-0.0850, -0.0327, -0.0011, ..., 0.0552, 0.0250, -0.0236], [-0.0567, -0.0512, 0.0741, ..., 0.1015, 0.0453, -0.0065], [-0.0738, -0.0334, 0.0383, ..., 0.0974, 0.0570, -0.0097], ..., [-0.0352, -0.0065, -0.0038, ..., -0.0257, 0.0273, -0.0275], [ 0.0552, 0.0570, 0.0491, ..., -0.0533, -0.0732, -0.0627], [-0.0079, 0.0119, 0.0202, ..., -0.0413, -0.0490, -0.0928]], ..., [[-0.0974, -0.0357, -0.0637, ..., -0.0637, -0.0228, 0.0032], [-0.0352, 0.0125, -0.0438, ..., -0.0205, 0.0805, 0.1438], [-0.0076, 0.0452, 0.0310, ..., -0.0428, -0.0087, 0.0439], ..., [-0.0556, -0.0168, 0.0125, ..., -0.0336, -0.0604, -0.0738], [-0.1158, -0.0726, -0.0050, ..., 0.0109, -0.0004, -0.0633], [-0.1226, -0.0931, -0.0844, ..., -0.0718, -0.0282, -0.0443]], [[-0.0787, -0.0310, -0.0008, ..., 0.0244, 0.0272, 0.0122], [ 0.0031, 0.0096, 0.0468, ..., 0.1062, 0.1036, 0.0854], [ 0.0196, 0.0422, 0.1191, ..., 0.0817, 0.0749, 0.0545], ..., [ 0.0364, 0.0224, 0.0021, ..., -0.0197, 0.0239, -0.0109], [ 0.0244, 0.0559, 0.0411, ..., -0.0529, -0.0173, -0.0235], [-0.0286, 0.0330, 0.0321, ..., 0.0262, 0.0654, 0.0311]], [[-0.0086, 0.0656, 0.0510, ..., -0.0015, -0.0074, -0.0071], [ 0.0841, 0.0682, 0.0510, ..., -0.0253, -0.0443, -0.0139], [ 0.0517, 0.0391, 0.0168, ..., -0.0678, -0.0927, -0.0420], ..., [-0.0126, -0.0465, -0.0627, ..., -0.0418, -0.0515, -0.0328], [ 0.0267, 0.0123, 0.0104, ..., 0.0231, -0.0101, 0.0244], [ 0.0172, 0.0445, 0.0412, ..., 0.0129, -0.0327, -0.0167]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-3.4750, -3.4560, -2.8273, ..., -3.2508, -3.8729, -2.8269], [-4.2592, -4.8648, -2.5765, ..., -1.2634, -1.9938, -2.1092], [-2.7763, -3.2505, -1.8529, ..., -2.0219, -1.5580, -1.5422], ..., [-1.7902, -2.4408, -1.2079, ..., -2.0084, -2.4411, -1.9580], [-1.6889, -2.7830, -2.4005, ..., -1.9301, -2.0826, -1.7352], [-1.4279, -2.3741, -2.9092, ..., -3.9762, -3.4731, -2.4181]], [[-1.9909, -2.7051, -2.3577, ..., -3.0502, -3.1455, -1.9688], [-2.5792, -3.3235, -2.4701, ..., -4.1309, -4.7480, -3.0812], [-2.1466, -2.2925, -1.1225, ..., -2.5902, -3.6139, -2.3158], ..., [-2.0956, -3.0973, -2.2434, ..., -1.5169, -2.2583, -1.6122], [-2.3290, -3.7623, -3.1273, ..., -2.5104, -3.6841, -2.5078], [-1.8428, -2.6170, -1.8207, ..., -1.3055, -2.4725, -1.5867]], [[-3.4129, -2.4648, -1.0742, ..., -1.1919, -1.9342, -1.6739], [-2.6977, -2.0980, -1.4507, ..., -0.8640, -1.0576, -0.5588], [-2.2977, -2.6378, -2.5816, ..., -2.1669, -1.5316, -0.7172], ..., [-3.3527, -3.5814, -2.9848, ..., -2.0961, -2.6056, -1.8142], [-4.3135, -4.9184, -4.3180, ..., -3.8032, -4.8414, -3.6772], [-2.7174, -3.1033, -3.0057, ..., -3.0211, -4.7558, -4.2106]], ..., [[-4.4429, -3.3553, -1.4595, ..., -1.8802, -2.5783, -1.9087], [-4.6435, -4.3449, -3.1467, ..., -2.3504, -2.0158, -1.1910], [-3.3648, -3.1884, -2.2892, ..., -2.9678, -2.6551, -1.7214], ..., [-2.8477, -2.9236, -2.2211, ..., -4.0577, -4.8453, -3.4708], [-3.9063, -4.3842, -3.3218, ..., -4.5455, -5.6346, -4.1392], [-2.8678, -3.3069, -2.2644, ..., -1.6910, -2.7187, -2.1092]], [[-2.5947, -2.0769, -1.6998, ..., -2.1328, -3.2101, -2.5518], [-2.1753, -1.3601, -0.5612, ..., -1.5786, -2.3167, -1.4304], [-1.8868, -1.4609, -1.4939, ..., -2.2960, -1.9507, -0.9916], ..., [-4.1552, -4.0864, -2.9043, ..., -2.2917, -2.6341, -1.9900], [-5.4539, -5.7471, -4.4933, ..., -4.1449, -4.8205, -3.3648], [-3.5695, -4.2426, -4.0575, ..., -3.9390, -4.0871, -2.6593]], [[-1.7293, -1.3352, -0.6515, ..., -2.7331, -3.6395, -2.9957], [-2.3236, -2.4487, -2.4042, ..., -4.6181, -4.5513, -3.0388], [-1.9516, -1.9545, -1.9636, ..., -5.1817, -4.5127, -2.4386], ..., [-2.8701, -2.9135, -1.2747, ..., -0.9129, -1.7076, -1.5889], [-3.5865, -4.6488, -3.6247, ..., -4.0014, -4.4134, -3.0185], [-3.1188, -3.7854, -2.9042, ..., -3.5254, -4.1202, -2.7234]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-4.0248e-02, -2.1918e-02, 1.6857e-02, ..., -3.5696e-03, -2.6573e-02, -4.6227e-02], [-2.7004e-02, -1.1469e-02, 4.3785e-02, ..., 3.2996e-02, 1.1239e-02, -1.7459e-02], [ 5.9849e-03, -1.3250e-03, 1.6657e-02, ..., 5.3861e-02, 5.6166e-02, 1.7476e-02], ..., [ 8.3230e-03, 4.1532e-02, 5.7824e-02, ..., 6.9979e-02, 2.0746e-02, -6.8109e-03], [-2.6846e-02, 4.4148e-03, 1.4690e-02, ..., -3.6798e-03, -7.0932e-02, -5.3330e-02], [-2.1289e-02, -1.7336e-02, -4.2136e-02, ..., -5.3113e-02, -7.2978e-02, -2.9946e-02]], [[-3.7065e-02, 1.3638e-02, 5.1883e-02, ..., -1.5602e-02, -4.2106e-02, -3.2605e-02], [-5.9112e-02, 8.4406e-03, 7.6438e-02, ..., 1.9861e-02, 3.6813e-02, 4.7601e-02], [-1.3119e-02, 2.3961e-02, 2.7430e-02, ..., -8.6907e-02, -3.4433e-03, 4.5861e-02], ..., [-1.3509e-02, 2.4247e-03, 2.8476e-02, ..., 5.4252e-02, 1.1840e-01, 9.6187e-02], [-2.8346e-02, -1.9278e-02, 3.3225e-02, ..., 7.3646e-02, 9.0414e-02, 6.1256e-02], [-2.8322e-02, -5.2114e-02, -3.8483e-02, ..., -1.2129e-04, 3.6583e-02, 1.6103e-02]], [[-7.1677e-03, 1.0149e-01, 1.6720e-01, ..., 9.0421e-02, 3.4992e-02, -1.8386e-03], [ 4.6779e-02, 1.7365e-01, 2.3382e-01, ..., 1.2050e-01, 2.4064e-02, -6.9508e-03], [ 1.8438e-02, 9.5620e-02, 1.1244e-01, ..., 6.1100e-02, -2.5235e-05, -1.9582e-02], ..., [-4.6476e-02, -8.5548e-02, -8.1671e-02, ..., -1.2316e-02, 2.3720e-02, 2.5891e-03], [-7.9118e-03, -4.0907e-02, -9.0682e-02, ..., -3.6402e-02, 1.4012e-03, -1.1529e-02], [-4.0327e-02, -4.7497e-02, -5.6817e-02, ..., -1.1304e-02, 1.4103e-02, -1.6755e-02]], ..., [[-7.1824e-02, -8.3558e-02, -6.0969e-02, ..., -3.1534e-02, 5.8441e-02, 4.9902e-02], [-5.4394e-02, -9.7608e-02, -1.1188e-01, ..., -5.6412e-02, 1.3519e-01, 1.1633e-01], [-2.6096e-02, -2.2610e-02, -1.3835e-02, ..., 3.4550e-02, 2.0898e-01, 1.6247e-01], ..., [-2.9306e-03, 2.2745e-02, 4.1931e-02, ..., 5.7627e-02, 1.1362e-01, 6.5168e-02], [ 3.3599e-03, 1.8140e-02, 3.4065e-02, ..., 1.5860e-02, 4.4836e-02, 2.5481e-02], [-1.0852e-02, -1.6917e-02, -8.4206e-03, ..., -2.0341e-02, -7.6384e-03, -1.5975e-02]], [[-9.4794e-02, -9.0133e-02, -2.8065e-02, ..., -2.7000e-02, -6.5505e-02, -7.7891e-02], [-2.9906e-02, 2.4624e-03, 6.2037e-02, ..., 2.1808e-02, -3.5462e-02, -6.9783e-02], [-5.9355e-03, 1.2592e-02, 2.6404e-02, ..., 6.4637e-02, 6.1290e-02, -1.7846e-02], ..., [-5.3652e-02, -4.5677e-02, -4.0731e-02, ..., -1.8724e-02, -2.1224e-03, -2.1797e-02], [-6.4489e-02, -6.2902e-02, -2.7807e-02, ..., -4.3309e-03, -6.6434e-02, -7.2191e-02], [-5.5928e-02, -7.3513e-02, -6.4857e-02, ..., -4.5293e-02, -8.1454e-02, -7.0142e-02]], [[ 1.5675e-02, 9.0438e-02, 8.0371e-02, ..., 2.1300e-02, -2.8058e-02, -7.7982e-02], [ 5.9975e-02, 9.8272e-02, 6.5212e-02, ..., 5.1632e-02, -3.6335e-02, -9.9438e-02], [ 4.6650e-02, 3.0553e-02, -5.7419e-03, ..., 2.3345e-02, -5.8012e-02, -1.1471e-01], ..., [ 1.1716e-02, -2.0707e-02, -2.9558e-02, ..., 2.8098e-02, -3.2145e-03, -7.5852e-02], [-3.7559e-02, -3.7850e-02, -9.6232e-03, ..., 3.5433e-02, -1.7212e-02, -7.3634e-02], [-4.4100e-02, -2.9511e-02, -2.4186e-03, ..., -1.3369e-02, -6.6702e-02, -9.5514e-02]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>)] Transfer Learning Input Shape's importance in Transfer Learning 1. torchvision.models.vgg.py 2. changing-input-dimension-for-alexnet 3. is-it-possible-to-give-variable-sized-images-as-input-to-a-convolutional-neural VGG16 We use an extremely simple tranfer learning model from torchvision.models and use this as a vanilla example to understand our problem statement. One should understand one of the fundamental reason why we use transfer learning is because the model is already pre-trained on millions of images; this is often the case as we usually benchmark our performance on model that are trained on imagenet . However, problem arises as we try to transfer our learned weights and layers from the said model, to our own custom dataset. Why? We will unveil the mystery soon, but first keep in mind two important concepts: Convolutional Layers are independent of the input size/shape. Dense or Fully Connected Layers are dependent on the input size/shape. Conclusion: As long as the pretrained model has Dense/FC layers, then how do we reconcile the fact that our input shape may be different from the ones that were trained on? Below: I present the source code of vgg16 and we will use it later. import torch import torch.nn as nn # from .utils import load_state_dict_from_url from typing import Union , List , Dict , Any , cast model_urls = { 'vgg11' : 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth' , 'vgg13' : 'https://download.pytorch.org/models/vgg13-c768596a.pth' , 'vgg16' : 'https://download.pytorch.org/models/vgg16-397923af.pth' , 'vgg19' : 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth' , 'vgg11_bn' : 'https://download.pytorch.org/models/vgg11_bn-6002323d.pth' , 'vgg13_bn' : 'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth' , 'vgg16_bn' : 'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth' , 'vgg19_bn' : 'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth' , } cfgs : Dict [ str , List [ Union [ str , int ]]] = { 'A' : [ 64 , 'MaxPool' , 128 , 'MaxPool' , 256 , 256 , 'MaxPool' , 512 , 512 , 'MaxPool' , 512 , 512 , 'MaxPool' ], 'B' : [ 64 , 64 , 'MaxPool' , 128 , 128 , 'MaxPool' , 256 , 256 , 'MaxPool' , 512 , 512 , 'MaxPool' , 512 , 512 , 'MaxPool' ], 'D' : [ 64 , 64 , 'MaxPool' , 128 , 128 , 'MaxPool' , 256 , 256 , 256 , 'MaxPool' , 512 , 512 , 512 , 'MaxPool' , 512 , 512 , 512 , 'MaxPool' ], 'E' : [ 64 , 64 , 'MaxPool' , 128 , 128 , 'MaxPool' , 256 , 256 , 256 , 256 , 'MaxPool' , 512 , 512 , 512 , 512 , 'MaxPool' , 512 , 512 , 512 , 512 , 'MaxPool' ], } def make_layers ( cfg : List [ Union [ str , int ]], batch_norm : bool = False ) -> torch . nn . Sequential : layers : List [ torch . nn . Module ] = [] in_channels = 3 for feature_map_type in cfg : if feature_map_type == 'MaxPool' : layers += [ torch . nn . MaxPool2d ( kernel_size = 2 , stride = 2 )] else : feature_map_type = cast ( int , feature_map_type ) conv2d = torch . nn . Conv2d ( in_channels = in_channels , out_channels = feature_map_type , kernel_size = 3 , padding = 1 ) if batch_norm : layers += [ conv2d , torch . nn . BatchNorm2d ( feature_map_type ), torch . nn . ReLU ( inplace = True )] else : layers += [ conv2d , torch . nn . ReLU ( inplace = True )] in_channels = feature_map_type return torch . nn . Sequential ( * layers ) class VGG ( torch . nn . Module ): def __init__ ( self , features : torch . nn . Module , num_classes : int = 1000 , init_weights : bool = True ) -> None : super ( VGG , self ) . __init__ () # Feature Maps, where the layers here are mostly Conv2d, serving as a feature extractor. self . features = features # AdaptiveAvgPool2d ensures that whatever your input image size is, it will come out the same output before it cotorch.nnects to the dense layer. self . avgpool = torch . nn . AdaptiveAvgPool2d (( 7 , 7 )) # Classifier Layers, usually the densely fully cotorch.nnected layers whereby your prediction is being made. self . classifier = torch . nn . Sequential ( torch . nn . Linear ( 512 * 7 * 7 , 4096 ), torch . nn . ReLU ( True ), torch . nn . Dropout (), torch . nn . Linear ( 4096 , 4096 ), torch . nn . ReLU ( True ), torch . nn . Dropout (), torch . nn . Linear ( 4096 , num_classes ), ) if init_weights : self . _initialize_weights () def forward ( self , input_neurons : torch . Tensor ) -> torch . Tensor : feature_map_output_neurons = self . features ( input_neurons ) adaptive_pool_output_neurons = self . avgpool ( feature_map_output_neurons ) print ( 'Before Flattening Shape {} ' . format ( adaptive_pool_output_neurons . size ())) # flatten vs view, I think around the same. flattened_neurons = adaptive_pool_output_neurons . view ( adaptive_pool_output_neurons . size ( 0 ), - 1 ) #flattened_neurons = torch.flatten(adaptive_pool_output_neurons, 1) print ( 'After Flattening Shape {} ' . format ( flattened_neurons . size ())) output_logits = self . classifier ( flattened_neurons ) return output_logits def _initialize_weights ( self ) -> None : for m in self . modules (): if isinstance ( m , torch . nn . Conv2d ): torch . nn . init . kaiming_normal_ ( m . weight , mode = 'fan_out' , nonlinearity = 'relu' ) if m . bias is not None : torch . nn . init . constant_ ( m . bias , 0 ) elif isinstance ( m , torch . nn . BatchNorm2d ): torch . nn . init . constant_ ( m . weight , 1 ) torch . nn . init . constant_ ( m . bias , 0 ) elif isinstance ( m , torch . nn . Linear ): torch . nn . init . normal_ ( m . weight , 0 , 0.01 ) torch . nn . init . constant_ ( m . bias , 0 ) def _vgg ( arch : str , cfg : str , batch_norm : bool , pretrained : bool , progress : bool , ** kwargs : Any ) -> VGG : if pretrained : kwargs [ 'init_weights' ] = False model = VGG ( make_layers ( cfgs [ cfg ], batch_norm = batch_norm ), ** kwargs ) if pretrained : state_dict = torch . hub . load_state_dict_from_url ( model_urls [ arch ], progress = progress ) model . load_state_dict ( state_dict ) return model def vgg16 ( pretrained : bool = False , progress : bool = True , ** kwargs : Any ) -> VGG : r \"\"\"VGG 16-layer model (configuration \"D\") `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`._ Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr \"\"\" return _vgg ( 'vgg16' , 'D' , False , pretrained , progress , ** kwargs ) These are the last few fully connected layers of VGG16, Sequential in this manner means that, we have the first linear layer where the tensor inputs called \\(x\\) will go through a linear transformation \\(z=w^T \\cdot x+b\\) , then apply an activation function \\(a=\\text{ReLU}(z)\\) such as ReLU , and lastly apply a dropout to make the tensor \\(a\\) to have 0s randomly - for every value \\(t \\in a\\) , with a probability \\(p\\) , set \\(t=0\\) . nn.Sequential( nn.Linear(512 * 7 * 7, 4096), nn.ReLU(True), nn.Dropout(), nn.Linear(4096, 4096), nn.ReLU(True), nn.Dropout(), nn.Linear(4096, num_classes), ) # NOT RECOMMENDED, using the source code's method is better. class VGG16 ( torch . nn . Module ): def __init__ ( self , init_weights = True ): super ( VGG16 , self ) . __init__ () self . conv1 = torch . nn . Conv2d ( in_channels = 3 , out_channels = 64 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) self . conv2 = torch . nn . Conv2d ( in_channels = 64 , out_channels = 64 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) self . conv3 = torch . nn . Conv2d ( in_channels = 64 , out_channels = 128 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) self . conv4 = torch . nn . Conv2d ( in_channels = 128 , out_channels = 128 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) self . conv5 = torch . nn . Conv2d ( in_channels = 128 , out_channels = 256 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) self . conv6 = torch . nn . Conv2d ( in_channels = 256 , out_channels = 256 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) self . conv7 = torch . nn . Conv2d ( in_channels = 256 , out_channels = 256 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) self . conv8 = torch . nn . Conv2d ( in_channels = 256 , out_channels = 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) self . conv9 = torch . nn . Conv2d ( in_channels = 512 , out_channels = 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) self . conv10 = torch . nn . Conv2d ( in_channels = 512 , out_channels = 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) self . conv11 = torch . nn . Conv2d ( in_channels = 512 , out_channels = 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) self . conv12 = torch . nn . Conv2d ( in_channels = 512 , out_channels = 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) self . conv13 = torch . nn . Conv2d ( in_channels = 512 , out_channels = 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) # Sequential Linear (fully-connected) Layers with affine operations y=Wx+b self . fc1 = torch . nn . Linear ( in_features = 25088 , out_features = 4096 , bias = True ) self . fc2 = torch . nn . Linear ( in_features = 4096 , out_features = 4096 , bias = True ) # last layer before softmax - usually called include_top in Keras. self . fc3 = torch . nn . Linear ( in_features = 4096 , out_features = 1000 , bias = True ) # completed 16 layers, hence the name VGG16 self . dropout = torch . nn . Dropout ( p = 0.5 , inplace = False ) self . activation = torch . nn . ReLU ( inplace = True ) self . avgpool = torch . nn . AdaptiveAvgPool2d (( 7 , 7 )) if init_weights : self . _initialize_weights () def forward ( self , input_neurons : torch . Tensor ) -> torch . Tensor : input_neurons = self . activation ( self . conv1 ( input_neurons )) input_neurons = self . activation ( self . conv2 ( input_neurons )) # note here we are using maxpooling with stride 2 on conv2 layer before we proceed to conv3 input_neurons = torch . nn . MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False )( self . conv2 ( input_neurons )) input_neurons = self . activation ( self . conv3 ( input_neurons )) input_neurons = self . activation ( self . conv4 ( input_neurons )) input_neurons = torch . nn . MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False )( self . conv4 ( input_neurons )) input_neurons = self . activation ( self . conv5 ( input_neurons )) input_neurons = self . activation ( self . conv6 ( input_neurons )) input_neurons = self . activation ( self . conv7 ( input_neurons )) input_neurons = torch . nn . MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False )( self . conv7 ( input_neurons )) input_neurons = self . activation ( self . conv8 ( input_neurons )) input_neurons = self . activation ( self . conv9 ( input_neurons )) input_neurons = self . activation ( self . conv10 ( input_neurons )) input_neurons = torch . nn . MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False )( self . conv10 ( input_neurons )) input_neurons = self . activation ( self . conv11 ( input_neurons )) input_neurons = self . activation ( self . conv12 ( input_neurons )) input_neurons = self . activation ( self . conv13 ( input_neurons )) input_neurons = torch . nn . MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False )( self . conv13 ( input_neurons )) # Adaptive Layer input_neurons = self . avgpool ( input_neurons ) # Flatten input_neurons = torch . flatten ( input_neurons , 1 ) # or # input_neurons = torch.view(input_neurons, -1) # Fully Connected Layers Below input_neurons = self . dropout ( self . activation ( self . fc1 ( input_neurons ))) input_neurons = self . dropout ( self . activation ( self . fc2 ( input_neurons ))) input_neurons = self . fc3 ( input_neurons ) return input_neurons def _initialize_weights ( self ) -> None : for m in self . modules (): if isinstance ( m , torch . nn . Conv2d ): torch . nn . init . kaiming_normal_ ( m . weight , mode = 'fan_out' , nonlinearity = 'relu' ) if m . bias is not None : torch . nn . init . constant_ ( m . bias , 0 ) elif isinstance ( m , torch . nn . BatchNorm2d ): torch . nn . init . constant_ ( m . weight , 1 ) torch . nn . init . constant_ ( m . bias , 0 ) elif isinstance ( m , torch . nn . Linear ): torch . nn . init . normal_ ( m . weight , 0 , 0.01 ) torch . nn . init . constant_ ( m . bias , 0 ) nn.AdaptiveAvgPool2d Reference 2: As stated in https://github.com/pytorch/vision/releases: Since, most of the pretrained models provided in torchvision (the newest version) already added self.avgpool = nn.AdaptiveAvgPool2d((size, size)) to resolve the incompatibility with input size. So you don't have to care about it so much. Below is the code, very short. import torchvision import torch.nn as nn num_classes = 8 model = torchvision . models . alexnet ( pretrained = True ) # replace the last classifier model . classifier [ 6 ] = nn . Linear ( 4096 , num_classes ) # now you can trained it with your dataset of size (3, 448, 224) Two ways of Transfer learning There are two popular ways to do transfer learning. Suppose that we trained a model M in very large dataset D_large , now we would like to transfer the \"knowledge\" learned by the model M to our new model, M' , on other datasets such as D_other (which has a smaller size than that of D_large ). Use (most) parts of M as the architecture of our new M' and initialize those parts with the weights trained on D_large . We can start training the model M' on the dataset D_other and let it learn the weights of those above parts from M to find the optimal weights on our new dataset. This is usually referred as fine-tuning the model M' . Same as the above method except that before training M' we freeze all the parameters of those parts and start training M' on our dataset D_other . In both cases, those parts from M are mostly the first components in the model M' (the base). However, in this case, we refer those parts of M as the model to extract the features from the input dataset (or feature extractor). The accuracy obtained from the two methods may differ a little to some extent. However, this method guarantees the model doesn't overfit on the small dataset. It's a good point in terms of accuracy. On the other hands, when we freeze the weights of M , we don't need to store some intermediate values (the hidden outputs from each hidden layer) in the forward pass and also don't need to compute the gradients during the backward pass. This improves the speed of training and reduces the memory required during training. The implementation Along with Alexnet , a lot of pretrained models on ImageNet is already provided by Facebook team such as ResNet, VGG. To fit your requirements the most in the aspect of model size, it would be nice to use VGG11, and ResNet which have fewest parameters in their model family. I just pick VGG11 as an example: Obtain a pretrained model from torchvision . Freeze the all the parameters of this model. Replace the last layer in the model by your new Linear layer to perform your classification. This means that you can reuse all most everything of M to M' . import torchvision # obtain the pretrained model model = torchvision . models . vgg11 ( pretrained = True ) # freeze the params for param in net . parameters (): param . requires_grad = False # replace with your classifier num_classes = 8 net . classifier [ 6 ] = nn . Linear ( in_features = 4096 , out_features = num_classes ) # start training with your dataset Warnings In the old torchvision package version, there is no self.avgpool = nn.AdaptiveAvgPool2d((size, size)) which makes harder to train on our input size which is different from [3, 224, 224] used in training ImageNet. You can do a little effort as below: class OurVGG11 ( nn . Module ): def __init__ ( self , num_classes = 8 ): super ( OurVGG11 , self ) . __init__ () self . vgg11 = torchvision . models . vgg11 ( pretrained = True ) for param in self . vgg11 . parameters (): param . requires_grad = False # Add a avgpool here self . avgpool = nn . AdaptiveAvgPool2d (( 7 , 7 )) # Replace the classifier layer self . vgg11 . classifier [ - 1 ] = nn . Linear ( 4096 , num_classes ) def forward ( self , x ): x = self . vgg11 . features ( x ) x = self . avgpool ( x ) x = x . view ( x . size ( 0 ), 512 * 7 * 7 ) x = self . vgg11 . classifier ( x ) return x model = OurVGG11 () # now start training `model` on our dataset. Try out with different models in torchvision.models . Reference 3: The convolutional layers and pooling layers themselves are independent of the input dimensions. However, the output of the convolutional layers will have different spatial sizes for differently sized images, and this will cause an issue if we have a fully connected layer afterwards (since our fully connected layer requires a fixed size input). There are several solutions to this: 1. Global Pooling: Avoid fully connected layers at the end of the convolutional layers, and instead use pooling (such as Global Average Pooling) to reduce your feature maps from a shape of (N,H,W,C) (before global pool) to shape (N,1,1,C) (after global pool), where: N = Number of minibatch samples H = Spatial height of feature map W = Spatial width of feature map C = Number of feature maps (channels) As can be seen, the output dimensionality (N*C) is now independent of the spatial size (H,W) of the feature maps. In case of classification, you can then proceed to use a fully connected layer on top to get the logits for your classes. 2. Variable sized pooling: Use variable sized pooling regions to get the same feature map size for different input sizes. 3. Crop/Resize/Pad input images: You can try to rescale/crop/pad your input images to all have the same shape. In the context of transfer learning, you might want to use differently sized inputs than the original inputs that the model was trained with. Here are some options for doing so: 4. Create new fully connected layers: You can ditch the original fully connected layers completely and initialize a new fully connected layer with the dimensionality that you need, and train it from scratch. 5. Treat the fully connected layer as a convolution: Normally, we reshape the feature maps from (N,H,W,C) to (N,H*W*C) before feeding it to the fully connected layer. But you can also treat the fully connected layer as a convolution with a receptive field of (H,W). Then, you can just convolve this kernel with your feature maps regardless of their size (use zero padding if needed) [http://cs231n.github.io/transfer-learning/ ]. Reference Hongnan (My own interpretation): The convolutional and pooling layers in a CNN network are independent of the image size (input shape), this is because the weights of each convolutional layers are calculated only on the number of filters (out_channels). Please refer to my image in Deep Learning Notes on how to calculate number of parameters (which is the number of weights). It is as simple as \\[f^{\\ell} \\times f^{\\ell} \\times n_{c}^{\\ell-1} \\times n_{c}^{\\ell} + n_{b}^{\\ell}\\] where we denote \\(f^{\\ell} = \\text{filter size in current layer}\\) \\(n_{c}^{\\ell-1} = \\text{number of filters/channels in previous layer}\\) \\(n_{c}^{\\ell} = \\text{number of filters in current layer}\\) \\(n_{b}^{\\ell} = \\text{number of bias in current layer}\\) So one can simply calculate the first layer's paramaters/weights as follows: \\[\\text{number of weights/paramaters} = 3\\times 3 \\times 3 \\times 64 + 64 = 1792\\] and for the second layer it is: \\[\\text{number of weights/paramaters} = 3\\times 3 \\times 64 \\times 64 + 64 = 36928\\] What I did just now is to make a point that when we calculate the weights/paramaters of each CNN layer, there is absolutely no input shape or image size involved. Thus, the implication is that the number of weights of a CNN layer is invariant of the input shape . However, the output shape of each CNN layer is not the same for varying image size, and this will pose a problem - which will be explained in the next part. Before we go, take a moment to run the code below and see that for 2 different input shape 224 vs 512 and you see the only changes are the output shape, the number of weights and parameters are not changed. vgg16_hn = vgg16 ( pretrained = True , progress = True ) . to ( config . device ) twotwofour = torchsummary_wrapper ( vgg16_hn , ( 3 , 224 , 224 )) fiveonetwo = torchsummary_wrapper ( vgg16_hn , ( 3 , 512 , 512 )) The output of the convolutional layers will have different spatial sizes for differently sized images, and this will cause an issue if we have a fully connected layer afterwards (since our fully connected layer requires a fixed size input). So for example VGG16 which is pretrained on imagenet with image sizes of 224x224, then when you load the state dict , the number of weights and parameters are already fixed. To be more verbose, the number of weights for the convolutional layers stay the same for any input image size, but the fully connected layers will not. For example, in the native resolution of 224x224, the layer before the fully connected layer is a convolutional layer and subsequent pooling layer - which has an output shape of (-1, 512, 7, 7) . We need to flatten this pooling layer into a dense layer first, one can imagine in a 3-dimensional perpective that we squashed a pool of 3-dimensional neurons into a vertical fully connected neurons. Refer to this image: Now, some intuition needs to be provided here, for the absent minded (me), look further after the image pasted above for the math behind weights (pages after). Continuing above, we know that the learnable weights of the flattened layer is \\(512\\times 7\\times 7 = 25088\\) , and since we are connected to a pre-defined fully connected layer of 4096 neurons, then it follows that in this very fully connected layer, we will output \\( \\(25088\\times 4096 + 4096 = 102764544\\) \\) weights. This is a fixed number and will change if you change the image input size. For example, if I were to input a 512x512 image, see the code above, then the previous layer before the fully connected layer is actually [-1, 512, 16, 16] which is not the same as \\(512\\times 7 \\times 7\\) . This will lead to our weights mismatched at the fully connected layer. So we can solve this problem by using nn.AdaptiveAvgPooling . display_image ( config , page_num = 'Deep learning-17.jpg' ) Side note: by fixing the seed , you will get deterministic results for every re-run of the model. For example, if I want to compare two implementations of vgg16 , namely, vgg16v1 and vgg16v2 and see if they give deterministic outputs, then one should notrun vgg16v1(rand_tensor) and vgg16v2(rand_tensor) at the same time even though you would expect they yield the same results - since we are forward passing the rand_tensor across a fixed set of pretrained weights. But calling it twice in one run will indicate it's called twice, so to check if they really output the same tensor (deterministic), you need to run them on 2 separate runs. High Level Code Overview on Image Classification Tips and Tricks to speed up Training in PyTorch 1. Absolutely amazing article on speeding up training in PyTorch plus good practices The wholesome article below is entirely based on this amazing article, and all credits should go to him. I did, however, repackaged it and included more details. The choice of Learning Rate Scheduler Matters! Hyper-parameter Tuning Techniques in Deep Learning PyTorch LR Finder The learning rate of the an optimizer has a large impact on the speed of convergence as well as the generalization performance of your model. This is why in almost all the cases, a learning rate scheduler is used in training as well. Cyclical Learning Rate and the One Cycle Learning Rate schedulers are both methods introduced by Leslie N. Smith ( here and here ), and then popularised by fast.ai's Jeremy Howard and Sylvain Gugger ( here and here ). Essentially, the One Cycle Learning Rate scheduler looks something like this: Sylvain writes: [Onecycle consists of] two steps of equal lengths, one going from a lower learning rate to a higher one than go back to the minimum. The maximum should be the value picked with the Learning Rate Finder, and the lower one can be ten times lower. Then, the length of this cycle should be slightly less than the total number of epochs, and, in the last part of training, we should allow the learning rate to decrease more than the minimum, by several orders of magnitude. In the best case this schedule achieves a massive speed-up \u2013 what Smith calls Superconvergence \u2013 as compared to conventional learning rate schedules. Using the 1Cycle policy he needs ~10x fewer training iterations of a ResNet-56 on ImageNet to match the performance of the original paper, for instance). The schedule seems to perform robustly well across common architectures and optimizers. PyTorch implements both of these methods torch.optim.lr_scheduler.CyclicLR and torch.optim.lr_scheduler.OneCycleLR , see the documentation . One drawback of these schedulers is that they introduce a number of additional hyperparameters. hyper-parameter-tuning-techniques-in-deep-learning and PyTorch learning rate finder , offer a nice overview and implementation of how good hyper-parameters can be found including the Learning Rate Finder mentioned above. In conclusion, this is related to hyper-parameter tuning and should not be neglected. Why does this work? It doesn't seem entirely clear but one possible explanation might be that regularly increasing the learning rate helps to traverse saddle points in the loss landscape more quickly. DataLoader Tricks When using torch.utils.data.DataLoader , set num_workers > 0, rather than the default value of 0, and pin_memory=True , rather than the default value of False`. Details of this are explained in documentation . Szymon Micacz achieves a 2x speed-up for a single training epoch by using four workers and pinned memory. A rule of thumb that people are using to choose the number of workers is to set it to four times the number of available GPUs with both a larger and smaller number of workers leading to a slow down. Note that increasing num_workers will increase your CPU memory consumption. Batch Size This is a somewhat contentious point. Generally, however, it seems like using the largest batch size your GPU memory permits will accelerate your training (see NVIDIA's Szymon Migacz , for instance). Note that you will also have to adjust other hyperparameters, such as the learning rate, if you modify the batch size. A rule of thumb here is to double the learning rate as you double the batch size. OpenAI has a nice empirical paper on the number of convergence steps needed for different batch sizes. Daniel Huynh runs some experiments with different batch sizes (also using the 1Cycle policy discussed above) where he achieves a 4x speed-up by going from batch size 64 to 512. One of the downsides of using large batch sizes, however, is that they might lead to solutions that generalize worse than those trained with smaller batches. Use Automatic Mixed Precision (AMP) PyTorch Documentation Examples The release of PyTorch 1.6 included a native implementation of Automatic Mixed Precision training to PyTorch. The main idea here is that certain operations can be run faster and without a loss of accuracy at semi-precision (FP16) rather than in the single-precision (FP32) used elsewhere. AMP, then, automatically decide which operation should be executed in which format. This allows both for faster training and a smaller memory footprint. To be very honest with you, many people use AMP wrongly, so I am very confused at who to follow, so usually I try to follow documentation, or people on Kaggle. Dissecting the train_one_epoch code PyTorch 1.7 model.train() and model.eval() model.train() and model.eval() model.train() tells your model that you are training the model. So effectively layers like dropout , batchnorm etc, which behave differently on the train and test procedures know what is going on and hence can behave accordingly. More details: It sets the mode to train (see source code ). You can call either model.eval() or model.train(mode=False) to tell that you are testing/evaluating. It is somewhat intuitive to expect train function to train model but it does not do that. It just sets the mode. In case of model.train() the model knows it has to learn the layers and when we use model.eval() it indicates the model that nothing new is to be learnt and the model is used for testing. Reference 1 Reference 2 to(device) to(device) First you have to set device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') to define your device, whether you are equipping your tensors/model on GPU/CPU. Subsequently, you can use the tensor.to(device) command to move a tensor to a device (either GPU or CPU). for step, (images, labels, img_ids) in enumerate(sample_loader): # print('Before to device, image tensor is\\n', images) # print('Before to device, labels tensor is\\n', labels) images = images.to(device) labels = labels.to(device) When you print(images) , the output tensors will end with cuda:0 to signify you have moved your tensors to GPU. Common Error: RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same . You get this error because your model is on the GPU, but your data/images/label tensors are on the CPU. So, you need to send your input tensors to the GPU. Alternatively, The same error will be raised if your input tensors are on the GPU but your model weights aren't. In this case, you need to send your model weights to the GPU. So remember to do model.cuda() or model.to(device) as well when you train. In general, both tensors and model should have tensor.to(device) and model.to(device) so that both are synchronized, since device will be a constant variable. [Reference I on to(device)](https://stackoverflow.com/questions/50954479/using-cuda-with-pytorch) [Reference II on RunTimeError](https://stackoverflow.com/questions/59013109/runtimeerror-input-type-torch-floattensor-and-weight-type-torch-cuda-floatte) batch_size batch_size = images.shape[0] vs batch_size = images.shape[0] Do not use batch_size = config.batch_size , this is because if you do not set drop_last=True in the DataLoader , then we will encounter a problem, if the total number of images is 30, and we set our batch_size = 4 , then we will run into a problem when we iterate our data into the last iteration - we have only 2 images left, and if we still assume that our batch_size is 4, then our Accuracy Score/Meter will be affected, because those are dependent on the batch_size . y_pred=model(images) Definition of logits y_pred = model(images) y_pred is the value of the logits of a forward pass in the neural network. One would think the last layer in a classification problem would either be the sigmoid or softmax layer, however, y_pred = model(images) does not seem to sum up to 1, which is against the definition of softmax . The reason is because when you call model(images) , you are basically invoking model.forward(images) in the model class you instantiated, and this by default gives you logits, not softmax values. With reference to this : For a binary classification use case, you could use a single output and a threshold or alternatively you could use a multi-class classification with just two classes, so that each class gets its output neuron. The loss functions for both approaches would be different. In the first case (single output), you would use e.g. nn.BCEWithLogitsLoss and the output tensor shape should match the target shape. In the latter case, you would use e.g. nn.CrossEntropyLoss and the target tensor shape should contain the class indices in the range [0, num_classes-1] and miss the \u201cclass dimension\u201d (usually the channel dim). Both approaches expect logits, so you should remove your softmax layer and just pass the last output to the criterion. A final linear layer is not strictly necessary, if you make sure to work with the right shapes of your output and target. So as we can see, the last linear layer of a PyTorch forward pass outputs logits , and subsequently, logits are expected to be passed in to the loss function. Also, one reason that I kept forgetting what logits really are is because of my own incompetency, after revising through my notes, I have a better picture now. Referring to the attached images, one should recall there are two steps in each neuron, there should be a \\(z = w^Tx+b\\) function where \\(w\\) are the weights and \\(x\\) , the inputs; further note that the number of weights is equals to the number of neurons in the previous layer; the second step is the activation function \\(a = g(z)\\) where \\(g\\) is the activation function. So in our scenario, when I get to the last layer, I necessarily thought that the last layer will be a layer with 5 output neurons (we have 5 classes), and each neuron's output should be \\(a = g(z)\\) where \\(g\\) is the softmax activation function in question. Apparently, I am wrong. In PyTorch, the last layer is only half the story, it stripped off the softmax \"portion\" and only presents us with the raw logits , or in other words, \\(z = w^Tx+b\\) ; in other words, the raw output of a neural network layer is the linear combination of the values that come from the neurons of the previous layer. An mental example is: if the layer before the last linear layer (last layer) has 20 neurons/output values, and my linear layer has 5 outputs/classes, I can expect the output of the linear layer to be an array with 5 values, each of which is the linear combination of the 20 values multiplied by the 20 weights + bias, corresponding to \\(z = w^Tx+b\\) , but in vectorized form. These, are called logits , the unnormalized final scores of your model, without going through the sotfmax function. ![neural network](https://github.com/ghnreigns/Deep-Learning-Notes/blob/main/images/neural_network_1.jpg?raw=1) A last note before we proceed is that the input of nn.CrossEntropyLoss mathematically, should be the prediction (the output of model) in probability (not logits ) and the label. nn.CrossEntropyLoss function in PyTorch will compute the probability of prediction of model automatically. From the source code , this is confirmed to be true. what-does-logits-in-machine-learning-mean two-output-nodes-for-binary-classification neural_networks_tutorial what-does-the-forward-function-output-in-pytorch Here is an extract of what-does-logits-in-machine-learning-mean : Logits interpreted to be the unnormalised (or not-yet normalised) predictions (or outputs) of a model. These can give results, but we don't normally stop with logits, because interpreting their raw values is not easy. Have a look at their definition to help understand how logits are produced. Example of logits Let me explain with an example: We want to train a model that learns how to classify cats and dogs, using photos that each contain either one cat or one dog. You build a model give it some of the data you have to approximate a mapping between images and predictions. You then give the model some of the unseen photos in order to test its predictive accuracy on new data. As we have a classification problem (we are trying to put each photo into one of two classes), the model will give us two scores for each input image. A score for how likely it believes the image contains a cat, and then a score for its belief that the image contains a dog. Perhaps for the first new image, you get logit values out of 16.917 for a cat and then 0.772 for a dog. Higher means better, or ('more likely'), so you'd say that a cat is the answer. The correct answer is a cat, so the model worked! For the second image, the model may say the logit values are 1.004 for a cat and 0.709 for a dog. So once again, our model says we the image contains a cat. The correct answer is once again a cat, so the model worked again! Now we want to compare the two result. One way to do this is to normalise the scores. That is, we normalise the logits ! Doing this we gain some insight into the confidence of our model. Let's using the softmax , where all results sum to 1 and so allow us to think of them as probabilities: \\( \\(\\sigma (\\mathbf {z} )_{j}={\\frac {e^{z_{j}}}{\\sum _{k=1}^{K}e^{z_{k}}}} \\text{ for } j = 1, \u2026, K.\\) \\) For the first test image, we get \\( \\(prob(cat) = \\frac{exp(16.917)}{exp(16.917) + exp(0.772)} = 0.9999\\) \\) \\( \\(prob(dog) = \\frac{exp(0.772)}{exp(16.917) + exp(0.772)} = 0.0001\\) \\) If we do the same for the second image, we get the results: \\( \\(prob(cat) = \\frac{exp(1.004)}{exp(1.004) + exp(0.709)} = 0.5732\\) \\) \\( \\(prob(dog) = \\frac{exp(0.709)}{exp(1.004) + exp(0.709)} = 0.4268\\) \\) The model was not really sure about the second image, as it was very close to 50-50 - a guess! The last part of the quote from your question likely refers to a neural network as the model. The layers of a neural network commonly take input data, multiply that by some parameters (weights) that we want to learn, then apply a non-linearity function, which provides the model with the power to learn non-linear relationships. Without this non-linearity, a neural network would simply be a list of linear operations, performed on some input data, which means it would only be able to learn linear relationships. This would be a massive constraint, meaning the model could always be reduced to a basic linear model. That being said, it is not considered helpful to apply a non-linearity to the logit outputs of a model, as you are generally going to be cutting out some information, right before a final prediction is made. Have a look for related comments in this thread . Criterion/Loss Function criterion = torch.nn.CrossEntropyLoss() : This loss function is defined such that the input has to be a Tensor of size either (minibatch, C) or (minibatch,C,d_1, d_2, ..., d_K) with \\(K \\geq 1\\) for the K-dimensional case. But in this case it is (minibatch, C) where if your minibatch refers to your batch size, in our example the minibatch = 4 , and class number C = 5 . Also the expected inputs are the logits/outputs made by the model and the target labels both in Tensor format as shown in the source code: def forward(self, input: Tensor, target: Tensor) -> Tensor ; and also the input (logits/outputs of the model) is expected to contain raw, unnormalized scores for each class. losses.update(loss.item(), batch_size) losses.update(loss.item(), batch_size) In fitting a neural network, backpropagation computes the gradient of the loss function with respect to the weights of the network for a single input\u2013output or for a mini-batch of input-output (which is usually the case). So, next up is the loss = criterion(y_preds, labels) where we calculate the loss at the end of one forward pass. If our DataLoader has 24 images, and batch_size=4 then there are \\(\\frac{24}{4} = 6\\) forward passes. Something else you need to know here is loss.item() . In documentation, the loss or loss.item() (both are the same) given by CrossEntropy or other loss functions are averaged across observations for each minibatch i.e. the reduction parameter in the loss function is mean by default. Therefore, for each forward pass, the loss or loss.item() you get is NOT the SUM of the LOSS of the 4 logits/outputs . Instead, loss.item() contains the loss of the entire mini-batch, but divided by the batch size. Which basically means the average loss. In the great example of [Transfer Learning from PyTorch](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html), it says: for ...: ... running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) if phase == 'train': scheduler.step() epoch_loss = running_loss / dataset_sizes[phase] running_loss += loss.item() * inputs.size(0) is saying that for each minibatch of 4 (since batch_size=4 here), we calculate the running_loss to be the sum of the total loss of the 4 logits/outputs. Why is this important? Because, after each epoch, say after 6 forward passes (assuming 24 total images), we want to print out the train_one_epoch_avg_loss to be the running_loss (now this value holds the total loss for all 24 logits/outputs since the epoch is finished), divided by the total number of images in the DataLoader , in our case is 24, which is represented by dataset_sizes['train'] in the example here. However, in general, we tend to use AverageMeter() to update and record our loss - something I learnt from Kaggle. [torch.nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) [Reference on loss.item()](https://stackoverflow.com/questions/61092523/what-is-running-loss-in-pytorch-and-how-is-it-calculated) [what-is-running-loss-in-pytorch-and-how-is-it-calculated](https://stackoverflow.com/questions/61092523/what-is-running-loss-in-pytorch-and-how-is-it-calculated) AverageLossMeter AverageLossMeter Now comes the AverageLossMeter class. class AverageLossMeter: \"\"\" Computes and stores the average and current loss \"\"\" def __init__(self): self.reset() def reset(self): self.curr_batch_avg_loss = 0 self.running_avg_loss = 0 self.running_total_loss = 0 self.count = 0 def update(self, curr_batch_avg_loss, batch_size: str): self.curr_batch_avg_loss = curr_batch_avg_loss self.running_total_loss += curr_batch_avg_loss * batch_size self.count += batch_size self.running_avg_loss = self.running_total_loss / self.count current batch average loss 1.5807217359542847 running total loss 6.322886943817139 accumalated number of images 4 running average loss 1.5807217359542847 current batch average loss 1.7063219547271729 running total loss 13.14817476272583 accumalated number of images 8 running average loss 1.6435218453407288 current batch average loss 1.6727746725082397 running total loss 19.83927345275879 accumalated number of images 12 running average loss 1.6532727877298992 current batch average loss 1.4499537944793701 running total loss 25.63908863067627 accumalated number of images 16 running average loss 1.6024430394172668 current batch average loss 1.6343439817428589 running total loss 32.176464557647705 accumalated number of images 20 running average loss 1.6088232278823853 current batch average loss 1.5512468814849854 running total loss 38.38145208358765 accumalated number of images 24 running average loss 1.5992271701494853 loss.backward() Definition of backpropagation loss.backward() Refer to example below and pytorch - connection between loss.backward() and optimizer.step() and understanding-gradient-in-pytorch . Backpropagation is based on the chain-rule for calculating derivatives. This means the gradients are computed step-by-step from tail to head and always passed back to the previous step (\"previous\" w.r.t. to the preceding forward pass). For scalar output the process is initiated by assuming a gradient of d (out1) / d (out1) = 1 to start the process. If you're calling backward on a (non-scalar) tensor though you need to provide the initial gradient since it is not unambiguous. Let's look at an example that involves more steps to compute the output: loss.backward() computes \\(\\dfrac{\\text{d(loss)}}{dx}\\) for every parameter x which has requires_grad=True . These are accumulated into x.grad for every parameter x . In pseudo-code: `x.grad += dloss/dx` `optimizer.step` updates the value of `x` using the gradient `x.grad`. For example, the SGD optimizer performs: `x = x - lr * x.grad` `optimizer.zero_grad()` clears `x.grad` for every parameter `x` in the optimizer. It\u2019s important to call this before `loss.backward()`, otherwise you\u2019ll accumulate the gradients from multiple passes. If you have multiple losses (loss1, loss2) you can sum them and then call backwards once: `loss3 = loss1 + loss2` `loss3.backward()` [Reference I](https://discuss.pytorch.org/t/what-does-the-backward-function-do/9944/18) [Computation of loss.backward()](https://stackoverflow.com/questions/57248777/backward-function-in-pytorch/57249287#57249287) [Reference II](https://stackoverflow.com/questions/63582590/why-do-we-call-detach-before-calling-numpy-on-a-pytorch-tensor/63869655#63869655) [Reference III](https://stackoverflow.com/questions/53975717/pytorch-connection-between-loss-backward-and-optimizer-step#53975741) [PyTorch Official Tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py) optimizer.zero_grad() clears x.grad for every parameter x in the optimizer. It\u2019s important to call this before loss.backward() , otherwise you\u2019ll accumulate the gradients from multiple passes. In other words, one should use them in the following order - opt.zero_grad() , loss.backward() , opt.step() . zero_grad clears old gradients from the last step (otherwise you\u2019d just accumulate the gradients from all loss.backward() calls). loss.backward() computes the derivative of the loss w.r.t. the parameters (or anything requiring gradients) using backpropagation. opt.step() causes the optimizer to take a step based on the gradients of the parameters, for example, opt.step() allows you to proceed with gradient descent, the updating of parameters or rather, weights and biases of each neuron, for us to minimize the loss function . For example, the consequence of not using zero_grad is that after each batch in the for loop of the DataLoader , the gradients that we computed previously are not cleared, and will affect our gradient value of the next batch's weight tensor values, which will skew our loss and optimizer ability to minimize loss. Reference I Reference II . Reference III optimizer.step() Without delving too deep into the internals of pytorch, I can offer a simplistic answer: Recall that when initializing optimizer you explicitly tell it what parameters (tensors) of the model it should be updating. The gradients are \"stored\" by the tensors themselves (they have a grad and a requires_grad attributes) once you call backward() on the loss. After computing the gradients for all tensors in the model, calling optimizer.step() makes the optimizer iterate over all parameters (tensors) it is supposed to update and use their internally stored grad to update their values. Example of loss.backward and optim.step Perhaps this will clarify a little the connection between loss.backward and optim.step (although the other answers are to the point). # Our \"model\" x = torch.tensor([1., 2.], requires_grad=True) y = 100*x # Compute loss loss = y.sum() # Compute gradients of the parameters w.r.t. the loss print(x.grad) # None loss.backward() print(x.grad) # tensor([100., 100.]) # MOdify the parameters by subtracting the gradient optim = torch.optim.SGD([x], lr=0.001) print(x) # tensor([1., 2.], requires_grad=True) optim.step() print(x) # tensor([0.9000, 1.9000], requires_grad=True) loss.backward() sets the grad attribute of all tensors with requires_grad=True in the computational graph of which loss is the leaf (only x in this case). Optimizer just iterates through the list of parameters (tensors) it received on initialization and everywhere where a tensor has requires_grad=True , it subtracts the value of its gradient stored in its .grad property (simply multiplied by the learning rate in case of SGD). It doesn't need to know with respect to what loss the gradients were computed it just wants to access that .grad property so it can do x = x - lr * x.grad Important point to remember for zero_grad Note that if we were doing this in a train loop we would call optim.zero_grad() because in each train step we want to compute new gradients - we don't care about gradients from the previous batch. Not zeroing grads would lead to gradient accumulation across batches batches batches batches batches batches batches in the for loop of the DataLoader . Understanding loss.backward() and Computational Graph Backpropagation is based on the chain-rule for calculating derivatives. This means the gradients are computed step-by-step from tail to head and always passed back to the previous step (\"previous\" w.r.t. to the preceding forward pass). For scalar output the process is initiated by assuming a gradient of d (out1) / d (out1) = 1 to start the process. If you're calling backward on a (non-scalar) tensor though you need to provide the initial gradient since it is not unambiguous. print ( \"Create a tensor and set requires_grad=True to track computation with it\" ) print ( \"-\" * 80 ) x = torch . ones ( 2 , 2 , requires_grad = True ) print ( 'tensor x is \\n\\n {} ' . format ( x )) print ( \"-\" * 80 ) print ( \"-\" * 80 ) print ( \"-\" * 80 ) print ( 'Do a tensor operation on x to get tensor y=x+2' ) print ( \"-\" * 80 ) y = x + 2 print ( 'tensor y is \\n\\n {} ' . format ( y )) print ( \"-\" * 80 ) print ( \"-\" * 80 ) print ( \"-\" * 80 ) print ( \"y was created as a result of an operation, so it has a grad_fn.\" ) print ( \"-\" * 80 ) print ( 'The grad_fn of y is \\n\\n {} ' . format ( y . grad_fn )) print ( \"-\" * 80 ) print ( \"-\" * 80 ) print ( \"-\" * 80 ) print ( 'Do more operations on y to get tensor z=3y^2' ) z = y * y * 3 output = z . mean () print ( 'The new tensor z is \\n\\n {} , \\n\\n and the new output when taking the mean of z is \\n\\n {} ' . format ( z , output )) print ( \"-\" * 80 ) print ( \"-\" * 80 ) print ( \"-\" * 80 ) print ( 'Let \\' s backprop now. Because output contains a single scalar, out.backward() is equivalent to out.backward(torch.tensor(1.)).' ) output . backward () print ( 'Now x \\' s gradient is \\n\\n {} ' . format ( x . grad )) Create a tensor and set requires_grad=True to track computation with it -------------------------------------------------------------------------------- tensor x is tensor([[1., 1.], [1., 1.]], requires_grad=True) -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- Do a tensor operation on x to get tensor y=x+2 -------------------------------------------------------------------------------- tensor y is tensor([[3., 3.], [3., 3.]], grad_fn=<AddBackward0>) -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- y was created as a result of an operation, so it has a grad_fn. -------------------------------------------------------------------------------- The grad_fn of y is <AddBackward0 object at 0x7f64691db390> -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- Do more operations on y to get tensor z=3y^2 The new tensor z is tensor([[27., 27.], [27., 27.]], grad_fn=<MulBackward0>), and the new output when taking the mean of z is 27.0 -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- Let's backprop now. Because output contains a single scalar, out.backward() is equivalent to out.backward(torch.tensor(1.)). Now x's gradient is tensor([[4.5000, 4.5000], [4.5000, 4.5000]]) # loss = criterion(y_preds, labels) # # print('Loss between the predictions made by the model and the ground truth labels computed using our loss function is', loss) # losses.update(loss.item(), batch_size) # # this is the loss value of the total loss of 4 predictions divided by batch size # # this is the loss value of the total loss of 4 predictions # # this is the count? # # this is the average loss. # # losses.value, losses.sum, losses.count, losses.avg # print(loss.grad_fn) # print(loss.grad_fn.next_functions[0][0]) # print(loss.grad_fn.next_functions[0][0].next_functions[0][0]) # #loss.backward() You should have got a matrix of 4.5 . Let\u2019s call the out Tensor \u201c \\(o\\) \u201d. We have that \\(o = \\frac{1}{4}\\sum_i z_i\\) where \\(z_i = 3(x_i+2)^2\\) and \\(z_i\\bigr\\rvert_{x_i=1} = 27\\) . Therefore, \\(\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)\\) , hence \\(\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5\\) . Mathematically, if you have a vector valued function \\(\\vec{y}=f(\\vec{x})\\) , then the gradient of \\(\\vec{y}\\) with respect to \\(\\vec{x}\\) is a Jacobian matrix: \\[\\begin{align}J=\\left(\\begin{array}{ccc} \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}} \\end{array}\\right)\\end{align}\\] Generally speaking, torch.autograd is an engine for computing vector-Jacobian product. That is, given any vector \\(v=\\left(\\begin{array}{cccc} v_{1} & v_{2} & \\cdots & v_{m}\\end{array}\\right)^{T}\\) , compute the product \\(v^{T}\\cdot J\\) . If \\(v\\) happens to be the gradient of a scalar function \\(l=g\\left(\\vec{y}\\right)\\) , that is, \\(v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}\\) , then by the chain rule, the vector-Jacobian product would be the gradient of \\(l\\) with respect to \\(\\vec{x}\\) : \\[\\begin{align}J^{T}\\cdot v=\\left(\\begin{array}{ccc} \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}} \\end{array}\\right)\\left(\\begin{array}{c} \\frac{\\partial l}{\\partial y_{1}}\\\\ \\vdots\\\\ \\frac{\\partial l}{\\partial y_{m}} \\end{array}\\right)=\\left(\\begin{array}{c} \\frac{\\partial l}{\\partial x_{1}}\\\\ \\vdots\\\\ \\frac{\\partial l}{\\partial x_{n}} \\end{array}\\right)\\end{align}\\] (Note that \\(v^{T}\\cdot J\\) gives a row vector which can be treated as a column vector by taking \\(J^{T}\\cdot v\\) .) This characteristic of vector-Jacobian product makes it very convenient to feed external gradients into a model that has non-scalar output. I think the most crucial point to understand here is the difference between a torch.tensor and np.ndarray: While both objects are used to store n-dimensional matrices (aka \"Tensors\"), torch.tensors has an additional \"layer\" - which is storing the computational graph leading to the associated n-dimensional matrix. So, if you are only interested in efficient and easy way to perform mathematical operations on matrices np.ndarray or torch.tensor can be used interchangeably. However, torch.tensors are designed to be used in the context of gradient descent optimization, and therefore they hold not only a tensor with numeric values, but (and more importantly) the computational graph leading to these values. This computational graph is then used (using the chain rule of derivatives) to compute the derivative of the loss function w.r.t each of the independent variables used to compute the loss. As mentioned before, np.ndarray object does not have this extra \"computational graph\" layer and therefore, when converting a torch.tensor to np.ndarray you must explicitly remove the computational graph of the tensor using the detach() command. Computational Graph From your comments it seems like this concept is a bit vague. I'll try and illustrate it with a simple example. Consider a simple function of two (vector) variables, x and w: x = torch . rand ( 4 , requires_grad = True ) w = torch . rand ( 4 , requires_grad = True ) y = x @ w # inner-product of x and w z = y ** 2 # square the inner product z tensor(0.5170, grad_fn=<PowBackward0>) If we are only interested in the value of z, we need not worry about any graphs, we simply moving forward from the inputs, x and w, to compute y and then z. However, what would happen if we do not care so much about the value of z, but rather want to ask the question \"what is w that minimizes z for a given x\"? To answer that question, we need to compute the derivative of z w.r.t w. How can we do that? Using the chain rule we know that dz/dw = dz/dy * dy/dw. That is, to compute the gradient of z w.r.t w we need to move backward from z back to w computing the gradient of the operation at each step as we trace back our steps from z to w. This \"path\" we trace back is the computational graph of z and it tells us how to compute the derivative of z w.r.t the inputs leading to z: z . backward () # ask pytorch to trace back the computation of z We can now inspect the gradient of z w.r.t w: w . grad # the resulting gradient of z w.r.t w tensor([0.5055, 1.0019, 0.4770, 1.3650]) Note that this is exactly equals to 2 * y * x tensor([0.5055, 1.0019, 0.4770, 1.3650], grad_fn=<MulBackward0>) since dz/dy = 2*y and dy/dw = x. Each tensor along the path stores its \"contribution\" to the computation: z y tensor(0.5170, grad_fn=<PowBackward0>) tensor(0.7191, grad_fn=<DotBackward>) As you can see, y and z stores not only the \"forward\" value of or y**2 but also the computational graph -- the grad_fn that is needed to compute the derivatives (using the chain rule) when tracing back the gradients from z (output) to w (inputs). These grad_fn are essential components to torch.tensors and without them one cannot compute derivatives of complicated functions. However, np.ndarrays do not have this capability at all and they do not have this information. 23 Nov IMPORTANT, THIS IS ALL UNDER THE ASSUMPTION THAT shuffle=False in our DataLoader because if shuffle=True , then we will mess up the train_labels = subset_train_df['label'].values and the predictions since the predictions will be made on SHUFFLED IMAGES!!!!!!! But I managed to fixed it, see point 4 and 5. Take note that softmax_preds = torch.nn.Softmax(dim=1)(input=logits).to('cpu').detach().numpy() may give you an error called Pytorch: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead . Basically, when you are in training phase, where require_grad=True , you need to use tensor.detach().numpy() instead of tensor.numpy() . So it is only needed when you set model.train() mode, and for model.eval() mode I think no need. Refer to point 1, I rewrote it in a more suggestive manner as such: note that when you call torch.nn.Softmax(dim=1)(input=y_preds) you are essentially calling torch.nn.Softmax(dim=1)(input=y_preds).forward(input) . Refer to Source Code . So after getting the softmax predict for the batch size of 4 here, we append them to a list called train_preds=[] , and then we go out of the for loop after each epoch, and then call predictions = np.concatenate(train_preds) whereby you concatenate all the predictions into a 2d-np-array. After which, we get train_labels = subset_train_df['label'].values where these are the groud-truth labels. Subsequently, we compare these ground truth with the predicted values using the function score = get_score(train_labels, predictions.argmax(1)) where predictions.argmax(1) returns an array of indices which tells us in each prediction, which indice has the largest number. This nicely corresponds with our class because our class is from 0 - 4, note in the event if our class is 1-5, a quick fix is to make the class into 0-4. Made changes to the Dataset because I want to return image_id as well, but he called it p so I renamed it to img_id . Successfully solved the problem of shuffle = True in DataLoader . Moving on to valid_fn , we need to know that we need to write model.eval() model.eval() is a kind of switch for some specific layers/parts of the model that behave differently during training and inference (evaluating) time. For example, Dropouts Layers, BatchNorm Layers etc. You need to turn off them during model evaluation, and .eval() will do it for you. In addition, the common practice for evaluating/validation is using torch.no_grad() in pair with model.eval() to turn off gradients computation: # evaluate model: model . eval () with torch . no_grad (): ... out_data = model ( data ) ... BUT, don't forget to turn back to training mode after eval step: # training step ... model . train () ... import torch import torchvision import torchvision.transforms as transforms transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))]) trainset = torchvision . datasets . CIFAR10 ( root = './data' , train = True , download = True , transform = transform ) trainloader = torch . utils . data . DataLoader ( trainset , batch_size = 4 , shuffle = True , num_workers = 2 ) Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value=''))) Extracting ./data/cifar-10-python.tar.gz to ./data # testset = torchvision.datasets.CIFAR10(root='./data', train=False, # download=True, transform=transform) # testloader = torch.utils.data.DataLoader(testset, batch_size=4, # shuffle=False, num_workers=2) # classes = ('plane', 'car', 'bird', 'cat', # 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') import torch.nn as nn import torch.nn.functional as F class Net ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () self . conv1 = nn . Conv2d ( 3 , 6 , 5 ) self . pool = nn . MaxPool2d ( 2 , 2 ) self . conv2 = nn . Conv2d ( 6 , 16 , 5 ) self . fc1 = nn . Linear ( 16 * 5 * 5 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , x ): x = self . pool ( F . relu ( self . conv1 ( x ))) x = self . pool ( F . relu ( self . conv2 ( x ))) x = x . view ( - 1 , 16 * 5 * 5 ) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x net = Net () import torch.optim as optim criterion = nn . CrossEntropyLoss () optimizer = optim . SGD ( net . parameters (), lr = 0.001 , momentum = 0.9 ) for epoch in range ( 2 ): # loop over the dataset multiple times running_loss = 0.0 for i , data in enumerate ( trainloader , 0 ): # get the inputs; data is a list of [inputs, labels] inputs , labels = data # zero the parameter gradients optimizer . zero_grad () # forward + backward + optimize outputs = net ( inputs ) print ( ' \\n\\n The logits are \\n ' , outputs ) loss = criterion ( outputs , labels ) print ( ' \\n\\n The loss is \\n ' , loss ) print ( ' \\n\\n The loss avg is \\n ' , loss . item ()) # so basically loss == loss.item() break loss . backward () optimizer . step () # print statistics running_loss += loss . item () if i % 2000 == 1999 : # print every 2000 mini-batches print ( '[ %d , %5d ] loss: %.3f ' % ( epoch + 1 , i + 1 , running_loss / 2000 )) running_loss = 0.0 print ( 'Finished Training' ) The logits are tensor([[ 0.0878, 0.0786, -0.0898, 0.0612, -0.0639, -0.0529, -0.0973, 0.0149, -0.1152, -0.0745], [ 0.0868, 0.0975, -0.0728, 0.0374, -0.0802, -0.0677, -0.1017, 0.0022, -0.0963, -0.0782], [ 0.0780, 0.0960, -0.0788, 0.0655, -0.0611, -0.0639, -0.0985, 0.0128, -0.1240, -0.0759], [ 0.0604, 0.0839, -0.0712, 0.0642, -0.0808, -0.0629, -0.0866, 0.0055, -0.1157, -0.0830]], grad_fn=<AddmmBackward>) The loss is tensor(2.2469, grad_fn=<NllLossBackward>) The loss avg is 2.246931552886963 The logits are tensor([[ 0.0920, 0.0988, -0.0781, 0.0741, -0.0729, -0.0437, -0.0753, 0.0017, -0.0969, -0.0751], [ 0.0822, 0.0904, -0.0852, 0.0491, -0.0769, -0.0646, -0.0970, 0.0130, -0.1071, -0.0702], [ 0.0768, 0.0865, -0.0799, 0.0551, -0.0681, -0.0570, -0.0914, 0.0002, -0.1133, -0.0707], [ 0.0814, 0.0849, -0.0938, 0.0467, -0.0732, -0.0569, -0.0966, 0.0104, -0.1145, -0.0769]], grad_fn=<AddmmBackward>) The loss is tensor(2.2669, grad_fn=<NllLossBackward>) The loss avg is 2.266934871673584 Finished Training from __future__ import print_function , division import torch import torch.nn as nn import torch.optim as optim from torch.optim import lr_scheduler import numpy as np import torchvision from torchvision import datasets , models , transforms import matplotlib.pyplot as plt import time import os import copy # Data augmentation and normalization for training # Just normalization for validation data_transforms = { 'train' : transforms . Compose ([ transforms . RandomResizedCrop ( 224 ), transforms . RandomHorizontalFlip (), transforms . ToTensor (), transforms . Normalize ([ 0.485 , 0.456 , 0.406 ], [ 0.229 , 0.224 , 0.225 ]) ]), 'val' : transforms . Compose ([ transforms . Resize ( 256 ), transforms . CenterCrop ( 224 ), transforms . ToTensor (), transforms . Normalize ([ 0.485 , 0.456 , 0.406 ], [ 0.229 , 0.224 , 0.225 ]) ]), } data_dir = './data/hymenoptera_data' image_datasets = { x : datasets . ImageFolder ( os . path . join ( data_dir , x ), data_transforms [ x ]) for x in [ 'train' , 'val' ]} dataloaders = { x : torch . utils . data . DataLoader ( image_datasets [ x ], batch_size = 4 , shuffle = True , num_workers = 4 ) for x in [ 'train' , 'val' ]} dataset_sizes = { x : len ( image_datasets [ x ]) for x in [ 'train' , 'val' ]} class_names = image_datasets [ 'train' ] . classes device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) --------------------------------------------------------------------------- FileNotFoundError Traceback (most recent call last) <ipython-input-24-1f4b70a4f27c> in <module> 19 image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), 20 data_transforms[x]) ---> 21 for x in ['train', 'val']} 22 dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, 23 shuffle=True, num_workers=4) <ipython-input-24-1f4b70a4f27c> in <dictcomp>(.0) 19 image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), 20 data_transforms[x]) ---> 21 for x in ['train', 'val']} 22 dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, 23 shuffle=True, num_workers=4) /opt/conda/lib/python3.7/site-packages/torchvision/datasets/folder.py in __init__(self, root, transform, target_transform, loader, is_valid_file) 227 transform=transform, 228 target_transform=target_transform, --> 229 is_valid_file=is_valid_file) 230 self.imgs = self.samples /opt/conda/lib/python3.7/site-packages/torchvision/datasets/folder.py in __init__(self, root, loader, extensions, transform, target_transform, is_valid_file) 106 super(DatasetFolder, self).__init__(root, transform=transform, 107 target_transform=target_transform) --> 108 classes, class_to_idx = self._find_classes(self.root) 109 samples = make_dataset(self.root, class_to_idx, extensions, is_valid_file) 110 if len(samples) == 0: /opt/conda/lib/python3.7/site-packages/torchvision/datasets/folder.py in _find_classes(self, dir) 135 No class is a subdirectory of another. 136 \"\"\" --> 137 classes = [d.name for d in os.scandir(dir) if d.is_dir()] 138 classes.sort() 139 class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)} FileNotFoundError: [Errno 2] No such file or directory: './data/hymenoptera_data/train' print ( 'The number of images in the training set is' , dataset_sizes [ 'train' ]) --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-25-281ff82a30a3> in <module> ----> 1 print('The number of images in the training set is', dataset_sizes['train']) NameError: name 'dataset_sizes' is not defined def train_model ( model , criterion , optimizer , scheduler , num_epochs = 25 ): since = time . time () best_model_wts = copy . deepcopy ( model . state_dict ()) best_acc = 0.0 for epoch in range ( num_epochs ): print ( 'Epoch {} / {} ' . format ( epoch , num_epochs - 1 )) print ( '-' * 10 ) # Each epoch has a training and validation phase for phase in [ 'train' , 'val' ]: if phase == 'train' : model . train () # Set model to training mode else : model . eval () # Set model to evaluate mode running_loss = 0.0 running_corrects = 0 # Iterate over data. for inputs , labels in dataloaders [ phase ]: inputs = inputs . to ( device ) labels = labels . to ( device ) # zero the parameter gradients optimizer . zero_grad () # forward # track history if only in train with torch . set_grad_enabled ( phase == 'train' ): outputs = model ( inputs ) _ , preds = torch . max ( outputs , 1 ) loss = criterion ( outputs , labels ) # backward + optimize only if in training phase if phase == 'train' : loss . backward () optimizer . step () # statistics print ( inputs . size ( 0 )) running_loss += loss . item () * inputs . size ( 0 ) running_corrects += torch . sum ( preds == labels . data ) if phase == 'train' : scheduler . step () epoch_loss = running_loss / dataset_sizes [ phase ] epoch_acc = running_corrects . double () / dataset_sizes [ phase ] print ( ' {} Loss: {:.4f} Acc: {:.4f} ' . format ( phase , epoch_loss , epoch_acc )) # deep copy the model if phase == 'val' and epoch_acc > best_acc : best_acc = epoch_acc best_model_wts = copy . deepcopy ( model . state_dict ()) print () time_elapsed = time . time () - since print ( 'Training complete in {:.0f} m {:.0f} s' . format ( time_elapsed // 60 , time_elapsed % 60 )) print ( 'Best val Acc: {:4f} ' . format ( best_acc )) # load best model weights model . load_state_dict ( best_model_wts ) return model model_ft = models . resnet18 ( pretrained = True ) num_ftrs = model_ft . fc . in_features # Here the size of each output sample is set to 2. # Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)). model_ft . fc = nn . Linear ( num_ftrs , 2 ) model_ft = model_ft . to ( device ) criterion = nn . CrossEntropyLoss () # Observe that all parameters are being optimized optimizer_ft = optim . SGD ( model_ft . parameters (), lr = 0.001 , momentum = 0.9 ) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler . StepLR ( optimizer_ft , step_size = 7 , gamma = 0.1 ) Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value=''))) model_ft = train_model ( model_ft , criterion , optimizer_ft , exp_lr_scheduler , num_epochs = 25 ) Epoch 0/24 ---------- --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-28-cc88ea5f8bd3> in <module> 1 model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, ----> 2 num_epochs=25) <ipython-input-26-9af9c454ae56> in train_model(model, criterion, optimizer, scheduler, num_epochs) 20 21 # Iterate over data. ---> 22 for inputs, labels in dataloaders[phase]: 23 inputs = inputs.to(device) 24 labels = labels.to(device) NameError: name 'dataloaders' is not defined","title":"Deep Learning Notes (Important)"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#config","text":"class GlobalConfig : seed = 1930 num_classes = 11 class_list = [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] batch_size = 16 n_epochs = 15 tensor_size = ( 8 , 3 , 64 , 64 ) # unpack the key dict scheduler = 'CosineAnnealingWarmRestarts' scheduler_params = { 'StepLR' : { 'step_size' : 2 , 'gamma' : 0.3 , 'last_epoch' : - 1 , 'verbose' : True }, 'ReduceLROnPlateau' : { 'mode' : 'max' , 'factor' : 0.5 , 'patience' : 0 , 'threshold' : 0.0001 , 'threshold_mode' : 'rel' , 'cooldown' : 0 , 'min_lr' : 1e-5 , 'eps' : 1e-08 , 'verbose' : True }, 'CosineAnnealingWarmRestarts' : { 'T_0' : 10 , 'T_mult' : 1 , 'eta_min' : 1e-6 , 'last_epoch' : - 1 , 'verbose' : True }} # do scheduler.step after optimizer.step train_step_scheduler = False val_step_scheduler = True # optimizer optimizer = 'AdamW' optimizer_params = { 'AdamW' :{ 'lr' : 1e-3 , 'betas' :( 0.9 , 0.999 ), 'eps' : 1e-08 , 'weight_decay' : 1e-6 , 'amsgrad' : False }, 'Adam' :{ 'lr' : 1e-4 , 'betas' :( 0.9 , 0.999 ), 'eps' : 1e-08 , 'weight_decay' : 1e-6 , 'amsgrad' : False },} # criterion criterion = 'BCEWithLogitsLoss' criterion_val = 'BCEWithLogitsLoss' criterion_params = { 'BCEWithLogitsLoss' : { 'weight' : None , 'size_average' : None , 'reduce' : None , 'reduction' : 'mean' , 'pos_weight' : None }, 'CrossEntropyLoss' : { 'weight' : None , 'size_average' : None , 'ignore_index' : - 100 , 'reduce' : None , 'reduction' : 'mean' }, 'LabelSmoothingLoss' : { 'classes' : 2 , 'smoothing' : 0.05 , 'dim' : - 1 }, 'FocalCosineLoss' : { 'alpha' : 1 , 'gamma' : 2 , 'xent' : 0.1 }} image_col_name = 'image_id' class_col_name = 'label' paths = { 'train_path' : '/content/kaggle/train_images_jpeg/' , 'test_path' : '../input/siim-isic-melanoma-classification/jpeg/test' , 'csv_path' : '/content/drive/My Drive/Cassava/input/cassava-leaf-disease-classification/train.csv' , 'log_path' : './log.text' , 'save_path' : '/content/drive/My Drive/Cassava/weights/tf_effnet_b4_ns/18-Jan-V1' , 'model_weight_path_folder' : '/content/drive/My Drive/pretrained-effnet-weights' , 'image_path' : '/content/drive/My Drive/deep-learning-notes/notebooks/images' } model_name = 'tf_efficientnet_b5_ns' device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) config = GlobalConfig","title":"Config"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#utils","text":"def seed_all ( seed : int = 1930 ): print ( \"I love my Grandpa so the seed number is {} \" . format ( seed )) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) # set PYTHONHASHSEED env var at fixed value torch . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) torch . cuda . manual_seed ( seed ) # pytorch (both CPU and CUDA) np . random . seed ( seed ) # for numpy pseudo-random generator random . seed ( seed ) # set fixed value for python built-in pseudo-random generator torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = False torch . backends . cudnn . enabled = False def seed_worker ( _worker_id ): worker_seed = torch . initial_seed () % 2 ** 32 np . random . seed ( worker_seed ) random . seed ( worker_seed ) ###################################################################### seed_all ( config . seed ) device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) ###################################################################### I love my Grandpa so the seed number is 1930 def make_rand_tensors ( config ): # Create a batch size of 8 random images with 3 channels and 64x64 -> [N,C,W,H] rand_tensor = torch . ones ( config . tensor_size , dtype = torch . float ) . to ( config . device ) return rand_tensor def torchsummary_wrapper ( model , image_size : Tuple ): model_summary = summary ( model , image_size ) return model_summary def display_image ( config , page_num ): image = os . path . join ( config . paths [ 'hongnan_notes' ], page_num ) return Image ( image )","title":"Utils"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#learning-the-syntax-of-pytorch","text":"torch.view() reference","title":"Learning the Syntax of PyTorch"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#torchnn-vs-torchnnfunctional","text":"torch.nn.Relu vs torch.nn.functional.Relu using-dropout-in-pytorch-nn-dropout-vs-f-dropout","title":"torch.nn vs torch.nn.functional"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#building-neural-networks","text":"","title":"Building Neural Networks"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#convolutional-layers","text":"import torch import torch.nn as nn import torch.optim as optim from torch.optim import lr_scheduler import numpy as np import torchvision from torchvision import datasets , models , transforms import matplotlib.pyplot as plt import time import os import copy plt . ion () # interactive mode ! unzip '/content/drive/My Drive/deep-learning-notes/notebooks/hymenoptera_data.zip' - d '/content/' Archive: /content/drive/My Drive/deep-learning-notes/notebooks/hymenoptera_data.zip creating: /content/hymenoptera_data/ creating: /content/hymenoptera_data/train/ creating: /content/hymenoptera_data/train/ants/ inflating: /content/hymenoptera_data/train/ants/0013035.jpg inflating: /content/hymenoptera_data/train/ants/1030023514_aad5c608f9.jpg inflating: /content/hymenoptera_data/train/ants/1095476100_3906d8afde.jpg inflating: /content/hymenoptera_data/train/ants/1099452230_d1949d3250.jpg inflating: /content/hymenoptera_data/train/ants/116570827_e9c126745d.jpg inflating: /content/hymenoptera_data/train/ants/1225872729_6f0856588f.jpg inflating: /content/hymenoptera_data/train/ants/1262877379_64fcada201.jpg inflating: /content/hymenoptera_data/train/ants/1269756697_0bce92cdab.jpg inflating: /content/hymenoptera_data/train/ants/1286984635_5119e80de1.jpg inflating: /content/hymenoptera_data/train/ants/132478121_2a430adea2.jpg inflating: /content/hymenoptera_data/train/ants/1360291657_dc248c5eea.jpg inflating: /content/hymenoptera_data/train/ants/1368913450_e146e2fb6d.jpg inflating: /content/hymenoptera_data/train/ants/1473187633_63ccaacea6.jpg inflating: /content/hymenoptera_data/train/ants/148715752_302c84f5a4.jpg inflating: /content/hymenoptera_data/train/ants/1489674356_09d48dde0a.jpg inflating: /content/hymenoptera_data/train/ants/149244013_c529578289.jpg inflating: /content/hymenoptera_data/train/ants/150801003_3390b73135.jpg inflating: /content/hymenoptera_data/train/ants/150801171_cd86f17ed8.jpg inflating: /content/hymenoptera_data/train/ants/154124431_65460430f2.jpg inflating: /content/hymenoptera_data/train/ants/162603798_40b51f1654.jpg inflating: /content/hymenoptera_data/train/ants/1660097129_384bf54490.jpg inflating: /content/hymenoptera_data/train/ants/167890289_dd5ba923f3.jpg inflating: /content/hymenoptera_data/train/ants/1693954099_46d4c20605.jpg inflating: /content/hymenoptera_data/train/ants/175998972.jpg inflating: /content/hymenoptera_data/train/ants/178538489_bec7649292.jpg inflating: /content/hymenoptera_data/train/ants/1804095607_0341701e1c.jpg inflating: /content/hymenoptera_data/train/ants/1808777855_2a895621d7.jpg inflating: /content/hymenoptera_data/train/ants/188552436_605cc9b36b.jpg inflating: /content/hymenoptera_data/train/ants/1917341202_d00a7f9af5.jpg inflating: /content/hymenoptera_data/train/ants/1924473702_daa9aacdbe.jpg inflating: /content/hymenoptera_data/train/ants/196057951_63bf063b92.jpg inflating: /content/hymenoptera_data/train/ants/196757565_326437f5fe.jpg inflating: /content/hymenoptera_data/train/ants/201558278_fe4caecc76.jpg inflating: /content/hymenoptera_data/train/ants/201790779_527f4c0168.jpg inflating: /content/hymenoptera_data/train/ants/2019439677_2db655d361.jpg inflating: /content/hymenoptera_data/train/ants/207947948_3ab29d7207.jpg inflating: /content/hymenoptera_data/train/ants/20935278_9190345f6b.jpg inflating: /content/hymenoptera_data/train/ants/224655713_3956f7d39a.jpg inflating: /content/hymenoptera_data/train/ants/2265824718_2c96f485da.jpg inflating: /content/hymenoptera_data/train/ants/2265825502_fff99cfd2d.jpg inflating: /content/hymenoptera_data/train/ants/226951206_d6bf946504.jpg inflating: /content/hymenoptera_data/train/ants/2278278459_6b99605e50.jpg inflating: /content/hymenoptera_data/train/ants/2288450226_a6e96e8fdf.jpg inflating: /content/hymenoptera_data/train/ants/2288481644_83ff7e4572.jpg inflating: /content/hymenoptera_data/train/ants/2292213964_ca51ce4bef.jpg inflating: /content/hymenoptera_data/train/ants/24335309_c5ea483bb8.jpg inflating: /content/hymenoptera_data/train/ants/245647475_9523dfd13e.jpg inflating: /content/hymenoptera_data/train/ants/255434217_1b2b3fe0a4.jpg inflating: /content/hymenoptera_data/train/ants/258217966_d9d90d18d3.jpg inflating: /content/hymenoptera_data/train/ants/275429470_b2d7d9290b.jpg inflating: /content/hymenoptera_data/train/ants/28847243_e79fe052cd.jpg inflating: /content/hymenoptera_data/train/ants/318052216_84dff3f98a.jpg inflating: /content/hymenoptera_data/train/ants/334167043_cbd1adaeb9.jpg inflating: /content/hymenoptera_data/train/ants/339670531_94b75ae47a.jpg inflating: /content/hymenoptera_data/train/ants/342438950_a3da61deab.jpg inflating: /content/hymenoptera_data/train/ants/36439863_0bec9f554f.jpg inflating: /content/hymenoptera_data/train/ants/374435068_7eee412ec4.jpg inflating: /content/hymenoptera_data/train/ants/382971067_0bfd33afe0.jpg inflating: /content/hymenoptera_data/train/ants/384191229_5779cf591b.jpg inflating: /content/hymenoptera_data/train/ants/386190770_672743c9a7.jpg inflating: /content/hymenoptera_data/train/ants/392382602_1b7bed32fa.jpg inflating: /content/hymenoptera_data/train/ants/403746349_71384f5b58.jpg inflating: /content/hymenoptera_data/train/ants/408393566_b5b694119b.jpg inflating: /content/hymenoptera_data/train/ants/424119020_6d57481dab.jpg inflating: /content/hymenoptera_data/train/ants/424873399_47658a91fb.jpg inflating: /content/hymenoptera_data/train/ants/450057712_771b3bfc91.jpg inflating: /content/hymenoptera_data/train/ants/45472593_bfd624f8dc.jpg inflating: /content/hymenoptera_data/train/ants/459694881_ac657d3187.jpg inflating: /content/hymenoptera_data/train/ants/460372577_f2f6a8c9fc.jpg inflating: /content/hymenoptera_data/train/ants/460874319_0a45ab4d05.jpg inflating: /content/hymenoptera_data/train/ants/466430434_4000737de9.jpg inflating: /content/hymenoptera_data/train/ants/470127037_513711fd21.jpg inflating: /content/hymenoptera_data/train/ants/474806473_ca6caab245.jpg inflating: /content/hymenoptera_data/train/ants/475961153_b8c13fd405.jpg inflating: /content/hymenoptera_data/train/ants/484293231_e53cfc0c89.jpg inflating: /content/hymenoptera_data/train/ants/49375974_e28ba6f17e.jpg inflating: /content/hymenoptera_data/train/ants/506249802_207cd979b4.jpg inflating: /content/hymenoptera_data/train/ants/506249836_717b73f540.jpg inflating: /content/hymenoptera_data/train/ants/512164029_c0a66b8498.jpg inflating: /content/hymenoptera_data/train/ants/512863248_43c8ce579b.jpg inflating: /content/hymenoptera_data/train/ants/518773929_734dbc5ff4.jpg inflating: /content/hymenoptera_data/train/ants/522163566_fec115ca66.jpg inflating: /content/hymenoptera_data/train/ants/522415432_2218f34bf8.jpg inflating: /content/hymenoptera_data/train/ants/531979952_bde12b3bc0.jpg inflating: /content/hymenoptera_data/train/ants/533848102_70a85ad6dd.jpg inflating: /content/hymenoptera_data/train/ants/535522953_308353a07c.jpg inflating: /content/hymenoptera_data/train/ants/540889389_48bb588b21.jpg inflating: /content/hymenoptera_data/train/ants/541630764_dbd285d63c.jpg inflating: /content/hymenoptera_data/train/ants/543417860_b14237f569.jpg inflating: /content/hymenoptera_data/train/ants/560966032_988f4d7bc4.jpg inflating: /content/hymenoptera_data/train/ants/5650366_e22b7e1065.jpg inflating: /content/hymenoptera_data/train/ants/6240329_72c01e663e.jpg inflating: /content/hymenoptera_data/train/ants/6240338_93729615ec.jpg inflating: /content/hymenoptera_data/train/ants/649026570_e58656104b.jpg inflating: /content/hymenoptera_data/train/ants/662541407_ff8db781e7.jpg inflating: /content/hymenoptera_data/train/ants/67270775_e9fdf77e9d.jpg inflating: /content/hymenoptera_data/train/ants/6743948_2b8c096dda.jpg inflating: /content/hymenoptera_data/train/ants/684133190_35b62c0c1d.jpg inflating: /content/hymenoptera_data/train/ants/69639610_95e0de17aa.jpg inflating: /content/hymenoptera_data/train/ants/707895295_009cf23188.jpg inflating: /content/hymenoptera_data/train/ants/7759525_1363d24e88.jpg inflating: /content/hymenoptera_data/train/ants/795000156_a9900a4a71.jpg inflating: /content/hymenoptera_data/train/ants/822537660_caf4ba5514.jpg inflating: /content/hymenoptera_data/train/ants/82852639_52b7f7f5e3.jpg inflating: /content/hymenoptera_data/train/ants/841049277_b28e58ad05.jpg inflating: /content/hymenoptera_data/train/ants/886401651_f878e888cd.jpg inflating: /content/hymenoptera_data/train/ants/892108839_f1aad4ca46.jpg inflating: /content/hymenoptera_data/train/ants/938946700_ca1c669085.jpg inflating: /content/hymenoptera_data/train/ants/957233405_25c1d1187b.jpg inflating: /content/hymenoptera_data/train/ants/9715481_b3cb4114ff.jpg inflating: /content/hymenoptera_data/train/ants/998118368_6ac1d91f81.jpg inflating: /content/hymenoptera_data/train/ants/ant photos.jpg inflating: /content/hymenoptera_data/train/ants/Ant_1.jpg inflating: /content/hymenoptera_data/train/ants/army-ants-red-picture.jpg inflating: /content/hymenoptera_data/train/ants/formica.jpeg inflating: /content/hymenoptera_data/train/ants/hormiga_co_por.jpg inflating: /content/hymenoptera_data/train/ants/imageNotFound.gif inflating: /content/hymenoptera_data/train/ants/kurokusa.jpg inflating: /content/hymenoptera_data/train/ants/MehdiabadiAnt2_600.jpg inflating: /content/hymenoptera_data/train/ants/Nepenthes_rafflesiana_ant.jpg inflating: /content/hymenoptera_data/train/ants/swiss-army-ant.jpg inflating: /content/hymenoptera_data/train/ants/termite-vs-ant.jpg inflating: /content/hymenoptera_data/train/ants/trap-jaw-ant-insect-bg.jpg inflating: /content/hymenoptera_data/train/ants/VietnameseAntMimicSpider.jpg creating: /content/hymenoptera_data/train/bees/ inflating: /content/hymenoptera_data/train/bees/1092977343_cb42b38d62.jpg inflating: /content/hymenoptera_data/train/bees/1093831624_fb5fbe2308.jpg inflating: /content/hymenoptera_data/train/bees/1097045929_1753d1c765.jpg inflating: /content/hymenoptera_data/train/bees/1232245714_f862fbe385.jpg inflating: /content/hymenoptera_data/train/bees/129236073_0985e91c7d.jpg inflating: /content/hymenoptera_data/train/bees/1295655112_7813f37d21.jpg inflating: /content/hymenoptera_data/train/bees/132511197_0b86ad0fff.jpg inflating: /content/hymenoptera_data/train/bees/132826773_dbbcb117b9.jpg inflating: /content/hymenoptera_data/train/bees/150013791_969d9a968b.jpg inflating: /content/hymenoptera_data/train/bees/1508176360_2972117c9d.jpg inflating: /content/hymenoptera_data/train/bees/154600396_53e1252e52.jpg inflating: /content/hymenoptera_data/train/bees/16838648_415acd9e3f.jpg inflating: /content/hymenoptera_data/train/bees/1691282715_0addfdf5e8.jpg inflating: /content/hymenoptera_data/train/bees/17209602_fe5a5a746f.jpg inflating: /content/hymenoptera_data/train/bees/174142798_e5ad6d76e0.jpg inflating: /content/hymenoptera_data/train/bees/1799726602_8580867f71.jpg inflating: /content/hymenoptera_data/train/bees/1807583459_4fe92b3133.jpg inflating: /content/hymenoptera_data/train/bees/196430254_46bd129ae7.jpg inflating: /content/hymenoptera_data/train/bees/196658222_3fffd79c67.jpg inflating: /content/hymenoptera_data/train/bees/198508668_97d818b6c4.jpg inflating: /content/hymenoptera_data/train/bees/2031225713_50ed499635.jpg inflating: /content/hymenoptera_data/train/bees/2037437624_2d7bce461f.jpg inflating: /content/hymenoptera_data/train/bees/2053200300_8911ef438a.jpg inflating: /content/hymenoptera_data/train/bees/205835650_e6f2614bee.jpg inflating: /content/hymenoptera_data/train/bees/208702903_42fb4d9748.jpg inflating: /content/hymenoptera_data/train/bees/21399619_3e61e5bb6f.jpg inflating: /content/hymenoptera_data/train/bees/2227611847_ec72d40403.jpg inflating: /content/hymenoptera_data/train/bees/2321139806_d73d899e66.jpg inflating: /content/hymenoptera_data/train/bees/2330918208_8074770c20.jpg inflating: /content/hymenoptera_data/train/bees/2345177635_caf07159b3.jpg inflating: /content/hymenoptera_data/train/bees/2358061370_9daabbd9ac.jpg inflating: /content/hymenoptera_data/train/bees/2364597044_3c3e3fc391.jpg inflating: /content/hymenoptera_data/train/bees/2384149906_2cd8b0b699.jpg inflating: /content/hymenoptera_data/train/bees/2397446847_04ef3cd3e1.jpg inflating: /content/hymenoptera_data/train/bees/2405441001_b06c36fa72.jpg inflating: /content/hymenoptera_data/train/bees/2445215254_51698ff797.jpg inflating: /content/hymenoptera_data/train/bees/2452236943_255bfd9e58.jpg inflating: /content/hymenoptera_data/train/bees/2467959963_a7831e9ff0.jpg inflating: /content/hymenoptera_data/train/bees/2470492904_837e97800d.jpg inflating: /content/hymenoptera_data/train/bees/2477324698_3d4b1b1cab.jpg inflating: /content/hymenoptera_data/train/bees/2477349551_e75c97cf4d.jpg inflating: /content/hymenoptera_data/train/bees/2486729079_62df0920be.jpg inflating: /content/hymenoptera_data/train/bees/2486746709_c43cec0e42.jpg inflating: /content/hymenoptera_data/train/bees/2493379287_4100e1dacc.jpg inflating: /content/hymenoptera_data/train/bees/2495722465_879acf9d85.jpg inflating: /content/hymenoptera_data/train/bees/2528444139_fa728b0f5b.jpg inflating: /content/hymenoptera_data/train/bees/2538361678_9da84b77e3.jpg inflating: /content/hymenoptera_data/train/bees/2551813042_8a070aeb2b.jpg inflating: /content/hymenoptera_data/train/bees/2580598377_a4caecdb54.jpg inflating: /content/hymenoptera_data/train/bees/2601176055_8464e6aa71.jpg inflating: /content/hymenoptera_data/train/bees/2610833167_79bf0bcae5.jpg inflating: /content/hymenoptera_data/train/bees/2610838525_fe8e3cae47.jpg inflating: /content/hymenoptera_data/train/bees/2617161745_fa3ebe85b4.jpg inflating: /content/hymenoptera_data/train/bees/2625499656_e3415e374d.jpg inflating: /content/hymenoptera_data/train/bees/2634617358_f32fd16bea.jpg inflating: /content/hymenoptera_data/train/bees/2638074627_6b3ae746a0.jpg inflating: /content/hymenoptera_data/train/bees/2645107662_b73a8595cc.jpg inflating: /content/hymenoptera_data/train/bees/2651621464_a2fa8722eb.jpg inflating: /content/hymenoptera_data/train/bees/2652877533_a564830cbf.jpg inflating: /content/hymenoptera_data/train/bees/266644509_d30bb16a1b.jpg inflating: /content/hymenoptera_data/train/bees/2683605182_9d2a0c66cf.jpg inflating: /content/hymenoptera_data/train/bees/2704348794_eb5d5178c2.jpg inflating: /content/hymenoptera_data/train/bees/2707440199_cd170bd512.jpg inflating: /content/hymenoptera_data/train/bees/2710368626_cb42882dc8.jpg inflating: /content/hymenoptera_data/train/bees/2722592222_258d473e17.jpg inflating: /content/hymenoptera_data/train/bees/2728759455_ce9bb8cd7a.jpg inflating: /content/hymenoptera_data/train/bees/2756397428_1d82a08807.jpg inflating: /content/hymenoptera_data/train/bees/2765347790_da6cf6cb40.jpg inflating: /content/hymenoptera_data/train/bees/2781170484_5d61835d63.jpg inflating: /content/hymenoptera_data/train/bees/279113587_b4843db199.jpg inflating: /content/hymenoptera_data/train/bees/2792000093_e8ae0718cf.jpg inflating: /content/hymenoptera_data/train/bees/2801728106_833798c909.jpg inflating: /content/hymenoptera_data/train/bees/2822388965_f6dca2a275.jpg inflating: /content/hymenoptera_data/train/bees/2861002136_52c7c6f708.jpg inflating: /content/hymenoptera_data/train/bees/2908916142_a7ac8b57a8.jpg inflating: /content/hymenoptera_data/train/bees/29494643_e3410f0d37.jpg inflating: /content/hymenoptera_data/train/bees/2959730355_416a18c63c.jpg inflating: /content/hymenoptera_data/train/bees/2962405283_22718d9617.jpg inflating: /content/hymenoptera_data/train/bees/3006264892_30e9cced70.jpg inflating: /content/hymenoptera_data/train/bees/3030189811_01d095b793.jpg inflating: /content/hymenoptera_data/train/bees/3030772428_8578335616.jpg inflating: /content/hymenoptera_data/train/bees/3044402684_3853071a87.jpg inflating: /content/hymenoptera_data/train/bees/3074585407_9854eb3153.jpg inflating: /content/hymenoptera_data/train/bees/3079610310_ac2d0ae7bc.jpg inflating: /content/hymenoptera_data/train/bees/3090975720_71f12e6de4.jpg inflating: /content/hymenoptera_data/train/bees/3100226504_c0d4f1e3f1.jpg inflating: /content/hymenoptera_data/train/bees/342758693_c56b89b6b6.jpg inflating: /content/hymenoptera_data/train/bees/354167719_22dca13752.jpg inflating: /content/hymenoptera_data/train/bees/359928878_b3b418c728.jpg inflating: /content/hymenoptera_data/train/bees/365759866_b15700c59b.jpg inflating: /content/hymenoptera_data/train/bees/36900412_92b81831ad.jpg inflating: /content/hymenoptera_data/train/bees/39672681_1302d204d1.jpg inflating: /content/hymenoptera_data/train/bees/39747887_42df2855ee.jpg inflating: /content/hymenoptera_data/train/bees/421515404_e87569fd8b.jpg inflating: /content/hymenoptera_data/train/bees/444532809_9e931e2279.jpg inflating: /content/hymenoptera_data/train/bees/446296270_d9e8b93ecf.jpg inflating: /content/hymenoptera_data/train/bees/452462677_7be43af8ff.jpg inflating: /content/hymenoptera_data/train/bees/452462695_40a4e5b559.jpg inflating: /content/hymenoptera_data/train/bees/457457145_5f86eb7e9c.jpg inflating: /content/hymenoptera_data/train/bees/465133211_80e0c27f60.jpg inflating: /content/hymenoptera_data/train/bees/469333327_358ba8fe8a.jpg inflating: /content/hymenoptera_data/train/bees/472288710_2abee16fa0.jpg inflating: /content/hymenoptera_data/train/bees/473618094_8ffdcab215.jpg inflating: /content/hymenoptera_data/train/bees/476347960_52edd72b06.jpg inflating: /content/hymenoptera_data/train/bees/478701318_bbd5e557b8.jpg inflating: /content/hymenoptera_data/train/bees/507288830_f46e8d4cb2.jpg inflating: /content/hymenoptera_data/train/bees/509247772_2db2d01374.jpg inflating: /content/hymenoptera_data/train/bees/513545352_fd3e7c7c5d.jpg inflating: /content/hymenoptera_data/train/bees/522104315_5d3cb2758e.jpg inflating: /content/hymenoptera_data/train/bees/537309131_532bfa59ea.jpg inflating: /content/hymenoptera_data/train/bees/586041248_3032e277a9.jpg inflating: /content/hymenoptera_data/train/bees/760526046_547e8b381f.jpg inflating: /content/hymenoptera_data/train/bees/760568592_45a52c847f.jpg inflating: /content/hymenoptera_data/train/bees/774440991_63a4aa0cbe.jpg inflating: /content/hymenoptera_data/train/bees/85112639_6e860b0469.jpg inflating: /content/hymenoptera_data/train/bees/873076652_eb098dab2d.jpg inflating: /content/hymenoptera_data/train/bees/90179376_abc234e5f4.jpg inflating: /content/hymenoptera_data/train/bees/92663402_37f379e57a.jpg inflating: /content/hymenoptera_data/train/bees/95238259_98470c5b10.jpg inflating: /content/hymenoptera_data/train/bees/969455125_58c797ef17.jpg inflating: /content/hymenoptera_data/train/bees/98391118_bdb1e80cce.jpg creating: /content/hymenoptera_data/val/ creating: /content/hymenoptera_data/val/ants/ inflating: /content/hymenoptera_data/val/ants/10308379_1b6c72e180.jpg inflating: /content/hymenoptera_data/val/ants/1053149811_f62a3410d3.jpg inflating: /content/hymenoptera_data/val/ants/1073564163_225a64f170.jpg inflating: /content/hymenoptera_data/val/ants/1119630822_cd325ea21a.jpg inflating: /content/hymenoptera_data/val/ants/1124525276_816a07c17f.jpg inflating: /content/hymenoptera_data/val/ants/11381045_b352a47d8c.jpg inflating: /content/hymenoptera_data/val/ants/119785936_dd428e40c3.jpg inflating: /content/hymenoptera_data/val/ants/1247887232_edcb61246c.jpg inflating: /content/hymenoptera_data/val/ants/1262751255_c56c042b7b.jpg inflating: /content/hymenoptera_data/val/ants/1337725712_2eb53cd742.jpg inflating: /content/hymenoptera_data/val/ants/1358854066_5ad8015f7f.jpg inflating: /content/hymenoptera_data/val/ants/1440002809_b268d9a66a.jpg inflating: /content/hymenoptera_data/val/ants/147542264_79506478c2.jpg inflating: /content/hymenoptera_data/val/ants/152286280_411648ec27.jpg inflating: /content/hymenoptera_data/val/ants/153320619_2aeb5fa0ee.jpg inflating: /content/hymenoptera_data/val/ants/153783656_85f9c3ac70.jpg inflating: /content/hymenoptera_data/val/ants/157401988_d0564a9d02.jpg inflating: /content/hymenoptera_data/val/ants/159515240_d5981e20d1.jpg inflating: /content/hymenoptera_data/val/ants/161076144_124db762d6.jpg inflating: /content/hymenoptera_data/val/ants/161292361_c16e0bf57a.jpg inflating: /content/hymenoptera_data/val/ants/170652283_ecdaff5d1a.jpg inflating: /content/hymenoptera_data/val/ants/17081114_79b9a27724.jpg inflating: /content/hymenoptera_data/val/ants/172772109_d0a8e15fb0.jpg inflating: /content/hymenoptera_data/val/ants/1743840368_b5ccda82b7.jpg inflating: /content/hymenoptera_data/val/ants/181942028_961261ef48.jpg inflating: /content/hymenoptera_data/val/ants/183260961_64ab754c97.jpg inflating: /content/hymenoptera_data/val/ants/2039585088_c6f47c592e.jpg inflating: /content/hymenoptera_data/val/ants/205398178_c395c5e460.jpg inflating: /content/hymenoptera_data/val/ants/208072188_f293096296.jpg inflating: /content/hymenoptera_data/val/ants/209615353_eeb38ba204.jpg inflating: /content/hymenoptera_data/val/ants/2104709400_8831b4fc6f.jpg inflating: /content/hymenoptera_data/val/ants/212100470_b485e7b7b9.jpg inflating: /content/hymenoptera_data/val/ants/2127908701_d49dc83c97.jpg inflating: /content/hymenoptera_data/val/ants/2191997003_379df31291.jpg inflating: /content/hymenoptera_data/val/ants/2211974567_ee4606b493.jpg inflating: /content/hymenoptera_data/val/ants/2219621907_47bc7cc6b0.jpg inflating: /content/hymenoptera_data/val/ants/2238242353_52c82441df.jpg inflating: /content/hymenoptera_data/val/ants/2255445811_dabcdf7258.jpg inflating: /content/hymenoptera_data/val/ants/239161491_86ac23b0a3.jpg inflating: /content/hymenoptera_data/val/ants/263615709_cfb28f6b8e.jpg inflating: /content/hymenoptera_data/val/ants/308196310_1db5ffa01b.jpg inflating: /content/hymenoptera_data/val/ants/319494379_648fb5a1c6.jpg inflating: /content/hymenoptera_data/val/ants/35558229_1fa4608a7a.jpg inflating: /content/hymenoptera_data/val/ants/412436937_4c2378efc2.jpg inflating: /content/hymenoptera_data/val/ants/436944325_d4925a38c7.jpg inflating: /content/hymenoptera_data/val/ants/445356866_6cb3289067.jpg inflating: /content/hymenoptera_data/val/ants/459442412_412fecf3fe.jpg inflating: /content/hymenoptera_data/val/ants/470127071_8b8ee2bd74.jpg inflating: /content/hymenoptera_data/val/ants/477437164_bc3e6e594a.jpg inflating: /content/hymenoptera_data/val/ants/488272201_c5aa281348.jpg inflating: /content/hymenoptera_data/val/ants/502717153_3e4865621a.jpg inflating: /content/hymenoptera_data/val/ants/518746016_bcc28f8b5b.jpg inflating: /content/hymenoptera_data/val/ants/540543309_ddbb193ee5.jpg inflating: /content/hymenoptera_data/val/ants/562589509_7e55469b97.jpg inflating: /content/hymenoptera_data/val/ants/57264437_a19006872f.jpg inflating: /content/hymenoptera_data/val/ants/573151833_ebbc274b77.jpg inflating: /content/hymenoptera_data/val/ants/649407494_9b6bc4949f.jpg inflating: /content/hymenoptera_data/val/ants/751649788_78dd7d16ce.jpg inflating: /content/hymenoptera_data/val/ants/768870506_8f115d3d37.jpg inflating: /content/hymenoptera_data/val/ants/800px-Meat_eater_ant_qeen_excavating_hole.jpg inflating: /content/hymenoptera_data/val/ants/8124241_36b290d372.jpg inflating: /content/hymenoptera_data/val/ants/8398478_50ef10c47a.jpg inflating: /content/hymenoptera_data/val/ants/854534770_31f6156383.jpg inflating: /content/hymenoptera_data/val/ants/892676922_4ab37dce07.jpg inflating: /content/hymenoptera_data/val/ants/94999827_36895faade.jpg inflating: /content/hymenoptera_data/val/ants/Ant-1818.jpg inflating: /content/hymenoptera_data/val/ants/ants-devouring-remains-of-large-dead-insect-on-red-tile-in-Stellenbosch-South-Africa-closeup-1-DHD.jpg inflating: /content/hymenoptera_data/val/ants/desert_ant.jpg inflating: /content/hymenoptera_data/val/ants/F.pergan.28(f).jpg inflating: /content/hymenoptera_data/val/ants/Hormiga.jpg creating: /content/hymenoptera_data/val/bees/ inflating: /content/hymenoptera_data/val/bees/1032546534_06907fe3b3.jpg inflating: /content/hymenoptera_data/val/bees/10870992_eebeeb3a12.jpg inflating: /content/hymenoptera_data/val/bees/1181173278_23c36fac71.jpg inflating: /content/hymenoptera_data/val/bees/1297972485_33266a18d9.jpg inflating: /content/hymenoptera_data/val/bees/1328423762_f7a88a8451.jpg inflating: /content/hymenoptera_data/val/bees/1355974687_1341c1face.jpg inflating: /content/hymenoptera_data/val/bees/144098310_a4176fd54d.jpg inflating: /content/hymenoptera_data/val/bees/1486120850_490388f84b.jpg inflating: /content/hymenoptera_data/val/bees/149973093_da3c446268.jpg inflating: /content/hymenoptera_data/val/bees/151594775_ee7dc17b60.jpg inflating: /content/hymenoptera_data/val/bees/151603988_2c6f7d14c7.jpg inflating: /content/hymenoptera_data/val/bees/1519368889_4270261ee3.jpg inflating: /content/hymenoptera_data/val/bees/152789693_220b003452.jpg inflating: /content/hymenoptera_data/val/bees/177677657_a38c97e572.jpg inflating: /content/hymenoptera_data/val/bees/1799729694_0c40101071.jpg inflating: /content/hymenoptera_data/val/bees/181171681_c5a1a82ded.jpg inflating: /content/hymenoptera_data/val/bees/187130242_4593a4c610.jpg inflating: /content/hymenoptera_data/val/bees/203868383_0fcbb48278.jpg inflating: /content/hymenoptera_data/val/bees/2060668999_e11edb10d0.jpg inflating: /content/hymenoptera_data/val/bees/2086294791_6f3789d8a6.jpg inflating: /content/hymenoptera_data/val/bees/2103637821_8d26ee6b90.jpg inflating: /content/hymenoptera_data/val/bees/2104135106_a65eede1de.jpg inflating: /content/hymenoptera_data/val/bees/215512424_687e1e0821.jpg inflating: /content/hymenoptera_data/val/bees/2173503984_9c6aaaa7e2.jpg inflating: /content/hymenoptera_data/val/bees/220376539_20567395d8.jpg inflating: /content/hymenoptera_data/val/bees/224841383_d050f5f510.jpg inflating: /content/hymenoptera_data/val/bees/2321144482_f3785ba7b2.jpg inflating: /content/hymenoptera_data/val/bees/238161922_55fa9a76ae.jpg inflating: /content/hymenoptera_data/val/bees/2407809945_fb525ef54d.jpg inflating: /content/hymenoptera_data/val/bees/2415414155_1916f03b42.jpg inflating: /content/hymenoptera_data/val/bees/2438480600_40a1249879.jpg inflating: /content/hymenoptera_data/val/bees/2444778727_4b781ac424.jpg inflating: /content/hymenoptera_data/val/bees/2457841282_7867f16639.jpg inflating: /content/hymenoptera_data/val/bees/2470492902_3572c90f75.jpg inflating: /content/hymenoptera_data/val/bees/2478216347_535c8fe6d7.jpg inflating: /content/hymenoptera_data/val/bees/2501530886_e20952b97d.jpg inflating: /content/hymenoptera_data/val/bees/2506114833_90a41c5267.jpg inflating: /content/hymenoptera_data/val/bees/2509402554_31821cb0b6.jpg inflating: /content/hymenoptera_data/val/bees/2525379273_dcb26a516d.jpg inflating: /content/hymenoptera_data/val/bees/26589803_5ba7000313.jpg inflating: /content/hymenoptera_data/val/bees/2668391343_45e272cd07.jpg inflating: /content/hymenoptera_data/val/bees/2670536155_c170f49cd0.jpg inflating: /content/hymenoptera_data/val/bees/2685605303_9eed79d59d.jpg inflating: /content/hymenoptera_data/val/bees/2702408468_d9ed795f4f.jpg inflating: /content/hymenoptera_data/val/bees/2709775832_85b4b50a57.jpg inflating: /content/hymenoptera_data/val/bees/2717418782_bd83307d9f.jpg inflating: /content/hymenoptera_data/val/bees/272986700_d4d4bf8c4b.jpg inflating: /content/hymenoptera_data/val/bees/2741763055_9a7bb00802.jpg inflating: /content/hymenoptera_data/val/bees/2745389517_250a397f31.jpg inflating: /content/hymenoptera_data/val/bees/2751836205_6f7b5eff30.jpg inflating: /content/hymenoptera_data/val/bees/2782079948_8d4e94a826.jpg inflating: /content/hymenoptera_data/val/bees/2809496124_5f25b5946a.jpg inflating: /content/hymenoptera_data/val/bees/2815838190_0a9889d995.jpg inflating: /content/hymenoptera_data/val/bees/2841437312_789699c740.jpg inflating: /content/hymenoptera_data/val/bees/2883093452_7e3a1eb53f.jpg inflating: /content/hymenoptera_data/val/bees/290082189_f66cb80bfc.jpg inflating: /content/hymenoptera_data/val/bees/296565463_d07a7bed96.jpg inflating: /content/hymenoptera_data/val/bees/3077452620_548c79fda0.jpg inflating: /content/hymenoptera_data/val/bees/348291597_ee836fbb1a.jpg inflating: /content/hymenoptera_data/val/bees/350436573_41f4ecb6c8.jpg inflating: /content/hymenoptera_data/val/bees/353266603_d3eac7e9a0.jpg inflating: /content/hymenoptera_data/val/bees/372228424_16da1f8884.jpg inflating: /content/hymenoptera_data/val/bees/400262091_701c00031c.jpg inflating: /content/hymenoptera_data/val/bees/416144384_961c326481.jpg inflating: /content/hymenoptera_data/val/bees/44105569_16720a960c.jpg inflating: /content/hymenoptera_data/val/bees/456097971_860949c4fc.jpg inflating: /content/hymenoptera_data/val/bees/464594019_1b24a28bb1.jpg inflating: /content/hymenoptera_data/val/bees/485743562_d8cc6b8f73.jpg inflating: /content/hymenoptera_data/val/bees/540976476_844950623f.jpg inflating: /content/hymenoptera_data/val/bees/54736755_c057723f64.jpg inflating: /content/hymenoptera_data/val/bees/57459255_752774f1b2.jpg inflating: /content/hymenoptera_data/val/bees/576452297_897023f002.jpg inflating: /content/hymenoptera_data/val/bees/586474709_ae436da045.jpg inflating: /content/hymenoptera_data/val/bees/590318879_68cf112861.jpg inflating: /content/hymenoptera_data/val/bees/59798110_2b6a3c8031.jpg inflating: /content/hymenoptera_data/val/bees/603709866_a97c7cfc72.jpg inflating: /content/hymenoptera_data/val/bees/603711658_4c8cd2201e.jpg inflating: /content/hymenoptera_data/val/bees/65038344_52a45d090d.jpg inflating: /content/hymenoptera_data/val/bees/6a00d8341c630a53ef00e553d0beb18834-800wi.jpg inflating: /content/hymenoptera_data/val/bees/72100438_73de9f17af.jpg inflating: /content/hymenoptera_data/val/bees/759745145_e8bc776ec8.jpg inflating: /content/hymenoptera_data/val/bees/936182217_c4caa5222d.jpg inflating: /content/hymenoptera_data/val/bees/abeja.jpg # Data augmentation and normalization for training # Just normalization for validation data_transforms = { 'train' : transforms . Compose ([ transforms . RandomResizedCrop ( 224 ), transforms . RandomHorizontalFlip (), transforms . ToTensor (), transforms . Normalize ([ 0.485 , 0.456 , 0.406 ], [ 0.229 , 0.224 , 0.225 ]) ]), 'val' : transforms . Compose ([ transforms . Resize ( 256 ), transforms . CenterCrop ( 224 ), transforms . ToTensor (), transforms . Normalize ([ 0.485 , 0.456 , 0.406 ], [ 0.229 , 0.224 , 0.225 ]) ]), } data_dir = 'hymenoptera_data' image_datasets = { x : datasets . ImageFolder ( os . path . join ( data_dir , x ), data_transforms [ x ]) for x in [ 'train' , 'val' ]} dataloaders = { x : torch . utils . data . DataLoader ( image_datasets [ x ], batch_size = 4 , shuffle = True , num_workers = 4 ) for x in [ 'train' , 'val' ]} dataset_sizes = { x : len ( image_datasets [ x ]) for x in [ 'train' , 'val' ]} class_names = image_datasets [ 'train' ] . classes device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) def imshow ( inp , title = None ): \"\"\"Imshow for Tensor.\"\"\" inp = inp . numpy () . transpose (( 1 , 2 , 0 )) mean = np . array ([ 0.485 , 0.456 , 0.406 ]) std = np . array ([ 0.229 , 0.224 , 0.225 ]) inp = std * inp + mean inp = np . clip ( inp , 0 , 1 ) plt . imshow ( inp ) if title is not None : plt . title ( title ) plt . pause ( 0.001 ) # pause a bit so that plots are updated # Get a batch of training data inputs , classes = next ( iter ( dataloaders [ 'train' ])) # Make a grid from batch out = torchvision . utils . make_grid ( inputs ) imshow ( out , title = [ class_names [ x ] for x in classes ]) def train_model ( model , criterion , optimizer , scheduler , num_epochs = 25 ): since = time . time () best_model_wts = copy . deepcopy ( model . state_dict ()) best_acc = 0.0 for epoch in range ( num_epochs ): print ( 'Epoch {} / {} ' . format ( epoch , num_epochs - 1 )) print ( '-' * 10 ) # Each epoch has a training and validation phase for phase in [ 'train' , 'val' ]: if phase == 'train' : model . train () # Set model to training mode else : model . eval () # Set model to evaluate mode running_loss = 0.0 running_corrects = 0 # Iterate over data. for inputs , labels in dataloaders [ phase ]: inputs = inputs . to ( device ) labels = labels . to ( device ) # zero the parameter gradients optimizer . zero_grad () # forward # track history if only in train with torch . set_grad_enabled ( phase == 'train' ): outputs = model ( inputs ) _ , preds = torch . max ( outputs , 1 ) loss = criterion ( outputs , labels ) # backward + optimize only if in training phase if phase == 'train' : loss . backward () optimizer . step () # statistics running_loss += loss . item () * inputs . size ( 0 ) running_corrects += torch . sum ( preds == labels . data ) if phase == 'train' : scheduler . step () epoch_loss = running_loss / dataset_sizes [ phase ] epoch_acc = running_corrects . double () / dataset_sizes [ phase ] print ( ' {} Loss: {:.4f} Acc: {:.4f} ' . format ( phase , epoch_loss , epoch_acc )) # deep copy the model if phase == 'val' and epoch_acc > best_acc : best_acc = epoch_acc best_model_wts = copy . deepcopy ( model . state_dict ()) print () time_elapsed = time . time () - since print ( 'Training complete in {:.0f} m {:.0f} s' . format ( time_elapsed // 60 , time_elapsed % 60 )) print ( 'Best val Acc: {:4f} ' . format ( best_acc )) # load best model weights model . load_state_dict ( best_model_wts ) return model model_ft = models . resnet18 ( pretrained = True ) num_ftrs = model_ft . fc . in_features # Here the size of each output sample is set to 2. # Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)). model_ft . fc = nn . Linear ( num_ftrs , 2 ) model_ft = model_ft . to ( device ) criterion = nn . CrossEntropyLoss () # Observe that all parameters are being optimized optimizer_ft = optim . SGD ( model_ft . parameters (), lr = 0.001 , momentum = 0.9 ) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler . StepLR ( optimizer_ft , step_size = 7 , gamma = 0.1 ) model_ft = train_model ( model_ft , criterion , optimizer_ft , exp_lr_scheduler , num_epochs = 1 ) Epoch 0/0 ---------- train Loss: 0.4348 Acc: 0.7951 val Loss: 0.1981 Acc: 0.9085 Training complete in 0m 3s Best val Acc: 0.908497","title":"Convolutional Layers"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#hooks-the-amazing-trick-you-should-know","text":"the-one-pytorch-trick-which-you-should-know import torch from torchvision.models import resnet34 device = torch . device ( 'cuda' ) if torch . cuda . is_available () else torch . device ( 'cpu' ) model = resnet34 ( pretrained = True ) model = model . to ( device ) class SaveOutput : def __init__ ( self ): self . outputs = [] def __call__ ( self , module , module_in , module_out ): self . outputs . append ( module_out ) def clear ( self ): self . outputs = [] Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth HBox(children=(FloatProgress(value=0.0, max=87306240.0), HTML(value=''))) save_output = SaveOutput () hook_handles = [] for layer in model . modules (): if isinstance ( layer , torch . nn . modules . conv . Conv2d ): handle = layer . register_forward_hook ( save_output ) hook_handles . append ( handle ) image = PIL . Image . open ( os . path . join ( config . paths [ 'image_path' ], 'cat.jpg' )) transform = transforms . Compose ([ transforms . Resize (( 224 , 224 )), transforms . ToTensor (), transforms . Normalize ([ 0.485 , 0.456 , 0.406 ], [ 0.229 , 0.224 , 0.225 ])]) X = transform ( image ) . unsqueeze ( dim = 0 ) . to ( device ) out = model ( X ) len ( save_output . outputs ) 36 save_output . outputs [tensor([[[[-8.4063e-02, -3.1156e-01, -3.7922e-01, ..., -3.1241e-01, -3.1991e-01, -7.8703e-02], [-6.6684e-01, -1.0324e+00, -1.1204e+00, ..., -1.5653e+00, -1.5471e+00, -6.4014e-01], [-3.4449e-01, -6.4493e-01, -7.2495e-01, ..., -1.2277e+00, -1.2087e+00, -4.5994e-01], ..., [ 2.1659e-02, -4.6106e-02, -2.1702e-01, ..., -1.0963e-01, -1.2138e-01, -1.5617e-01], [-3.2920e-02, -1.3041e-01, -3.1823e-01, ..., -2.6584e-02, -3.2380e-02, -1.1487e-01], [ 4.4561e-01, -2.9023e-01, -1.0615e+00, ..., 4.9755e-01, 4.4903e-01, 2.2949e-01]], [[-4.6110e-02, -3.0740e-02, -1.0826e-01, ..., -7.0238e-02, -9.2460e-02, -2.0240e+00], [-3.5144e-02, -1.7679e-02, -4.9731e-02, ..., -1.2231e-02, -9.3168e-03, -2.8310e+00], [-7.0530e-02, -3.8022e-02, -4.4606e-02, ..., -4.5727e-03, 2.5627e-02, -2.7854e+00], ..., [-7.3700e-01, 6.4940e-01, -5.2753e-01, ..., 1.7030e-02, -3.7055e-02, -2.0967e-02], [-7.4464e-01, 6.5585e-01, -5.2193e-01, ..., 4.4152e-02, -8.7973e-03, 4.6403e-02], [-6.1189e-01, 5.5873e-01, -4.2375e-01, ..., 6.6532e-02, 3.7737e-02, 6.0204e-02]], [[-2.3457e-01, -1.2645e-01, -1.7406e-01, ..., -4.1817e-01, -4.3566e-01, -1.6011e+00], [-6.6614e-02, -2.3226e-02, -6.2236e-02, ..., 2.1789e-02, 3.5555e-02, 2.7135e-01], [-4.7556e-02, -7.3814e-02, -4.9432e-02, ..., -5.7698e-02, -7.4298e-02, -6.5381e-02], ..., [ 3.9153e-03, -3.1434e-02, -2.9721e-02, ..., -1.0715e-01, -6.9462e-02, -6.0394e-02], [ 1.6745e-02, -3.0932e-02, -2.6025e-02, ..., -6.5645e-02, -1.0768e-01, -6.5954e-02], [ 1.7977e-01, 1.5711e-01, -9.2721e-02, ..., -1.2713e-01, -1.2850e-01, -3.8657e-02]], ..., [[ 8.5574e-02, 1.1745e-01, 1.5548e-01, ..., 4.0933e-01, 4.1436e-01, 9.9362e-01], [-2.6615e-01, 4.7584e-02, 6.8852e-02, ..., 1.1224e-01, 1.0466e-01, 5.7341e-02], [-2.5215e-01, 2.7188e-02, 8.2145e-02, ..., 1.0604e-01, 1.0107e-01, 2.3964e-02], ..., [-1.5416e-01, 5.6333e-02, -1.2265e-01, ..., 8.9357e-02, 7.2297e-02, 4.5511e-02], [-1.6092e-01, 5.2559e-02, -1.2424e-01, ..., 6.9515e-02, 1.0962e-01, 5.3230e-02], [ 7.6358e-02, 5.4660e-02, -4.3079e-01, ..., -4.0607e-03, 1.3518e-02, -7.2620e-02]], [[ 3.1630e-01, 8.0494e-01, 1.0661e+00, ..., 2.1891e+00, 2.1899e+00, 1.3774e+00], [ 6.3669e-01, 1.2647e+00, 1.4631e+00, ..., 3.0832e+00, 3.0457e+00, 1.9488e+00], [ 2.7135e-01, 6.2898e-01, 6.9927e-01, ..., 2.0648e+00, 2.0256e+00, 1.1266e+00], ..., [-9.1054e-02, -3.8858e-02, 3.3645e-01, ..., -1.9231e-01, -1.5554e-01, -1.0143e-01], [-8.0800e-02, -1.7947e-02, 3.7280e-01, ..., -2.6777e-01, -2.3381e-01, -1.4636e-01], [-2.4702e-01, -5.7148e-02, 5.2341e-01, ..., -1.4570e-01, -1.0016e-01, -4.2076e-02]], [[ 1.8776e-07, 2.7738e-07, 3.4406e-07, ..., 5.4105e-07, 5.3934e-07, 3.5630e-07], [ 2.7450e-07, 4.1102e-07, 5.1028e-07, ..., 8.2784e-07, 8.2309e-07, 5.3809e-07], [ 2.9428e-07, 4.4021e-07, 5.3925e-07, ..., 8.8300e-07, 8.7416e-07, 5.5885e-07], ..., [-2.8161e-08, 6.2136e-08, 1.4658e-07, ..., 2.8521e-07, 2.8608e-07, 1.9511e-07], [-3.0600e-08, 5.6445e-08, 1.3730e-07, ..., 2.6854e-07, 2.7299e-07, 1.8746e-07], [-1.4595e-08, 4.4339e-08, 9.0041e-08, ..., 1.8611e-07, 1.8967e-07, 1.3514e-07]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-7.1307e-01, -8.3134e-01, -8.4162e-01, ..., -8.3447e-01, -8.8322e-01, -8.0581e-01], [-8.5292e-01, -7.7469e-01, -8.0102e-01, ..., -7.7430e-01, -7.3565e-01, -5.8939e-01], [-8.3598e-01, -8.0091e-01, -9.0385e-01, ..., -8.0386e-01, -8.3582e-01, -7.0583e-01], ..., [-1.0630e+00, -1.0327e+00, -9.0952e-01, ..., -8.5371e-01, -9.6862e-01, -1.0368e+00], [-1.1039e+00, -1.0192e+00, -8.9569e-01, ..., -8.6189e-01, -1.0371e+00, -1.1618e+00], [-8.5768e-01, -9.8439e-01, -8.9813e-01, ..., -1.0932e+00, -1.2086e+00, -1.2490e+00]], [[-3.9721e-02, 1.8612e-02, -5.6467e-02, ..., -7.6485e-01, -9.7524e-01, -3.5409e-01], [ 3.4262e-01, 3.3146e-01, 1.9839e-01, ..., 1.2355e-01, 9.3290e-02, 9.5830e-02], [ 1.3413e-01, 1.1606e-01, 2.6072e-02, ..., -6.3812e-02, -5.7438e-02, -2.1188e-03], ..., [-1.8598e-02, -1.2766e-01, -2.4095e-02, ..., 1.4271e-01, 1.1860e-01, -4.7149e-02], [-9.9174e-03, -1.2616e-01, 4.7130e-03, ..., 1.3165e-01, -4.2112e-02, -1.3193e-01], [-2.8945e-01, -5.4195e-02, 3.9133e-02, ..., -2.2187e+00, -2.4321e+00, -1.8812e+00]], [[-6.8958e-01, -2.9590e-01, -3.4411e-01, ..., -2.2276e-01, -3.4415e-01, -1.8591e+00], [-7.0155e-01, -4.5380e-01, -5.7219e-01, ..., -4.8057e-01, -6.6103e-01, -1.7357e+00], [-4.6282e-01, -3.0516e-01, -4.1870e-01, ..., -3.9162e-01, -6.0281e-01, -1.7183e+00], ..., [-1.5330e+00, -5.3029e-01, -3.5363e-01, ..., -1.6527e-01, -5.8343e-01, -2.1110e+00], [-1.5865e+00, -5.5779e-01, -2.7971e-01, ..., -4.0614e-01, -9.4208e-01, -2.4309e+00], [-1.5805e+00, -7.1949e-01, -3.7832e-01, ..., -6.8876e-01, -1.1460e+00, -2.1012e+00]], ..., [[-9.6639e-01, -1.0328e+00, -1.0597e+00, ..., -1.1950e+00, -1.2843e+00, -1.2610e+00], [-1.1809e+00, -1.1479e+00, -1.2401e+00, ..., -1.3782e+00, -1.3827e+00, -1.0675e+00], [-1.1952e+00, -1.2641e+00, -1.3910e+00, ..., -1.3886e+00, -1.3968e+00, -1.0293e+00], ..., [-2.8220e+00, -1.7530e+00, -1.6388e+00, ..., -1.2392e+00, -1.2937e+00, -1.1474e+00], [-2.8713e+00, -1.6669e+00, -1.5304e+00, ..., -1.2188e+00, -1.3438e+00, -1.1898e+00], [-2.7238e+00, -1.5046e+00, -1.3087e+00, ..., -9.7235e-01, -1.0158e+00, -9.8973e-01]], [[-2.5968e+00, -1.6794e+00, -1.4204e+00, ..., -1.1400e+00, -1.2257e+00, -1.2097e+00], [-2.6432e+00, -1.9258e+00, -1.7191e+00, ..., -1.3148e+00, -1.1469e+00, -1.2445e+00], [-2.3157e+00, -2.0002e+00, -2.0134e+00, ..., -1.4132e+00, -1.2471e+00, -1.5705e+00], ..., [-2.5916e+00, -3.0395e+00, -1.8192e+00, ..., -1.3918e+00, -1.3557e+00, -2.4565e+00], [-2.6827e+00, -3.0189e+00, -1.6226e+00, ..., -1.2810e+00, -1.2225e+00, -2.5779e+00], [-2.3964e+00, -2.7206e+00, -1.5952e+00, ..., -1.3886e+00, -1.3188e+00, -2.3821e+00]], [[ 9.3492e-01, 2.9573e-01, 2.0938e-01, ..., 1.5254e-01, 9.0258e-03, -7.4639e-01], [ 6.4624e-01, -9.7434e-02, -1.3152e-01, ..., 1.3951e-01, -1.5915e-01, -1.7132e+00], [ 5.5572e-01, -1.0975e-01, -1.4220e-01, ..., -4.1307e-02, -3.4354e-01, -1.7407e+00], ..., [-9.0136e-01, -8.2252e-02, -6.0170e-02, ..., 7.6841e-02, 1.1324e-01, -1.2357e+00], [-7.7420e-01, -2.0381e-02, -8.0946e-02, ..., 8.4156e-02, 1.3501e-01, -1.3044e+00], [-3.3190e-01, 8.8170e-02, -1.7382e-02, ..., -3.6395e-01, -4.6098e-01, -9.1564e-01]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-0.0067, 0.0934, 0.2068, ..., 0.2558, 0.2704, 0.1711], [-0.0525, -0.0926, 0.0183, ..., 0.2465, 0.2182, 0.1562], [ 0.0718, 0.0516, 0.0838, ..., 0.3026, 0.2200, 0.1786], ..., [ 0.0680, 0.0353, 0.0985, ..., 0.0679, -0.0684, -0.0239], [ 0.1705, 0.0419, 0.0697, ..., -0.0153, -0.0345, 0.0361], [ 0.1610, -0.1131, -0.0392, ..., 0.0911, 0.0280, 0.0677]], [[ 0.1090, 0.0759, 0.1343, ..., 0.0514, 0.0668, 0.0639], [ 0.1767, 0.1108, 0.1808, ..., 0.0470, 0.0197, 0.0140], [ 0.2040, 0.1734, 0.2154, ..., 0.0737, 0.0707, 0.0720], ..., [ 0.2990, 0.3060, 0.3207, ..., 0.3023, 0.2745, 0.1356], [ 0.2607, 0.2654, 0.2734, ..., 0.3501, 0.3430, 0.2238], [ 0.1456, 0.1358, 0.1019, ..., 0.1606, 0.1911, 0.1065]], [[ 0.1066, 0.0436, 0.0515, ..., 0.1293, 0.0921, -0.0538], [-0.2412, -0.3833, -0.2128, ..., -0.2032, -0.2209, -0.0512], [-0.2085, -0.1475, -0.0593, ..., -0.1508, -0.2108, -0.0181], ..., [-0.0955, -0.1585, -0.1726, ..., -0.1232, -0.0873, -0.0838], [-0.2223, -0.3166, -0.2828, ..., -0.2100, -0.1755, -0.1166], [-0.2328, -0.0936, -0.1716, ..., -0.1858, -0.1902, -0.0887]], ..., [[ 0.0435, -0.0346, -0.0193, ..., -0.0864, -0.0844, -0.0661], [-0.0094, -0.0602, 0.0022, ..., -0.1411, -0.1402, -0.0684], [ 0.0354, 0.0333, 0.0703, ..., -0.1200, -0.1259, -0.0280], ..., [ 0.0172, 0.0704, 0.0566, ..., 0.1495, 0.1632, 0.0931], [-0.0317, 0.0102, -0.0065, ..., 0.1540, 0.1796, 0.1241], [-0.0509, -0.0178, -0.0326, ..., 0.0413, 0.0676, 0.0497]], [[-0.3162, -0.2310, -0.1908, ..., -0.2563, -0.2508, -0.1339], [-0.2608, -0.2658, -0.3400, ..., -0.3716, -0.3720, -0.0844], [-0.1699, -0.3054, -0.3627, ..., -0.4232, -0.3379, -0.0883], ..., [-0.1122, -0.3011, -0.3755, ..., -0.0965, -0.1681, 0.0880], [-0.2009, -0.3449, -0.3103, ..., -0.1556, -0.2994, 0.1678], [-0.0541, -0.2684, -0.1538, ..., -0.0941, -0.1122, 0.0796]], [[ 0.2191, 0.2895, 0.3184, ..., 0.3778, 0.4290, 0.3253], [ 0.3126, 0.3261, 0.3293, ..., 0.4939, 0.6433, 0.4783], [ 0.2731, 0.2239, 0.2208, ..., 0.4430, 0.5743, 0.4407], ..., [ 0.1634, 0.1989, 0.3175, ..., 0.1045, 0.0940, 0.1423], [ 0.1457, 0.1518, 0.2690, ..., 0.1048, 0.0339, 0.0605], [ 0.0594, 0.0695, 0.1093, ..., 0.0717, 0.0124, 0.0449]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[ 0.1288, -0.2503, -0.1333, ..., 0.3151, 0.4780, -0.0678], [-0.0787, -0.1792, 0.0485, ..., 0.4072, 0.4981, 0.3600], [-0.1732, -0.2616, 0.1061, ..., 0.4136, 0.4425, 0.3536], ..., [ 0.3060, 0.4366, 0.2526, ..., 0.0864, 0.3143, 0.0586], [ 0.3617, 0.2622, 0.2624, ..., 0.1249, 0.4111, -0.2825], [-0.4700, -0.8507, -0.8230, ..., -1.6068, -1.5898, -1.3000]], [[ 0.1018, -0.3544, -0.3491, ..., -0.4679, -0.7683, -1.1598], [ 0.3575, -0.4819, -0.7272, ..., -0.7274, -1.1150, -1.3365], [ 0.3272, -0.4256, -0.6880, ..., -0.7192, -1.0137, -1.2675], ..., [ 0.1819, -1.3496, -1.1823, ..., -0.5881, -0.5162, -0.5722], [ 0.1448, -1.2669, -0.9025, ..., -0.6687, -0.3966, -0.2392], [ 0.2109, -0.8323, -0.4376, ..., -0.2245, 0.0652, -0.1672]], [[-2.3957, -1.9007, -1.9473, ..., -1.6839, -1.8994, -2.1858], [-1.2779, -1.4584, -1.7060, ..., -1.2947, -1.5838, -2.1106], [-1.2658, -1.6701, -1.8100, ..., -1.5734, -1.5880, -2.0974], ..., [-1.2345, -1.9685, -1.6607, ..., -1.5137, -1.6177, -1.7467], [-1.2292, -1.9627, -1.5617, ..., -1.7320, -1.6819, -1.7406], [-1.1952, -1.7024, -1.2822, ..., -1.4485, -1.3326, -1.3642]], ..., [[-0.1999, 0.7422, 0.7286, ..., 0.6396, 0.4921, 0.5735], [-1.8417, -0.0774, 0.0263, ..., 0.1794, 0.2562, 0.3815], [-1.6356, 0.1033, 0.0197, ..., 0.1362, 0.1361, -0.0085], ..., [-0.2457, 0.3278, 0.3608, ..., 0.0249, -0.0697, 0.5960], [-0.1247, 0.2252, 0.2401, ..., -0.4984, -0.5210, 0.4292], [-0.0336, 0.2188, 0.0487, ..., 0.5139, 0.6008, 1.5812]], [[-0.8325, -0.2705, -0.1320, ..., -0.0169, 0.0082, 0.7680], [-0.4834, -0.6586, -0.3605, ..., -0.2657, -0.3053, -0.7996], [-0.5342, -0.6958, -0.3008, ..., 0.2130, 0.0545, -0.5790], ..., [-0.4812, -0.5852, -0.1952, ..., -0.3104, -0.3786, -0.7488], [-0.4629, -0.5718, -0.3029, ..., -0.2068, -0.0184, -0.5921], [-0.5736, -0.6432, -0.3380, ..., -0.2731, -0.3399, -0.5874]], [[ 0.6537, 0.2888, 0.3033, ..., 1.1874, 1.1648, 1.3152], [ 0.9450, 0.6366, 0.6243, ..., 1.1310, 1.0945, 1.3916], [ 1.1746, 0.9364, 0.9100, ..., 1.3841, 1.2866, 1.4843], ..., [ 1.2870, 1.4445, 1.4908, ..., 0.4462, 0.4064, 0.9422], [ 1.2159, 1.3198, 1.4482, ..., 1.2616, 1.2065, 1.4373], [-0.4371, -0.9823, -0.9968, ..., 0.9042, 0.7254, 1.2273]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-0.0690, 0.0205, 0.0238, ..., 0.0504, 0.0489, 0.0620], [-0.0085, -0.2284, -0.2074, ..., -0.0862, -0.2175, -0.0029], [ 0.1603, 0.0254, -0.0896, ..., -0.1621, -0.2845, 0.0653], ..., [-0.0922, -0.0500, -0.0803, ..., 0.1579, -0.0258, 0.1260], [ 0.1160, -0.0575, -0.1284, ..., -0.1459, 0.0544, 0.1429], [ 0.1766, -0.3412, -0.2090, ..., -0.2005, -0.1526, -0.0480]], [[ 0.0569, -0.0271, 0.0484, ..., 0.0785, 0.0668, 0.0726], [ 0.1256, -0.0083, 0.1261, ..., 0.1567, 0.1875, 0.0977], [ 0.0912, -0.0189, 0.1105, ..., 0.1244, 0.1274, 0.0368], ..., [ 0.0576, 0.0070, 0.0357, ..., -0.1868, -0.2296, -0.1509], [ 0.1553, 0.0701, 0.0321, ..., -0.2530, -0.2889, -0.2127], [ 0.0855, 0.0397, -0.0359, ..., -0.1913, -0.2375, -0.1456]], [[ 0.1201, 0.0909, 0.0267, ..., 0.0299, 0.1325, 0.0393], [-0.0334, 0.0592, 0.0434, ..., 0.0855, 0.1506, 0.1683], [-0.1773, -0.0331, 0.0200, ..., 0.0043, 0.0764, 0.0877], ..., [-0.0717, 0.0951, 0.0525, ..., -0.0263, 0.0365, 0.0329], [-0.0570, 0.0173, -0.0354, ..., -0.0474, -0.0211, 0.0258], [-0.1326, 0.0810, -0.0064, ..., -0.0219, -0.0343, 0.0497]], ..., [[ 0.0337, 0.0411, 0.0187, ..., -0.0363, -0.0619, -0.0256], [-0.0246, 0.0176, 0.0450, ..., -0.0725, -0.1047, -0.0112], [-0.0102, 0.0386, 0.0480, ..., -0.0989, -0.1209, 0.0060], ..., [ 0.0540, 0.0942, 0.0474, ..., 0.0919, 0.0928, 0.1310], [ 0.0908, 0.1295, 0.0543, ..., 0.1005, 0.1049, 0.1038], [ 0.0257, 0.0705, 0.0354, ..., 0.0861, 0.0639, 0.0800]], [[-0.0520, -0.1138, -0.1880, ..., -0.2184, -0.2333, -0.2963], [-0.0347, 0.2291, -0.0918, ..., 0.0682, 0.1796, -0.1232], [-0.0566, 0.0762, -0.2317, ..., -0.1599, 0.0066, -0.0630], ..., [-0.1647, 0.0511, -0.1918, ..., -0.0743, -0.0133, -0.0464], [-0.2029, 0.0046, -0.0944, ..., 0.0204, -0.0767, -0.0127], [ 0.0343, -0.0258, -0.0422, ..., -0.0718, -0.1464, 0.0529]], [[ 0.0959, -0.1765, -0.1379, ..., -0.2142, -0.2033, 0.0204], [ 0.2175, -0.1815, -0.1862, ..., -0.2911, -0.1999, -0.0215], [ 0.3199, -0.1060, -0.1423, ..., -0.2524, -0.1460, 0.0433], ..., [ 0.2595, -0.0375, -0.0292, ..., 0.1552, 0.2469, 0.2323], [ 0.2947, 0.0060, -0.0351, ..., 0.3238, 0.4278, 0.3459], [ 0.2347, 0.1015, 0.0621, ..., 0.3857, 0.4319, 0.3006]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-0.7312, -1.0573, -0.8696, ..., -0.9803, -1.2274, -0.4400], [-0.4258, -1.6297, -1.3173, ..., -1.3742, -1.6037, -0.2364], [-0.5914, -1.0902, -1.3082, ..., -1.8181, -1.7968, -0.3723], ..., [-0.0704, -1.8710, -1.3881, ..., -1.1977, -1.1060, -0.3352], [ 0.0396, -2.2010, -1.4669, ..., -1.7222, -0.9263, -0.2770], [ 0.2731, -1.0796, -0.5575, ..., -0.3376, -0.2581, -0.1647]], [[ 0.7737, 0.8047, 0.0747, ..., 0.1028, 0.4023, -0.9467], [ 0.6843, 1.3533, 0.0049, ..., 0.0604, 0.2824, -0.6501], [ 0.3333, 1.1694, 0.5182, ..., -0.0359, 0.2724, -0.2666], ..., [ 1.1152, 1.0655, -0.3863, ..., -0.1095, 0.0656, 0.4892], [ 1.3597, 1.0931, -0.1453, ..., -0.5847, -0.5435, 0.5415], [ 1.5063, 0.8190, -0.0748, ..., -0.2666, 0.0716, 0.4723]], [[-0.0421, 0.1598, 0.3452, ..., 0.6064, 0.5386, 0.9047], [ 0.0706, 0.1349, 0.2835, ..., 0.7040, 0.5819, 1.0330], [ 0.1681, 0.2871, 0.3497, ..., 0.8788, 0.6498, 0.8639], ..., [ 0.9277, 1.1339, 0.8439, ..., 0.2173, -0.0893, 0.3682], [ 1.2604, 1.4140, 1.0508, ..., 0.1766, -0.1631, 0.3723], [ 0.8026, 0.9001, 0.7016, ..., 0.1117, -0.3394, 0.0032]], ..., [[ 0.3208, -0.5787, -0.2745, ..., -0.4792, -0.7870, -1.1380], [ 0.1350, -0.8132, -0.8228, ..., -0.6536, -0.6146, -1.3862], [ 0.1851, -0.2499, -0.5231, ..., -1.0034, -0.4775, -1.1100], ..., [ 0.3556, -1.2090, -1.0370, ..., -0.5365, 0.0756, -0.8619], [ 0.4460, -0.9401, -0.7835, ..., -0.5803, 0.0291, -0.5543], [-0.0055, -1.0113, -0.8210, ..., -0.3147, -0.1591, -0.7711]], [[ 1.0562, 0.7463, 1.1578, ..., 0.7318, 0.2068, 0.3523], [ 0.5896, 0.3341, 1.3005, ..., 0.6428, -0.1286, -0.1280], [ 0.6546, 0.4339, 1.3679, ..., 1.0451, 0.3791, -0.1241], ..., [ 0.3876, 1.0999, 1.5070, ..., 0.8180, 0.7997, -0.3036], [ 0.5483, 1.5749, 1.3768, ..., 1.0305, 0.9950, -0.3318], [-0.3145, 0.1694, 0.0642, ..., 0.0511, -0.0087, -0.5342]], [[ 0.4556, 0.0650, 0.5178, ..., 0.5446, 0.9076, -1.0967], [-0.2459, -0.5475, -0.1135, ..., 0.3136, 0.9088, -1.6499], [-0.2022, -0.3190, -0.1806, ..., 0.0904, 0.7406, -1.3833], ..., [-0.8970, 0.2649, -0.0789, ..., -0.0769, 1.1681, -2.2012], [-1.2325, 0.4407, -0.1711, ..., -0.0385, 1.1555, -2.5085], [-0.5395, 0.5949, 0.0827, ..., 0.2193, 0.9333, -1.8239]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-3.7964e-01, -4.0134e-01, -1.7608e-01, ..., -2.5551e-01, -2.3841e-01, -2.2295e-01], [ 3.9957e-02, -1.3263e-01, 8.1712e-02, ..., 2.3840e-01, 1.3429e-01, 7.8315e-02], [ 3.1333e-03, -1.0131e-02, -6.6429e-02, ..., -2.0292e-01, -2.8177e-01, -5.3561e-02], ..., [-2.1268e-01, 1.5780e-01, 6.7509e-02, ..., 1.0054e-01, -1.3563e-01, -2.1604e-01], [ 1.4755e-02, 8.2588e-02, -1.2901e-01, ..., -3.6963e-01, -8.8912e-02, -3.1725e-02], [-1.5123e-02, -4.9199e-01, -3.9780e-01, ..., -4.1590e-01, -4.3884e-01, -4.5005e-01]], [[-2.4282e-02, -8.2193e-02, -8.2178e-02, ..., -1.3298e-01, 5.8532e-02, -7.8329e-02], [-3.2592e-03, -1.6045e-01, -1.6002e-01, ..., -1.8618e-01, 2.2344e-03, -2.6714e-01], [ 1.4457e-02, -7.3774e-02, -1.1208e-01, ..., -1.1209e-01, 3.8288e-02, -2.6211e-01], ..., [ 2.3041e-01, 1.1639e-01, 2.1283e-02, ..., 1.4279e-02, 3.2593e-02, -1.5568e-01], [ 2.3745e-01, 7.5388e-02, -6.5470e-02, ..., 1.2928e-01, 1.2930e-01, -9.1972e-02], [ 7.6496e-02, -1.1406e-01, -2.1356e-01, ..., -8.5138e-02, -8.2041e-02, -1.7543e-01]], [[ 3.8545e-02, 1.7309e-02, -1.7679e-01, ..., -7.5756e-02, -6.3311e-02, -8.1885e-02], [ 2.0244e-01, 2.7231e-01, 3.9004e-02, ..., 1.4688e-02, -4.5041e-02, 5.9030e-02], [ 4.0409e-02, 2.9233e-01, 1.1852e-01, ..., -3.0633e-02, 4.2346e-02, 1.1974e-01], ..., [ 1.0777e-01, 1.0976e-01, 5.1145e-02, ..., 8.7173e-02, 6.6064e-02, 1.9359e-01], [ 8.7519e-02, -1.2386e-02, 7.9483e-02, ..., 3.3585e-02, 1.0019e-01, 3.8054e-01], [ 1.6395e-01, 3.0325e-01, 8.2967e-02, ..., 1.3820e-01, 3.1377e-01, 4.5450e-01]], ..., [[ 1.3808e-01, 2.0525e-01, 1.7885e-01, ..., 2.8482e-01, 2.8515e-01, 1.2578e-01], [ 9.4201e-04, 1.4381e-01, 1.7652e-01, ..., 2.2403e-01, 2.0144e-01, 1.4126e-01], [-1.8357e-02, 7.3374e-02, 1.4470e-01, ..., 1.3975e-01, 1.9496e-01, 1.3552e-01], ..., [ 1.2864e-01, 1.7703e-02, 7.7141e-04, ..., 1.4196e-01, 1.6583e-01, 7.5212e-02], [ 8.7795e-02, 1.2203e-01, 4.5601e-02, ..., 4.8339e-02, 1.1783e-01, 3.1923e-02], [ 8.8624e-02, 2.0204e-01, 1.8055e-01, ..., 9.0144e-02, 1.2010e-01, 7.4829e-02]], [[-1.2925e-01, -2.7588e-02, -4.4342e-02, ..., 8.2830e-02, -1.7703e-02, -6.3619e-02], [-1.4586e-01, -9.6416e-03, 5.4718e-02, ..., -3.5866e-02, -1.2122e-01, -2.9893e-02], [-3.0947e-01, -8.5613e-02, -1.7647e-02, ..., -2.0618e-01, -4.2163e-02, -4.4049e-04], ..., [-2.7113e-01, -1.3855e-01, -2.0130e-01, ..., -2.0272e-01, -1.9512e-01, -1.2623e-01], [-2.4112e-01, -1.5066e-01, -1.6733e-01, ..., -1.9722e-01, -3.0106e-01, -1.2402e-01], [-2.4192e-01, -1.7451e-01, 2.7594e-02, ..., -1.0973e-02, -1.1463e-01, -6.7437e-02]], [[ 2.1856e-01, 1.2767e-01, 8.7509e-02, ..., 1.7413e-01, 1.3547e-01, 2.4634e-01], [ 3.9917e-01, 2.7012e-01, 1.7836e-01, ..., 3.1700e-01, 3.3307e-01, 3.6791e-01], [ 1.9946e-01, 9.8512e-02, -6.8485e-03, ..., 1.6686e-01, 2.0509e-01, 2.7482e-01], ..., [ 3.2994e-01, 2.2487e-01, 1.6483e-01, ..., 1.2141e-01, 1.8807e-01, 2.3022e-01], [ 3.5468e-01, 3.0923e-01, 1.8328e-01, ..., 1.1140e-01, 1.1644e-01, 1.4330e-01], [ 2.4101e-01, 2.4712e-01, 1.5476e-01, ..., 1.7904e-01, 1.7466e-01, 1.4160e-01]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-9.0091e-01, 6.3870e-01, -3.0263e-01, ..., -6.5964e-01, 7.8636e-01, -1.0163e+00], [-1.1825e+00, 6.3744e-01, -3.5650e-01, ..., -8.8556e-02, 3.4580e-01, -1.0833e-01], [-1.1111e+00, 3.4459e-01, -7.5116e-01, ..., -1.6278e-01, -2.8018e-01, 2.0702e-01], ..., [ 7.5050e-01, -9.8482e-01, 4.0760e-01, ..., 6.2197e-01, 1.2554e-01, -6.7755e-01], [-1.5456e+00, 1.3781e-01, 6.0317e-01, ..., 1.3621e+00, -1.3546e-01, 1.2409e-01], [-1.1613e+00, -3.4615e-01, 1.3554e-01, ..., 4.6713e-02, -1.0961e-01, 5.2183e-01]], [[ 6.4298e-01, 2.1897e-01, 1.5417e-01, ..., 6.1239e-01, 6.5252e-01, 1.9946e-01], [-2.1554e-01, -4.5524e-01, 3.1487e-01, ..., 1.2128e+00, 1.7485e+00, 1.2734e-02], [-4.8627e-02, -8.1735e-02, -1.5425e-01, ..., -3.5698e-02, 9.1563e-01, 1.5359e+00], ..., [-8.6189e-01, -1.1113e-01, -1.0548e-01, ..., -2.2268e-01, 2.1504e-02, -4.1924e-01], [-7.7746e-01, -6.7780e-01, 6.5219e-02, ..., -2.7417e-01, -1.9657e-01, 3.0158e-02], [-4.2939e-01, -1.1187e-01, 8.3300e-01, ..., -3.0947e-01, 1.2986e-02, -7.6760e-02]], [[-2.2114e-01, -3.9811e-01, -4.3892e-01, ..., -4.2268e-01, -6.8673e-01, -4.9937e-01], [-7.4385e-02, 1.0765e-01, -6.6805e-01, ..., -2.9073e-01, -7.2485e-01, 3.2642e-01], [ 1.7151e-01, -9.0508e-02, -1.4112e-01, ..., 2.7432e-01, 3.4726e-02, 8.5700e-02], ..., [ 4.4768e-02, 7.3492e-01, 1.7399e-01, ..., 1.5529e-01, -5.5666e-02, -1.0640e+00], [ 5.0144e-01, 4.9841e-01, 6.9592e-04, ..., -2.5502e-01, -5.8249e-01, -4.9665e-03], [ 3.5961e-01, -6.5753e-01, -2.9218e-01, ..., 7.3599e-03, -4.0838e-01, 2.6860e-02]], ..., [[ 7.9693e-01, 1.1981e+00, -2.6719e-02, ..., -1.4044e-01, 9.2718e-01, -5.7674e-01], [ 8.0784e-01, -1.5681e+00, 1.8397e-01, ..., 8.7765e-01, -6.4868e-01, 6.9011e-01], [-2.4827e-02, 1.6731e-01, -1.2616e-01, ..., -3.6915e-01, 2.0152e-01, -3.9128e-01], ..., [-1.1125e+00, 4.7629e-01, 8.7911e-01, ..., -9.4275e-04, -9.2995e-01, 2.9742e-01], [ 5.7873e-01, 6.8500e-01, -5.4961e-01, ..., -3.0391e-01, 5.8966e-01, -1.5808e-01], [-5.0518e-01, 5.9546e-01, -4.2340e-01, ..., -7.0296e-02, 1.5118e-01, 5.6469e-01]], [[-1.1372e-01, 1.9317e-01, -2.0616e-01, ..., -4.5876e-02, 2.9818e-02, -1.4028e-01], [ 2.7155e-02, 4.5951e-01, 1.8610e-01, ..., 4.6667e-01, -3.9038e-01, 1.1254e+00], [ 2.1434e-01, 5.7286e-02, -2.0534e-01, ..., -2.1218e-01, 3.6298e-02, 5.4128e-01], ..., [-1.6878e+00, -1.5219e+00, 8.6590e-02, ..., 5.5942e-01, 3.5859e-01, 1.3370e+00], [-1.2910e+00, 2.3102e-01, 6.7816e-01, ..., -2.6203e-01, -2.4199e-01, -1.0794e-01], [-4.2782e-02, 7.7749e-01, 9.3433e-01, ..., 9.2322e-01, 1.2178e+00, 1.8176e+00]], [[-7.9935e-01, -1.2774e+00, -4.3572e-01, ..., -7.2118e-02, -2.2340e-01, -4.4651e-01], [-1.7296e+00, -2.1770e+00, -8.5848e-01, ..., -6.6132e-01, -3.8605e-01, -7.1216e-01], [-1.2839e+00, -2.0997e+00, -1.4358e+00, ..., -1.1509e+00, -7.3437e-01, -1.2747e+00], ..., [-1.7467e+00, -2.7322e+00, -1.0077e+00, ..., -9.5499e-01, -6.3091e-01, -1.0631e+00], [-2.0026e+00, -1.7775e+00, -9.2295e-01, ..., -6.0964e-01, -4.7979e-01, -5.8898e-01], [-1.4926e+00, -1.1507e+00, -3.1636e-01, ..., -1.2428e-01, -4.7847e-01, -4.4640e-01]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[ 9.5913e-02, -1.6541e-01, -5.6267e-02, ..., -1.8978e-01, -1.0216e-02, -2.0282e-02], [-7.9051e-02, -1.5625e-01, -9.1682e-02, ..., -1.2345e-01, -5.0356e-02, 6.8900e-02], [-1.3790e-01, -1.4030e-01, -7.8608e-02, ..., -1.1149e-01, -3.1122e-01, -2.4360e-01], ..., [-2.0402e-01, -2.7970e-01, -5.8880e-02, ..., -1.2766e-01, -1.5945e-01, -1.4166e-04], [ 1.0777e-01, -5.8746e-02, -3.4232e-01, ..., -6.7850e-02, -1.2919e-01, 2.3733e-02], [-1.5006e-01, 1.8371e-01, -2.5554e-01, ..., -2.5197e-01, -8.3513e-03, 7.0744e-02]], [[-1.1158e-02, -2.5456e-02, -3.6755e-02, ..., 6.3668e-02, -1.7311e-01, 1.1011e-01], [-6.5492e-02, 9.1241e-02, -5.5219e-02, ..., 3.3896e-02, -8.6943e-02, 2.9966e-02], [-2.4388e-02, -2.0955e-02, -1.2032e-02, ..., -4.6190e-03, 1.3383e-02, 5.6576e-02], ..., [ 6.9491e-03, 1.1501e-01, -1.6926e-04, ..., 3.8924e-02, -1.5815e-02, 3.5392e-02], [-7.1771e-02, -1.1710e-01, 1.3267e-02, ..., -1.1833e-01, 4.6548e-02, 3.4670e-02], [-2.0510e-02, 4.5852e-02, -9.4286e-03, ..., 7.4114e-02, 2.8800e-02, -4.0978e-02]], [[ 2.8442e-01, 1.8822e-01, 2.3620e-01, ..., 1.2417e-01, 2.6348e-01, 3.2023e-01], [ 3.0938e-02, 7.4200e-02, 2.4235e-01, ..., 1.3457e-01, 1.1945e-01, 9.4352e-02], [ 1.0847e-01, 1.3481e-01, -2.0223e-02, ..., 1.2688e-01, 1.3234e-01, 2.9863e-02], ..., [-9.2610e-02, 2.1935e-02, -1.5643e-01, ..., -4.6844e-02, -1.0332e-01, 8.3054e-02], [ 3.7796e-02, 2.5773e-01, 1.3152e-02, ..., -6.5266e-02, -2.9678e-02, -1.7658e-01], [ 3.2729e-01, 2.8362e-01, 1.0696e-01, ..., 2.2743e-01, 1.2019e-02, 1.5634e-01]], ..., [[-3.0847e-01, -2.5876e-01, -3.6083e-01, ..., -2.5877e-01, -3.1287e-01, -3.7233e-01], [-1.3438e-01, -1.4756e-01, -2.5999e-01, ..., -2.9874e-01, -2.4534e-01, -1.3022e-01], [ 7.5756e-02, -9.8625e-02, -1.1781e-01, ..., -6.3175e-02, -1.2338e-01, 8.6862e-02], ..., [-1.4622e-01, -3.0403e-01, -1.7429e-01, ..., -1.5258e-01, -1.9483e-01, -1.2083e-04], [ 5.3101e-02, -1.3620e-01, -1.3141e-01, ..., 2.1619e-01, -8.5160e-02, -4.4242e-02], [-7.8481e-02, 8.2884e-02, 1.2997e-02, ..., 7.7851e-02, 1.4876e-01, 5.9686e-02]], [[-4.4822e-01, -4.4531e-01, -5.4460e-01, ..., -4.2265e-01, -5.7580e-01, -3.2392e-01], [-4.1808e-01, -4.3049e-01, -5.1591e-01, ..., -4.7325e-01, -4.2901e-01, -3.3930e-01], [-3.9814e-01, -3.4293e-01, -4.2360e-01, ..., -5.9320e-01, -4.5428e-01, -3.0691e-01], ..., [-5.2022e-01, -4.3604e-01, -2.8243e-01, ..., -3.7317e-01, -3.8210e-01, -5.0886e-01], [-7.2720e-01, -4.8945e-01, -1.4572e-01, ..., -4.5263e-01, -4.7482e-01, -6.6300e-01], [-5.8000e-01, -3.1295e-01, -3.2944e-01, ..., -4.0389e-01, -4.3888e-01, -3.2057e-01]], [[ 9.7382e-02, 9.4367e-02, 7.4948e-03, ..., 2.4979e-01, 3.8360e-01, 2.1761e-01], [-2.4950e-01, -8.9020e-02, -1.3273e-01, ..., 8.9120e-02, 2.7409e-01, -1.7473e-01], [-5.4640e-02, 1.3543e-01, -3.0125e-02, ..., 1.0683e-01, 1.2142e-01, 2.9367e-01], ..., [ 6.0568e-01, -6.5119e-02, -8.1571e-02, ..., 6.2436e-02, -4.4642e-02, 8.2873e-02], [ 5.8876e-02, -8.7217e-02, 7.7449e-02, ..., -1.0470e-01, -2.2360e-02, -7.8556e-03], [ 7.5289e-03, 1.0554e-01, 5.1145e-02, ..., 1.0968e-01, 1.1641e-01, -6.0809e-02]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-4.2273e-01, -2.0987e-01, -1.8386e-01, ..., -1.6742e-01, -1.4549e-01, -2.3816e-01], [ 5.2229e-02, -2.1747e-01, 1.6554e-03, ..., 9.4766e-02, 2.8188e-03, 2.2632e-02], [-2.1932e-01, -4.8531e-02, 1.7961e-02, ..., -1.8766e-01, -1.5018e-01, -2.3167e-01], ..., [-2.6574e-01, -2.2816e-01, -1.6534e-02, ..., -1.1980e-01, -9.9002e-02, -1.0444e-01], [-1.4383e-01, -4.1912e-02, 1.2204e-01, ..., 1.0591e-02, -7.6314e-03, -3.2738e-01], [-1.8492e-01, -6.7604e-02, -2.1401e-02, ..., -2.0158e-01, -1.5867e-01, -5.3975e-02]], [[ 1.3357e+00, 1.1842e+00, 1.3085e+00, ..., 8.3036e-01, 8.1247e-01, 1.1123e+00], [ 9.6221e-01, 9.3270e-01, 1.3355e+00, ..., 6.1226e-01, 2.3988e-01, 4.3982e-01], [ 8.8059e-01, 7.4175e-01, 8.8665e-01, ..., 8.1106e-01, 4.9318e-01, 7.6294e-01], ..., [ 1.9570e-01, -8.9729e-02, -1.5378e-01, ..., 1.0819e+00, 9.9820e-01, 1.2328e+00], [-1.6463e-01, -1.0758e-01, -1.3862e-01, ..., 1.1708e+00, 9.6169e-01, 1.4559e+00], [-2.3193e-01, -2.6281e-01, -1.7272e-01, ..., 1.1984e+00, 1.3402e+00, 1.5838e+00]], [[-2.9064e-01, -3.3235e-01, -3.0300e-01, ..., -1.4648e-01, -1.0491e-01, -8.5999e-02], [ 3.0795e-01, -3.6245e-01, 1.7980e-01, ..., 2.4748e-01, 8.4595e-02, 8.1217e-02], [-3.0408e-02, -2.1951e-02, -1.8527e-02, ..., -2.8260e-03, -4.4726e-02, -4.8306e-02], ..., [ 3.6590e-02, -2.2649e-01, -5.1185e-02, ..., -3.1439e-02, -4.5245e-01, -2.1810e-02], [ 1.1245e-01, -3.2933e-01, 8.5335e-03, ..., 4.0620e-02, 1.7344e-01, -3.1027e-01], [ 2.3234e-01, 2.2664e-01, -7.7205e-02, ..., -2.2093e-01, -4.8593e-02, 2.4661e-02]], ..., [[-9.3261e-01, -7.6260e-01, -7.3332e-01, ..., -7.7190e-01, -6.0499e-01, -5.9985e-01], [-8.5653e-01, -2.1887e-01, -5.4355e-01, ..., -3.8094e-01, -5.8377e-01, -5.0749e-01], [-4.9705e-01, -1.9928e-01, -4.5073e-01, ..., 1.2020e-03, -1.8645e-01, -1.2595e-01], ..., [-6.3847e-01, -3.3980e-01, -5.2701e-01, ..., -3.0330e-01, -2.8109e-01, -1.6475e-01], [-6.8109e-01, -5.0743e-01, -5.2533e-01, ..., -5.2017e-01, -6.2944e-01, -6.2879e-01], [-2.7850e-01, -6.1580e-01, -5.2393e-01, ..., -5.9199e-01, -6.9086e-01, -6.9969e-01]], [[ 2.5167e-01, 4.3919e-02, 1.0668e-02, ..., -7.8221e-02, 2.1594e-01, 1.3642e-01], [ 2.4208e-01, 2.5012e-01, 2.5683e-01, ..., 2.6633e-01, -5.7677e-02, 3.9456e-01], [ 3.7880e-01, 1.2930e-01, -3.5076e-02, ..., 2.1143e-01, 1.0497e-01, 4.1619e-01], ..., [ 3.1514e-01, 2.0994e-01, 1.2845e-01, ..., 2.5211e-01, 9.9327e-02, 1.8318e-01], [ 3.1479e-01, 1.2375e-01, 1.1239e-01, ..., 2.0128e-01, 1.0337e-01, 3.0883e-01], [ 3.8773e-01, 1.0302e-01, 2.2351e-01, ..., -7.8994e-02, 6.5807e-03, -1.0335e-01]], [[ 1.2423e-03, -1.0223e-01, 9.5612e-02, ..., -9.4084e-02, -4.2660e-02, 7.5494e-02], [-5.1509e-01, -5.2124e-02, -2.5991e-01, ..., -3.1778e-01, -2.3171e-02, -2.8931e-01], [-2.6891e-01, -7.5821e-02, -2.3992e-01, ..., 1.3733e-01, 2.6749e-02, -9.0869e-02], ..., [-1.9027e-01, -1.9019e-01, -1.7996e-01, ..., -1.0913e-01, 1.2905e-01, -9.7400e-02], [-4.8791e-01, -2.2153e-01, -1.8444e-01, ..., -1.9465e-01, -2.4405e-01, 5.0802e-02], [-4.1235e-01, -4.6859e-01, -3.2017e-01, ..., -2.3232e-01, -1.8305e-01, -4.1211e-01]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[ 0.0551, -0.5440, -0.6146, ..., 0.1801, -0.3121, 0.2243], [ 0.2878, -0.2937, 0.0180, ..., -0.3412, -0.9847, 0.3291], [-0.2521, -0.2032, 0.6301, ..., -0.9992, -1.3043, 0.1372], ..., [-0.4388, -2.0501, -0.9857, ..., -1.4562, -1.1338, -0.2438], [-0.6619, -1.0800, -0.6420, ..., -0.9531, -0.9155, -0.5830], [-0.9757, -0.4903, -0.7896, ..., -0.0207, 0.0709, -0.3874]], [[-0.5626, -0.8982, -0.6051, ..., -0.4369, -0.3069, -0.0818], [-0.8539, -1.2124, -1.1421, ..., -1.0073, -1.0924, -0.4628], [-0.6097, -0.7027, -0.6621, ..., -1.2089, -1.3066, -0.7056], ..., [-0.5803, -0.5633, -0.2863, ..., -0.4478, -0.9250, -0.5883], [-0.6545, -0.8446, -0.5428, ..., -0.3727, -0.2968, 0.0574], [-0.6146, -0.6378, -0.3994, ..., -0.5582, -0.4681, -0.0292]], [[ 0.5193, 0.6878, 0.3467, ..., 0.2677, 0.1245, 0.1870], [ 0.1295, -0.1289, -0.1073, ..., 0.2351, -0.0065, 0.0047], [-0.1027, -0.4202, -0.5724, ..., -0.3395, -0.0770, 0.0321], ..., [-0.0276, 0.5198, 0.3101, ..., -0.0947, -0.1507, -0.2880], [-0.3556, -0.0265, -0.3138, ..., 0.0869, 0.0704, 0.0083], [ 0.1238, 0.6825, 0.2532, ..., -1.6974, -1.8882, -1.3618]], ..., [[ 0.8641, 0.6122, 0.5621, ..., 0.4198, 0.8420, 0.1588], [ 0.6023, 0.1794, 0.3502, ..., -0.0430, 0.5074, -0.0994], [ 0.1859, 0.1100, 0.5256, ..., 0.5270, 1.0384, -0.0507], ..., [ 0.1272, 0.4405, 0.4913, ..., 0.1201, -0.0852, -0.2832], [-0.0867, 0.4949, 0.4213, ..., 0.3035, 0.3179, -0.0499], [ 0.1581, 0.7226, 0.5054, ..., 0.4537, 0.4473, 0.4062]], [[-0.8049, -1.0994, -1.1123, ..., -0.5673, -0.4921, -0.0453], [-0.5760, -0.7352, -0.4998, ..., -0.4954, -0.2212, 0.0798], [-0.2649, 0.0948, -0.3051, ..., -0.3232, -0.4791, -0.2479], ..., [-0.2960, -1.0196, -0.1207, ..., -0.2136, -0.4638, 0.2480], [ 0.3223, -0.2110, -0.0877, ..., -0.2047, -0.7102, 0.3825], [-0.4926, -0.8470, -0.7704, ..., 0.5240, 0.2731, 0.9488]], [[ 0.7633, 0.5597, 0.6460, ..., 0.8756, 0.7951, 0.4977], [ 1.0543, 0.6982, 1.0793, ..., 1.1163, 1.2171, 0.3768], [ 0.6776, 0.4172, 0.4710, ..., 1.1850, 1.4658, 0.7831], ..., [ 0.8799, 0.0338, 0.0235, ..., 1.1350, 1.4108, 0.9740], [ 0.4392, -0.0140, 0.3868, ..., 0.5046, 0.7000, 0.4616], [ 0.2597, 0.1772, 0.4445, ..., 0.7297, 0.6433, 0.2840]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[ 5.0589e-02, 3.0239e-02, -6.5652e-02, ..., 2.9136e-02, 2.0472e-02, 3.3116e-02], [-4.3490e-02, -9.9013e-02, -1.4477e-01, ..., -1.2799e-01, -1.2968e-01, -1.1374e-01], [-1.7633e-02, 8.8080e-02, 9.0812e-02, ..., 8.1622e-02, 4.6130e-02, 9.6210e-03], ..., [-3.2275e-02, -1.1952e-01, -8.8798e-02, ..., -7.3871e-02, 7.0270e-02, 9.4732e-02], [-3.6789e-04, -1.6536e-01, -1.2563e-01, ..., -3.9373e-03, -1.1487e-04, 9.4181e-02], [-3.9141e-02, -6.5069e-02, -7.1126e-03, ..., -1.3528e-03, 2.9227e-02, 7.1437e-02]], [[ 4.9216e-01, 3.2340e-01, 3.7978e-01, ..., 8.5515e-02, 2.5145e-01, 2.8095e-01], [ 1.8159e-01, -1.3266e-01, 1.4115e-01, ..., 1.5686e-02, -9.7150e-03, -4.6657e-02], [ 9.7674e-02, -1.9594e-01, -9.3590e-02, ..., 1.4430e-01, 3.0088e-01, 1.6222e-01], ..., [ 2.1367e-01, 1.3881e-01, 1.8991e-02, ..., -3.0949e-02, 3.1051e-01, 1.7396e-01], [ 1.1380e-01, -7.6312e-02, -1.8014e-02, ..., 2.8777e-02, 2.3656e-01, 7.9741e-02], [ 2.0087e-01, 1.1959e-01, 1.2988e-01, ..., -1.5768e-01, -9.1760e-02, -1.4441e-01]], [[-1.3740e-01, -1.9611e-01, -2.0091e-01, ..., -1.5659e-01, -1.2471e-01, -1.8021e-01], [-1.7893e-01, -2.0166e-01, -1.0795e-01, ..., 5.2685e-02, -6.0237e-02, -8.4450e-02], [-1.2058e-01, -6.8945e-04, -3.3964e-03, ..., 9.2430e-02, -1.5144e-02, 1.0227e-02], ..., [ 1.2023e-01, 9.7808e-02, 6.1309e-02, ..., -1.4919e-01, -8.5181e-02, 1.0121e-01], [ 1.0048e-01, -7.8018e-03, -9.2056e-02, ..., -1.8924e-01, -2.1661e-01, -7.0055e-02], [ 8.6329e-02, -5.4492e-02, -1.6985e-01, ..., -2.0948e-01, -1.6376e-01, -2.4448e-02]], ..., [[-2.8329e-01, -3.6258e-01, -3.0192e-01, ..., -9.3842e-02, -1.5187e-01, -2.5100e-02], [-1.6744e-01, -1.9852e-01, -2.5101e-01, ..., 1.1434e-01, 2.3056e-03, 9.7841e-03], [-2.3171e-02, -5.7884e-02, -1.3461e-01, ..., -2.6371e-02, -1.9229e-01, -2.0811e-01], ..., [-6.9217e-02, -2.0768e-01, -2.8877e-02, ..., -5.8385e-02, 2.6056e-02, 4.0350e-02], [-8.4423e-02, -2.5802e-01, -2.9243e-02, ..., -1.7626e-01, -2.2544e-01, -1.3741e-01], [-1.7964e-02, -1.3211e-02, 1.2998e-01, ..., 4.6026e-02, 4.3756e-02, 2.3945e-02]], [[-3.3565e-01, -5.4160e-01, -6.0805e-01, ..., -3.8045e-01, -4.3757e-01, -3.0454e-01], [-1.4515e-01, -3.4559e-01, -4.1628e-01, ..., -3.6153e-01, -3.3174e-01, -9.0514e-02], [-1.2495e-01, -3.9581e-01, -3.5128e-01, ..., -2.6482e-01, -2.4520e-01, -1.8402e-01], ..., [-2.2884e-01, -6.1938e-01, -5.5678e-01, ..., -6.8449e-01, -5.8617e-01, -2.8077e-01], [-3.3351e-01, -5.2314e-01, -4.2504e-01, ..., -5.2789e-01, -6.0944e-01, -3.6275e-01], [-1.5605e-01, -3.6710e-01, -3.8210e-01, ..., -4.0686e-01, -4.1307e-01, -1.7890e-01]], [[-2.1081e-01, -2.9553e-01, -3.2521e-01, ..., -2.0666e-01, -1.2164e-01, -1.2524e-01], [-2.4101e-01, -2.0451e-01, -1.5815e-01, ..., -3.4341e-02, 4.8311e-02, 2.4075e-02], [-2.1997e-01, -2.2960e-01, -2.4427e-01, ..., 2.7745e-02, 6.6867e-02, 4.3925e-02], ..., [-7.6800e-03, -9.8346e-02, -2.0452e-01, ..., -1.2589e-01, 1.1875e-01, 7.0566e-02], [-8.1622e-02, -1.5807e-01, -2.0304e-01, ..., -1.0088e-01, 1.0701e-02, -1.2372e-02], [-9.3799e-02, -1.8981e-01, -2.0816e-01, ..., -2.0537e-01, -1.6077e-01, -1.8046e-01]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-0.1642, 0.6111, 0.6719, ..., 0.3935, 0.4093, 0.1318], [-0.2679, 0.4372, 0.4897, ..., 0.5854, 0.5598, 0.4889], [-0.0614, 0.0641, -0.0846, ..., 0.9225, 0.8031, 0.8296], ..., [-0.0864, -0.0523, -0.1517, ..., -0.3154, -0.4248, 0.7720], [-0.0210, -0.0107, -0.4515, ..., -0.3691, -0.4755, -0.3588], [ 0.2661, 0.2514, 0.4845, ..., 0.4435, 0.6367, -0.1002]], [[-0.2612, -0.2832, -0.6690, ..., -0.6152, -0.5100, -0.5664], [-0.5795, -0.6801, -1.0398, ..., -1.1175, -1.3070, -0.9341], [-0.4090, -0.7280, -0.6889, ..., -0.6297, -1.0082, -0.7394], ..., [ 0.2380, -0.9826, -0.7688, ..., -0.3479, -0.6661, -0.5269], [-0.8029, -1.2784, -1.1533, ..., -0.5312, -1.0361, -0.8832], [-0.6589, -0.5647, -0.3576, ..., -0.0677, -0.3550, -0.3389]], [[ 0.0297, -0.4572, -0.3457, ..., 0.0052, 0.3190, 0.2809], [-0.2332, -0.5157, -0.3640, ..., -0.4438, -0.3500, -0.5304], [-0.4940, -0.6755, -0.6056, ..., -0.2920, -0.5922, -0.1759], ..., [ 0.5285, 0.7424, -0.5153, ..., -0.4805, -0.3844, 0.4224], [ 0.3556, -0.6536, -0.6248, ..., 0.0121, 0.2211, 0.0086], [ 0.4613, -0.0046, -0.2656, ..., -0.1414, -0.0830, -0.6322]], ..., [[ 0.1395, -0.0938, -0.0249, ..., -0.2357, -0.2905, -0.4863], [-0.0598, 0.2485, 0.3585, ..., -0.4103, -0.3441, -0.6461], [-0.1429, 0.5415, 0.5160, ..., -0.2196, -0.0928, -0.3425], ..., [ 1.1019, -0.0367, -0.5643, ..., 0.0514, 0.1681, 0.1267], [ 0.9417, -0.0935, -0.6389, ..., -0.0209, 0.0145, 0.0313], [ 0.5574, 0.2148, -0.5317, ..., -0.3683, -0.4497, -0.1123]], [[ 0.0148, 0.0874, -0.7874, ..., 0.4710, 0.6335, -0.4608], [ 0.2926, 0.2670, -0.9552, ..., 0.2511, 0.6559, -0.3782], [ 0.5292, -0.1217, -0.6817, ..., -0.1196, 0.1346, -0.6021], ..., [ 0.2037, -0.5333, -0.2021, ..., 0.2152, 0.5675, -1.0983], [ 0.4190, -0.6143, 0.7062, ..., 0.4490, 0.7827, -0.9449], [ 0.1773, -0.5181, 1.0329, ..., 0.5269, 0.5063, -0.6873]], [[ 0.0336, -0.1246, -0.1963, ..., -0.3679, 0.0404, -0.4178], [-0.3616, -0.3861, -0.0552, ..., -0.0681, -0.0149, -0.3101], [-0.2323, -0.6257, -0.3293, ..., 0.0823, -0.1018, -0.2396], ..., [ 0.2612, -0.2018, -0.5177, ..., -0.7051, -0.2484, -0.2144], [ 0.2345, -0.1775, -0.4967, ..., -0.4719, -0.0954, -0.0237], [-1.0177, -0.5352, -0.4000, ..., -0.2615, -0.4057, -0.2122]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[ 1.3626e-03, -2.4663e-02, 4.6051e-02, ..., -9.1565e-03, 2.6631e-03, -2.3246e-02], [-1.3125e-01, -5.8948e-02, 7.5310e-02, ..., -4.9859e-02, -2.3506e-02, -4.2598e-02], [ 6.0517e-03, 7.9837e-02, 1.3708e-01, ..., -6.7412e-02, -1.6596e-01, -1.9063e-01], ..., [-1.0412e-01, 1.0787e-01, 1.9226e-02, ..., -1.7789e-01, -1.4185e-01, -1.1365e-01], [ 1.9537e-01, -8.7989e-02, -1.1354e-01, ..., -8.3751e-02, -8.7740e-02, -7.4591e-02], [ 1.0447e-01, -5.5854e-02, -5.7268e-02, ..., -5.2313e-02, 8.7574e-02, 4.5343e-02]], [[-4.4239e-02, -2.1504e-02, 4.3372e-03, ..., -1.2835e-02, -7.4009e-02, -4.2044e-02], [-5.3044e-02, -8.7785e-02, -1.2914e-01, ..., -4.7358e-02, -1.3473e-01, -6.2772e-02], [-9.3551e-02, -1.3260e-01, -1.3236e-01, ..., 1.3536e-01, 1.0653e-01, 1.2801e-01], ..., [-1.8869e-03, -6.6526e-02, -1.0954e-02, ..., 5.1577e-02, 2.0316e-01, 1.2561e-01], [-2.7441e-02, -9.5117e-02, 1.7368e-02, ..., 1.2589e-01, 2.3397e-01, 1.5138e-01], [-5.1650e-02, -5.0134e-02, 5.3820e-02, ..., 1.3207e-02, -7.1995e-04, -9.8985e-02]], [[-1.3365e-02, -4.0225e-02, 1.1895e-02, ..., -4.4649e-02, -2.2931e-02, -4.4239e-02], [ 8.7024e-03, -7.6718e-03, 5.0184e-02, ..., 7.8267e-03, -2.9017e-02, -1.0779e-02], [ 6.0618e-02, 1.2400e-02, 1.8309e-02, ..., -2.1703e-02, -4.2587e-02, -8.2061e-03], ..., [-4.6708e-03, -4.0856e-02, -1.2657e-01, ..., -7.2252e-02, 1.7713e-02, 5.6455e-02], [ 1.2032e-01, 8.5076e-03, -4.6789e-02, ..., 7.3983e-03, -6.1203e-02, 9.5531e-04], [ 9.6192e-02, 3.0080e-02, -2.5663e-02, ..., -2.1131e-02, -2.0930e-02, 5.3250e-03]], ..., [[ 8.0602e-02, 2.0411e-02, 6.5288e-02, ..., 6.6563e-02, 1.1606e-02, 2.7365e-02], [ 7.2081e-02, 8.9982e-02, 1.4145e-01, ..., 4.2192e-02, 5.2420e-02, 1.0945e-01], [ 5.4702e-02, 8.3627e-02, 3.0349e-02, ..., 9.6957e-02, 1.3886e-01, 1.6291e-01], ..., [ 1.2677e-01, -7.0860e-02, 4.2757e-02, ..., 5.4127e-02, 1.3381e-01, 1.5856e-01], [-6.8892e-02, -1.3785e-01, 6.4451e-02, ..., 9.7933e-02, 9.6460e-02, 1.6184e-01], [ 8.2498e-03, -7.8099e-04, 5.8352e-02, ..., 4.5872e-03, 3.9126e-02, 1.2256e-01]], [[-1.6818e-01, -1.2923e-01, -1.6857e-01, ..., -5.4568e-02, -1.2155e-01, -1.6396e-01], [-4.4197e-01, -3.9939e-01, -3.0251e-01, ..., -2.8055e-01, -3.1912e-01, -2.5067e-01], [-3.9235e-01, -3.4749e-01, -2.4425e-01, ..., -3.1263e-01, -3.2038e-01, -2.7487e-01], ..., [-5.7203e-01, -3.7472e-01, -2.5183e-01, ..., -3.1058e-01, -3.8692e-01, -3.0409e-01], [-7.4071e-01, -4.7059e-01, -2.6943e-01, ..., -2.5183e-01, -3.5373e-01, -2.9527e-01], [-5.1734e-01, -3.9472e-01, -2.5330e-01, ..., -4.2560e-01, -3.7086e-01, -3.3770e-01]], [[-6.9101e-03, -5.5174e-02, -5.2258e-02, ..., -5.5020e-02, -3.8445e-02, -1.0077e-01], [-5.0044e-02, -3.1171e-02, -2.5651e-02, ..., -5.5442e-02, -3.2126e-02, -1.1928e-01], [ 7.3592e-02, 8.5570e-02, 4.4706e-02, ..., -2.2834e-02, 5.2341e-02, -3.3763e-03], ..., [ 1.5344e-01, -1.2464e-01, -7.3227e-02, ..., -9.2346e-02, 1.2123e-01, 8.5718e-02], [ 6.0997e-02, -1.2332e-01, -1.0832e-02, ..., -7.4599e-02, -7.8659e-02, -1.0594e-01], [-4.7824e-02, -7.0945e-02, -1.7477e-02, ..., -8.5501e-02, -1.0939e-01, -1.5686e-01]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-4.2873e-01, -4.6934e-01, -4.1325e-02, ..., -3.2636e-02, 7.4030e-02, 5.8232e-03], [-5.5538e-01, -1.0775e+00, -5.9274e-01, ..., -9.1038e-01, -3.8420e-01, -3.6700e-01], [-4.9508e-01, -1.0233e+00, -2.1957e-01, ..., -5.9944e-01, -3.6409e-01, -1.1305e-01], ..., [-6.3640e-01, -2.6598e-01, -2.8498e-01, ..., -3.0316e-01, -3.0609e-01, -9.3499e-01], [ 1.0858e-01, 9.1596e-02, -3.1969e-01, ..., -4.5866e-01, -2.9367e-01, -9.2299e-01], [ 6.3381e-01, 1.1834e-01, -1.6743e-01, ..., -1.6575e-01, -1.6571e-01, -2.6300e-01]], [[ 4.0905e-03, -4.3001e-01, -5.7072e-01, ..., 8.1292e-04, -6.4482e-01, -3.6637e-01], [-3.0681e-01, 1.5747e-02, -2.6038e-01, ..., 3.0524e-01, -2.2313e-01, 4.0327e-02], [-5.6381e-01, 1.9786e-03, -1.3212e-01, ..., 6.2308e-02, -5.2619e-01, -1.3217e-01], ..., [-7.0862e-01, -9.1414e-01, -2.5121e-01, ..., -4.9914e-01, -7.5175e-01, -1.7616e-01], [-2.1418e-01, -4.5349e-01, -3.8520e-01, ..., -3.2243e-01, -4.9149e-01, -3.4339e-01], [ 9.3637e-02, -8.7578e-02, -2.4526e-01, ..., -1.3802e-01, -3.6111e-01, -5.0108e-01]], [[ 7.3805e-01, 1.4611e-01, -2.7105e-01, ..., -1.2570e-01, -2.0609e-01, -7.4954e-02], [ 8.2128e-01, 3.0414e-01, -2.0269e-01, ..., 1.8832e-01, 3.5639e-01, 8.4530e-01], [ 5.3060e-01, -1.9782e-01, -3.9589e-01, ..., -3.7223e-01, -3.4382e-01, -8.0846e-02], ..., [-5.9906e-01, -6.1805e-01, -3.0498e-01, ..., -8.5905e-01, -1.4049e+00, -7.6847e-01], [-3.7451e-01, -3.8290e-01, -7.8562e-02, ..., -2.9276e-02, -6.0355e-01, -3.1064e-01], [ 8.4362e-01, 5.4807e-01, 2.0263e-01, ..., 3.1692e-01, 1.3642e-01, 6.8857e-01]], ..., [[ 5.1337e-01, 3.9596e-01, 2.2522e-01, ..., 4.1894e-01, 1.0254e-01, -4.0375e-01], [-5.3977e-03, -3.4549e-01, -5.9793e-01, ..., -1.7841e-01, -3.0228e-01, -8.4002e-01], [-2.3763e-01, -6.0266e-01, -3.2253e-01, ..., -1.2647e-01, -2.9172e-01, -1.0146e+00], ..., [-8.1082e-02, -4.2886e-01, 3.3912e-01, ..., -7.1773e-01, -3.2211e-01, -4.3716e-01], [ 3.6547e-02, 6.8812e-01, -3.2904e-01, ..., -1.8101e-01, -4.1893e-01, -7.3784e-01], [-3.4927e-01, 1.3177e+00, 2.0435e-01, ..., -1.9723e-02, -6.6064e-02, -2.3001e-01]], [[ 6.9231e-01, -1.0600e-01, -2.5855e-01, ..., -4.6072e-01, 9.7681e-01, -7.9610e-01], [-5.8889e-01, -1.1106e+00, -3.1060e-02, ..., -3.3787e-01, 5.3565e-01, -5.3625e-01], [-5.6028e-01, -7.4135e-01, -1.4226e-01, ..., 4.8697e-02, 1.7423e-01, -7.5924e-01], ..., [-5.8227e-01, -3.0304e-01, -1.0266e+00, ..., -9.6372e-01, -5.2071e-01, -5.8520e-01], [-3.2737e-01, -5.4993e-01, -1.3333e+00, ..., -5.4997e-01, -2.9026e-01, -8.1955e-01], [ 5.2538e-02, 5.0382e-01, -6.4723e-01, ..., -3.1758e-01, -2.8657e-01, -6.7551e-01]], [[-9.3298e-01, -8.5845e-01, -1.0554e+00, ..., -8.7959e-01, -7.9436e-01, -1.1912e+00], [-1.0732e+00, -9.6579e-01, -8.0460e-01, ..., -1.0780e+00, -5.7669e-01, -1.0331e+00], [-1.1648e+00, -9.7872e-01, -5.6461e-01, ..., -9.1402e-01, -4.6951e-01, -8.8273e-01], ..., [-1.5256e+00, -1.6750e-01, 2.1887e-01, ..., -9.9437e-01, -7.7857e-01, -1.0885e+00], [-1.3656e+00, 2.4792e-01, -5.4816e-01, ..., -1.2917e+00, -1.4464e+00, -1.6200e+00], [-7.4554e-01, 7.8284e-01, -6.8172e-02, ..., -4.8101e-01, -8.3025e-01, -8.2073e-01]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[ 0.0988, 0.1280, 0.0348, ..., 0.0244, 0.0435, 0.0492], [-0.0232, 0.0532, 0.0509, ..., -0.1039, -0.0123, 0.0517], [-0.0195, -0.0130, 0.0014, ..., 0.0052, 0.1269, 0.0927], ..., [ 0.0612, -0.0726, -0.0816, ..., -0.0858, 0.0101, 0.0488], [ 0.0246, -0.0265, -0.0734, ..., -0.0315, 0.0121, 0.0841], [-0.0066, -0.0592, -0.0653, ..., 0.0848, 0.0086, 0.0533]], [[ 0.0054, 0.0172, 0.0109, ..., 0.0438, -0.0023, 0.0667], [ 0.0028, 0.0551, -0.0138, ..., 0.0370, -0.0277, 0.0705], [-0.0194, 0.0452, -0.0279, ..., 0.0167, -0.0203, 0.0328], ..., [ 0.0789, -0.0072, -0.0014, ..., 0.0319, -0.0005, 0.0071], [ 0.0310, -0.0774, 0.0336, ..., -0.0352, -0.0523, 0.0280], [ 0.0317, -0.0733, 0.0212, ..., -0.0258, -0.0484, 0.0304]], [[ 0.0447, 0.0728, 0.0419, ..., 0.0372, 0.0194, 0.0387], [ 0.0556, 0.0240, 0.0387, ..., 0.0497, 0.0876, 0.0938], [-0.0082, 0.0247, 0.0094, ..., -0.0385, 0.0543, 0.0157], ..., [-0.0077, -0.0965, -0.0529, ..., 0.0185, 0.0220, 0.0010], [ 0.0037, -0.1304, -0.0403, ..., 0.0028, -0.0538, 0.0186], [ 0.0494, 0.0494, -0.0566, ..., -0.0360, 0.0525, 0.0500]], ..., [[ 0.1733, 0.1157, 0.0382, ..., 0.0869, 0.1794, 0.1358], [-0.0386, -0.0362, 0.0066, ..., 0.0206, -0.0064, -0.0312], [ 0.0028, -0.0682, -0.0012, ..., -0.0308, -0.0326, 0.0439], ..., [ 0.1916, 0.0499, -0.0054, ..., 0.0241, 0.0702, 0.0216], [ 0.1830, 0.0439, 0.0128, ..., 0.0936, 0.0854, 0.0686], [ 0.1746, 0.0807, -0.0055, ..., -0.0044, 0.0453, 0.0411]], [[-0.0622, -0.0869, -0.0834, ..., -0.0582, -0.0993, -0.1090], [-0.0826, -0.2253, -0.1340, ..., -0.1373, -0.1944, -0.1553], [-0.1094, -0.1520, -0.1378, ..., -0.1157, -0.2341, -0.2295], ..., [-0.2469, -0.1926, 0.0333, ..., -0.1980, -0.3922, -0.2547], [-0.0622, -0.0641, 0.0081, ..., -0.1415, -0.1819, -0.0514], [-0.1384, -0.0867, -0.0205, ..., -0.1600, -0.1566, -0.0787]], [[ 0.1696, 0.0830, 0.0843, ..., 0.0816, 0.1086, 0.0569], [ 0.0366, 0.1129, 0.0805, ..., 0.0165, 0.0319, -0.0460], [-0.0262, 0.0282, 0.0309, ..., 0.0037, 0.0894, 0.0436], ..., [ 0.1452, 0.0994, 0.0504, ..., 0.0656, 0.2144, 0.1010], [ 0.1647, 0.0498, 0.0299, ..., -0.0269, 0.1108, 0.0720], [ 0.1522, 0.0643, -0.0155, ..., 0.0290, 0.0490, 0.0259]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[ 4.1243e-01, 3.6304e-01, 8.8107e-01, ..., 6.0127e-01, 6.5288e-01, 9.4326e-01], [ 6.0697e-01, 6.4844e-01, 8.7361e-01, ..., 6.2816e-01, 6.8928e-01, 5.3948e-01], [-1.1460e-03, 4.0684e-01, 6.6968e-01, ..., 2.4982e-01, 3.0477e-01, 4.4353e-01], ..., [ 1.0008e+00, 3.6334e-01, 3.7838e-01, ..., 6.2974e-02, 1.3335e-01, 7.1240e-01], [ 9.4002e-01, 4.6166e-01, 3.9414e-01, ..., 1.1289e-01, 2.4865e-01, 1.6548e+00], [ 4.5392e-01, 3.8610e-01, 2.1027e-01, ..., 3.0638e-01, 4.6619e-01, 7.9911e-01]], [[-4.5250e-01, -2.0844e-01, -2.4178e-01, ..., -2.8272e-01, -3.9787e-01, -8.4349e-01], [-8.0248e-01, -2.7057e-01, -4.7645e-01, ..., -3.7947e-01, -1.8241e-01, 6.5178e-02], [-5.1991e-01, -6.0929e-01, -3.5325e-01, ..., -5.2016e-01, -2.4186e-01, -6.5532e-01], ..., [-5.9643e-01, 5.7050e-01, -8.3123e-01, ..., -4.4619e-01, -2.6789e-01, -5.2685e-01], [ 1.7776e-01, 1.0316e-01, -6.1159e-01, ..., -8.4718e-01, -9.2070e-02, -6.6023e-01], [-4.6888e-01, -3.4122e-02, -8.5488e-02, ..., -1.0417e+00, -1.8465e-02, -4.5028e-01]], [[ 1.3665e+00, 6.7283e-01, 8.2696e-01, ..., 1.1734e+00, 5.8562e-01, 1.1521e+00], [-3.5310e-01, -3.1397e-01, 1.0160e-01, ..., 2.0190e-01, -1.2062e-01, 1.1574e-02], [ 1.8670e-01, -7.1362e-01, -2.7392e-01, ..., -4.8404e-01, -5.5818e-01, -1.8868e-01], ..., [ 5.7915e-01, 4.0021e-01, 1.3172e-01, ..., -2.4053e-01, 3.9218e-02, 4.3768e-01], [ 4.3819e-01, -4.0679e-01, -2.1769e-03, ..., -8.8953e-02, 8.1186e-02, 3.6979e-01], [ 1.4249e+00, -2.3837e-02, -1.5262e-01, ..., 5.2375e-01, 2.9434e-01, 2.4603e-01]], ..., [[ 3.8155e-02, -1.6977e-01, -2.0032e-01, ..., -5.6964e-01, -2.9177e-01, -9.4349e-02], [ 2.4736e-01, 1.3977e-01, -1.2234e-01, ..., -4.4522e-01, 6.4455e-02, 3.9772e-01], [-6.1862e-01, -2.3263e-01, 2.2372e-01, ..., -1.3316e-01, 1.3979e-01, 4.3874e-01], ..., [-4.4341e-02, 1.2667e-01, -1.4757e-01, ..., -4.7259e-02, -8.4698e-02, -3.8401e-01], [ 5.8723e-02, -4.6699e-01, 2.9764e-01, ..., 1.5851e-02, 2.0578e-01, -2.7948e-01], [ 3.0847e-01, -7.3721e-01, -8.6126e-01, ..., -1.1998e+00, -4.6212e-01, 1.3137e+00]], [[ 8.6156e-02, -2.0926e-01, -7.1230e-01, ..., -1.3362e-01, 3.1789e-01, 3.6001e-01], [-1.2724e-01, -1.2035e-01, -6.0503e-01, ..., -1.7572e-01, -2.1856e-01, -1.0504e-01], [ 6.0878e-02, 2.0468e-01, 6.8006e-02, ..., 7.5139e-01, 2.5093e-02, -1.5781e-01], ..., [ 8.0393e-01, 6.9006e-02, 1.1580e+00, ..., -6.4039e-01, -7.3785e-02, 6.7414e-01], [ 8.7700e-01, -4.3417e-01, -2.9335e-01, ..., -1.4228e-01, 2.3888e-01, 7.7920e-01], [ 5.2887e-01, -3.4151e-01, -3.1082e-01, ..., 2.4944e-01, -8.7037e-02, 7.1645e-01]], [[ 1.2785e-01, -2.4757e-01, 4.0018e-01, ..., -7.1277e-01, -4.4163e-01, -3.2978e-01], [-3.8121e-01, -4.8290e-01, -4.8577e-01, ..., -1.7976e+00, -1.0235e+00, -8.7740e-01], [-4.0046e-01, -3.5255e-01, 2.1916e-02, ..., -1.6129e+00, -1.1431e+00, -9.3650e-01], ..., [-1.4788e+00, -6.9077e-01, -4.5148e-01, ..., -8.8217e-01, -9.8117e-01, -1.0065e+00], [-1.0895e+00, -9.9224e-01, -6.8925e-01, ..., -5.7167e-01, -6.9166e-01, -1.7308e+00], [-1.4956e+00, -9.4706e-01, -8.6554e-01, ..., -1.4528e+00, -7.2765e-01, -1.3770e-01]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-0.1236, -0.0602, -0.1390, ..., -0.4963, -0.4020, -0.1198], [-0.0350, -0.0439, -0.2125, ..., -0.0424, 0.0160, 0.1509], [-0.2220, -0.1353, -0.3867, ..., -0.1576, -0.0601, -0.0723], ..., [-0.1874, -0.1648, -0.3391, ..., 0.0121, -0.2412, -0.3454], [-0.1325, -0.4435, -0.3661, ..., -0.3154, -0.1932, -0.2613], [-0.3495, -0.1867, -0.0766, ..., -0.2538, -0.1465, -0.1136]], [[-0.3884, -0.2835, -0.3478, ..., -0.3397, -0.6106, -0.5631], [-0.2955, -0.1269, 0.0024, ..., -0.0864, -0.4283, -0.4903], [-0.0700, -0.0156, -0.0923, ..., 0.1401, -0.0755, -0.4042], ..., [ 0.1401, -0.3010, -0.3625, ..., -0.2610, -0.4198, -0.2596], [-0.1279, -0.3806, -0.5162, ..., -0.3461, -0.4323, -0.2634], [-0.2910, -0.3711, -0.3404, ..., -0.1880, -0.1061, -0.1619]], [[-0.2050, -0.5653, -0.7059, ..., -0.5888, -0.4452, -0.2084], [-0.3620, -0.8297, -0.5641, ..., -0.7035, -0.6533, -0.4250], [-0.2378, -0.5462, 0.0864, ..., -0.2322, -0.3490, -0.2139], ..., [-0.6504, -0.7616, 0.0459, ..., 0.2952, 0.3382, 0.0308], [-0.6238, -0.5782, 0.0213, ..., 0.0991, -0.1207, 0.2721], [-0.2482, -0.3077, 0.1315, ..., -0.0717, -0.3099, 0.0063]], ..., [[ 0.0926, -0.0044, -0.0968, ..., 0.0761, -0.0428, 0.1569], [ 0.1673, 0.0863, -0.1966, ..., 0.2720, 0.1123, 0.2954], [ 0.1487, 0.1765, 0.2782, ..., 0.0290, -0.0423, 0.4044], ..., [ 0.1914, -0.1436, -0.2107, ..., -0.1952, 0.0748, -0.0246], [-0.2369, 0.0786, -0.0242, ..., -0.1436, -0.2877, 0.1694], [-0.0514, 0.2114, 0.3048, ..., 0.1482, -0.3507, 0.4003]], [[-0.1723, 0.0760, 0.0721, ..., 0.0341, 0.1266, -0.2085], [-0.2347, -0.3643, 0.0663, ..., -0.3313, -0.1482, -0.2671], [-0.3051, -0.3792, 0.2800, ..., -0.1511, -0.1272, -0.2918], ..., [ 0.0721, 0.0340, 0.2539, ..., 0.4027, 0.0683, -0.3045], [ 0.0758, -0.1852, -0.1356, ..., 0.1785, 0.0511, -0.0705], [-0.2175, -0.0551, -0.2782, ..., 0.0408, -0.0052, -0.0972]], [[ 0.2038, 0.0062, 0.1736, ..., 0.1367, 0.2460, 0.1823], [ 0.1573, -0.2326, 0.0208, ..., -0.3015, 0.0737, 0.1335], [ 0.1387, 0.0569, 0.3607, ..., -0.0596, 0.1768, 0.2967], ..., [ 0.7866, 0.5105, 0.4362, ..., 0.0980, 0.3943, 0.5658], [ 0.5503, 0.0597, 0.1589, ..., -0.0902, 0.3756, 0.5093], [ 0.1137, -0.0809, -0.0159, ..., 0.1156, 0.3735, 0.4709]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-3.2580e-02, 4.6716e-03, -6.6709e-02, ..., -4.7522e-03, -2.2099e-03, 9.0466e-02], [-1.6302e-01, -2.9018e-02, -2.1720e-01, ..., -4.3875e-02, 1.0886e-02, 2.8372e-02], [-2.5479e-01, -1.4047e-01, -1.7275e-01, ..., 8.0531e-02, 1.9334e-01, 1.5276e-02], ..., [-2.1139e-01, -1.4825e-01, -4.9402e-03, ..., 9.2946e-02, -1.0612e-01, -1.7040e-01], [-2.0970e-01, -1.6195e-01, -1.7520e-01, ..., -6.7448e-02, 4.9452e-02, -1.1551e-01], [-2.7232e-01, -9.8204e-02, -8.8646e-02, ..., 8.4115e-03, 1.1206e-01, -1.0140e-02]], [[ 4.8812e-02, -6.0470e-02, -5.8316e-02, ..., -8.2867e-02, -9.3440e-02, -7.1303e-02], [-1.4589e-01, -1.4611e-01, -1.8509e-01, ..., 7.8155e-02, -5.8147e-03, -1.0605e-01], [-7.0111e-02, -2.2644e-01, -8.3414e-02, ..., 2.0901e-01, 8.1771e-02, -1.6470e-02], ..., [ 1.8304e-01, -8.1817e-03, 6.9792e-02, ..., -1.0695e-01, -1.7610e-01, -1.3897e-01], [ 2.4655e-01, -1.4628e-01, -2.1027e-01, ..., -2.4190e-01, 8.1670e-03, 2.2009e-01], [ 1.0761e-02, -2.2890e-01, -2.6206e-01, ..., -1.8678e-01, 5.0541e-02, 1.7424e-01]], [[-6.4699e-01, -6.5046e-01, -6.4946e-01, ..., -6.3103e-01, -5.8418e-01, -6.0015e-01], [-3.5077e-01, -2.5295e-01, -3.1632e-01, ..., -2.6033e-01, -3.3463e-01, -4.6449e-01], [-3.1468e-01, -3.2959e-01, -2.4537e-01, ..., -3.4004e-01, -2.1492e-01, -1.9570e-01], ..., [-5.4013e-01, -2.5121e-01, -1.6403e-01, ..., -1.5129e-02, -2.1977e-01, -2.5057e-01], [-3.8462e-01, -2.4791e-01, -2.3765e-01, ..., -2.2953e-01, -4.0156e-01, -2.9487e-01], [-8.4561e-01, -1.3793e-01, -1.1995e-01, ..., -3.2368e-01, -1.9963e-01, -2.4796e-01]], ..., [[ 5.7024e-01, 6.6056e-01, 9.5901e-02, ..., -2.4350e-02, 2.7821e-01, 1.2417e-01], [-7.0457e-02, 3.7125e-02, -8.5546e-02, ..., -7.4320e-02, -1.3726e-01, 6.1165e-03], [-9.4908e-02, -4.6271e-02, 2.8854e-02, ..., -4.7003e-02, 1.2840e-01, -2.9733e-02], ..., [-1.0554e-01, -6.0221e-02, -1.7736e-01, ..., 7.9574e-03, -2.8799e-01, -2.2224e-01], [-5.7899e-02, -1.0701e-01, -1.7920e-01, ..., -1.4876e-01, -2.7093e-01, -7.9749e-02], [-4.4996e-02, -2.9580e-01, 2.2398e-01, ..., -2.4354e-01, -2.1907e-01, -8.4004e-02]], [[ 3.4024e-01, -1.5116e-01, -7.1770e-02, ..., -1.6114e-01, 9.6471e-02, 3.4851e-01], [-3.5924e-01, -1.8450e-01, 9.4673e-02, ..., -3.2239e-01, -3.4507e-01, -7.4684e-02], [-2.6030e-01, -2.0770e-01, -3.4150e-01, ..., -2.8970e-01, 2.1401e-01, -9.7496e-02], ..., [ 1.9954e-01, -2.0608e-01, -1.2124e-01, ..., 1.2212e-01, 9.7532e-02, -3.5990e-01], [ 4.1975e-01, -2.3223e-01, -2.0307e-01, ..., -3.8297e-01, -3.5574e-01, -6.8504e-02], [ 1.8114e-04, -2.0356e-01, 1.0351e-01, ..., -2.3819e-01, -1.4674e-01, -1.1833e-01]], [[ 3.0758e-01, 2.7873e-01, 5.4953e-02, ..., 1.4411e-01, 1.1629e-01, 1.0042e-01], [ 1.7667e-01, -1.8428e-01, 5.3541e-02, ..., 1.0744e-01, 3.9272e-02, 1.7413e-01], [ 2.0926e-03, 1.1734e-01, 4.6211e-02, ..., -2.7263e-01, 5.6385e-03, -6.9367e-02], ..., [ 2.7898e-01, 9.0060e-02, 1.0426e-01, ..., -5.5958e-02, -9.9089e-02, 1.5218e-01], [ 7.2136e-02, -2.8248e-02, 5.7768e-02, ..., -1.7842e-01, -4.6636e-02, 3.7414e-01], [ 3.3351e-01, -1.6085e-01, 6.2398e-02, ..., -2.3159e-02, 2.2459e-01, 1.4017e-01]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[ 0.8268, 0.4441, 0.3694, ..., 0.4115, 0.7633, 0.8978], [-0.0194, -0.7248, -0.6000, ..., -0.7206, -0.5846, -0.1153], [ 0.1886, -0.4891, -0.5042, ..., -0.5467, -0.4565, -0.1056], ..., [ 1.0802, 0.5223, 0.2821, ..., -0.1917, -0.3600, -0.3545], [ 1.0232, 0.4767, 0.1597, ..., -0.3930, -0.4350, -0.0377], [ 1.0075, 0.7341, 0.2484, ..., 0.2522, -0.0150, 0.2700]], [[-0.5701, -0.7030, -0.7707, ..., -0.6556, -0.8253, -0.5767], [-0.5188, -0.8589, -0.7814, ..., -0.9747, -1.1042, -0.6838], [-0.5817, -0.6517, -0.4354, ..., -0.3231, -0.6983, -0.5580], ..., [-0.5730, -0.7506, -0.6265, ..., -0.5238, -0.5434, -0.4181], [-0.3847, -0.6073, -0.8639, ..., -0.4829, -0.4861, 0.0185], [-0.4507, -0.4574, -0.5627, ..., -0.2748, -0.5572, -0.3460]], [[ 0.1596, 0.0046, -0.2229, ..., 0.2172, 0.4685, 0.4965], [ 0.2181, 0.2517, 0.3000, ..., -0.0341, -0.2611, -0.2299], [ 0.4062, 0.3518, 0.4570, ..., 0.0234, -0.1292, -0.0786], ..., [-0.3141, -0.1117, 0.1558, ..., 0.0288, -0.2321, -0.1655], [ 0.0597, -0.0387, 0.2186, ..., 0.3252, 0.3253, 0.2932], [ 0.0218, -0.1647, -0.0439, ..., 0.1506, 0.2616, 0.2033]], ..., [[-0.3023, -0.5082, -0.4832, ..., -0.8931, -0.7944, -0.5214], [-0.4516, -0.2861, -0.5033, ..., -0.8971, -1.2599, -0.8366], [-0.5091, -0.2623, -0.6127, ..., -0.9415, -1.1568, -0.9192], ..., [-0.8310, -1.0234, -1.1779, ..., -0.3845, -0.4947, -0.1517], [-0.9886, -0.7029, -0.4522, ..., -0.5521, -0.8839, -0.6843], [-0.3870, -0.2183, -0.0513, ..., -0.3436, -0.6146, -0.6278]], [[ 0.0665, 0.0452, -0.1549, ..., 0.1974, -0.2540, -0.0679], [-0.3426, 0.0577, -0.8722, ..., -0.5749, -0.8167, -0.1882], [-0.7388, 0.0540, -0.1547, ..., -0.7001, -1.1010, -0.0070], ..., [-0.4458, -0.0720, -0.0663, ..., -0.1730, -0.5392, -0.0321], [ 0.0398, 0.3934, -0.1953, ..., -0.4248, -0.6574, -0.3950], [-0.1572, 0.5568, 0.0729, ..., -0.4862, -0.5172, -0.2908]], [[-0.9220, -1.2343, -0.1687, ..., -0.2934, -0.1706, -0.1749], [-0.9835, -1.1848, -0.7574, ..., -0.2448, -0.1289, 0.0472], [-1.0677, -1.1468, -0.6967, ..., -0.5125, -0.0795, 0.3138], ..., [-0.1524, -0.4937, -0.6800, ..., 0.0790, -0.0247, -0.2594], [-0.4430, -0.3362, -0.4801, ..., -0.0096, -0.7177, -0.2634], [-0.3126, -0.4949, -0.1230, ..., -0.0975, -0.8262, -0.2739]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[ 0.0891, -0.0343, -0.0670, ..., 0.0482, 0.0205, 0.0690], [ 0.0079, -0.0992, -0.1780, ..., -0.0172, 0.0695, 0.0783], [-0.1336, -0.1911, -0.1928, ..., -0.0254, -0.0152, -0.0049], ..., [-0.0549, -0.0683, -0.1709, ..., -0.0291, -0.0146, -0.0118], [-0.0996, -0.1211, -0.2715, ..., 0.0197, -0.0492, -0.0516], [-0.0667, -0.0486, -0.1591, ..., -0.0957, -0.0715, -0.1130]], [[-0.0414, -0.0187, -0.0580, ..., -0.0443, -0.0396, -0.0592], [-0.0774, -0.0847, -0.1103, ..., -0.0007, 0.0389, 0.0294], [-0.0698, -0.0738, -0.0786, ..., -0.0846, -0.1175, -0.1000], ..., [-0.0121, -0.1405, -0.0540, ..., -0.1062, -0.1952, -0.1241], [-0.0941, -0.1619, -0.0185, ..., -0.2264, -0.1543, -0.0991], [-0.0718, -0.0770, 0.0399, ..., -0.1147, 0.0332, 0.0185]], [[-0.0967, -0.1833, -0.1092, ..., -0.2087, -0.1302, -0.0594], [-0.1555, -0.2623, -0.1557, ..., -0.2220, -0.2110, -0.1157], [-0.1416, -0.1988, -0.1886, ..., -0.1003, -0.1234, -0.0743], ..., [-0.1254, -0.2000, -0.1661, ..., -0.0280, -0.0018, -0.0108], [-0.1081, -0.1427, -0.1374, ..., -0.1261, -0.1929, -0.0767], [-0.0184, -0.0225, -0.0251, ..., -0.0505, -0.0375, 0.0290]], ..., [[-0.2503, -0.2906, -0.2482, ..., -0.0599, -0.1782, -0.1961], [-0.2494, -0.3674, -0.2781, ..., -0.1314, -0.1071, -0.1463], [-0.0903, -0.1135, -0.0733, ..., -0.0469, 0.0529, -0.0597], ..., [-0.1460, -0.1257, -0.0230, ..., 0.0024, 0.0556, -0.0626], [-0.1900, -0.0814, -0.0467, ..., -0.0095, -0.0219, -0.0222], [-0.1668, -0.0776, -0.0633, ..., -0.0105, -0.1150, -0.0501]], [[-0.0462, -0.0058, -0.0168, ..., -0.0012, -0.0573, 0.0469], [-0.1890, -0.1472, -0.0599, ..., -0.1150, -0.0626, 0.0218], [-0.0405, -0.0680, 0.0672, ..., -0.0256, 0.0763, 0.0875], ..., [ 0.0059, -0.0371, -0.0750, ..., 0.0666, 0.0448, -0.0269], [ 0.0553, -0.0393, -0.1183, ..., 0.0850, 0.0264, -0.0066], [ 0.0347, -0.0798, -0.1994, ..., 0.0171, -0.0586, -0.0245]], [[-0.0178, -0.0318, -0.0838, ..., 0.0773, 0.0054, -0.0698], [-0.0545, -0.0315, -0.0842, ..., 0.0640, -0.0761, -0.1300], [-0.0559, -0.0413, -0.0776, ..., -0.1542, -0.1934, -0.1077], ..., [ 0.0022, 0.1460, 0.1342, ..., -0.1196, -0.1491, -0.1003], [ 0.0401, 0.1450, 0.1216, ..., -0.0765, -0.1312, -0.1566], [ 0.0010, 0.0654, 0.0558, ..., -0.0464, -0.0703, -0.1186]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-0.2521, -0.8738, -0.6532, ..., 0.0576, -0.2081, -0.3561], [-0.3011, -0.5276, -0.6430, ..., -0.1986, -0.4507, -0.3705], [-0.4933, -0.8762, -1.2042, ..., 0.0192, -0.5263, -0.4188], ..., [-0.4822, -0.5530, -1.0430, ..., -0.2959, -0.1074, -0.3416], [-0.5835, -0.6165, -0.9733, ..., -1.1879, -0.3531, -0.4250], [-0.6776, -0.6156, -0.5577, ..., -0.5834, -0.3649, -0.2203]], [[ 0.0177, -0.1427, -0.3212, ..., -0.2198, -0.2059, 0.0977], [-0.2004, -0.5116, -0.3690, ..., -0.4446, -0.8544, -0.5728], [-0.2213, -0.3730, -0.3643, ..., -0.2383, -0.4089, -0.3251], ..., [-0.1507, -1.1372, -0.9870, ..., -0.6122, -0.4332, -0.1330], [-0.3038, -0.9152, -0.8302, ..., -0.3705, -0.4816, -0.3446], [-0.4832, -0.5504, -0.2201, ..., -0.2456, -0.3247, -0.3117]], [[-0.0672, -0.1554, 0.0552, ..., -0.2345, -0.7497, -0.6175], [ 0.2831, 0.0600, -0.0768, ..., -0.3010, -0.7343, -0.7601], [-0.1145, -0.3830, -0.6809, ..., -0.5276, -0.5555, -0.6247], ..., [-0.2057, -0.7569, -0.5689, ..., -0.6663, -0.5145, 0.0535], [-0.6855, -0.7507, -0.1757, ..., -0.1187, -0.5039, 0.0934], [-0.4590, -0.5180, -0.2126, ..., -0.4002, -0.3137, -0.5877]], ..., [[-0.5173, -0.7319, -0.5628, ..., -0.7071, -0.6873, -0.4385], [-0.7093, -1.0123, -0.5945, ..., -0.5060, -0.6159, -0.7295], [-0.5527, -0.6056, -0.2464, ..., -0.7749, -0.8456, -0.8498], ..., [-0.9676, -1.5211, -1.0507, ..., -0.5427, -0.3854, -0.7520], [-0.9118, -1.0320, -0.8958, ..., -0.7115, -0.5381, -0.4616], [-0.5716, -0.8103, -0.7555, ..., -0.4182, -0.5253, -0.5415]], [[-0.5313, -0.3614, -0.5246, ..., -0.6295, -0.5290, -0.4729], [-0.2976, -0.3723, -0.5166, ..., -0.8591, -0.6088, -0.5045], [-0.7066, -0.2102, -0.0457, ..., -0.8686, -0.8044, -0.7096], ..., [-0.9559, -0.4253, -0.2284, ..., -0.5182, -0.3698, -0.6497], [-1.1067, -0.8729, -0.9031, ..., -0.4912, -0.2269, -0.6201], [-0.8432, -0.5654, -0.6099, ..., -0.4652, -0.3318, -0.4693]], [[ 0.1028, 0.1567, 0.1607, ..., 0.8193, 0.1068, 0.1878], [ 0.0629, 0.0170, 0.6012, ..., 0.4266, 0.1424, 0.2063], [ 0.1214, 0.1671, 1.2869, ..., 0.3030, 0.1830, 0.3461], ..., [-0.1943, 0.3957, -0.0051, ..., 0.3658, 0.8822, 0.6292], [ 0.2586, 0.7748, -0.0087, ..., 0.5417, 0.2212, -0.0846], [ 0.5294, 0.7278, 0.1215, ..., 0.1824, -0.2166, -0.2289]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[ 5.9690e-02, 8.1791e-02, 8.8460e-02, ..., 1.2052e-02, -2.9440e-02, 4.1868e-02], [ 2.0751e-01, 1.9978e-01, 3.1553e-01, ..., 5.5059e-02, 7.1169e-02, 7.4423e-02], [ 9.4788e-02, 9.9758e-02, 5.5432e-02, ..., -2.8651e-02, -9.6179e-02, -2.8936e-02], ..., [-5.7275e-02, -6.1507e-02, -2.1981e-02, ..., 5.6889e-02, 4.3233e-02, 8.5275e-02], [-7.9141e-02, -2.5249e-02, -6.8972e-02, ..., 3.9908e-02, -6.9416e-02, 2.6843e-03], [ 2.1747e-02, 1.2513e-01, 7.1213e-02, ..., -2.8586e-02, -1.3100e-02, -2.4738e-02]], [[-2.7763e-02, -1.1040e-01, -5.0566e-02, ..., -7.8674e-02, -3.8930e-02, 2.7603e-02], [-4.1623e-02, -9.8588e-02, -4.6023e-02, ..., -5.2057e-02, -9.7615e-02, -7.2089e-02], [-5.5634e-02, -9.4488e-02, -5.6073e-02, ..., -2.9765e-02, -1.4687e-01, -1.0436e-01], ..., [ 3.8538e-02, -3.3256e-02, -2.8461e-02, ..., -8.6775e-03, -6.4014e-02, -6.6953e-02], [-1.2763e-02, -9.8569e-02, -8.2613e-02, ..., -2.2940e-02, -7.3775e-02, -8.6286e-02], [-8.1326e-02, -1.1877e-01, -8.2915e-02, ..., -2.1756e-02, -5.5721e-02, -5.7234e-02]], [[ 5.3683e-02, -7.5098e-02, -1.4165e-01, ..., -1.2710e-01, -3.9675e-02, -2.3752e-03], [-9.5284e-02, -1.8529e-01, -1.0618e-01, ..., -9.2752e-02, -9.2028e-03, -2.8579e-02], [ 1.0723e-02, -2.3845e-02, 5.8893e-02, ..., -8.4111e-02, -1.0929e-02, -3.8908e-02], ..., [-1.2620e-01, -1.2239e-01, -6.0111e-02, ..., 1.9814e-02, -7.4255e-02, -1.9455e-02], [-1.8121e-01, -1.2122e-01, 3.9839e-02, ..., -8.2767e-02, -1.8340e-01, -1.2343e-01], [-9.4623e-02, -2.1298e-02, 9.8767e-02, ..., -1.3415e-01, -1.4433e-01, -9.5447e-02]], ..., [[-1.4666e-01, -2.1602e-01, -9.7738e-02, ..., -6.1896e-02, -5.6454e-02, -5.9906e-02], [ 1.9899e-02, 5.1323e-03, 1.3906e-02, ..., 1.5137e-02, 8.3006e-02, 3.2444e-02], [-3.5191e-02, -5.4830e-02, -2.9264e-04, ..., -5.2180e-02, -3.4717e-02, -2.7403e-02], ..., [ 2.9712e-02, 1.5094e-02, -1.5655e-02, ..., 1.8559e-02, -1.6583e-02, -1.3607e-02], [ 1.0858e-02, -7.2396e-02, -3.3979e-02, ..., -1.7926e-02, -4.6190e-02, -5.3141e-02], [-3.7275e-02, -9.7014e-02, 1.0602e-02, ..., -5.7990e-02, -2.2373e-02, -1.8351e-02]], [[ 2.2787e-03, 7.1163e-02, 1.6819e-02, ..., 9.2681e-02, 1.1505e-01, 2.3904e-02], [ 4.7862e-02, 1.1637e-01, 9.4310e-02, ..., 1.1634e-01, 1.8279e-01, 5.3646e-02], [-1.1675e-01, -7.2314e-02, 3.4619e-02, ..., -6.8577e-03, 2.9380e-02, -2.8520e-02], ..., [ 2.4590e-02, 4.2514e-02, 6.2918e-02, ..., -2.6829e-02, -1.1360e-02, -9.5783e-02], [-3.6852e-02, -6.3597e-02, -6.8385e-02, ..., -7.8031e-02, -2.6182e-02, -3.7326e-02], [-2.0531e-02, -4.0638e-02, -1.0545e-01, ..., -3.3283e-02, -1.7214e-02, -8.7905e-03]], [[ 1.4262e-01, 1.8939e-01, 4.6325e-02, ..., 5.6332e-03, -1.7036e-02, 8.5665e-02], [ 8.7077e-02, 1.5266e-01, 1.1495e-02, ..., 2.6765e-02, -7.1896e-02, 1.6017e-02], [ 2.0477e-02, 3.8997e-02, -5.1062e-02, ..., -3.8573e-02, -1.3501e-01, -4.8769e-02], ..., [ 6.8783e-02, 8.6703e-02, 3.7755e-02, ..., -1.4440e-02, -4.4981e-02, 4.6754e-02], [ 1.5211e-01, 1.4666e-01, 1.0651e-01, ..., 6.6362e-03, -2.3379e-02, 2.2118e-02], [ 1.5475e-01, 1.1244e-01, 5.5967e-02, ..., 2.7780e-02, 5.5084e-02, 3.2660e-02]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[ 0.4845, 0.0795, -0.2955, ..., -0.7637, -0.7079, -0.2085], [ 0.0782, -0.6192, -0.6169, ..., -0.5698, -0.6532, -0.1965], [-0.1840, -0.7758, -0.5835, ..., 0.4777, -0.3596, -0.5760], ..., [-0.4258, -0.6596, -0.4395, ..., -1.2278, -1.2216, -0.2326], [-0.7650, -0.4104, -0.0632, ..., -1.1424, -1.1432, -0.1594], [-0.4026, 0.0630, 0.2351, ..., -0.2858, -0.3669, 0.2083]], [[ 0.6445, 0.0482, 0.4370, ..., -0.4058, 0.3200, 0.3312], [-0.1693, -0.6156, -0.5838, ..., -0.3975, 0.0058, 0.0147], [ 0.0864, -0.4110, -0.5953, ..., -0.2295, -0.2403, -0.0342], ..., [ 0.1388, -0.2530, 0.2279, ..., -0.3627, -0.2018, 0.1189], [ 0.4692, -0.2895, 0.3101, ..., -0.4736, -0.0019, 0.2250], [ 0.3214, -0.2667, 0.1720, ..., 0.0590, 0.4691, 0.3283]], [[-0.1366, -0.6080, -0.5196, ..., 0.2603, -0.2752, -0.2446], [-0.2837, -1.1259, -0.9564, ..., 0.0706, -0.4083, -0.1435], [-0.0544, -0.5661, -0.5452, ..., -0.0945, -0.1330, -0.0922], ..., [-0.0720, -0.4958, -0.2366, ..., 0.2443, -0.0310, 0.0820], [-0.0304, -0.4755, -0.2775, ..., 0.1328, -0.2953, -0.1548], [ 0.0987, -0.4032, -0.2415, ..., -0.2818, -0.3963, -0.1066]], ..., [[ 0.3596, 0.2900, 0.1245, ..., 0.1821, 0.4045, 0.2309], [ 0.0965, -0.0323, -0.2414, ..., 0.0500, 0.1201, 0.0346], [-0.0176, 0.1297, -0.1019, ..., -0.3010, -0.2776, -0.0885], ..., [-0.0956, -0.2160, 0.2131, ..., 0.3172, 0.2728, 0.4095], [-0.0683, 0.0664, 0.2848, ..., 0.4504, 0.4178, 0.0137], [ 0.1541, 0.3728, 0.4992, ..., 0.1085, 0.6247, 0.3332]], [[-0.1562, -0.6261, -0.3393, ..., -0.0509, 0.1059, -0.1578], [-0.3468, -0.7332, -0.9827, ..., 0.0946, -0.1183, -0.4941], [-0.3986, -0.8786, -1.0388, ..., -0.1285, 0.0028, -0.2330], ..., [-0.5062, -0.3236, 0.2055, ..., -0.7337, -0.4309, 0.0667], [-0.6180, -0.3479, -0.0436, ..., -0.7868, -0.4855, 0.0638], [-0.3072, -0.2761, -0.2037, ..., 0.3174, 0.1394, 0.1267]], [[-0.2980, -0.6261, -0.3754, ..., -0.8803, -0.5116, -0.2325], [-0.2628, -0.9277, -0.9636, ..., -0.4362, -0.4065, -0.3207], [-0.2065, -0.8519, -1.0033, ..., -0.6636, -0.4973, -0.1509], ..., [-0.4886, -1.0695, -0.6502, ..., -0.6194, -0.6536, 0.0325], [-0.5144, -0.6333, -0.1508, ..., -0.9391, -1.1019, -0.7397], [-0.0694, -0.0629, 0.0898, ..., -0.4796, -0.1971, -0.3615]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-1.6082e-01, -8.7399e-02, -1.3319e-03, ..., -1.1923e-01, -1.4567e-01, -1.0296e-01], [-3.6143e-01, -5.2402e-01, -3.1734e-01, ..., -3.7360e-02, -1.4727e-01, -2.1146e-01], [-1.2748e-01, -2.0289e-01, -1.7263e-01, ..., 3.2821e-02, 2.3899e-02, -1.7116e-02], ..., [-5.4805e-02, -8.7460e-02, -7.2353e-02, ..., -8.5676e-02, -4.8657e-02, 4.4028e-02], [-7.9688e-02, -1.0489e-01, -9.2847e-02, ..., -2.2595e-02, -1.0795e-01, -7.0397e-02], [-9.7414e-02, -1.1942e-01, -9.3385e-02, ..., -6.3040e-02, -1.0286e-02, -2.8162e-02]], [[-8.3590e-02, -1.1097e-01, -7.6247e-02, ..., -8.4737e-03, 5.6614e-05, -2.6328e-02], [-3.3130e-02, -2.6316e-02, -6.1830e-02, ..., -3.6645e-02, 1.5747e-02, 1.4232e-02], [-4.1686e-02, -7.8000e-02, -7.1547e-02, ..., -3.7017e-02, -2.4838e-03, -2.6782e-02], ..., [-1.2196e-02, -2.3432e-02, -2.0606e-03, ..., -8.6377e-02, -1.2465e-01, -9.6318e-02], [ 7.4525e-03, -1.4041e-02, -4.3532e-02, ..., -6.8818e-02, -6.5830e-02, -7.8353e-02], [ 1.1506e-02, 9.2946e-03, -2.4731e-02, ..., -3.2439e-02, 2.1590e-03, 3.9627e-02]], [[-3.9879e-01, -3.8665e-01, -1.3869e-01, ..., -1.1499e-01, -1.2475e-01, -1.3711e-01], [-6.7526e-01, -7.6506e-01, -2.8433e-01, ..., -1.8596e-01, -2.0662e-01, -2.4935e-01], [-2.1169e-01, -1.6909e-01, 5.4648e-02, ..., -1.5494e-02, 1.9123e-02, -2.2116e-02], ..., [-3.6328e-02, -1.6013e-03, -3.8489e-02, ..., -1.0460e-01, -1.5847e-01, -1.7731e-01], [-6.4490e-02, -7.4539e-02, -1.0174e-01, ..., -1.0484e-01, -1.5456e-01, -1.5496e-01], [-7.7448e-02, -1.5199e-01, -2.2827e-01, ..., -9.5927e-02, -1.1893e-01, -9.5688e-02]], ..., [[-6.4548e-02, -8.8192e-02, -4.6773e-02, ..., -6.9743e-03, 2.2594e-03, -3.4030e-02], [ 1.3396e-01, 1.1073e-01, 3.6814e-02, ..., 4.9052e-03, 4.7609e-02, 2.7674e-02], [ 1.5980e-02, 5.0351e-03, 9.3483e-03, ..., 1.8490e-02, 4.5325e-02, -4.3205e-04], ..., [-5.8525e-03, 4.7031e-02, 4.5665e-02, ..., 4.1247e-02, 4.7514e-02, -2.1914e-02], [ 2.8747e-04, 4.0925e-02, 7.1120e-02, ..., 1.4343e-02, 3.0187e-02, -1.0466e-03], [-1.6504e-02, -8.5669e-03, 3.8790e-02, ..., -3.4095e-02, -1.7662e-02, -1.4444e-02]], [[-3.3402e-01, -2.9618e-01, -9.9874e-02, ..., -8.1035e-02, -1.4801e-01, -1.0487e-01], [-3.1593e-01, -2.6937e-01, -7.6912e-02, ..., -5.0427e-02, -9.1080e-02, -7.3436e-02], [-1.6077e-01, -6.5540e-02, 4.2919e-02, ..., -4.2545e-02, -3.7607e-02, -1.2003e-02], ..., [-3.7154e-03, 3.9980e-02, 1.2839e-02, ..., -1.6639e-02, -3.1090e-02, -6.4658e-02], [-2.3084e-02, 2.3174e-02, -1.3319e-02, ..., -1.1787e-02, 9.9478e-03, 2.9969e-02], [-1.9074e-02, -1.7670e-02, -7.8655e-02, ..., -2.6532e-03, -2.2898e-02, 1.5542e-02]], [[ 1.7509e-01, -2.9027e-05, -3.3672e-02, ..., 3.0066e-02, 2.1173e-02, -7.9550e-03], [-1.0897e-01, -3.1960e-01, -1.1253e-01, ..., -2.4740e-02, -9.3286e-02, -1.2415e-01], [-5.8501e-03, -1.0101e-01, 1.9465e-02, ..., -3.2247e-02, -2.0805e-02, -4.5185e-03], ..., [ 7.1553e-03, 3.4583e-02, 6.0450e-02, ..., -1.8143e-02, -8.8677e-03, -5.4216e-02], [ 3.3659e-02, 6.5209e-02, 9.3663e-02, ..., 4.1290e-02, -5.5905e-03, -5.6606e-02], [ 2.9368e-02, 3.4554e-02, 1.7176e-02, ..., 2.8348e-02, -2.7019e-02, -6.9664e-02]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[ 0.5098, 0.1736, 0.3925, ..., 0.3039, 0.4451, 0.3184], [-0.0598, -0.2861, -0.2505, ..., 0.1970, 0.1205, -0.0464], [ 0.4337, -0.0479, -0.1914, ..., 0.3584, 0.1753, -0.0405], ..., [-0.2482, -0.4840, -0.1365, ..., -0.3502, -0.2638, -0.1277], [ 0.0931, -0.1315, -0.0144, ..., -0.0708, -0.1416, 0.2576], [ 0.5693, 0.0493, -0.1726, ..., -0.0663, -0.1311, 0.2075]], [[ 0.0800, 0.3527, 0.2951, ..., -0.2240, 0.0408, 0.1746], [ 0.1391, 0.1942, 0.1127, ..., 0.1342, 0.2865, 0.1706], [ 0.1639, -0.0116, -0.0705, ..., -0.1466, -0.0055, 0.0638], ..., [ 0.1223, -0.3129, -0.2357, ..., 0.6392, 0.5878, 0.5443], [-0.0431, -0.3688, -0.2213, ..., -0.0247, -0.2139, 0.1065], [ 0.3570, 0.1945, 0.1925, ..., -0.0640, 0.1018, 0.0997]], [[ 0.6032, 0.0028, -0.0716, ..., -0.1254, 0.0954, 0.3884], [-0.1856, -0.4956, -0.4756, ..., -0.5205, -0.6469, -0.2358], [-0.1939, -0.6479, -0.3986, ..., -0.5717, -0.6354, -0.2821], ..., [-0.2615, -0.3921, 0.1904, ..., -0.4963, -0.5437, -0.2692], [-0.1927, -0.5685, -0.1972, ..., -0.2620, -0.4574, 0.1973], [ 0.1334, -0.2671, -0.1990, ..., -0.0171, -0.2605, 0.1129]], ..., [[-0.5754, -0.4448, -0.3716, ..., -0.5752, -0.8019, -0.6547], [-0.5028, -0.7751, -0.7857, ..., -0.4929, -0.5696, -0.3475], [-0.2166, -0.4623, -0.5127, ..., -0.5050, -0.4840, -0.1737], ..., [-0.9109, -1.7282, -1.1727, ..., -0.5710, -0.4864, -0.2089], [-0.9949, -1.4755, -1.4009, ..., -0.8807, -1.0772, -0.4198], [-0.5473, -0.7397, -0.7279, ..., -0.9085, -1.1764, -0.5110]], [[ 0.4664, -0.5403, -0.2714, ..., 0.2430, 0.3114, -0.2147], [ 0.0738, -0.6117, -0.7808, ..., 0.1921, -0.3795, -0.4145], [-0.0733, -0.6342, -0.9034, ..., 0.4000, -0.7577, -0.3841], ..., [-0.4114, -1.1774, -0.6825, ..., -0.4739, -0.1066, -0.1945], [-0.1428, -0.7037, -0.5981, ..., -0.6284, -0.3815, -0.3180], [ 0.0949, -0.1319, -0.2956, ..., -0.1477, -0.3470, -0.5407]], [[-0.4962, -0.0778, -0.5292, ..., -0.0404, 0.0530, -0.1162], [-0.0377, -0.0302, -0.4661, ..., -0.5547, -0.2383, 0.1106], [-0.1918, 0.2446, 0.0900, ..., -0.6465, -0.5852, 0.0785], ..., [ 0.1349, -0.1471, -0.2929, ..., -0.5733, -0.7522, 0.1922], [-0.2976, -0.0762, -0.4357, ..., -0.1059, -0.0564, 0.0568], [-0.2566, -0.1907, -0.3015, ..., -0.0227, -0.2228, -0.0242]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-0.0510, -0.0698, 0.0040, ..., -0.0179, 0.0021, -0.0078], [-0.1222, -0.0798, -0.0631, ..., 0.0161, -0.0895, -0.1311], [-0.0843, -0.1224, -0.1109, ..., -0.0426, -0.2030, -0.1647], ..., [-0.0870, -0.0207, -0.0474, ..., -0.1326, -0.0437, -0.0309], [-0.1607, -0.1051, -0.0448, ..., 0.0469, 0.0465, -0.0133], [-0.1061, -0.1567, -0.0659, ..., -0.0701, -0.0325, 0.0012]], [[-0.1245, -0.1477, -0.0338, ..., -0.0827, -0.0504, -0.1058], [ 0.1969, 0.0248, 0.0013, ..., -0.0502, 0.0326, -0.0411], [ 0.0264, -0.0859, -0.0659, ..., -0.1118, -0.0909, -0.1030], ..., [-0.1272, -0.0633, 0.0459, ..., -0.0523, -0.0801, -0.0876], [-0.1067, -0.0668, 0.0022, ..., -0.1192, -0.1293, -0.1347], [-0.0381, -0.0477, -0.0597, ..., -0.0926, -0.1077, -0.0823]], [[-0.0635, -0.1400, -0.1012, ..., -0.0068, 0.0314, 0.0102], [-0.0933, -0.1811, -0.1031, ..., -0.0319, 0.0461, 0.0047], [-0.0964, -0.1068, 0.0097, ..., -0.0365, -0.0181, -0.0691], ..., [-0.0430, -0.0466, -0.0025, ..., 0.0205, -0.0102, -0.0511], [-0.0682, -0.0683, -0.0129, ..., -0.0089, -0.0188, -0.0494], [-0.0579, -0.0713, -0.0010, ..., -0.0311, -0.0175, -0.0290]], ..., [[-0.0800, -0.0812, -0.0242, ..., -0.0869, -0.0772, -0.0011], [-0.0381, -0.0167, -0.0238, ..., -0.0680, -0.0347, -0.0412], [-0.0834, -0.0793, -0.1422, ..., -0.0534, -0.0114, -0.0406], ..., [-0.0159, 0.0017, -0.0306, ..., -0.0497, -0.0632, -0.0877], [-0.0268, 0.0149, -0.0126, ..., -0.0441, 0.0213, 0.0090], [ 0.0006, -0.0310, -0.0287, ..., -0.0085, -0.0128, -0.0178]], [[-0.0556, -0.0731, -0.0343, ..., -0.0642, -0.0661, -0.0520], [ 0.0388, 0.0129, 0.0487, ..., 0.0453, 0.0838, 0.0257], [-0.0910, -0.0915, -0.0123, ..., 0.0324, -0.0015, -0.0794], ..., [ 0.0483, -0.0290, -0.0291, ..., -0.0449, -0.0434, -0.0987], [ 0.0601, -0.0490, -0.0457, ..., 0.0019, 0.0606, 0.0470], [ 0.0335, -0.0144, -0.0024, ..., 0.0107, 0.0047, 0.0004]], [[-0.0654, -0.1429, -0.0705, ..., -0.0970, -0.0802, -0.1169], [ 0.0387, -0.0708, -0.0757, ..., -0.0317, -0.0100, -0.0739], [-0.0517, -0.1478, -0.1113, ..., -0.0170, -0.0384, -0.0802], ..., [-0.0656, -0.1084, -0.0880, ..., -0.0044, -0.0288, -0.0500], [-0.0497, -0.0753, -0.0420, ..., -0.0368, -0.0288, -0.0468], [-0.0154, -0.0363, -0.0092, ..., -0.0728, -0.0481, -0.0592]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[ 0.9101, 1.0766, 0.8197, ..., 0.3239, 0.6147, 0.6619], [ 0.3469, -0.0430, 0.0586, ..., 0.0741, -0.0061, 0.2298], [ 0.4233, 0.2378, -0.1834, ..., -0.3338, -0.4337, -0.0405], ..., [-0.2806, -0.7089, -0.6877, ..., -0.2932, -0.1125, 0.2156], [-0.1780, -0.5839, -0.3999, ..., -0.3161, -0.3349, -0.1509], [ 0.0418, -0.2564, -0.0946, ..., -0.1442, -0.0363, -0.1216]], [[ 0.8149, 0.4860, 0.3185, ..., 0.1834, 0.6390, 0.6432], [ 0.7706, 0.3752, 0.3433, ..., 0.0663, 0.5357, 0.4820], [ 0.1882, -0.2527, -0.2498, ..., -0.0375, -0.0892, -0.1041], ..., [ 0.2684, -0.4433, -0.2683, ..., -0.0835, -0.0633, -0.0604], [ 0.2136, -0.3107, -0.3213, ..., -0.1884, -0.2071, -0.0392], [ 0.2493, -0.0326, -0.0568, ..., -0.1346, -0.4392, -0.2407]], [[-0.7696, -0.7801, -0.3502, ..., -0.4878, -0.4838, -0.3692], [-0.8561, -0.7158, -0.3122, ..., -0.6538, -0.7723, -0.5182], [-0.4039, -0.3073, -0.2708, ..., -0.7497, -0.6898, -0.4187], ..., [-0.1388, -0.7416, -0.4934, ..., -0.4124, -0.5719, -0.9950], [ 0.0069, -0.7505, -0.6551, ..., -0.8837, -0.7356, -0.9177], [-0.1235, -0.8004, -0.8288, ..., -0.6875, -0.8000, -0.8135]], ..., [[ 0.5193, 0.0885, -0.1648, ..., -0.2513, -0.0280, 0.2250], [-0.1095, -0.4824, -0.6343, ..., -0.4968, -0.4093, -0.1937], [-0.1244, -0.5217, -0.5575, ..., -0.3860, -0.2766, -0.0532], ..., [-0.0555, -0.4405, -0.6392, ..., -0.7580, -0.7131, -0.6700], [ 0.0155, -0.8437, -0.9014, ..., -1.1372, -0.8675, -0.2615], [ 0.1376, -0.5330, -0.7336, ..., -0.1720, -0.0350, 0.0740]], [[-0.4117, -0.6053, -0.9020, ..., -0.6382, -0.6521, -0.1293], [-0.9844, -1.0386, -0.9518, ..., -1.1631, -1.2422, -0.5029], [-0.4233, -0.5345, -0.6717, ..., -0.6881, -1.0350, -0.4479], ..., [-0.0976, -0.1517, -0.1754, ..., 0.2021, -0.5755, -0.2413], [ 0.0702, -0.4755, -0.4894, ..., 0.3783, -0.3654, 0.0330], [-0.0632, -0.3983, -0.3590, ..., -0.3914, -0.5590, 0.2499]], [[-0.3513, -0.2525, -0.2272, ..., -0.4691, -0.4100, -0.0667], [-0.0315, -0.2985, -0.1417, ..., -0.8414, -0.7026, -0.2416], [ 0.0771, -0.1426, 0.0438, ..., -0.4894, -0.7221, -0.3022], ..., [-0.5155, -0.8266, -0.2681, ..., -0.1801, -0.8270, -0.3935], [-0.4917, -0.6305, 0.0745, ..., -0.6882, -1.1467, -0.6796], [-0.0945, -0.0862, 0.2665, ..., -0.4855, -0.6020, -0.2635]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[ 6.4430e-02, -1.6318e-01, -2.3215e-01, ..., -1.9166e-01, -1.6584e-01, -1.0940e-01], [-1.5872e-01, -3.8746e-01, -3.0614e-01, ..., -3.7735e-01, -3.5435e-01, -2.8609e-01], [-2.0114e-02, -1.1159e-01, -3.7295e-02, ..., -6.2077e-02, -1.4846e-01, -1.0356e-01], ..., [-1.1273e-01, -1.3277e-01, -9.7977e-02, ..., -7.3987e-02, -5.9154e-02, -3.6332e-02], [-1.3821e-01, -2.3602e-01, -1.7948e-01, ..., -5.4057e-02, -8.1132e-02, -8.4313e-02], [-8.6350e-02, -1.5658e-01, -9.1837e-02, ..., -1.3030e-01, -1.0429e-01, -6.4923e-02]], [[ 8.9676e-02, -6.1121e-02, -1.4292e-01, ..., -1.2495e-01, 4.0534e-02, 6.2829e-02], [-4.9366e-02, -2.0399e-01, -2.4697e-01, ..., -2.5667e-01, -1.6171e-01, -1.1283e-01], [ 2.2497e-02, -3.9560e-02, -8.7142e-02, ..., -1.9836e-01, -1.4036e-01, -1.1564e-01], ..., [-1.5411e-01, -1.2683e-01, -2.1520e-02, ..., -1.2022e-01, -1.8644e-01, -1.1395e-01], [-1.1874e-01, -1.5701e-01, -6.2235e-02, ..., -7.0218e-02, -1.0617e-01, -8.0756e-02], [-7.2681e-02, -1.4645e-01, -9.7536e-02, ..., -6.5988e-02, 2.5555e-03, -6.0063e-03]], [[ 1.8542e-01, 1.7175e-01, 1.1776e-01, ..., 4.2249e-02, 1.5117e-01, 1.2253e-01], [ 7.3441e-01, 8.1271e-01, 5.0550e-01, ..., 2.7646e-01, 5.0381e-01, 4.1670e-01], [ 1.3243e-01, 7.8072e-02, 4.7040e-02, ..., 8.7921e-02, 3.5772e-02, -1.3040e-03], ..., [-3.9274e-02, 1.2257e-02, 1.1344e-02, ..., 4.4609e-02, 2.8281e-02, 6.8394e-03], [-5.1679e-02, -1.6965e-02, -4.5877e-02, ..., -5.1035e-02, -5.9237e-04, 3.0533e-02], [ 6.0087e-02, 3.7373e-02, -1.3914e-02, ..., 7.0179e-03, 8.6234e-02, 1.1765e-01]], ..., [[ 6.0558e-03, 1.5564e-01, 5.5820e-02, ..., -4.4163e-02, -3.2737e-03, 8.2676e-02], [-2.2683e-01, -1.2114e-01, -9.8398e-02, ..., -1.3539e-01, -1.0991e-01, -1.1122e-01], [-1.3119e-01, -4.0930e-03, -4.1464e-02, ..., -1.4224e-02, -8.3651e-02, -1.7913e-01], ..., [-4.0930e-02, 1.0420e-02, -3.7359e-02, ..., -5.6199e-02, -6.6513e-02, -8.7126e-02], [-9.0039e-02, 3.0807e-02, -1.4512e-04, ..., -1.1594e-01, -3.8810e-02, -4.3311e-02], [-8.2347e-02, -1.1363e-02, -3.4658e-03, ..., -5.7998e-02, -4.6701e-03, -7.7030e-03]], [[ 5.5289e-02, -1.5680e-03, -1.1800e-01, ..., -2.2549e-01, -4.5773e-02, -2.2674e-02], [-2.0767e-01, -1.3274e-01, -1.2607e-01, ..., -1.9728e-01, -1.5975e-01, -1.3349e-01], [-1.4214e-01, -3.7481e-02, -3.9965e-02, ..., -4.2012e-02, -6.0136e-02, -1.1219e-01], ..., [-1.4361e-02, -8.7165e-02, -4.7337e-02, ..., -1.0478e-01, -6.8615e-02, -9.3372e-02], [-8.2169e-02, -1.5698e-01, -1.4019e-01, ..., -1.2017e-01, -3.3156e-03, -1.3669e-02], [-9.5183e-02, -1.0866e-01, -8.0570e-02, ..., -5.5362e-02, -3.9940e-02, -2.6436e-02]], [[ 5.6359e-02, 1.1743e-01, 3.2198e-02, ..., -3.8483e-03, 8.7708e-02, 6.4603e-02], [-1.1675e-01, -6.1985e-02, -5.4091e-02, ..., -4.9418e-02, -1.4927e-02, -4.6136e-02], [-3.6973e-02, 4.3101e-02, 5.4578e-03, ..., 2.4467e-02, 2.7021e-02, -4.8823e-03], ..., [ 5.6195e-03, -2.5033e-03, -1.4855e-02, ..., 2.3181e-02, -2.8707e-03, -1.0791e-02], [-1.2482e-02, 5.2544e-04, -1.2498e-02, ..., 1.4234e-02, 4.9095e-02, 4.3880e-03], [-1.7698e-02, 2.2928e-03, -7.0659e-03, ..., -3.8163e-03, 3.9203e-02, -2.6743e-03]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[ 1.7339e-01, -9.4808e-02, -6.1271e-01, ..., 1.4380e-01, -2.0960e-01, -3.2730e-01], [-6.3560e-02, -2.1000e-01, -6.8458e-01, ..., -9.9222e-01, 2.9317e-01, -5.5872e-02], [-1.5248e-01, -4.8554e-01, -4.8676e-01, ..., 6.1018e-02, 6.5937e-01, -1.3268e-01], ..., [-4.8350e-01, -8.6408e-01, -6.1364e-01, ..., -9.9484e-01, -7.1179e-01, -1.4440e-01], [-1.7778e-01, -1.4797e-01, -4.1957e-01, ..., -8.4083e-01, -6.0669e-01, -5.0201e-02], [-5.5214e-01, -4.5085e-01, -2.3836e-01, ..., -5.5610e-01, -8.8097e-01, -4.0863e-01]], [[-4.3246e-03, 7.2340e-02, -4.0673e-01, ..., 4.4244e-02, -1.3516e-01, -2.7559e-02], [-6.7222e-02, -1.0078e-01, -1.2998e-01, ..., -2.3212e-01, -4.1464e-01, -4.0327e-01], [-1.5512e-01, 7.9317e-02, 1.5356e-01, ..., -5.7165e-01, -4.5463e-01, -5.9033e-01], ..., [-1.8650e-01, -1.3346e-01, -2.2597e-02, ..., -9.4229e-02, -3.2536e-01, -3.4481e-01], [-6.8029e-01, -1.1684e-03, 9.0839e-02, ..., 2.6727e-01, 3.1889e-01, -2.6609e-02], [-7.7352e-01, -3.5479e-01, -5.5215e-01, ..., -4.0914e-01, 1.0151e-01, -4.4220e-01]], [[ 1.3809e+00, 7.1808e-02, -4.7408e-02, ..., -3.0978e-01, 2.6252e-01, 5.1537e-01], [ 2.0916e-01, 5.9198e-02, 3.5689e-01, ..., 2.4617e-01, -4.3292e-01, -3.2945e-01], [-9.1277e-03, 1.2988e-01, -3.6940e-01, ..., -1.1850e-01, 1.5383e-01, -1.4952e-02], ..., [-2.0953e-01, -1.1567e-01, -1.5326e-01, ..., -4.2803e-01, -4.6014e-01, -1.9415e-01], [-1.4530e-01, -1.3514e-01, -2.9253e-01, ..., -1.2311e-01, -1.1232e-01, -2.9425e-01], [ 1.9769e-01, 2.3179e-01, -2.5285e-01, ..., -3.0741e-01, -2.1593e-01, 2.9725e-02]], ..., [[ 2.5535e-01, 7.2656e-03, -1.2095e-01, ..., 3.7620e-02, 1.8716e-03, 2.3830e-01], [ 1.2931e-01, -1.1446e-01, -6.7117e-01, ..., -3.7731e-01, -1.8579e-01, 2.5818e-01], [ 1.0917e-02, -2.3147e-01, -6.8273e-01, ..., -1.5688e+00, -3.1911e-01, 1.9214e-01], ..., [ 4.9276e-01, 2.3696e-01, -1.7069e-01, ..., -2.3560e-02, -4.2982e-01, 4.3347e-02], [-1.5764e-01, 1.2234e-01, -2.6147e-01, ..., 4.9637e-02, -3.0250e-01, -9.2175e-02], [-4.0238e-01, -2.6120e-01, -4.6877e-01, ..., -8.3150e-01, -2.5967e-01, -6.1447e-01]], [[ 5.0277e-01, -2.5111e-01, -2.9781e-02, ..., 1.2346e-01, 1.2271e-01, -5.3489e-02], [ 5.4571e-01, -2.0467e-01, -5.6036e-01, ..., -6.4822e-01, -2.9836e-01, -3.3093e-01], [ 1.5907e-01, -3.0992e-01, -7.3655e-01, ..., 1.6752e-01, -2.4167e-01, -5.0056e-02], ..., [ 6.1304e-01, 1.5488e-01, -1.9809e-01, ..., 1.3724e-01, -1.7476e-01, -2.0033e-01], [ 1.4995e-01, 3.0548e-01, -1.9005e-01, ..., 1.7044e-01, -6.0262e-01, 1.1476e-02], [-1.3003e-01, -7.5153e-01, -2.9897e-01, ..., -4.0668e-01, -7.2825e-01, 6.4905e-02]], [[ 1.1347e-01, 4.4911e-02, -7.4415e-02, ..., -3.1216e-01, -2.3538e-01, 1.1548e-01], [-2.2218e-02, -2.2825e-01, -6.4981e-02, ..., 5.8519e-02, 1.0134e-01, -1.1251e-01], [-1.0316e-02, 4.1192e-01, -4.0713e-01, ..., -2.2387e-02, -8.5106e-02, -2.5153e-01], ..., [-3.3986e-01, -4.1138e-01, 3.5781e-01, ..., 2.1808e-01, -2.5594e-01, 1.1899e-01], [ 5.5571e-02, -1.4490e-01, -2.9625e-01, ..., 8.0132e-02, 1.0265e-01, 2.1682e-01], [ 6.5714e-02, -1.2890e-01, 1.6180e-01, ..., 1.4383e-01, 3.4399e-01, 6.4371e-01]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[ 0.0960, 0.0307, -0.0699, ..., 0.0172, 0.0034, -0.0184], [-0.1651, -0.2210, -0.1714, ..., -0.2050, -0.0549, -0.1189], [ 0.0070, 0.0068, 0.1551, ..., 0.0701, 0.0633, 0.0086], ..., [-0.0674, -0.0241, 0.0063, ..., 0.0346, 0.0357, 0.0131], [-0.1675, -0.0529, 0.1768, ..., 0.1033, 0.0940, 0.0378], [-0.1387, -0.1719, -0.0859, ..., 0.0721, 0.1203, 0.1045]], [[-0.0699, 0.0097, 0.0019, ..., -0.1541, -0.2316, -0.1260], [-0.0322, -0.0661, 0.1079, ..., -0.0795, -0.3717, -0.2026], [ 0.0063, 0.0893, 0.2031, ..., 0.1351, -0.1136, -0.0444], ..., [ 0.1117, 0.0801, 0.0366, ..., -0.0718, 0.0056, -0.0384], [ 0.0280, 0.0044, -0.0808, ..., -0.1459, -0.0720, -0.0570], [ 0.0297, 0.0227, 0.0394, ..., 0.1448, 0.1450, -0.0226]], [[-0.0899, -0.0195, -0.0080, ..., -0.0544, -0.1548, -0.0921], [-0.0682, 0.0799, 0.0049, ..., -0.1934, -0.2153, -0.0923], [-0.1031, -0.0454, -0.0174, ..., -0.0893, -0.1499, -0.0551], ..., [-0.0286, -0.0889, -0.1280, ..., -0.1297, -0.0631, -0.0738], [-0.1076, -0.1124, -0.1168, ..., -0.2121, -0.0860, -0.1526], [-0.1690, -0.0428, -0.0780, ..., -0.1965, -0.1253, -0.1400]], ..., [[-0.1522, -0.1636, -0.1202, ..., -0.0121, -0.0746, -0.1115], [-0.0114, -0.1939, -0.1007, ..., -0.0196, 0.0041, -0.0400], [-0.0623, -0.1104, -0.0841, ..., -0.0666, 0.0758, 0.0979], ..., [-0.0540, -0.1088, -0.0957, ..., -0.1641, -0.0596, -0.0542], [-0.1630, -0.1650, -0.1441, ..., -0.1307, -0.1170, -0.0429], [-0.0806, -0.1046, -0.0604, ..., -0.0512, 0.1480, 0.0605]], [[-0.0534, -0.0888, -0.1538, ..., -0.1150, -0.0616, -0.1154], [-0.0053, -0.2251, -0.1865, ..., -0.2248, -0.1754, -0.0764], [ 0.0032, -0.1462, -0.0746, ..., 0.0086, -0.0652, 0.0277], ..., [-0.0308, 0.0306, 0.0580, ..., -0.0542, -0.0445, -0.0155], [-0.0855, -0.0960, -0.0220, ..., -0.1566, -0.1331, 0.0572], [-0.1060, -0.0912, 0.0406, ..., -0.0249, -0.0581, 0.0064]], [[ 0.1796, 0.2143, 0.0647, ..., -0.0623, -0.0173, 0.0643], [-0.0477, -0.0548, -0.0255, ..., -0.2377, -0.2283, -0.0758], [ 0.0988, 0.1510, 0.1109, ..., 0.0127, -0.0725, -0.0160], ..., [-0.0108, -0.0191, -0.0287, ..., -0.0878, -0.1053, -0.0423], [-0.0221, 0.0222, 0.0112, ..., -0.0241, -0.0022, 0.0472], [ 0.0500, 0.1050, 0.0466, ..., 0.0299, 0.0499, 0.1599]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-0.4231, -0.0231, -0.0056, ..., -0.0265, -0.1730, -0.1304], [-0.1795, 0.0069, -0.0698, ..., -0.0588, 0.1447, 0.0358], [-0.0325, 0.0794, 0.0769, ..., 0.1704, 0.1836, -0.0286], ..., [-0.0966, -0.0066, 0.0286, ..., -0.0133, 0.0371, -0.1012], [ 0.0106, 0.0447, 0.0192, ..., 0.0716, 0.0035, -0.0776], [-0.0816, 0.0074, 0.0128, ..., -0.0761, -0.0472, 0.1071]], [[-0.1681, -0.0969, 0.0657, ..., -0.0468, -0.1331, -0.3370], [-0.1509, -0.0407, 0.2268, ..., 0.1119, -0.0907, -0.1011], [-0.0282, 0.0365, 0.0505, ..., 0.1827, 0.0482, -0.1284], ..., [-0.0058, 0.0085, -0.0955, ..., -0.1342, -0.1231, 0.0110], [ 0.1221, -0.1235, -0.1002, ..., -0.0979, -0.1216, -0.1265], [-0.0721, -0.1120, 0.0545, ..., 0.3044, -0.0122, -0.0502]], [[ 0.0532, -0.1504, -0.0791, ..., -0.0050, 0.0386, -0.0034], [-0.0949, 0.0850, -0.1386, ..., -0.1174, -0.0495, -0.0722], [-0.0897, -0.0400, -0.1848, ..., -0.0402, -0.0539, 0.0251], ..., [-0.0268, -0.1440, -0.1328, ..., -0.1762, -0.1322, -0.0544], [-0.2909, -0.1445, -0.0061, ..., -0.2415, -0.0795, -0.0177], [-0.3183, -0.1925, -0.2452, ..., -0.4863, -0.0652, -0.0086]], ..., [[-0.2409, -0.1967, -0.0914, ..., -0.1281, -0.2115, -0.2216], [-0.1904, -0.0376, -0.1786, ..., 0.0372, 0.1565, 0.0133], [-0.1434, -0.0010, -0.0514, ..., -0.3211, 0.0617, -0.1025], ..., [-0.1048, 0.0027, -0.0717, ..., -0.2067, -0.0891, -0.1054], [-0.2565, -0.0988, -0.1175, ..., -0.1354, -0.0973, -0.1034], [-0.2623, -0.0985, -0.0262, ..., -0.1220, -0.0202, 0.0135]], [[-0.2197, -0.0848, 0.0301, ..., -0.2480, -0.1971, 0.0863], [ 0.0555, 0.0098, -0.1250, ..., 0.0984, -0.0698, 0.0870], [-0.0286, -0.0368, 0.1240, ..., -0.1178, -0.1522, -0.0569], ..., [-0.0732, -0.0158, -0.0738, ..., 0.0430, -0.0608, -0.0357], [-0.2239, -0.0886, -0.0522, ..., 0.0225, -0.2635, -0.1263], [ 0.0212, -0.1778, -0.0208, ..., -0.1523, -0.1107, -0.1440]], [[ 0.0253, 0.0283, 0.0140, ..., 0.0084, -0.0078, 0.1810], [ 0.0963, 0.0438, -0.1671, ..., -0.1635, -0.2097, -0.0558], [ 0.0865, 0.0705, -0.1457, ..., -0.1799, -0.1054, -0.0024], ..., [ 0.1376, 0.0282, 0.0467, ..., 0.0566, 0.0660, -0.0354], [-0.1233, 0.0141, 0.0358, ..., -0.0034, -0.0085, -0.0285], [ 0.0108, 0.0045, 0.1608, ..., -0.0417, -0.0297, 0.0972]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-0.4958, -1.6956, -0.6618, ..., -0.9556, -1.6244, -0.7040], [-1.6635, -2.5068, -1.4933, ..., -1.6722, -2.4164, -1.5556], [-0.7953, -0.8299, -0.3875, ..., -0.9296, -1.3441, -0.9987], ..., [-1.8495, -2.9274, -1.7244, ..., -0.8433, -0.9403, -0.4768], [-1.1743, -2.8202, -2.3558, ..., -2.1676, -2.2784, -1.2847], [-0.6261, -1.4864, -0.9453, ..., -1.0875, -1.3397, -0.8367]], [[ 0.2355, -0.8715, -1.2119, ..., -1.2575, -0.6488, 0.0329], [-0.3182, -1.5435, -1.9886, ..., -2.0758, -1.5624, -0.3341], [-0.3772, -1.9734, -2.4813, ..., -2.1128, -1.6591, -0.9725], ..., [-0.7824, -1.4839, -1.3397, ..., -1.5276, -2.2641, -1.6701], [-1.3119, -2.0934, -1.6868, ..., -1.9044, -2.1447, -1.2452], [-0.6846, -1.6288, -1.7022, ..., -0.8273, -0.4320, 0.1732]], [[ 0.0166, -1.7161, -2.3600, ..., -3.1715, -2.6389, -1.7020], [-1.2275, -2.8275, -3.6183, ..., -4.1244, -3.2650, -1.9257], [-0.1512, -0.1212, -0.3152, ..., -1.9853, -2.1037, -1.3129], ..., [-0.4808, -0.8547, -0.7786, ..., -0.8631, -1.9802, -1.4388], [-0.3164, -1.0302, -0.7526, ..., -1.7501, -2.7765, -1.1548], [-0.8256, -1.4198, -1.1582, ..., -1.0846, -0.4300, 0.4089]], ..., [[-1.2897, -1.8389, -0.8400, ..., -0.6759, -0.5749, -0.1012], [-1.9064, -3.2975, -2.6617, ..., -2.9298, -2.4174, -1.3648], [-1.6253, -2.6109, -2.5476, ..., -3.3257, -3.4099, -2.1412], ..., [-1.2108, -2.3047, -1.9960, ..., -1.4551, -1.7386, -0.9149], [-1.3869, -2.4565, -2.1475, ..., -1.0893, -1.9669, -1.2323], [-0.6772, -1.1768, -1.0358, ..., -0.3274, -1.3807, -0.8585]], [[-1.8343, -0.8251, -1.0899, ..., -1.2018, -1.3029, -0.1550], [-0.5182, 0.0390, 0.0699, ..., -1.9062, -1.7981, 0.4875], [-0.8099, -0.7638, -1.8919, ..., -1.0373, -2.1952, -0.7600], ..., [-1.0898, -1.5434, -1.1102, ..., -2.0837, -2.9177, -0.6845], [-0.8162, -1.4854, -1.4141, ..., -2.6396, -3.5088, -0.8067], [-1.2074, -1.2848, -0.3816, ..., -0.5490, -2.6263, -0.3691]], [[-1.4092, -0.9987, -0.0146, ..., -0.7766, -2.0979, -1.7557], [-2.2071, -2.9781, -1.7933, ..., -2.5191, -2.7386, -1.6995], [-1.3005, -1.7724, -1.4228, ..., -1.1738, -1.2907, -1.2182], ..., [-0.3932, -1.2425, -2.0961, ..., -2.0225, -1.9730, -1.6157], [-1.0402, -1.3846, -1.5287, ..., -1.3333, -2.1565, -2.2994], [-1.1169, -1.1005, -0.9754, ..., -0.4825, -1.5659, -1.9117]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-0.0620, -0.0237, -0.0037, ..., 0.0130, -0.0199, -0.0416], [-0.1411, -0.1366, -0.0297, ..., -0.0025, -0.0703, -0.0618], [-0.0425, 0.0070, 0.0373, ..., 0.0118, -0.0123, -0.0235], ..., [-0.0277, -0.0095, -0.0008, ..., 0.0151, 0.0450, 0.0469], [-0.0916, -0.0925, -0.0732, ..., -0.1455, -0.1627, -0.0588], [-0.0897, -0.0521, -0.0247, ..., -0.0691, -0.1293, -0.0736]], [[-0.0568, -0.0628, -0.0028, ..., 0.0254, 0.0397, 0.0392], [-0.0439, -0.0591, 0.0522, ..., 0.0254, 0.0312, 0.0318], [-0.0353, -0.0663, -0.0202, ..., 0.0170, 0.0434, 0.0318], ..., [ 0.0143, 0.0021, 0.0039, ..., 0.0114, -0.0168, -0.0155], [ 0.0041, -0.0421, -0.0107, ..., 0.0293, 0.0091, 0.0105], [ 0.0061, -0.0431, 0.0033, ..., 0.0891, 0.1117, 0.0562]], [[-0.0850, -0.0327, -0.0011, ..., 0.0552, 0.0250, -0.0236], [-0.0567, -0.0512, 0.0741, ..., 0.1015, 0.0453, -0.0065], [-0.0738, -0.0334, 0.0383, ..., 0.0974, 0.0570, -0.0097], ..., [-0.0352, -0.0065, -0.0038, ..., -0.0257, 0.0273, -0.0275], [ 0.0552, 0.0570, 0.0491, ..., -0.0533, -0.0732, -0.0627], [-0.0079, 0.0119, 0.0202, ..., -0.0413, -0.0490, -0.0928]], ..., [[-0.0974, -0.0357, -0.0637, ..., -0.0637, -0.0228, 0.0032], [-0.0352, 0.0125, -0.0438, ..., -0.0205, 0.0805, 0.1438], [-0.0076, 0.0452, 0.0310, ..., -0.0428, -0.0087, 0.0439], ..., [-0.0556, -0.0168, 0.0125, ..., -0.0336, -0.0604, -0.0738], [-0.1158, -0.0726, -0.0050, ..., 0.0109, -0.0004, -0.0633], [-0.1226, -0.0931, -0.0844, ..., -0.0718, -0.0282, -0.0443]], [[-0.0787, -0.0310, -0.0008, ..., 0.0244, 0.0272, 0.0122], [ 0.0031, 0.0096, 0.0468, ..., 0.1062, 0.1036, 0.0854], [ 0.0196, 0.0422, 0.1191, ..., 0.0817, 0.0749, 0.0545], ..., [ 0.0364, 0.0224, 0.0021, ..., -0.0197, 0.0239, -0.0109], [ 0.0244, 0.0559, 0.0411, ..., -0.0529, -0.0173, -0.0235], [-0.0286, 0.0330, 0.0321, ..., 0.0262, 0.0654, 0.0311]], [[-0.0086, 0.0656, 0.0510, ..., -0.0015, -0.0074, -0.0071], [ 0.0841, 0.0682, 0.0510, ..., -0.0253, -0.0443, -0.0139], [ 0.0517, 0.0391, 0.0168, ..., -0.0678, -0.0927, -0.0420], ..., [-0.0126, -0.0465, -0.0627, ..., -0.0418, -0.0515, -0.0328], [ 0.0267, 0.0123, 0.0104, ..., 0.0231, -0.0101, 0.0244], [ 0.0172, 0.0445, 0.0412, ..., 0.0129, -0.0327, -0.0167]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-3.4750, -3.4560, -2.8273, ..., -3.2508, -3.8729, -2.8269], [-4.2592, -4.8648, -2.5765, ..., -1.2634, -1.9938, -2.1092], [-2.7763, -3.2505, -1.8529, ..., -2.0219, -1.5580, -1.5422], ..., [-1.7902, -2.4408, -1.2079, ..., -2.0084, -2.4411, -1.9580], [-1.6889, -2.7830, -2.4005, ..., -1.9301, -2.0826, -1.7352], [-1.4279, -2.3741, -2.9092, ..., -3.9762, -3.4731, -2.4181]], [[-1.9909, -2.7051, -2.3577, ..., -3.0502, -3.1455, -1.9688], [-2.5792, -3.3235, -2.4701, ..., -4.1309, -4.7480, -3.0812], [-2.1466, -2.2925, -1.1225, ..., -2.5902, -3.6139, -2.3158], ..., [-2.0956, -3.0973, -2.2434, ..., -1.5169, -2.2583, -1.6122], [-2.3290, -3.7623, -3.1273, ..., -2.5104, -3.6841, -2.5078], [-1.8428, -2.6170, -1.8207, ..., -1.3055, -2.4725, -1.5867]], [[-3.4129, -2.4648, -1.0742, ..., -1.1919, -1.9342, -1.6739], [-2.6977, -2.0980, -1.4507, ..., -0.8640, -1.0576, -0.5588], [-2.2977, -2.6378, -2.5816, ..., -2.1669, -1.5316, -0.7172], ..., [-3.3527, -3.5814, -2.9848, ..., -2.0961, -2.6056, -1.8142], [-4.3135, -4.9184, -4.3180, ..., -3.8032, -4.8414, -3.6772], [-2.7174, -3.1033, -3.0057, ..., -3.0211, -4.7558, -4.2106]], ..., [[-4.4429, -3.3553, -1.4595, ..., -1.8802, -2.5783, -1.9087], [-4.6435, -4.3449, -3.1467, ..., -2.3504, -2.0158, -1.1910], [-3.3648, -3.1884, -2.2892, ..., -2.9678, -2.6551, -1.7214], ..., [-2.8477, -2.9236, -2.2211, ..., -4.0577, -4.8453, -3.4708], [-3.9063, -4.3842, -3.3218, ..., -4.5455, -5.6346, -4.1392], [-2.8678, -3.3069, -2.2644, ..., -1.6910, -2.7187, -2.1092]], [[-2.5947, -2.0769, -1.6998, ..., -2.1328, -3.2101, -2.5518], [-2.1753, -1.3601, -0.5612, ..., -1.5786, -2.3167, -1.4304], [-1.8868, -1.4609, -1.4939, ..., -2.2960, -1.9507, -0.9916], ..., [-4.1552, -4.0864, -2.9043, ..., -2.2917, -2.6341, -1.9900], [-5.4539, -5.7471, -4.4933, ..., -4.1449, -4.8205, -3.3648], [-3.5695, -4.2426, -4.0575, ..., -3.9390, -4.0871, -2.6593]], [[-1.7293, -1.3352, -0.6515, ..., -2.7331, -3.6395, -2.9957], [-2.3236, -2.4487, -2.4042, ..., -4.6181, -4.5513, -3.0388], [-1.9516, -1.9545, -1.9636, ..., -5.1817, -4.5127, -2.4386], ..., [-2.8701, -2.9135, -1.2747, ..., -0.9129, -1.7076, -1.5889], [-3.5865, -4.6488, -3.6247, ..., -4.0014, -4.4134, -3.0185], [-3.1188, -3.7854, -2.9042, ..., -3.5254, -4.1202, -2.7234]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>), tensor([[[[-4.0248e-02, -2.1918e-02, 1.6857e-02, ..., -3.5696e-03, -2.6573e-02, -4.6227e-02], [-2.7004e-02, -1.1469e-02, 4.3785e-02, ..., 3.2996e-02, 1.1239e-02, -1.7459e-02], [ 5.9849e-03, -1.3250e-03, 1.6657e-02, ..., 5.3861e-02, 5.6166e-02, 1.7476e-02], ..., [ 8.3230e-03, 4.1532e-02, 5.7824e-02, ..., 6.9979e-02, 2.0746e-02, -6.8109e-03], [-2.6846e-02, 4.4148e-03, 1.4690e-02, ..., -3.6798e-03, -7.0932e-02, -5.3330e-02], [-2.1289e-02, -1.7336e-02, -4.2136e-02, ..., -5.3113e-02, -7.2978e-02, -2.9946e-02]], [[-3.7065e-02, 1.3638e-02, 5.1883e-02, ..., -1.5602e-02, -4.2106e-02, -3.2605e-02], [-5.9112e-02, 8.4406e-03, 7.6438e-02, ..., 1.9861e-02, 3.6813e-02, 4.7601e-02], [-1.3119e-02, 2.3961e-02, 2.7430e-02, ..., -8.6907e-02, -3.4433e-03, 4.5861e-02], ..., [-1.3509e-02, 2.4247e-03, 2.8476e-02, ..., 5.4252e-02, 1.1840e-01, 9.6187e-02], [-2.8346e-02, -1.9278e-02, 3.3225e-02, ..., 7.3646e-02, 9.0414e-02, 6.1256e-02], [-2.8322e-02, -5.2114e-02, -3.8483e-02, ..., -1.2129e-04, 3.6583e-02, 1.6103e-02]], [[-7.1677e-03, 1.0149e-01, 1.6720e-01, ..., 9.0421e-02, 3.4992e-02, -1.8386e-03], [ 4.6779e-02, 1.7365e-01, 2.3382e-01, ..., 1.2050e-01, 2.4064e-02, -6.9508e-03], [ 1.8438e-02, 9.5620e-02, 1.1244e-01, ..., 6.1100e-02, -2.5235e-05, -1.9582e-02], ..., [-4.6476e-02, -8.5548e-02, -8.1671e-02, ..., -1.2316e-02, 2.3720e-02, 2.5891e-03], [-7.9118e-03, -4.0907e-02, -9.0682e-02, ..., -3.6402e-02, 1.4012e-03, -1.1529e-02], [-4.0327e-02, -4.7497e-02, -5.6817e-02, ..., -1.1304e-02, 1.4103e-02, -1.6755e-02]], ..., [[-7.1824e-02, -8.3558e-02, -6.0969e-02, ..., -3.1534e-02, 5.8441e-02, 4.9902e-02], [-5.4394e-02, -9.7608e-02, -1.1188e-01, ..., -5.6412e-02, 1.3519e-01, 1.1633e-01], [-2.6096e-02, -2.2610e-02, -1.3835e-02, ..., 3.4550e-02, 2.0898e-01, 1.6247e-01], ..., [-2.9306e-03, 2.2745e-02, 4.1931e-02, ..., 5.7627e-02, 1.1362e-01, 6.5168e-02], [ 3.3599e-03, 1.8140e-02, 3.4065e-02, ..., 1.5860e-02, 4.4836e-02, 2.5481e-02], [-1.0852e-02, -1.6917e-02, -8.4206e-03, ..., -2.0341e-02, -7.6384e-03, -1.5975e-02]], [[-9.4794e-02, -9.0133e-02, -2.8065e-02, ..., -2.7000e-02, -6.5505e-02, -7.7891e-02], [-2.9906e-02, 2.4624e-03, 6.2037e-02, ..., 2.1808e-02, -3.5462e-02, -6.9783e-02], [-5.9355e-03, 1.2592e-02, 2.6404e-02, ..., 6.4637e-02, 6.1290e-02, -1.7846e-02], ..., [-5.3652e-02, -4.5677e-02, -4.0731e-02, ..., -1.8724e-02, -2.1224e-03, -2.1797e-02], [-6.4489e-02, -6.2902e-02, -2.7807e-02, ..., -4.3309e-03, -6.6434e-02, -7.2191e-02], [-5.5928e-02, -7.3513e-02, -6.4857e-02, ..., -4.5293e-02, -8.1454e-02, -7.0142e-02]], [[ 1.5675e-02, 9.0438e-02, 8.0371e-02, ..., 2.1300e-02, -2.8058e-02, -7.7982e-02], [ 5.9975e-02, 9.8272e-02, 6.5212e-02, ..., 5.1632e-02, -3.6335e-02, -9.9438e-02], [ 4.6650e-02, 3.0553e-02, -5.7419e-03, ..., 2.3345e-02, -5.8012e-02, -1.1471e-01], ..., [ 1.1716e-02, -2.0707e-02, -2.9558e-02, ..., 2.8098e-02, -3.2145e-03, -7.5852e-02], [-3.7559e-02, -3.7850e-02, -9.6232e-03, ..., 3.5433e-02, -1.7212e-02, -7.3634e-02], [-4.4100e-02, -2.9511e-02, -2.4186e-03, ..., -1.3369e-02, -6.6702e-02, -9.5514e-02]]]], device='cuda:0', grad_fn=<CudnnConvolutionBackward>)]","title":"Hooks - The amazing trick you should know"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#transfer-learning","text":"","title":"Transfer Learning"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#input-shapes-importance-in-transfer-learning","text":"1. torchvision.models.vgg.py 2. changing-input-dimension-for-alexnet 3. is-it-possible-to-give-variable-sized-images-as-input-to-a-convolutional-neural","title":"Input Shape's importance in Transfer Learning"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#vgg16","text":"We use an extremely simple tranfer learning model from torchvision.models and use this as a vanilla example to understand our problem statement. One should understand one of the fundamental reason why we use transfer learning is because the model is already pre-trained on millions of images; this is often the case as we usually benchmark our performance on model that are trained on imagenet . However, problem arises as we try to transfer our learned weights and layers from the said model, to our own custom dataset. Why? We will unveil the mystery soon, but first keep in mind two important concepts: Convolutional Layers are independent of the input size/shape. Dense or Fully Connected Layers are dependent on the input size/shape. Conclusion: As long as the pretrained model has Dense/FC layers, then how do we reconcile the fact that our input shape may be different from the ones that were trained on? Below: I present the source code of vgg16 and we will use it later. import torch import torch.nn as nn # from .utils import load_state_dict_from_url from typing import Union , List , Dict , Any , cast model_urls = { 'vgg11' : 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth' , 'vgg13' : 'https://download.pytorch.org/models/vgg13-c768596a.pth' , 'vgg16' : 'https://download.pytorch.org/models/vgg16-397923af.pth' , 'vgg19' : 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth' , 'vgg11_bn' : 'https://download.pytorch.org/models/vgg11_bn-6002323d.pth' , 'vgg13_bn' : 'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth' , 'vgg16_bn' : 'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth' , 'vgg19_bn' : 'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth' , } cfgs : Dict [ str , List [ Union [ str , int ]]] = { 'A' : [ 64 , 'MaxPool' , 128 , 'MaxPool' , 256 , 256 , 'MaxPool' , 512 , 512 , 'MaxPool' , 512 , 512 , 'MaxPool' ], 'B' : [ 64 , 64 , 'MaxPool' , 128 , 128 , 'MaxPool' , 256 , 256 , 'MaxPool' , 512 , 512 , 'MaxPool' , 512 , 512 , 'MaxPool' ], 'D' : [ 64 , 64 , 'MaxPool' , 128 , 128 , 'MaxPool' , 256 , 256 , 256 , 'MaxPool' , 512 , 512 , 512 , 'MaxPool' , 512 , 512 , 512 , 'MaxPool' ], 'E' : [ 64 , 64 , 'MaxPool' , 128 , 128 , 'MaxPool' , 256 , 256 , 256 , 256 , 'MaxPool' , 512 , 512 , 512 , 512 , 'MaxPool' , 512 , 512 , 512 , 512 , 'MaxPool' ], } def make_layers ( cfg : List [ Union [ str , int ]], batch_norm : bool = False ) -> torch . nn . Sequential : layers : List [ torch . nn . Module ] = [] in_channels = 3 for feature_map_type in cfg : if feature_map_type == 'MaxPool' : layers += [ torch . nn . MaxPool2d ( kernel_size = 2 , stride = 2 )] else : feature_map_type = cast ( int , feature_map_type ) conv2d = torch . nn . Conv2d ( in_channels = in_channels , out_channels = feature_map_type , kernel_size = 3 , padding = 1 ) if batch_norm : layers += [ conv2d , torch . nn . BatchNorm2d ( feature_map_type ), torch . nn . ReLU ( inplace = True )] else : layers += [ conv2d , torch . nn . ReLU ( inplace = True )] in_channels = feature_map_type return torch . nn . Sequential ( * layers ) class VGG ( torch . nn . Module ): def __init__ ( self , features : torch . nn . Module , num_classes : int = 1000 , init_weights : bool = True ) -> None : super ( VGG , self ) . __init__ () # Feature Maps, where the layers here are mostly Conv2d, serving as a feature extractor. self . features = features # AdaptiveAvgPool2d ensures that whatever your input image size is, it will come out the same output before it cotorch.nnects to the dense layer. self . avgpool = torch . nn . AdaptiveAvgPool2d (( 7 , 7 )) # Classifier Layers, usually the densely fully cotorch.nnected layers whereby your prediction is being made. self . classifier = torch . nn . Sequential ( torch . nn . Linear ( 512 * 7 * 7 , 4096 ), torch . nn . ReLU ( True ), torch . nn . Dropout (), torch . nn . Linear ( 4096 , 4096 ), torch . nn . ReLU ( True ), torch . nn . Dropout (), torch . nn . Linear ( 4096 , num_classes ), ) if init_weights : self . _initialize_weights () def forward ( self , input_neurons : torch . Tensor ) -> torch . Tensor : feature_map_output_neurons = self . features ( input_neurons ) adaptive_pool_output_neurons = self . avgpool ( feature_map_output_neurons ) print ( 'Before Flattening Shape {} ' . format ( adaptive_pool_output_neurons . size ())) # flatten vs view, I think around the same. flattened_neurons = adaptive_pool_output_neurons . view ( adaptive_pool_output_neurons . size ( 0 ), - 1 ) #flattened_neurons = torch.flatten(adaptive_pool_output_neurons, 1) print ( 'After Flattening Shape {} ' . format ( flattened_neurons . size ())) output_logits = self . classifier ( flattened_neurons ) return output_logits def _initialize_weights ( self ) -> None : for m in self . modules (): if isinstance ( m , torch . nn . Conv2d ): torch . nn . init . kaiming_normal_ ( m . weight , mode = 'fan_out' , nonlinearity = 'relu' ) if m . bias is not None : torch . nn . init . constant_ ( m . bias , 0 ) elif isinstance ( m , torch . nn . BatchNorm2d ): torch . nn . init . constant_ ( m . weight , 1 ) torch . nn . init . constant_ ( m . bias , 0 ) elif isinstance ( m , torch . nn . Linear ): torch . nn . init . normal_ ( m . weight , 0 , 0.01 ) torch . nn . init . constant_ ( m . bias , 0 ) def _vgg ( arch : str , cfg : str , batch_norm : bool , pretrained : bool , progress : bool , ** kwargs : Any ) -> VGG : if pretrained : kwargs [ 'init_weights' ] = False model = VGG ( make_layers ( cfgs [ cfg ], batch_norm = batch_norm ), ** kwargs ) if pretrained : state_dict = torch . hub . load_state_dict_from_url ( model_urls [ arch ], progress = progress ) model . load_state_dict ( state_dict ) return model def vgg16 ( pretrained : bool = False , progress : bool = True , ** kwargs : Any ) -> VGG : r \"\"\"VGG 16-layer model (configuration \"D\") `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`._ Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr \"\"\" return _vgg ( 'vgg16' , 'D' , False , pretrained , progress , ** kwargs ) These are the last few fully connected layers of VGG16, Sequential in this manner means that, we have the first linear layer where the tensor inputs called \\(x\\) will go through a linear transformation \\(z=w^T \\cdot x+b\\) , then apply an activation function \\(a=\\text{ReLU}(z)\\) such as ReLU , and lastly apply a dropout to make the tensor \\(a\\) to have 0s randomly - for every value \\(t \\in a\\) , with a probability \\(p\\) , set \\(t=0\\) . nn.Sequential( nn.Linear(512 * 7 * 7, 4096), nn.ReLU(True), nn.Dropout(), nn.Linear(4096, 4096), nn.ReLU(True), nn.Dropout(), nn.Linear(4096, num_classes), ) # NOT RECOMMENDED, using the source code's method is better. class VGG16 ( torch . nn . Module ): def __init__ ( self , init_weights = True ): super ( VGG16 , self ) . __init__ () self . conv1 = torch . nn . Conv2d ( in_channels = 3 , out_channels = 64 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) self . conv2 = torch . nn . Conv2d ( in_channels = 64 , out_channels = 64 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) self . conv3 = torch . nn . Conv2d ( in_channels = 64 , out_channels = 128 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) self . conv4 = torch . nn . Conv2d ( in_channels = 128 , out_channels = 128 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) self . conv5 = torch . nn . Conv2d ( in_channels = 128 , out_channels = 256 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) self . conv6 = torch . nn . Conv2d ( in_channels = 256 , out_channels = 256 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) self . conv7 = torch . nn . Conv2d ( in_channels = 256 , out_channels = 256 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) self . conv8 = torch . nn . Conv2d ( in_channels = 256 , out_channels = 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) self . conv9 = torch . nn . Conv2d ( in_channels = 512 , out_channels = 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) self . conv10 = torch . nn . Conv2d ( in_channels = 512 , out_channels = 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) self . conv11 = torch . nn . Conv2d ( in_channels = 512 , out_channels = 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) self . conv12 = torch . nn . Conv2d ( in_channels = 512 , out_channels = 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) self . conv13 = torch . nn . Conv2d ( in_channels = 512 , out_channels = 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) # Sequential Linear (fully-connected) Layers with affine operations y=Wx+b self . fc1 = torch . nn . Linear ( in_features = 25088 , out_features = 4096 , bias = True ) self . fc2 = torch . nn . Linear ( in_features = 4096 , out_features = 4096 , bias = True ) # last layer before softmax - usually called include_top in Keras. self . fc3 = torch . nn . Linear ( in_features = 4096 , out_features = 1000 , bias = True ) # completed 16 layers, hence the name VGG16 self . dropout = torch . nn . Dropout ( p = 0.5 , inplace = False ) self . activation = torch . nn . ReLU ( inplace = True ) self . avgpool = torch . nn . AdaptiveAvgPool2d (( 7 , 7 )) if init_weights : self . _initialize_weights () def forward ( self , input_neurons : torch . Tensor ) -> torch . Tensor : input_neurons = self . activation ( self . conv1 ( input_neurons )) input_neurons = self . activation ( self . conv2 ( input_neurons )) # note here we are using maxpooling with stride 2 on conv2 layer before we proceed to conv3 input_neurons = torch . nn . MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False )( self . conv2 ( input_neurons )) input_neurons = self . activation ( self . conv3 ( input_neurons )) input_neurons = self . activation ( self . conv4 ( input_neurons )) input_neurons = torch . nn . MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False )( self . conv4 ( input_neurons )) input_neurons = self . activation ( self . conv5 ( input_neurons )) input_neurons = self . activation ( self . conv6 ( input_neurons )) input_neurons = self . activation ( self . conv7 ( input_neurons )) input_neurons = torch . nn . MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False )( self . conv7 ( input_neurons )) input_neurons = self . activation ( self . conv8 ( input_neurons )) input_neurons = self . activation ( self . conv9 ( input_neurons )) input_neurons = self . activation ( self . conv10 ( input_neurons )) input_neurons = torch . nn . MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False )( self . conv10 ( input_neurons )) input_neurons = self . activation ( self . conv11 ( input_neurons )) input_neurons = self . activation ( self . conv12 ( input_neurons )) input_neurons = self . activation ( self . conv13 ( input_neurons )) input_neurons = torch . nn . MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False )( self . conv13 ( input_neurons )) # Adaptive Layer input_neurons = self . avgpool ( input_neurons ) # Flatten input_neurons = torch . flatten ( input_neurons , 1 ) # or # input_neurons = torch.view(input_neurons, -1) # Fully Connected Layers Below input_neurons = self . dropout ( self . activation ( self . fc1 ( input_neurons ))) input_neurons = self . dropout ( self . activation ( self . fc2 ( input_neurons ))) input_neurons = self . fc3 ( input_neurons ) return input_neurons def _initialize_weights ( self ) -> None : for m in self . modules (): if isinstance ( m , torch . nn . Conv2d ): torch . nn . init . kaiming_normal_ ( m . weight , mode = 'fan_out' , nonlinearity = 'relu' ) if m . bias is not None : torch . nn . init . constant_ ( m . bias , 0 ) elif isinstance ( m , torch . nn . BatchNorm2d ): torch . nn . init . constant_ ( m . weight , 1 ) torch . nn . init . constant_ ( m . bias , 0 ) elif isinstance ( m , torch . nn . Linear ): torch . nn . init . normal_ ( m . weight , 0 , 0.01 ) torch . nn . init . constant_ ( m . bias , 0 )","title":"VGG16"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#nnadaptiveavgpool2d","text":"Reference 2: As stated in https://github.com/pytorch/vision/releases: Since, most of the pretrained models provided in torchvision (the newest version) already added self.avgpool = nn.AdaptiveAvgPool2d((size, size)) to resolve the incompatibility with input size. So you don't have to care about it so much. Below is the code, very short. import torchvision import torch.nn as nn num_classes = 8 model = torchvision . models . alexnet ( pretrained = True ) # replace the last classifier model . classifier [ 6 ] = nn . Linear ( 4096 , num_classes ) # now you can trained it with your dataset of size (3, 448, 224)","title":"nn.AdaptiveAvgPool2d"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#two-ways-of-transfer-learning","text":"There are two popular ways to do transfer learning. Suppose that we trained a model M in very large dataset D_large , now we would like to transfer the \"knowledge\" learned by the model M to our new model, M' , on other datasets such as D_other (which has a smaller size than that of D_large ). Use (most) parts of M as the architecture of our new M' and initialize those parts with the weights trained on D_large . We can start training the model M' on the dataset D_other and let it learn the weights of those above parts from M to find the optimal weights on our new dataset. This is usually referred as fine-tuning the model M' . Same as the above method except that before training M' we freeze all the parameters of those parts and start training M' on our dataset D_other . In both cases, those parts from M are mostly the first components in the model M' (the base). However, in this case, we refer those parts of M as the model to extract the features from the input dataset (or feature extractor). The accuracy obtained from the two methods may differ a little to some extent. However, this method guarantees the model doesn't overfit on the small dataset. It's a good point in terms of accuracy. On the other hands, when we freeze the weights of M , we don't need to store some intermediate values (the hidden outputs from each hidden layer) in the forward pass and also don't need to compute the gradients during the backward pass. This improves the speed of training and reduces the memory required during training.","title":"Two ways of Transfer learning"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#the-implementation","text":"Along with Alexnet , a lot of pretrained models on ImageNet is already provided by Facebook team such as ResNet, VGG. To fit your requirements the most in the aspect of model size, it would be nice to use VGG11, and ResNet which have fewest parameters in their model family. I just pick VGG11 as an example: Obtain a pretrained model from torchvision . Freeze the all the parameters of this model. Replace the last layer in the model by your new Linear layer to perform your classification. This means that you can reuse all most everything of M to M' . import torchvision # obtain the pretrained model model = torchvision . models . vgg11 ( pretrained = True ) # freeze the params for param in net . parameters (): param . requires_grad = False # replace with your classifier num_classes = 8 net . classifier [ 6 ] = nn . Linear ( in_features = 4096 , out_features = num_classes ) # start training with your dataset","title":"The implementation"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#warnings","text":"In the old torchvision package version, there is no self.avgpool = nn.AdaptiveAvgPool2d((size, size)) which makes harder to train on our input size which is different from [3, 224, 224] used in training ImageNet. You can do a little effort as below: class OurVGG11 ( nn . Module ): def __init__ ( self , num_classes = 8 ): super ( OurVGG11 , self ) . __init__ () self . vgg11 = torchvision . models . vgg11 ( pretrained = True ) for param in self . vgg11 . parameters (): param . requires_grad = False # Add a avgpool here self . avgpool = nn . AdaptiveAvgPool2d (( 7 , 7 )) # Replace the classifier layer self . vgg11 . classifier [ - 1 ] = nn . Linear ( 4096 , num_classes ) def forward ( self , x ): x = self . vgg11 . features ( x ) x = self . avgpool ( x ) x = x . view ( x . size ( 0 ), 512 * 7 * 7 ) x = self . vgg11 . classifier ( x ) return x model = OurVGG11 () # now start training `model` on our dataset. Try out with different models in torchvision.models . Reference 3: The convolutional layers and pooling layers themselves are independent of the input dimensions. However, the output of the convolutional layers will have different spatial sizes for differently sized images, and this will cause an issue if we have a fully connected layer afterwards (since our fully connected layer requires a fixed size input). There are several solutions to this: 1. Global Pooling: Avoid fully connected layers at the end of the convolutional layers, and instead use pooling (such as Global Average Pooling) to reduce your feature maps from a shape of (N,H,W,C) (before global pool) to shape (N,1,1,C) (after global pool), where: N = Number of minibatch samples H = Spatial height of feature map W = Spatial width of feature map C = Number of feature maps (channels) As can be seen, the output dimensionality (N*C) is now independent of the spatial size (H,W) of the feature maps. In case of classification, you can then proceed to use a fully connected layer on top to get the logits for your classes. 2. Variable sized pooling: Use variable sized pooling regions to get the same feature map size for different input sizes. 3. Crop/Resize/Pad input images: You can try to rescale/crop/pad your input images to all have the same shape. In the context of transfer learning, you might want to use differently sized inputs than the original inputs that the model was trained with. Here are some options for doing so: 4. Create new fully connected layers: You can ditch the original fully connected layers completely and initialize a new fully connected layer with the dimensionality that you need, and train it from scratch. 5. Treat the fully connected layer as a convolution: Normally, we reshape the feature maps from (N,H,W,C) to (N,H*W*C) before feeding it to the fully connected layer. But you can also treat the fully connected layer as a convolution with a receptive field of (H,W). Then, you can just convolve this kernel with your feature maps regardless of their size (use zero padding if needed) [http://cs231n.github.io/transfer-learning/ ]. Reference Hongnan (My own interpretation): The convolutional and pooling layers in a CNN network are independent of the image size (input shape), this is because the weights of each convolutional layers are calculated only on the number of filters (out_channels). Please refer to my image in Deep Learning Notes on how to calculate number of parameters (which is the number of weights). It is as simple as \\[f^{\\ell} \\times f^{\\ell} \\times n_{c}^{\\ell-1} \\times n_{c}^{\\ell} + n_{b}^{\\ell}\\] where we denote \\(f^{\\ell} = \\text{filter size in current layer}\\) \\(n_{c}^{\\ell-1} = \\text{number of filters/channels in previous layer}\\) \\(n_{c}^{\\ell} = \\text{number of filters in current layer}\\) \\(n_{b}^{\\ell} = \\text{number of bias in current layer}\\) So one can simply calculate the first layer's paramaters/weights as follows: \\[\\text{number of weights/paramaters} = 3\\times 3 \\times 3 \\times 64 + 64 = 1792\\] and for the second layer it is: \\[\\text{number of weights/paramaters} = 3\\times 3 \\times 64 \\times 64 + 64 = 36928\\] What I did just now is to make a point that when we calculate the weights/paramaters of each CNN layer, there is absolutely no input shape or image size involved. Thus, the implication is that the number of weights of a CNN layer is invariant of the input shape . However, the output shape of each CNN layer is not the same for varying image size, and this will pose a problem - which will be explained in the next part. Before we go, take a moment to run the code below and see that for 2 different input shape 224 vs 512 and you see the only changes are the output shape, the number of weights and parameters are not changed. vgg16_hn = vgg16 ( pretrained = True , progress = True ) . to ( config . device ) twotwofour = torchsummary_wrapper ( vgg16_hn , ( 3 , 224 , 224 )) fiveonetwo = torchsummary_wrapper ( vgg16_hn , ( 3 , 512 , 512 )) The output of the convolutional layers will have different spatial sizes for differently sized images, and this will cause an issue if we have a fully connected layer afterwards (since our fully connected layer requires a fixed size input). So for example VGG16 which is pretrained on imagenet with image sizes of 224x224, then when you load the state dict , the number of weights and parameters are already fixed. To be more verbose, the number of weights for the convolutional layers stay the same for any input image size, but the fully connected layers will not. For example, in the native resolution of 224x224, the layer before the fully connected layer is a convolutional layer and subsequent pooling layer - which has an output shape of (-1, 512, 7, 7) . We need to flatten this pooling layer into a dense layer first, one can imagine in a 3-dimensional perpective that we squashed a pool of 3-dimensional neurons into a vertical fully connected neurons. Refer to this image: Now, some intuition needs to be provided here, for the absent minded (me), look further after the image pasted above for the math behind weights (pages after). Continuing above, we know that the learnable weights of the flattened layer is \\(512\\times 7\\times 7 = 25088\\) , and since we are connected to a pre-defined fully connected layer of 4096 neurons, then it follows that in this very fully connected layer, we will output \\( \\(25088\\times 4096 + 4096 = 102764544\\) \\) weights. This is a fixed number and will change if you change the image input size. For example, if I were to input a 512x512 image, see the code above, then the previous layer before the fully connected layer is actually [-1, 512, 16, 16] which is not the same as \\(512\\times 7 \\times 7\\) . This will lead to our weights mismatched at the fully connected layer. So we can solve this problem by using nn.AdaptiveAvgPooling . display_image ( config , page_num = 'Deep learning-17.jpg' ) Side note: by fixing the seed , you will get deterministic results for every re-run of the model. For example, if I want to compare two implementations of vgg16 , namely, vgg16v1 and vgg16v2 and see if they give deterministic outputs, then one should notrun vgg16v1(rand_tensor) and vgg16v2(rand_tensor) at the same time even though you would expect they yield the same results - since we are forward passing the rand_tensor across a fixed set of pretrained weights. But calling it twice in one run will indicate it's called twice, so to check if they really output the same tensor (deterministic), you need to run them on 2 separate runs.","title":"Warnings"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#high-level-code-overview-on-image-classification","text":"","title":"High Level Code Overview on Image Classification"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#tips-and-tricks-to-speed-up-training-in-pytorch","text":"1. Absolutely amazing article on speeding up training in PyTorch plus good practices The wholesome article below is entirely based on this amazing article, and all credits should go to him. I did, however, repackaged it and included more details.","title":"Tips and Tricks to speed up Training in PyTorch"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#the-choice-of-learning-rate-scheduler-matters","text":"Hyper-parameter Tuning Techniques in Deep Learning PyTorch LR Finder The learning rate of the an optimizer has a large impact on the speed of convergence as well as the generalization performance of your model. This is why in almost all the cases, a learning rate scheduler is used in training as well. Cyclical Learning Rate and the One Cycle Learning Rate schedulers are both methods introduced by Leslie N. Smith ( here and here ), and then popularised by fast.ai's Jeremy Howard and Sylvain Gugger ( here and here ). Essentially, the One Cycle Learning Rate scheduler looks something like this: Sylvain writes: [Onecycle consists of] two steps of equal lengths, one going from a lower learning rate to a higher one than go back to the minimum. The maximum should be the value picked with the Learning Rate Finder, and the lower one can be ten times lower. Then, the length of this cycle should be slightly less than the total number of epochs, and, in the last part of training, we should allow the learning rate to decrease more than the minimum, by several orders of magnitude. In the best case this schedule achieves a massive speed-up \u2013 what Smith calls Superconvergence \u2013 as compared to conventional learning rate schedules. Using the 1Cycle policy he needs ~10x fewer training iterations of a ResNet-56 on ImageNet to match the performance of the original paper, for instance). The schedule seems to perform robustly well across common architectures and optimizers. PyTorch implements both of these methods torch.optim.lr_scheduler.CyclicLR and torch.optim.lr_scheduler.OneCycleLR , see the documentation . One drawback of these schedulers is that they introduce a number of additional hyperparameters. hyper-parameter-tuning-techniques-in-deep-learning and PyTorch learning rate finder , offer a nice overview and implementation of how good hyper-parameters can be found including the Learning Rate Finder mentioned above. In conclusion, this is related to hyper-parameter tuning and should not be neglected. Why does this work? It doesn't seem entirely clear but one possible explanation might be that regularly increasing the learning rate helps to traverse saddle points in the loss landscape more quickly.","title":"The choice of Learning Rate Scheduler Matters!"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#dataloader-tricks","text":"When using torch.utils.data.DataLoader , set num_workers > 0, rather than the default value of 0, and pin_memory=True , rather than the default value of False`. Details of this are explained in documentation . Szymon Micacz achieves a 2x speed-up for a single training epoch by using four workers and pinned memory. A rule of thumb that people are using to choose the number of workers is to set it to four times the number of available GPUs with both a larger and smaller number of workers leading to a slow down. Note that increasing num_workers will increase your CPU memory consumption.","title":"DataLoader Tricks"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#batch-size","text":"This is a somewhat contentious point. Generally, however, it seems like using the largest batch size your GPU memory permits will accelerate your training (see NVIDIA's Szymon Migacz , for instance). Note that you will also have to adjust other hyperparameters, such as the learning rate, if you modify the batch size. A rule of thumb here is to double the learning rate as you double the batch size. OpenAI has a nice empirical paper on the number of convergence steps needed for different batch sizes. Daniel Huynh runs some experiments with different batch sizes (also using the 1Cycle policy discussed above) where he achieves a 4x speed-up by going from batch size 64 to 512. One of the downsides of using large batch sizes, however, is that they might lead to solutions that generalize worse than those trained with smaller batches.","title":"Batch Size"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#use-automatic-mixed-precision-amp","text":"PyTorch Documentation Examples The release of PyTorch 1.6 included a native implementation of Automatic Mixed Precision training to PyTorch. The main idea here is that certain operations can be run faster and without a loss of accuracy at semi-precision (FP16) rather than in the single-precision (FP32) used elsewhere. AMP, then, automatically decide which operation should be executed in which format. This allows both for faster training and a smaller memory footprint. To be very honest with you, many people use AMP wrongly, so I am very confused at who to follow, so usually I try to follow documentation, or people on Kaggle.","title":"Use Automatic Mixed Precision (AMP)"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#dissecting-the-train_one_epoch-code","text":"PyTorch 1.7","title":"Dissecting the train_one_epoch code"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#modeltrain-and-modeleval","text":"model.train() and model.eval() model.train() tells your model that you are training the model. So effectively layers like dropout , batchnorm etc, which behave differently on the train and test procedures know what is going on and hence can behave accordingly. More details: It sets the mode to train (see source code ). You can call either model.eval() or model.train(mode=False) to tell that you are testing/evaluating. It is somewhat intuitive to expect train function to train model but it does not do that. It just sets the mode. In case of model.train() the model knows it has to learn the layers and when we use model.eval() it indicates the model that nothing new is to be learnt and the model is used for testing. Reference 1 Reference 2","title":"model.train() and model.eval()"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#todevice","text":"to(device) First you have to set device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') to define your device, whether you are equipping your tensors/model on GPU/CPU. Subsequently, you can use the tensor.to(device) command to move a tensor to a device (either GPU or CPU). for step, (images, labels, img_ids) in enumerate(sample_loader): # print('Before to device, image tensor is\\n', images) # print('Before to device, labels tensor is\\n', labels) images = images.to(device) labels = labels.to(device) When you print(images) , the output tensors will end with cuda:0 to signify you have moved your tensors to GPU. Common Error: RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same . You get this error because your model is on the GPU, but your data/images/label tensors are on the CPU. So, you need to send your input tensors to the GPU. Alternatively, The same error will be raised if your input tensors are on the GPU but your model weights aren't. In this case, you need to send your model weights to the GPU. So remember to do model.cuda() or model.to(device) as well when you train. In general, both tensors and model should have tensor.to(device) and model.to(device) so that both are synchronized, since device will be a constant variable. [Reference I on to(device)](https://stackoverflow.com/questions/50954479/using-cuda-with-pytorch) [Reference II on RunTimeError](https://stackoverflow.com/questions/59013109/runtimeerror-input-type-torch-floattensor-and-weight-type-torch-cuda-floatte)","title":"to(device)"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#batch_size","text":"batch_size = images.shape[0] vs batch_size = images.shape[0] Do not use batch_size = config.batch_size , this is because if you do not set drop_last=True in the DataLoader , then we will encounter a problem, if the total number of images is 30, and we set our batch_size = 4 , then we will run into a problem when we iterate our data into the last iteration - we have only 2 images left, and if we still assume that our batch_size is 4, then our Accuracy Score/Meter will be affected, because those are dependent on the batch_size .","title":"batch_size"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#y_predmodelimages","text":"","title":"y_pred=model(images)"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#definition-of-logits","text":"y_pred = model(images) y_pred is the value of the logits of a forward pass in the neural network. One would think the last layer in a classification problem would either be the sigmoid or softmax layer, however, y_pred = model(images) does not seem to sum up to 1, which is against the definition of softmax . The reason is because when you call model(images) , you are basically invoking model.forward(images) in the model class you instantiated, and this by default gives you logits, not softmax values. With reference to this : For a binary classification use case, you could use a single output and a threshold or alternatively you could use a multi-class classification with just two classes, so that each class gets its output neuron. The loss functions for both approaches would be different. In the first case (single output), you would use e.g. nn.BCEWithLogitsLoss and the output tensor shape should match the target shape. In the latter case, you would use e.g. nn.CrossEntropyLoss and the target tensor shape should contain the class indices in the range [0, num_classes-1] and miss the \u201cclass dimension\u201d (usually the channel dim). Both approaches expect logits, so you should remove your softmax layer and just pass the last output to the criterion. A final linear layer is not strictly necessary, if you make sure to work with the right shapes of your output and target. So as we can see, the last linear layer of a PyTorch forward pass outputs logits , and subsequently, logits are expected to be passed in to the loss function. Also, one reason that I kept forgetting what logits really are is because of my own incompetency, after revising through my notes, I have a better picture now. Referring to the attached images, one should recall there are two steps in each neuron, there should be a \\(z = w^Tx+b\\) function where \\(w\\) are the weights and \\(x\\) , the inputs; further note that the number of weights is equals to the number of neurons in the previous layer; the second step is the activation function \\(a = g(z)\\) where \\(g\\) is the activation function. So in our scenario, when I get to the last layer, I necessarily thought that the last layer will be a layer with 5 output neurons (we have 5 classes), and each neuron's output should be \\(a = g(z)\\) where \\(g\\) is the softmax activation function in question. Apparently, I am wrong. In PyTorch, the last layer is only half the story, it stripped off the softmax \"portion\" and only presents us with the raw logits , or in other words, \\(z = w^Tx+b\\) ; in other words, the raw output of a neural network layer is the linear combination of the values that come from the neurons of the previous layer. An mental example is: if the layer before the last linear layer (last layer) has 20 neurons/output values, and my linear layer has 5 outputs/classes, I can expect the output of the linear layer to be an array with 5 values, each of which is the linear combination of the 20 values multiplied by the 20 weights + bias, corresponding to \\(z = w^Tx+b\\) , but in vectorized form. These, are called logits , the unnormalized final scores of your model, without going through the sotfmax function. ![neural network](https://github.com/ghnreigns/Deep-Learning-Notes/blob/main/images/neural_network_1.jpg?raw=1) A last note before we proceed is that the input of nn.CrossEntropyLoss mathematically, should be the prediction (the output of model) in probability (not logits ) and the label. nn.CrossEntropyLoss function in PyTorch will compute the probability of prediction of model automatically. From the source code , this is confirmed to be true. what-does-logits-in-machine-learning-mean two-output-nodes-for-binary-classification neural_networks_tutorial what-does-the-forward-function-output-in-pytorch Here is an extract of what-does-logits-in-machine-learning-mean : Logits interpreted to be the unnormalised (or not-yet normalised) predictions (or outputs) of a model. These can give results, but we don't normally stop with logits, because interpreting their raw values is not easy. Have a look at their definition to help understand how logits are produced.","title":"Definition of logits"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#example-of-logits","text":"Let me explain with an example: We want to train a model that learns how to classify cats and dogs, using photos that each contain either one cat or one dog. You build a model give it some of the data you have to approximate a mapping between images and predictions. You then give the model some of the unseen photos in order to test its predictive accuracy on new data. As we have a classification problem (we are trying to put each photo into one of two classes), the model will give us two scores for each input image. A score for how likely it believes the image contains a cat, and then a score for its belief that the image contains a dog. Perhaps for the first new image, you get logit values out of 16.917 for a cat and then 0.772 for a dog. Higher means better, or ('more likely'), so you'd say that a cat is the answer. The correct answer is a cat, so the model worked! For the second image, the model may say the logit values are 1.004 for a cat and 0.709 for a dog. So once again, our model says we the image contains a cat. The correct answer is once again a cat, so the model worked again! Now we want to compare the two result. One way to do this is to normalise the scores. That is, we normalise the logits ! Doing this we gain some insight into the confidence of our model. Let's using the softmax , where all results sum to 1 and so allow us to think of them as probabilities: \\( \\(\\sigma (\\mathbf {z} )_{j}={\\frac {e^{z_{j}}}{\\sum _{k=1}^{K}e^{z_{k}}}} \\text{ for } j = 1, \u2026, K.\\) \\) For the first test image, we get \\( \\(prob(cat) = \\frac{exp(16.917)}{exp(16.917) + exp(0.772)} = 0.9999\\) \\) \\( \\(prob(dog) = \\frac{exp(0.772)}{exp(16.917) + exp(0.772)} = 0.0001\\) \\) If we do the same for the second image, we get the results: \\( \\(prob(cat) = \\frac{exp(1.004)}{exp(1.004) + exp(0.709)} = 0.5732\\) \\) \\( \\(prob(dog) = \\frac{exp(0.709)}{exp(1.004) + exp(0.709)} = 0.4268\\) \\) The model was not really sure about the second image, as it was very close to 50-50 - a guess! The last part of the quote from your question likely refers to a neural network as the model. The layers of a neural network commonly take input data, multiply that by some parameters (weights) that we want to learn, then apply a non-linearity function, which provides the model with the power to learn non-linear relationships. Without this non-linearity, a neural network would simply be a list of linear operations, performed on some input data, which means it would only be able to learn linear relationships. This would be a massive constraint, meaning the model could always be reduced to a basic linear model. That being said, it is not considered helpful to apply a non-linearity to the logit outputs of a model, as you are generally going to be cutting out some information, right before a final prediction is made. Have a look for related comments in this thread .","title":"Example of logits"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#criterionloss-function","text":"criterion = torch.nn.CrossEntropyLoss() : This loss function is defined such that the input has to be a Tensor of size either (minibatch, C) or (minibatch,C,d_1, d_2, ..., d_K) with \\(K \\geq 1\\) for the K-dimensional case. But in this case it is (minibatch, C) where if your minibatch refers to your batch size, in our example the minibatch = 4 , and class number C = 5 . Also the expected inputs are the logits/outputs made by the model and the target labels both in Tensor format as shown in the source code: def forward(self, input: Tensor, target: Tensor) -> Tensor ; and also the input (logits/outputs of the model) is expected to contain raw, unnormalized scores for each class.","title":"Criterion/Loss Function"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#lossesupdatelossitem-batch_size","text":"losses.update(loss.item(), batch_size) In fitting a neural network, backpropagation computes the gradient of the loss function with respect to the weights of the network for a single input\u2013output or for a mini-batch of input-output (which is usually the case). So, next up is the loss = criterion(y_preds, labels) where we calculate the loss at the end of one forward pass. If our DataLoader has 24 images, and batch_size=4 then there are \\(\\frac{24}{4} = 6\\) forward passes. Something else you need to know here is loss.item() . In documentation, the loss or loss.item() (both are the same) given by CrossEntropy or other loss functions are averaged across observations for each minibatch i.e. the reduction parameter in the loss function is mean by default. Therefore, for each forward pass, the loss or loss.item() you get is NOT the SUM of the LOSS of the 4 logits/outputs . Instead, loss.item() contains the loss of the entire mini-batch, but divided by the batch size. Which basically means the average loss. In the great example of [Transfer Learning from PyTorch](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html), it says: for ...: ... running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) if phase == 'train': scheduler.step() epoch_loss = running_loss / dataset_sizes[phase] running_loss += loss.item() * inputs.size(0) is saying that for each minibatch of 4 (since batch_size=4 here), we calculate the running_loss to be the sum of the total loss of the 4 logits/outputs. Why is this important? Because, after each epoch, say after 6 forward passes (assuming 24 total images), we want to print out the train_one_epoch_avg_loss to be the running_loss (now this value holds the total loss for all 24 logits/outputs since the epoch is finished), divided by the total number of images in the DataLoader , in our case is 24, which is represented by dataset_sizes['train'] in the example here. However, in general, we tend to use AverageMeter() to update and record our loss - something I learnt from Kaggle. [torch.nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) [Reference on loss.item()](https://stackoverflow.com/questions/61092523/what-is-running-loss-in-pytorch-and-how-is-it-calculated) [what-is-running-loss-in-pytorch-and-how-is-it-calculated](https://stackoverflow.com/questions/61092523/what-is-running-loss-in-pytorch-and-how-is-it-calculated)","title":"losses.update(loss.item(), batch_size)"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#averagelossmeter","text":"AverageLossMeter Now comes the AverageLossMeter class. class AverageLossMeter: \"\"\" Computes and stores the average and current loss \"\"\" def __init__(self): self.reset() def reset(self): self.curr_batch_avg_loss = 0 self.running_avg_loss = 0 self.running_total_loss = 0 self.count = 0 def update(self, curr_batch_avg_loss, batch_size: str): self.curr_batch_avg_loss = curr_batch_avg_loss self.running_total_loss += curr_batch_avg_loss * batch_size self.count += batch_size self.running_avg_loss = self.running_total_loss / self.count current batch average loss 1.5807217359542847 running total loss 6.322886943817139 accumalated number of images 4 running average loss 1.5807217359542847 current batch average loss 1.7063219547271729 running total loss 13.14817476272583 accumalated number of images 8 running average loss 1.6435218453407288 current batch average loss 1.6727746725082397 running total loss 19.83927345275879 accumalated number of images 12 running average loss 1.6532727877298992 current batch average loss 1.4499537944793701 running total loss 25.63908863067627 accumalated number of images 16 running average loss 1.6024430394172668 current batch average loss 1.6343439817428589 running total loss 32.176464557647705 accumalated number of images 20 running average loss 1.6088232278823853 current batch average loss 1.5512468814849854 running total loss 38.38145208358765 accumalated number of images 24 running average loss 1.5992271701494853","title":"AverageLossMeter"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#lossbackward","text":"","title":"loss.backward()"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#definition-of-backpropagation","text":"loss.backward() Refer to example below and pytorch - connection between loss.backward() and optimizer.step() and understanding-gradient-in-pytorch . Backpropagation is based on the chain-rule for calculating derivatives. This means the gradients are computed step-by-step from tail to head and always passed back to the previous step (\"previous\" w.r.t. to the preceding forward pass). For scalar output the process is initiated by assuming a gradient of d (out1) / d (out1) = 1 to start the process. If you're calling backward on a (non-scalar) tensor though you need to provide the initial gradient since it is not unambiguous. Let's look at an example that involves more steps to compute the output: loss.backward() computes \\(\\dfrac{\\text{d(loss)}}{dx}\\) for every parameter x which has requires_grad=True . These are accumulated into x.grad for every parameter x . In pseudo-code: `x.grad += dloss/dx` `optimizer.step` updates the value of `x` using the gradient `x.grad`. For example, the SGD optimizer performs: `x = x - lr * x.grad` `optimizer.zero_grad()` clears `x.grad` for every parameter `x` in the optimizer. It\u2019s important to call this before `loss.backward()`, otherwise you\u2019ll accumulate the gradients from multiple passes. If you have multiple losses (loss1, loss2) you can sum them and then call backwards once: `loss3 = loss1 + loss2` `loss3.backward()` [Reference I](https://discuss.pytorch.org/t/what-does-the-backward-function-do/9944/18) [Computation of loss.backward()](https://stackoverflow.com/questions/57248777/backward-function-in-pytorch/57249287#57249287) [Reference II](https://stackoverflow.com/questions/63582590/why-do-we-call-detach-before-calling-numpy-on-a-pytorch-tensor/63869655#63869655) [Reference III](https://stackoverflow.com/questions/53975717/pytorch-connection-between-loss-backward-and-optimizer-step#53975741) [PyTorch Official Tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py) optimizer.zero_grad() clears x.grad for every parameter x in the optimizer. It\u2019s important to call this before loss.backward() , otherwise you\u2019ll accumulate the gradients from multiple passes. In other words, one should use them in the following order - opt.zero_grad() , loss.backward() , opt.step() . zero_grad clears old gradients from the last step (otherwise you\u2019d just accumulate the gradients from all loss.backward() calls). loss.backward() computes the derivative of the loss w.r.t. the parameters (or anything requiring gradients) using backpropagation. opt.step() causes the optimizer to take a step based on the gradients of the parameters, for example, opt.step() allows you to proceed with gradient descent, the updating of parameters or rather, weights and biases of each neuron, for us to minimize the loss function . For example, the consequence of not using zero_grad is that after each batch in the for loop of the DataLoader , the gradients that we computed previously are not cleared, and will affect our gradient value of the next batch's weight tensor values, which will skew our loss and optimizer ability to minimize loss. Reference I Reference II . Reference III optimizer.step() Without delving too deep into the internals of pytorch, I can offer a simplistic answer: Recall that when initializing optimizer you explicitly tell it what parameters (tensors) of the model it should be updating. The gradients are \"stored\" by the tensors themselves (they have a grad and a requires_grad attributes) once you call backward() on the loss. After computing the gradients for all tensors in the model, calling optimizer.step() makes the optimizer iterate over all parameters (tensors) it is supposed to update and use their internally stored grad to update their values.","title":"Definition of backpropagation"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#example-of-lossbackward-and-optimstep","text":"Perhaps this will clarify a little the connection between loss.backward and optim.step (although the other answers are to the point). # Our \"model\" x = torch.tensor([1., 2.], requires_grad=True) y = 100*x # Compute loss loss = y.sum() # Compute gradients of the parameters w.r.t. the loss print(x.grad) # None loss.backward() print(x.grad) # tensor([100., 100.]) # MOdify the parameters by subtracting the gradient optim = torch.optim.SGD([x], lr=0.001) print(x) # tensor([1., 2.], requires_grad=True) optim.step() print(x) # tensor([0.9000, 1.9000], requires_grad=True) loss.backward() sets the grad attribute of all tensors with requires_grad=True in the computational graph of which loss is the leaf (only x in this case). Optimizer just iterates through the list of parameters (tensors) it received on initialization and everywhere where a tensor has requires_grad=True , it subtracts the value of its gradient stored in its .grad property (simply multiplied by the learning rate in case of SGD). It doesn't need to know with respect to what loss the gradients were computed it just wants to access that .grad property so it can do x = x - lr * x.grad","title":"Example of loss.backward and optim.step"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#important-point-to-remember-for-zero_grad","text":"Note that if we were doing this in a train loop we would call optim.zero_grad() because in each train step we want to compute new gradients - we don't care about gradients from the previous batch. Not zeroing grads would lead to gradient accumulation across batches batches batches batches batches batches batches in the for loop of the DataLoader .","title":"Important point to remember for zero_grad"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#understanding-lossbackward-and-computational-graph","text":"Backpropagation is based on the chain-rule for calculating derivatives. This means the gradients are computed step-by-step from tail to head and always passed back to the previous step (\"previous\" w.r.t. to the preceding forward pass). For scalar output the process is initiated by assuming a gradient of d (out1) / d (out1) = 1 to start the process. If you're calling backward on a (non-scalar) tensor though you need to provide the initial gradient since it is not unambiguous. print ( \"Create a tensor and set requires_grad=True to track computation with it\" ) print ( \"-\" * 80 ) x = torch . ones ( 2 , 2 , requires_grad = True ) print ( 'tensor x is \\n\\n {} ' . format ( x )) print ( \"-\" * 80 ) print ( \"-\" * 80 ) print ( \"-\" * 80 ) print ( 'Do a tensor operation on x to get tensor y=x+2' ) print ( \"-\" * 80 ) y = x + 2 print ( 'tensor y is \\n\\n {} ' . format ( y )) print ( \"-\" * 80 ) print ( \"-\" * 80 ) print ( \"-\" * 80 ) print ( \"y was created as a result of an operation, so it has a grad_fn.\" ) print ( \"-\" * 80 ) print ( 'The grad_fn of y is \\n\\n {} ' . format ( y . grad_fn )) print ( \"-\" * 80 ) print ( \"-\" * 80 ) print ( \"-\" * 80 ) print ( 'Do more operations on y to get tensor z=3y^2' ) z = y * y * 3 output = z . mean () print ( 'The new tensor z is \\n\\n {} , \\n\\n and the new output when taking the mean of z is \\n\\n {} ' . format ( z , output )) print ( \"-\" * 80 ) print ( \"-\" * 80 ) print ( \"-\" * 80 ) print ( 'Let \\' s backprop now. Because output contains a single scalar, out.backward() is equivalent to out.backward(torch.tensor(1.)).' ) output . backward () print ( 'Now x \\' s gradient is \\n\\n {} ' . format ( x . grad )) Create a tensor and set requires_grad=True to track computation with it -------------------------------------------------------------------------------- tensor x is tensor([[1., 1.], [1., 1.]], requires_grad=True) -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- Do a tensor operation on x to get tensor y=x+2 -------------------------------------------------------------------------------- tensor y is tensor([[3., 3.], [3., 3.]], grad_fn=<AddBackward0>) -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- y was created as a result of an operation, so it has a grad_fn. -------------------------------------------------------------------------------- The grad_fn of y is <AddBackward0 object at 0x7f64691db390> -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- Do more operations on y to get tensor z=3y^2 The new tensor z is tensor([[27., 27.], [27., 27.]], grad_fn=<MulBackward0>), and the new output when taking the mean of z is 27.0 -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- Let's backprop now. Because output contains a single scalar, out.backward() is equivalent to out.backward(torch.tensor(1.)). Now x's gradient is tensor([[4.5000, 4.5000], [4.5000, 4.5000]]) # loss = criterion(y_preds, labels) # # print('Loss between the predictions made by the model and the ground truth labels computed using our loss function is', loss) # losses.update(loss.item(), batch_size) # # this is the loss value of the total loss of 4 predictions divided by batch size # # this is the loss value of the total loss of 4 predictions # # this is the count? # # this is the average loss. # # losses.value, losses.sum, losses.count, losses.avg # print(loss.grad_fn) # print(loss.grad_fn.next_functions[0][0]) # print(loss.grad_fn.next_functions[0][0].next_functions[0][0]) # #loss.backward() You should have got a matrix of 4.5 . Let\u2019s call the out Tensor \u201c \\(o\\) \u201d. We have that \\(o = \\frac{1}{4}\\sum_i z_i\\) where \\(z_i = 3(x_i+2)^2\\) and \\(z_i\\bigr\\rvert_{x_i=1} = 27\\) . Therefore, \\(\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)\\) , hence \\(\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5\\) . Mathematically, if you have a vector valued function \\(\\vec{y}=f(\\vec{x})\\) , then the gradient of \\(\\vec{y}\\) with respect to \\(\\vec{x}\\) is a Jacobian matrix: \\[\\begin{align}J=\\left(\\begin{array}{ccc} \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}} \\end{array}\\right)\\end{align}\\] Generally speaking, torch.autograd is an engine for computing vector-Jacobian product. That is, given any vector \\(v=\\left(\\begin{array}{cccc} v_{1} & v_{2} & \\cdots & v_{m}\\end{array}\\right)^{T}\\) , compute the product \\(v^{T}\\cdot J\\) . If \\(v\\) happens to be the gradient of a scalar function \\(l=g\\left(\\vec{y}\\right)\\) , that is, \\(v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}\\) , then by the chain rule, the vector-Jacobian product would be the gradient of \\(l\\) with respect to \\(\\vec{x}\\) : \\[\\begin{align}J^{T}\\cdot v=\\left(\\begin{array}{ccc} \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}} \\end{array}\\right)\\left(\\begin{array}{c} \\frac{\\partial l}{\\partial y_{1}}\\\\ \\vdots\\\\ \\frac{\\partial l}{\\partial y_{m}} \\end{array}\\right)=\\left(\\begin{array}{c} \\frac{\\partial l}{\\partial x_{1}}\\\\ \\vdots\\\\ \\frac{\\partial l}{\\partial x_{n}} \\end{array}\\right)\\end{align}\\] (Note that \\(v^{T}\\cdot J\\) gives a row vector which can be treated as a column vector by taking \\(J^{T}\\cdot v\\) .) This characteristic of vector-Jacobian product makes it very convenient to feed external gradients into a model that has non-scalar output. I think the most crucial point to understand here is the difference between a torch.tensor and np.ndarray: While both objects are used to store n-dimensional matrices (aka \"Tensors\"), torch.tensors has an additional \"layer\" - which is storing the computational graph leading to the associated n-dimensional matrix. So, if you are only interested in efficient and easy way to perform mathematical operations on matrices np.ndarray or torch.tensor can be used interchangeably. However, torch.tensors are designed to be used in the context of gradient descent optimization, and therefore they hold not only a tensor with numeric values, but (and more importantly) the computational graph leading to these values. This computational graph is then used (using the chain rule of derivatives) to compute the derivative of the loss function w.r.t each of the independent variables used to compute the loss. As mentioned before, np.ndarray object does not have this extra \"computational graph\" layer and therefore, when converting a torch.tensor to np.ndarray you must explicitly remove the computational graph of the tensor using the detach() command. Computational Graph From your comments it seems like this concept is a bit vague. I'll try and illustrate it with a simple example. Consider a simple function of two (vector) variables, x and w: x = torch . rand ( 4 , requires_grad = True ) w = torch . rand ( 4 , requires_grad = True ) y = x @ w # inner-product of x and w z = y ** 2 # square the inner product z tensor(0.5170, grad_fn=<PowBackward0>) If we are only interested in the value of z, we need not worry about any graphs, we simply moving forward from the inputs, x and w, to compute y and then z. However, what would happen if we do not care so much about the value of z, but rather want to ask the question \"what is w that minimizes z for a given x\"? To answer that question, we need to compute the derivative of z w.r.t w. How can we do that? Using the chain rule we know that dz/dw = dz/dy * dy/dw. That is, to compute the gradient of z w.r.t w we need to move backward from z back to w computing the gradient of the operation at each step as we trace back our steps from z to w. This \"path\" we trace back is the computational graph of z and it tells us how to compute the derivative of z w.r.t the inputs leading to z: z . backward () # ask pytorch to trace back the computation of z We can now inspect the gradient of z w.r.t w: w . grad # the resulting gradient of z w.r.t w tensor([0.5055, 1.0019, 0.4770, 1.3650]) Note that this is exactly equals to 2 * y * x tensor([0.5055, 1.0019, 0.4770, 1.3650], grad_fn=<MulBackward0>) since dz/dy = 2*y and dy/dw = x. Each tensor along the path stores its \"contribution\" to the computation: z y tensor(0.5170, grad_fn=<PowBackward0>) tensor(0.7191, grad_fn=<DotBackward>) As you can see, y and z stores not only the \"forward\" value of or y**2 but also the computational graph -- the grad_fn that is needed to compute the derivatives (using the chain rule) when tracing back the gradients from z (output) to w (inputs). These grad_fn are essential components to torch.tensors and without them one cannot compute derivatives of complicated functions. However, np.ndarrays do not have this capability at all and they do not have this information.","title":"Understanding loss.backward() and Computational Graph"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/Deep_Learning_Notes_%28Important%29/#23-nov","text":"IMPORTANT, THIS IS ALL UNDER THE ASSUMPTION THAT shuffle=False in our DataLoader because if shuffle=True , then we will mess up the train_labels = subset_train_df['label'].values and the predictions since the predictions will be made on SHUFFLED IMAGES!!!!!!! But I managed to fixed it, see point 4 and 5. Take note that softmax_preds = torch.nn.Softmax(dim=1)(input=logits).to('cpu').detach().numpy() may give you an error called Pytorch: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead . Basically, when you are in training phase, where require_grad=True , you need to use tensor.detach().numpy() instead of tensor.numpy() . So it is only needed when you set model.train() mode, and for model.eval() mode I think no need. Refer to point 1, I rewrote it in a more suggestive manner as such: note that when you call torch.nn.Softmax(dim=1)(input=y_preds) you are essentially calling torch.nn.Softmax(dim=1)(input=y_preds).forward(input) . Refer to Source Code . So after getting the softmax predict for the batch size of 4 here, we append them to a list called train_preds=[] , and then we go out of the for loop after each epoch, and then call predictions = np.concatenate(train_preds) whereby you concatenate all the predictions into a 2d-np-array. After which, we get train_labels = subset_train_df['label'].values where these are the groud-truth labels. Subsequently, we compare these ground truth with the predicted values using the function score = get_score(train_labels, predictions.argmax(1)) where predictions.argmax(1) returns an array of indices which tells us in each prediction, which indice has the largest number. This nicely corresponds with our class because our class is from 0 - 4, note in the event if our class is 1-5, a quick fix is to make the class into 0-4. Made changes to the Dataset because I want to return image_id as well, but he called it p so I renamed it to img_id . Successfully solved the problem of shuffle = True in DataLoader . Moving on to valid_fn , we need to know that we need to write model.eval() model.eval() is a kind of switch for some specific layers/parts of the model that behave differently during training and inference (evaluating) time. For example, Dropouts Layers, BatchNorm Layers etc. You need to turn off them during model evaluation, and .eval() will do it for you. In addition, the common practice for evaluating/validation is using torch.no_grad() in pair with model.eval() to turn off gradients computation: # evaluate model: model . eval () with torch . no_grad (): ... out_data = model ( data ) ... BUT, don't forget to turn back to training mode after eval step: # training step ... model . train () ... import torch import torchvision import torchvision.transforms as transforms transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))]) trainset = torchvision . datasets . CIFAR10 ( root = './data' , train = True , download = True , transform = transform ) trainloader = torch . utils . data . DataLoader ( trainset , batch_size = 4 , shuffle = True , num_workers = 2 ) Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value=''))) Extracting ./data/cifar-10-python.tar.gz to ./data # testset = torchvision.datasets.CIFAR10(root='./data', train=False, # download=True, transform=transform) # testloader = torch.utils.data.DataLoader(testset, batch_size=4, # shuffle=False, num_workers=2) # classes = ('plane', 'car', 'bird', 'cat', # 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') import torch.nn as nn import torch.nn.functional as F class Net ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () self . conv1 = nn . Conv2d ( 3 , 6 , 5 ) self . pool = nn . MaxPool2d ( 2 , 2 ) self . conv2 = nn . Conv2d ( 6 , 16 , 5 ) self . fc1 = nn . Linear ( 16 * 5 * 5 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , x ): x = self . pool ( F . relu ( self . conv1 ( x ))) x = self . pool ( F . relu ( self . conv2 ( x ))) x = x . view ( - 1 , 16 * 5 * 5 ) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x net = Net () import torch.optim as optim criterion = nn . CrossEntropyLoss () optimizer = optim . SGD ( net . parameters (), lr = 0.001 , momentum = 0.9 ) for epoch in range ( 2 ): # loop over the dataset multiple times running_loss = 0.0 for i , data in enumerate ( trainloader , 0 ): # get the inputs; data is a list of [inputs, labels] inputs , labels = data # zero the parameter gradients optimizer . zero_grad () # forward + backward + optimize outputs = net ( inputs ) print ( ' \\n\\n The logits are \\n ' , outputs ) loss = criterion ( outputs , labels ) print ( ' \\n\\n The loss is \\n ' , loss ) print ( ' \\n\\n The loss avg is \\n ' , loss . item ()) # so basically loss == loss.item() break loss . backward () optimizer . step () # print statistics running_loss += loss . item () if i % 2000 == 1999 : # print every 2000 mini-batches print ( '[ %d , %5d ] loss: %.3f ' % ( epoch + 1 , i + 1 , running_loss / 2000 )) running_loss = 0.0 print ( 'Finished Training' ) The logits are tensor([[ 0.0878, 0.0786, -0.0898, 0.0612, -0.0639, -0.0529, -0.0973, 0.0149, -0.1152, -0.0745], [ 0.0868, 0.0975, -0.0728, 0.0374, -0.0802, -0.0677, -0.1017, 0.0022, -0.0963, -0.0782], [ 0.0780, 0.0960, -0.0788, 0.0655, -0.0611, -0.0639, -0.0985, 0.0128, -0.1240, -0.0759], [ 0.0604, 0.0839, -0.0712, 0.0642, -0.0808, -0.0629, -0.0866, 0.0055, -0.1157, -0.0830]], grad_fn=<AddmmBackward>) The loss is tensor(2.2469, grad_fn=<NllLossBackward>) The loss avg is 2.246931552886963 The logits are tensor([[ 0.0920, 0.0988, -0.0781, 0.0741, -0.0729, -0.0437, -0.0753, 0.0017, -0.0969, -0.0751], [ 0.0822, 0.0904, -0.0852, 0.0491, -0.0769, -0.0646, -0.0970, 0.0130, -0.1071, -0.0702], [ 0.0768, 0.0865, -0.0799, 0.0551, -0.0681, -0.0570, -0.0914, 0.0002, -0.1133, -0.0707], [ 0.0814, 0.0849, -0.0938, 0.0467, -0.0732, -0.0569, -0.0966, 0.0104, -0.1145, -0.0769]], grad_fn=<AddmmBackward>) The loss is tensor(2.2669, grad_fn=<NllLossBackward>) The loss avg is 2.266934871673584 Finished Training from __future__ import print_function , division import torch import torch.nn as nn import torch.optim as optim from torch.optim import lr_scheduler import numpy as np import torchvision from torchvision import datasets , models , transforms import matplotlib.pyplot as plt import time import os import copy # Data augmentation and normalization for training # Just normalization for validation data_transforms = { 'train' : transforms . Compose ([ transforms . RandomResizedCrop ( 224 ), transforms . RandomHorizontalFlip (), transforms . ToTensor (), transforms . Normalize ([ 0.485 , 0.456 , 0.406 ], [ 0.229 , 0.224 , 0.225 ]) ]), 'val' : transforms . Compose ([ transforms . Resize ( 256 ), transforms . CenterCrop ( 224 ), transforms . ToTensor (), transforms . Normalize ([ 0.485 , 0.456 , 0.406 ], [ 0.229 , 0.224 , 0.225 ]) ]), } data_dir = './data/hymenoptera_data' image_datasets = { x : datasets . ImageFolder ( os . path . join ( data_dir , x ), data_transforms [ x ]) for x in [ 'train' , 'val' ]} dataloaders = { x : torch . utils . data . DataLoader ( image_datasets [ x ], batch_size = 4 , shuffle = True , num_workers = 4 ) for x in [ 'train' , 'val' ]} dataset_sizes = { x : len ( image_datasets [ x ]) for x in [ 'train' , 'val' ]} class_names = image_datasets [ 'train' ] . classes device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) --------------------------------------------------------------------------- FileNotFoundError Traceback (most recent call last) <ipython-input-24-1f4b70a4f27c> in <module> 19 image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), 20 data_transforms[x]) ---> 21 for x in ['train', 'val']} 22 dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, 23 shuffle=True, num_workers=4) <ipython-input-24-1f4b70a4f27c> in <dictcomp>(.0) 19 image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), 20 data_transforms[x]) ---> 21 for x in ['train', 'val']} 22 dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, 23 shuffle=True, num_workers=4) /opt/conda/lib/python3.7/site-packages/torchvision/datasets/folder.py in __init__(self, root, transform, target_transform, loader, is_valid_file) 227 transform=transform, 228 target_transform=target_transform, --> 229 is_valid_file=is_valid_file) 230 self.imgs = self.samples /opt/conda/lib/python3.7/site-packages/torchvision/datasets/folder.py in __init__(self, root, loader, extensions, transform, target_transform, is_valid_file) 106 super(DatasetFolder, self).__init__(root, transform=transform, 107 target_transform=target_transform) --> 108 classes, class_to_idx = self._find_classes(self.root) 109 samples = make_dataset(self.root, class_to_idx, extensions, is_valid_file) 110 if len(samples) == 0: /opt/conda/lib/python3.7/site-packages/torchvision/datasets/folder.py in _find_classes(self, dir) 135 No class is a subdirectory of another. 136 \"\"\" --> 137 classes = [d.name for d in os.scandir(dir) if d.is_dir()] 138 classes.sort() 139 class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)} FileNotFoundError: [Errno 2] No such file or directory: './data/hymenoptera_data/train' print ( 'The number of images in the training set is' , dataset_sizes [ 'train' ]) --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-25-281ff82a30a3> in <module> ----> 1 print('The number of images in the training set is', dataset_sizes['train']) NameError: name 'dataset_sizes' is not defined def train_model ( model , criterion , optimizer , scheduler , num_epochs = 25 ): since = time . time () best_model_wts = copy . deepcopy ( model . state_dict ()) best_acc = 0.0 for epoch in range ( num_epochs ): print ( 'Epoch {} / {} ' . format ( epoch , num_epochs - 1 )) print ( '-' * 10 ) # Each epoch has a training and validation phase for phase in [ 'train' , 'val' ]: if phase == 'train' : model . train () # Set model to training mode else : model . eval () # Set model to evaluate mode running_loss = 0.0 running_corrects = 0 # Iterate over data. for inputs , labels in dataloaders [ phase ]: inputs = inputs . to ( device ) labels = labels . to ( device ) # zero the parameter gradients optimizer . zero_grad () # forward # track history if only in train with torch . set_grad_enabled ( phase == 'train' ): outputs = model ( inputs ) _ , preds = torch . max ( outputs , 1 ) loss = criterion ( outputs , labels ) # backward + optimize only if in training phase if phase == 'train' : loss . backward () optimizer . step () # statistics print ( inputs . size ( 0 )) running_loss += loss . item () * inputs . size ( 0 ) running_corrects += torch . sum ( preds == labels . data ) if phase == 'train' : scheduler . step () epoch_loss = running_loss / dataset_sizes [ phase ] epoch_acc = running_corrects . double () / dataset_sizes [ phase ] print ( ' {} Loss: {:.4f} Acc: {:.4f} ' . format ( phase , epoch_loss , epoch_acc )) # deep copy the model if phase == 'val' and epoch_acc > best_acc : best_acc = epoch_acc best_model_wts = copy . deepcopy ( model . state_dict ()) print () time_elapsed = time . time () - since print ( 'Training complete in {:.0f} m {:.0f} s' . format ( time_elapsed // 60 , time_elapsed % 60 )) print ( 'Best val Acc: {:4f} ' . format ( best_acc )) # load best model weights model . load_state_dict ( best_model_wts ) return model model_ft = models . resnet18 ( pretrained = True ) num_ftrs = model_ft . fc . in_features # Here the size of each output sample is set to 2. # Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)). model_ft . fc = nn . Linear ( num_ftrs , 2 ) model_ft = model_ft . to ( device ) criterion = nn . CrossEntropyLoss () # Observe that all parameters are being optimized optimizer_ft = optim . SGD ( model_ft . parameters (), lr = 0.001 , momentum = 0.9 ) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler . StepLR ( optimizer_ft , step_size = 7 , gamma = 0.1 ) Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value=''))) model_ft = train_model ( model_ft , criterion , optimizer_ft , exp_lr_scheduler , num_epochs = 25 ) Epoch 0/24 ---------- --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-28-cc88ea5f8bd3> in <module> 1 model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, ----> 2 num_epochs=25) <ipython-input-26-9af9c454ae56> in train_model(model, criterion, optimizer, scheduler, num_epochs) 20 21 # Iterate over data. ---> 22 for inputs, labels in dataloaders[phase]: 23 inputs = inputs.to(device) 24 labels = labels.to(device) NameError: name 'dataloaders' is not defined","title":"23 Nov"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/PyTorch%20Utilities/","text":"Imports, Config and Seeding import timm import torch import torchvision from typing import Dict , Union , Callable , OrderedDict , Tuple import os , random import numpy as np import torch.nn as nn def seed_all ( seed : int = 1992 ) -> None : \"\"\"Seed all random number generators.\"\"\" print ( f \"Using Seed Number { seed } \" ) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) # set PYTHONHASHSEED env var at fixed value torch . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) torch . cuda . manual_seed ( seed ) # pytorch (both CPU and CUDA) np . random . seed ( seed ) # for numpy pseudo-random generator # set fixed value for python built-in pseudo-random generator random . seed ( seed ) torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = True torch . backends . cudnn . enabled = True def seed_worker ( _worker_id ) -> None : \"\"\"Seed a worker with the given ID.\"\"\" worker_seed = torch . initial_seed () % 2 ** 32 np . random . seed ( worker_seed ) random . seed ( worker_seed ) seed_all ( seed = 1992 ) Using Seed Number 1992 DEVICE = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) Convolutional Neural Networks Terminologies Kernel Filter Receptive Field X = torch . tensor ([[ 0.0 , 1.0 , 2.0 ], [ 3.0 , 4.0 , 5.0 ], [ 6.0 , 7.0 , 8.0 ]]) K = torch . tensor ([[ 0.0 , 1.0 ], [ 2.0 , 3.0 ]]) def my_conv2d ( x : torch . Tensor , kernel : torch . Tensor ) -> torch . Tensor : kernel_height , kernel_width = kernel . shape input_height , input_width = x . shape feature_map_height , feature_map_width = ( input_height - kernel_height + 1 , input_width - kernel_width + 1 , ) feature_map = torch . zeros ( size = ( feature_map_height , feature_map_width )) for height_index in range ( feature_map_height ): for width_index in range ( feature_map_width ): # 1st iter: height_index = 0, width_index = 0 # 2nd iter: height_index = 0, width_index = 1 receptive_field = x [ height_index : height_index + kernel_height , width_index : width_index + kernel_width , ] feature_map [ height_index , width_index ] = ( receptive_field * kernel ) . sum () return feature_map my_conv2d ( X , K ) tensor([[19., 25.], [37., 43.]]) class MyConv2D ( nn . Module ): def __init__ ( self , kernel_size : Tuple [ int , int ]): super () . __init__ () self . kernel = nn . Parameter ( torch . rand ( size = ( kernel_size [ 0 ], kernel_size [ 1 ])) ) self . bias = nn . Parameter ( torch . zeros ( 1 )) def forward ( self , x ): return my_conv2d ( x , self . kernel ) + self . bias conv2d = MyConv2D (( 2 , 2 )) conv2d ( X ) tensor([[ 7.3671, 10.2563], [16.0346, 18.9238]], grad_fn=<AddBackward0>) CNN Conv2d Layer Output Dimensions Calculation Given: n: Input image's height/width f: Filter/Kernel Size p: Padding Size s: Stride Given an image of size \\(n \\times n\\) and a kernel of \\(f \\times f\\) , our output shape is \\[ o = n - f + 1 \\] and if we pad the image with 1 extra layer outside, that means our input image is of size \\((n + 2*1) \\times (n + 2*1)\\) , if you see an image you will be clear why adding one layer around means input width and height add 2 times the padding \\(p\\) . hence our new output shape is: \\[ o = (n+2) - f + 1 \\] and a generic formula for padding equals \\(p\\) will yield \\[ o = (n + 2p) - f + 1 \\] Note that: If \\(p=0\\) , then this is valid padding where the output shape is \\(n - f + 1\\) ; If \\(p=\\frac{f-1}{2}\\) , then this is same padding where the output shape is equals to the original input shape \\(n\\) . Same padding can be deduced by setting \\[ (n+2p) - f + 1 = n \\implies p = \\frac{f-1}{2} \\] where by construction \\(f\\) must be odd in order to get a whole number for the padding \\(p\\) , and that's why most kernels/filters are of odd shape. With stride into action our final shape is: \\[ o = \\dfrac{n - f + 2p}{s} + 1 \\] where sometimes \\(o\\) is applied by \\(\\text{floor}(o)\\) if the \\(o\\) is non-integer. CNN Pooling Layer Output Dimensions Calculation For general pooling operations: Given: n: Input image's height/width f: Filter/Kernel Size s: Stride Given an image of size \\(n \\times n\\) and a kernel of \\(f \\times f\\) , our output shape after pooling is: \\[ o = \\text{floor}\\left(\\dfrac{n - f}{s} + 1\\right) \\] with floor applied to \\(o\\) . LeNet import torch from torch import nn lenet = nn . Sequential ( nn . Conv2d ( in_channels = 1 , out_channels = 6 , kernel_size = 5 , padding = 2 , stride = 1 ), nn . Sigmoid (), nn . AvgPool2d ( kernel_size = 2 , stride = 2 ), nn . Conv2d ( in_channels = 6 , out_channels = 16 , kernel_size = 5 , padding = 0 , stride = 1 ), nn . Sigmoid (), nn . AvgPool2d ( kernel_size = 2 , stride = 2 ), nn . Flatten (), nn . Linear ( 16 * 5 * 5 , 120 ), nn . Sigmoid (), nn . Linear ( 120 , 84 ), nn . Sigmoid (), nn . Linear ( 84 , 10 ), ) X: (1, 1, 28, 28) 1st conv2d layer: a conv2d layer that applies a filter containing 6 kernels, where each kernel is of size \\(5 \\times 5\\) with a padding of 2 and stride of 1 n = 28 f = 5 p = 2 ; Note in particular the padding of 2 is derived from the formula \\(p = (f-1)/2 = 4/2=2\\) to get same padding! s = 1 shape: \\(o = \\frac{28-5+4}{1} + 1 = 28\\) The final output shape is (1, 6, 28, 28) where 1 is the batch size 6 is the number of kernels applied, for each kernel our output shape is 28 by 28 28, 28 is the output shape by the kernels 1st avgpool2d layer: n = 28 f = 2 s = 2 o = 14 The final output shape is (1, 6, 14, 14) where the kernels are halved in size. 2nd conv2d layer: n = 14 f = 5 p = 0 s = 1 o = 10 The final output shape is (1, 16, 10, 10) X = torch . rand ( size = ( 1 , 1 , 28 , 28 ), dtype = torch . float32 ) for layer in lenet : X = layer ( X ) print ( layer . __class__ . __name__ , \"output shape: \\t \" , X . shape ) Conv2d output shape: torch.Size([1, 6, 28, 28]) Sigmoid output shape: torch.Size([1, 6, 28, 28]) AvgPool2d output shape: torch.Size([1, 6, 14, 14]) Conv2d output shape: torch.Size([1, 16, 10, 10]) Sigmoid output shape: torch.Size([1, 16, 10, 10]) AvgPool2d output shape: torch.Size([1, 16, 5, 5]) Flatten output shape: torch.Size([1, 400]) Linear output shape: torch.Size([1, 120]) Sigmoid output shape: torch.Size([1, 120]) Linear output shape: torch.Size([1, 84]) Sigmoid output shape: torch.Size([1, 84]) Linear output shape: torch.Size([1, 10]) X = torch . rand ( size = ( 1 , 1 , 28 , 28 ), dtype = torch . float32 ) for layer in lenet : X = layer ( X ) if hasattr ( layer , \"weight\" ): print ( layer , \"layer weight shape: \\t \" , layer . weight . shape ) Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) layer weight shape: torch.Size([6, 1, 5, 5]) Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) layer weight shape: torch.Size([16, 6, 5, 5]) Linear(in_features=400, out_features=120, bias=True) layer weight shape: torch.Size([120, 400]) Linear(in_features=120, out_features=84, bias=True) layer weight shape: torch.Size([84, 120]) Linear(in_features=84, out_features=10, bias=True) layer weight shape: torch.Size([10, 84]) Toy Models I created two versions of the same model. The Sequential method has a more compact form, but often is more difficult to extract layers. class ToyModel ( torch . nn . Module ): def __init__ ( self ): super () . __init__ () self . cl1 = torch . nn . Linear ( 25 , 60 ) self . cl2 = torch . nn . Linear ( 60 , 16 ) self . fc1 = torch . nn . Linear ( 16 , 120 ) self . fc2 = torch . nn . Linear ( 120 , 84 ) self . fc3 = torch . nn . Linear ( 84 , 10 ) def forward ( self , x ): \"\"\"Forward pass of the model. Args: x ([type]): [description] Returns: [type]: [description] \"\"\" x = torch . nn . ReLU ()( self . cl1 ( x )) x = torch . nn . ReLU ()( self . cl2 ( x )) x = torch . nn . ReLU ()( self . fc1 ( x )) x = torch . nn . ReLU ()( self . fc2 ( x )) x = torch . nn . LogSoftmax ( dim = 1 )( self . fc3 ( x )) return x class ToySequentialModel ( torch . nn . Module ): # Create a sequential model pytorch same as ToyModel. def __init__ ( self ) -> None : super () . __init__ () self . backbone = torch . nn . Sequential ( OrderedDict ( [ ( \"cl1\" , torch . nn . Linear ( 25 , 60 )), ( \"cl_relu1\" , torch . nn . ReLU ()), ( \"cl2\" , torch . nn . Linear ( 60 , 16 )), ( \"cl_relu2\" , torch . nn . ReLU ()), ] ) ) self . head = torch . nn . Sequential ( OrderedDict ( [ ( \"fc1\" , torch . nn . Linear ( 16 , 120 )), ( \"fc_relu_1\" , torch . nn . ReLU ()), ( \"fc2\" , torch . nn . Linear ( 120 , 84 )), ( \"fc_relu_2\" , torch . nn . ReLU ()), ( \"fc3\" , torch . nn . Linear ( 84 , 10 )), ( \"fc_log_softmax\" , torch . nn . LogSoftmax ( dim = 1 )), ] ) ) def forward ( self , x ): \"\"\"Forward pass of the model. Args: x ([type]): [description] Returns: [type]: [description] \"\"\" x = self . backbone ( x ) x = self . head ( x ) return x Named Modules Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. for name , layer in ToySequentialModel () . named_modules (): print ( name ) backbone backbone.cl1 backbone.cl_relu1 backbone.cl2 backbone.cl_relu2 head head.fc1 head.fc_relu_1 head.fc2 head.fc_relu_2 head.fc3 head.fc_log_softmax Get Convolutional Layers def get_conv_layers ( model : Callable , layer_type : str = \"Conv2d\" ) -> Dict [ str , str ]: \"\"\"Create a function that give me the convolutional layers of PyTorch model. This function is created to be used in conjunction with Visualization of Feature Maps. Args: model (Union[torchvision.models, timm.models]): A PyTorch model. layer_type (str): The type of layer to be extracted. Returns: conv_layers (Dict[str, str]): {\"layer1.0.conv1\": layer1.0.conv1, ...} Example: >>> resnet18_pretrained_true = timm.create_model(model_name = \"resnet34\", pretrained=True, num_classes=10).to(DEVICE) >>> conv_layers = get_conv_layers(resnet18_pretrained_true, layer_type=\"Conv2d\") \"\"\" if layer_type == \"Conv2d\" : _layer_type = torch . nn . Conv2d elif layer_type == \"Conv1d\" : _layer_type = torch . nn . Conv1d conv_layers = {} for name , layer in model . named_modules (): if isinstance ( layer , _layer_type ): conv_layers [ name ] = name return conv_layers resnet18_pretrained_true = timm . create_model ( model_name = \"resnet34\" , pretrained = True , num_classes = 10 ) . to ( DEVICE ) >>> resnet18_pretrained_true = timm . create_model ( model_name = \"resnet34\" , pretrained = True , num_classes = 10 ) . to ( DEVICE ) >>> conv_layers = get_conv_layers ( resnet18_pretrained_true , layer_type = \"Conv2d\" ) >>> print ( conv_layers ) {'conv1': 'conv1', 'layer1.0.conv1': 'layer1.0.conv1', 'layer1.0.conv2': 'layer1.0.conv2', 'layer1.1.conv1': 'layer1.1.conv1', 'layer1.1.conv2': 'layer1.1.conv2', 'layer1.2.conv1': 'layer1.2.conv1', 'layer1.2.conv2': 'layer1.2.conv2', 'layer2.0.conv1': 'layer2.0.conv1', 'layer2.0.conv2': 'layer2.0.conv2', 'layer2.0.downsample.0': 'layer2.0.downsample.0', 'layer2.1.conv1': 'layer2.1.conv1', 'layer2.1.conv2': 'layer2.1.conv2', 'layer2.2.conv1': 'layer2.2.conv1', 'layer2.2.conv2': 'layer2.2.conv2', 'layer2.3.conv1': 'layer2.3.conv1', 'layer2.3.conv2': 'layer2.3.conv2', 'layer3.0.conv1': 'layer3.0.conv1', 'layer3.0.conv2': 'layer3.0.conv2', 'layer3.0.downsample.0': 'layer3.0.downsample.0', 'layer3.1.conv1': 'layer3.1.conv1', 'layer3.1.conv2': 'layer3.1.conv2', 'layer3.2.conv1': 'layer3.2.conv1', 'layer3.2.conv2': 'layer3.2.conv2', 'layer3.3.conv1': 'layer3.3.conv1', 'layer3.3.conv2': 'layer3.3.conv2', 'layer3.4.conv1': 'layer3.4.conv1', 'layer3.4.conv2': 'layer3.4.conv2', 'layer3.5.conv1': 'layer3.5.conv1', 'layer3.5.conv2': 'layer3.5.conv2', 'layer4.0.conv1': 'layer4.0.conv1', 'layer4.0.conv2': 'layer4.0.conv2', 'layer4.0.downsample.0': 'layer4.0.downsample.0', 'layer4.1.conv1': 'layer4.1.conv1', 'layer4.1.conv2': 'layer4.1.conv2', 'layer4.2.conv1': 'layer4.2.conv1', 'layer4.2.conv2': 'layer4.2.conv2'} activation = {} def get_intermediate_features ( name : str ) -> Callable : \"\"\"Get the intermediate features of a model. Forward Hook. This is using forward hook with reference https://discuss.pytorch.org/t/how-can-l-load-my-best-model-as-a-feature-extractor-evaluator/17254/5 Args: name (str): name of the layer. Returns: Callable: [description] \"\"\" def hook ( model , input , output ): activation [ name ] = output . detach () return hook # The below is testing the forward hook functionalities, especially getting intermediate features. # Note that both models are same organically but created differently. # Due to seeding issues, you can check whether they are the same output or not by running them separately. # We also used assertion to check that the output from model(x) is same as torch.nn.LogSoftmax(dim=1)(fc3_output) use_sequential_model = True x = torch . randn ( 1 , 25 ) if not use_sequential_model : model = ToyModel () model . fc2 . register_forward_hook ( get_intermediate_features ( \"fc2\" )) model . fc3 . register_forward_hook ( get_intermediate_features ( \"fc3\" )) output = model ( x ) print ( activation ) fc2_output = activation [ \"fc2\" ] fc3_output = activation [ \"fc3\" ] # assert output and logsoftmax fc3_output are the same assert torch . allclose ( output , torch . nn . LogSoftmax ( dim = 1 )( fc3_output )) else : sequential_model = ToySequentialModel () # Do this if you want all, if not you can see below. # for name, layer in sequential_model.named_modules(): # layer.register_forward_hook(get_intermediate_features(name)) sequential_model . head . fc2 . register_forward_hook ( get_intermediate_features ( \"head.fc2\" ) ) sequential_model . head . fc3 . register_forward_hook ( get_intermediate_features ( \"head.fc3\" ) ) sequential_model_output = sequential_model ( x ) print ( activation ) fc2_output = activation [ \"head.fc2\" ] fc3_output = activation [ \"head.fc3\" ] assert torch . allclose ( sequential_model_output , torch . nn . LogSoftmax ( dim = 1 )( fc3_output ) ) {'head.fc2': tensor([[ 0.0697, 0.0544, -0.0157, -0.1059, -0.0464, -0.0090, 0.0532, -0.1273, -0.0286, -0.0151, 0.0963, 0.2205, 0.0745, -0.0110, -0.1127, -0.0367, -0.0681, 0.0463, -0.0833, 0.1288, 0.1058, 0.0976, -0.0251, 0.0980, -0.0110, 0.1170, -0.0650, 0.2091, -0.1773, 0.0363, -0.1452, 0.0036, 0.0112, -0.0304, -0.0620, -0.0658, -0.0543, 0.0072, 0.0436, 0.0703, 0.0254, -0.0614, 0.0164, -0.1003, -0.0396, 0.0349, 0.0089, -0.1243, -0.1037, -0.0491, 0.0627, -0.1347, 0.0010, -0.1290, -0.0280, -0.0344, 0.1487, -0.1764, -0.0233, 0.0082, 0.1270, 0.0368, 0.0103, -0.0929, 0.0038, 0.1346, -0.0688, -0.0437, -0.1205, -0.1596, -0.0240, -0.1001, -0.0300, -0.1119, 0.0344, -0.1587, 0.0329, -0.0424, 0.0999, 0.0732, 0.1116, 0.0220, -0.0570, 0.0232]]), 'head.fc3': tensor([[ 0.0256, -0.0924, 0.0456, 0.0972, 0.0107, 0.0527, 0.0208, 0.0373, 0.0451, 0.0712]])} How to freeze layers # resnet18_pretrained_true = timm.create_model(model_name = \"resnet34\", pretrained=True, num_classes=10).to(DEVICE) norm = torch . nn . InstanceNorm2d ( num_features = 3 , track_running_stats = True ) print ( norm . running_mean , norm . running_var ) tensor([0., 0., 0.]) tensor([1., 1., 1.]) x = torch . randn ( 2 , 3 , 24 , 24 ) out = norm ( x ) print ( norm . running_mean , norm . running_var ) out = norm ( x ) print ( norm . running_mean , norm . running_var ) out = norm ( x ) print ( norm . running_mean , norm . running_var ) tensor([-1.3414e-03, -4.7338e-05, 1.1239e-03]) tensor([1.0010, 0.9984, 0.9989]) tensor([-2.5486e-03, -8.9943e-05, 2.1355e-03]) tensor([1.0018, 0.9969, 0.9979]) tensor([-0.0036, -0.0001, 0.0030]) tensor([1.0026, 0.9956, 0.9970]) norm . eval () out = norm ( x ) print ( norm . running_mean , norm . running_var ) tensor([-0.0160, -0.0018, 0.0068]) tensor([1.0002, 1.0082, 0.9904]) def freeze_batchnorm_layers ( model : Callable ) -> None : \"\"\"Freeze the batchnorm layers of a PyTorch model. Args: model (CustomNeuralNet): model to be frozen. Example: >>> model = timm.create_model(\"efficientnet_b0\", pretrained=True) >>> model.apply(freeze_batchnorm_layers) # to freeze during training \"\"\" # https://discuss.pytorch.org/t/how-to-freeze-bn-layers-while-training-the-rest-of-network-mean-and-var-wont-freeze/89736/19 # https://discuss.pytorch.org/t/should-i-use-model-eval-when-i-freeze-batchnorm-layers-to-finetune/39495/3 classname = model . __class__ . __name__ for module in model . modules (): if isinstance ( module , torch . nn . InstanceNorm2d ): module . eval () if isinstance ( module , torch . nn . BatchNorm2d ): if hasattr ( module , \"weight\" ): module . weight . requires_grad_ ( False ) if hasattr ( module , \"bias\" ): module . bias . requires_grad_ ( False ) module . eval () norm . apply ( freeze_batchnorm_layers ) InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True) out = norm ( x ) norm . running_mean , norm . running_var (tensor([-0.0036, -0.0001, 0.0030]), tensor([1.0026, 0.9956, 0.9970]))","title":"PyTorch Utilities"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/PyTorch%20Utilities/#imports-config-and-seeding","text":"import timm import torch import torchvision from typing import Dict , Union , Callable , OrderedDict , Tuple import os , random import numpy as np import torch.nn as nn def seed_all ( seed : int = 1992 ) -> None : \"\"\"Seed all random number generators.\"\"\" print ( f \"Using Seed Number { seed } \" ) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) # set PYTHONHASHSEED env var at fixed value torch . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) torch . cuda . manual_seed ( seed ) # pytorch (both CPU and CUDA) np . random . seed ( seed ) # for numpy pseudo-random generator # set fixed value for python built-in pseudo-random generator random . seed ( seed ) torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = True torch . backends . cudnn . enabled = True def seed_worker ( _worker_id ) -> None : \"\"\"Seed a worker with the given ID.\"\"\" worker_seed = torch . initial_seed () % 2 ** 32 np . random . seed ( worker_seed ) random . seed ( worker_seed ) seed_all ( seed = 1992 ) Using Seed Number 1992 DEVICE = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" )","title":"Imports, Config and Seeding"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/PyTorch%20Utilities/#convolutional-neural-networks","text":"","title":"Convolutional Neural Networks"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/PyTorch%20Utilities/#terminologies","text":"Kernel Filter Receptive Field X = torch . tensor ([[ 0.0 , 1.0 , 2.0 ], [ 3.0 , 4.0 , 5.0 ], [ 6.0 , 7.0 , 8.0 ]]) K = torch . tensor ([[ 0.0 , 1.0 ], [ 2.0 , 3.0 ]]) def my_conv2d ( x : torch . Tensor , kernel : torch . Tensor ) -> torch . Tensor : kernel_height , kernel_width = kernel . shape input_height , input_width = x . shape feature_map_height , feature_map_width = ( input_height - kernel_height + 1 , input_width - kernel_width + 1 , ) feature_map = torch . zeros ( size = ( feature_map_height , feature_map_width )) for height_index in range ( feature_map_height ): for width_index in range ( feature_map_width ): # 1st iter: height_index = 0, width_index = 0 # 2nd iter: height_index = 0, width_index = 1 receptive_field = x [ height_index : height_index + kernel_height , width_index : width_index + kernel_width , ] feature_map [ height_index , width_index ] = ( receptive_field * kernel ) . sum () return feature_map my_conv2d ( X , K ) tensor([[19., 25.], [37., 43.]]) class MyConv2D ( nn . Module ): def __init__ ( self , kernel_size : Tuple [ int , int ]): super () . __init__ () self . kernel = nn . Parameter ( torch . rand ( size = ( kernel_size [ 0 ], kernel_size [ 1 ])) ) self . bias = nn . Parameter ( torch . zeros ( 1 )) def forward ( self , x ): return my_conv2d ( x , self . kernel ) + self . bias conv2d = MyConv2D (( 2 , 2 )) conv2d ( X ) tensor([[ 7.3671, 10.2563], [16.0346, 18.9238]], grad_fn=<AddBackward0>)","title":"Terminologies"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/PyTorch%20Utilities/#cnn-conv2d-layer-output-dimensions-calculation","text":"Given: n: Input image's height/width f: Filter/Kernel Size p: Padding Size s: Stride Given an image of size \\(n \\times n\\) and a kernel of \\(f \\times f\\) , our output shape is \\[ o = n - f + 1 \\] and if we pad the image with 1 extra layer outside, that means our input image is of size \\((n + 2*1) \\times (n + 2*1)\\) , if you see an image you will be clear why adding one layer around means input width and height add 2 times the padding \\(p\\) . hence our new output shape is: \\[ o = (n+2) - f + 1 \\] and a generic formula for padding equals \\(p\\) will yield \\[ o = (n + 2p) - f + 1 \\] Note that: If \\(p=0\\) , then this is valid padding where the output shape is \\(n - f + 1\\) ; If \\(p=\\frac{f-1}{2}\\) , then this is same padding where the output shape is equals to the original input shape \\(n\\) . Same padding can be deduced by setting \\[ (n+2p) - f + 1 = n \\implies p = \\frac{f-1}{2} \\] where by construction \\(f\\) must be odd in order to get a whole number for the padding \\(p\\) , and that's why most kernels/filters are of odd shape. With stride into action our final shape is: \\[ o = \\dfrac{n - f + 2p}{s} + 1 \\] where sometimes \\(o\\) is applied by \\(\\text{floor}(o)\\) if the \\(o\\) is non-integer.","title":"CNN Conv2d Layer Output Dimensions Calculation"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/PyTorch%20Utilities/#cnn-pooling-layer-output-dimensions-calculation","text":"For general pooling operations: Given: n: Input image's height/width f: Filter/Kernel Size s: Stride Given an image of size \\(n \\times n\\) and a kernel of \\(f \\times f\\) , our output shape after pooling is: \\[ o = \\text{floor}\\left(\\dfrac{n - f}{s} + 1\\right) \\] with floor applied to \\(o\\) .","title":"CNN Pooling Layer Output Dimensions Calculation"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/PyTorch%20Utilities/#lenet","text":"import torch from torch import nn lenet = nn . Sequential ( nn . Conv2d ( in_channels = 1 , out_channels = 6 , kernel_size = 5 , padding = 2 , stride = 1 ), nn . Sigmoid (), nn . AvgPool2d ( kernel_size = 2 , stride = 2 ), nn . Conv2d ( in_channels = 6 , out_channels = 16 , kernel_size = 5 , padding = 0 , stride = 1 ), nn . Sigmoid (), nn . AvgPool2d ( kernel_size = 2 , stride = 2 ), nn . Flatten (), nn . Linear ( 16 * 5 * 5 , 120 ), nn . Sigmoid (), nn . Linear ( 120 , 84 ), nn . Sigmoid (), nn . Linear ( 84 , 10 ), ) X: (1, 1, 28, 28) 1st conv2d layer: a conv2d layer that applies a filter containing 6 kernels, where each kernel is of size \\(5 \\times 5\\) with a padding of 2 and stride of 1 n = 28 f = 5 p = 2 ; Note in particular the padding of 2 is derived from the formula \\(p = (f-1)/2 = 4/2=2\\) to get same padding! s = 1 shape: \\(o = \\frac{28-5+4}{1} + 1 = 28\\) The final output shape is (1, 6, 28, 28) where 1 is the batch size 6 is the number of kernels applied, for each kernel our output shape is 28 by 28 28, 28 is the output shape by the kernels 1st avgpool2d layer: n = 28 f = 2 s = 2 o = 14 The final output shape is (1, 6, 14, 14) where the kernels are halved in size. 2nd conv2d layer: n = 14 f = 5 p = 0 s = 1 o = 10 The final output shape is (1, 16, 10, 10) X = torch . rand ( size = ( 1 , 1 , 28 , 28 ), dtype = torch . float32 ) for layer in lenet : X = layer ( X ) print ( layer . __class__ . __name__ , \"output shape: \\t \" , X . shape ) Conv2d output shape: torch.Size([1, 6, 28, 28]) Sigmoid output shape: torch.Size([1, 6, 28, 28]) AvgPool2d output shape: torch.Size([1, 6, 14, 14]) Conv2d output shape: torch.Size([1, 16, 10, 10]) Sigmoid output shape: torch.Size([1, 16, 10, 10]) AvgPool2d output shape: torch.Size([1, 16, 5, 5]) Flatten output shape: torch.Size([1, 400]) Linear output shape: torch.Size([1, 120]) Sigmoid output shape: torch.Size([1, 120]) Linear output shape: torch.Size([1, 84]) Sigmoid output shape: torch.Size([1, 84]) Linear output shape: torch.Size([1, 10]) X = torch . rand ( size = ( 1 , 1 , 28 , 28 ), dtype = torch . float32 ) for layer in lenet : X = layer ( X ) if hasattr ( layer , \"weight\" ): print ( layer , \"layer weight shape: \\t \" , layer . weight . shape ) Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) layer weight shape: torch.Size([6, 1, 5, 5]) Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) layer weight shape: torch.Size([16, 6, 5, 5]) Linear(in_features=400, out_features=120, bias=True) layer weight shape: torch.Size([120, 400]) Linear(in_features=120, out_features=84, bias=True) layer weight shape: torch.Size([84, 120]) Linear(in_features=84, out_features=10, bias=True) layer weight shape: torch.Size([10, 84])","title":"LeNet"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/PyTorch%20Utilities/#toy-models","text":"I created two versions of the same model. The Sequential method has a more compact form, but often is more difficult to extract layers. class ToyModel ( torch . nn . Module ): def __init__ ( self ): super () . __init__ () self . cl1 = torch . nn . Linear ( 25 , 60 ) self . cl2 = torch . nn . Linear ( 60 , 16 ) self . fc1 = torch . nn . Linear ( 16 , 120 ) self . fc2 = torch . nn . Linear ( 120 , 84 ) self . fc3 = torch . nn . Linear ( 84 , 10 ) def forward ( self , x ): \"\"\"Forward pass of the model. Args: x ([type]): [description] Returns: [type]: [description] \"\"\" x = torch . nn . ReLU ()( self . cl1 ( x )) x = torch . nn . ReLU ()( self . cl2 ( x )) x = torch . nn . ReLU ()( self . fc1 ( x )) x = torch . nn . ReLU ()( self . fc2 ( x )) x = torch . nn . LogSoftmax ( dim = 1 )( self . fc3 ( x )) return x class ToySequentialModel ( torch . nn . Module ): # Create a sequential model pytorch same as ToyModel. def __init__ ( self ) -> None : super () . __init__ () self . backbone = torch . nn . Sequential ( OrderedDict ( [ ( \"cl1\" , torch . nn . Linear ( 25 , 60 )), ( \"cl_relu1\" , torch . nn . ReLU ()), ( \"cl2\" , torch . nn . Linear ( 60 , 16 )), ( \"cl_relu2\" , torch . nn . ReLU ()), ] ) ) self . head = torch . nn . Sequential ( OrderedDict ( [ ( \"fc1\" , torch . nn . Linear ( 16 , 120 )), ( \"fc_relu_1\" , torch . nn . ReLU ()), ( \"fc2\" , torch . nn . Linear ( 120 , 84 )), ( \"fc_relu_2\" , torch . nn . ReLU ()), ( \"fc3\" , torch . nn . Linear ( 84 , 10 )), ( \"fc_log_softmax\" , torch . nn . LogSoftmax ( dim = 1 )), ] ) ) def forward ( self , x ): \"\"\"Forward pass of the model. Args: x ([type]): [description] Returns: [type]: [description] \"\"\" x = self . backbone ( x ) x = self . head ( x ) return x","title":"Toy Models"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/PyTorch%20Utilities/#named-modules","text":"Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. for name , layer in ToySequentialModel () . named_modules (): print ( name ) backbone backbone.cl1 backbone.cl_relu1 backbone.cl2 backbone.cl_relu2 head head.fc1 head.fc_relu_1 head.fc2 head.fc_relu_2 head.fc3 head.fc_log_softmax","title":"Named Modules"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/PyTorch%20Utilities/#get-convolutional-layers","text":"def get_conv_layers ( model : Callable , layer_type : str = \"Conv2d\" ) -> Dict [ str , str ]: \"\"\"Create a function that give me the convolutional layers of PyTorch model. This function is created to be used in conjunction with Visualization of Feature Maps. Args: model (Union[torchvision.models, timm.models]): A PyTorch model. layer_type (str): The type of layer to be extracted. Returns: conv_layers (Dict[str, str]): {\"layer1.0.conv1\": layer1.0.conv1, ...} Example: >>> resnet18_pretrained_true = timm.create_model(model_name = \"resnet34\", pretrained=True, num_classes=10).to(DEVICE) >>> conv_layers = get_conv_layers(resnet18_pretrained_true, layer_type=\"Conv2d\") \"\"\" if layer_type == \"Conv2d\" : _layer_type = torch . nn . Conv2d elif layer_type == \"Conv1d\" : _layer_type = torch . nn . Conv1d conv_layers = {} for name , layer in model . named_modules (): if isinstance ( layer , _layer_type ): conv_layers [ name ] = name return conv_layers resnet18_pretrained_true = timm . create_model ( model_name = \"resnet34\" , pretrained = True , num_classes = 10 ) . to ( DEVICE ) >>> resnet18_pretrained_true = timm . create_model ( model_name = \"resnet34\" , pretrained = True , num_classes = 10 ) . to ( DEVICE ) >>> conv_layers = get_conv_layers ( resnet18_pretrained_true , layer_type = \"Conv2d\" ) >>> print ( conv_layers ) {'conv1': 'conv1', 'layer1.0.conv1': 'layer1.0.conv1', 'layer1.0.conv2': 'layer1.0.conv2', 'layer1.1.conv1': 'layer1.1.conv1', 'layer1.1.conv2': 'layer1.1.conv2', 'layer1.2.conv1': 'layer1.2.conv1', 'layer1.2.conv2': 'layer1.2.conv2', 'layer2.0.conv1': 'layer2.0.conv1', 'layer2.0.conv2': 'layer2.0.conv2', 'layer2.0.downsample.0': 'layer2.0.downsample.0', 'layer2.1.conv1': 'layer2.1.conv1', 'layer2.1.conv2': 'layer2.1.conv2', 'layer2.2.conv1': 'layer2.2.conv1', 'layer2.2.conv2': 'layer2.2.conv2', 'layer2.3.conv1': 'layer2.3.conv1', 'layer2.3.conv2': 'layer2.3.conv2', 'layer3.0.conv1': 'layer3.0.conv1', 'layer3.0.conv2': 'layer3.0.conv2', 'layer3.0.downsample.0': 'layer3.0.downsample.0', 'layer3.1.conv1': 'layer3.1.conv1', 'layer3.1.conv2': 'layer3.1.conv2', 'layer3.2.conv1': 'layer3.2.conv1', 'layer3.2.conv2': 'layer3.2.conv2', 'layer3.3.conv1': 'layer3.3.conv1', 'layer3.3.conv2': 'layer3.3.conv2', 'layer3.4.conv1': 'layer3.4.conv1', 'layer3.4.conv2': 'layer3.4.conv2', 'layer3.5.conv1': 'layer3.5.conv1', 'layer3.5.conv2': 'layer3.5.conv2', 'layer4.0.conv1': 'layer4.0.conv1', 'layer4.0.conv2': 'layer4.0.conv2', 'layer4.0.downsample.0': 'layer4.0.downsample.0', 'layer4.1.conv1': 'layer4.1.conv1', 'layer4.1.conv2': 'layer4.1.conv2', 'layer4.2.conv1': 'layer4.2.conv1', 'layer4.2.conv2': 'layer4.2.conv2'} activation = {} def get_intermediate_features ( name : str ) -> Callable : \"\"\"Get the intermediate features of a model. Forward Hook. This is using forward hook with reference https://discuss.pytorch.org/t/how-can-l-load-my-best-model-as-a-feature-extractor-evaluator/17254/5 Args: name (str): name of the layer. Returns: Callable: [description] \"\"\" def hook ( model , input , output ): activation [ name ] = output . detach () return hook # The below is testing the forward hook functionalities, especially getting intermediate features. # Note that both models are same organically but created differently. # Due to seeding issues, you can check whether they are the same output or not by running them separately. # We also used assertion to check that the output from model(x) is same as torch.nn.LogSoftmax(dim=1)(fc3_output) use_sequential_model = True x = torch . randn ( 1 , 25 ) if not use_sequential_model : model = ToyModel () model . fc2 . register_forward_hook ( get_intermediate_features ( \"fc2\" )) model . fc3 . register_forward_hook ( get_intermediate_features ( \"fc3\" )) output = model ( x ) print ( activation ) fc2_output = activation [ \"fc2\" ] fc3_output = activation [ \"fc3\" ] # assert output and logsoftmax fc3_output are the same assert torch . allclose ( output , torch . nn . LogSoftmax ( dim = 1 )( fc3_output )) else : sequential_model = ToySequentialModel () # Do this if you want all, if not you can see below. # for name, layer in sequential_model.named_modules(): # layer.register_forward_hook(get_intermediate_features(name)) sequential_model . head . fc2 . register_forward_hook ( get_intermediate_features ( \"head.fc2\" ) ) sequential_model . head . fc3 . register_forward_hook ( get_intermediate_features ( \"head.fc3\" ) ) sequential_model_output = sequential_model ( x ) print ( activation ) fc2_output = activation [ \"head.fc2\" ] fc3_output = activation [ \"head.fc3\" ] assert torch . allclose ( sequential_model_output , torch . nn . LogSoftmax ( dim = 1 )( fc3_output ) ) {'head.fc2': tensor([[ 0.0697, 0.0544, -0.0157, -0.1059, -0.0464, -0.0090, 0.0532, -0.1273, -0.0286, -0.0151, 0.0963, 0.2205, 0.0745, -0.0110, -0.1127, -0.0367, -0.0681, 0.0463, -0.0833, 0.1288, 0.1058, 0.0976, -0.0251, 0.0980, -0.0110, 0.1170, -0.0650, 0.2091, -0.1773, 0.0363, -0.1452, 0.0036, 0.0112, -0.0304, -0.0620, -0.0658, -0.0543, 0.0072, 0.0436, 0.0703, 0.0254, -0.0614, 0.0164, -0.1003, -0.0396, 0.0349, 0.0089, -0.1243, -0.1037, -0.0491, 0.0627, -0.1347, 0.0010, -0.1290, -0.0280, -0.0344, 0.1487, -0.1764, -0.0233, 0.0082, 0.1270, 0.0368, 0.0103, -0.0929, 0.0038, 0.1346, -0.0688, -0.0437, -0.1205, -0.1596, -0.0240, -0.1001, -0.0300, -0.1119, 0.0344, -0.1587, 0.0329, -0.0424, 0.0999, 0.0732, 0.1116, 0.0220, -0.0570, 0.0232]]), 'head.fc3': tensor([[ 0.0256, -0.0924, 0.0456, 0.0972, 0.0107, 0.0527, 0.0208, 0.0373, 0.0451, 0.0712]])}","title":"Get Convolutional Layers"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/PyTorch%20Utilities/#how-to-freeze-layers","text":"# resnet18_pretrained_true = timm.create_model(model_name = \"resnet34\", pretrained=True, num_classes=10).to(DEVICE) norm = torch . nn . InstanceNorm2d ( num_features = 3 , track_running_stats = True ) print ( norm . running_mean , norm . running_var ) tensor([0., 0., 0.]) tensor([1., 1., 1.]) x = torch . randn ( 2 , 3 , 24 , 24 ) out = norm ( x ) print ( norm . running_mean , norm . running_var ) out = norm ( x ) print ( norm . running_mean , norm . running_var ) out = norm ( x ) print ( norm . running_mean , norm . running_var ) tensor([-1.3414e-03, -4.7338e-05, 1.1239e-03]) tensor([1.0010, 0.9984, 0.9989]) tensor([-2.5486e-03, -8.9943e-05, 2.1355e-03]) tensor([1.0018, 0.9969, 0.9979]) tensor([-0.0036, -0.0001, 0.0030]) tensor([1.0026, 0.9956, 0.9970]) norm . eval () out = norm ( x ) print ( norm . running_mean , norm . running_var ) tensor([-0.0160, -0.0018, 0.0068]) tensor([1.0002, 1.0082, 0.9904]) def freeze_batchnorm_layers ( model : Callable ) -> None : \"\"\"Freeze the batchnorm layers of a PyTorch model. Args: model (CustomNeuralNet): model to be frozen. Example: >>> model = timm.create_model(\"efficientnet_b0\", pretrained=True) >>> model.apply(freeze_batchnorm_layers) # to freeze during training \"\"\" # https://discuss.pytorch.org/t/how-to-freeze-bn-layers-while-training-the-rest-of-network-mean-and-var-wont-freeze/89736/19 # https://discuss.pytorch.org/t/should-i-use-model-eval-when-i-freeze-batchnorm-layers-to-finetune/39495/3 classname = model . __class__ . __name__ for module in model . modules (): if isinstance ( module , torch . nn . InstanceNorm2d ): module . eval () if isinstance ( module , torch . nn . BatchNorm2d ): if hasattr ( module , \"weight\" ): module . weight . requires_grad_ ( False ) if hasattr ( module , \"bias\" ): module . bias . requires_grad_ ( False ) module . eval () norm . apply ( freeze_batchnorm_layers ) InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True) out = norm ( x ) norm . running_mean , norm . running_var (tensor([-0.0036, -0.0001, 0.0030]), tensor([1.0026, 0.9956, 0.9970]))","title":"How to freeze layers"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/","text":"Object Detection https://everitt257.github.io/post/2018/08/10/object_detection.html https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9 https://viso.ai/deep-learning/yolov3-overview/#:~:text=YOLOv3%20(You%20Only%20Look%20Once%2C%20Version%203)%20is%20a,network%20to%20detect%20an%20object. https://sheng-fang.github.io/2020-04-25-review_yolo/ https://deep-learning-study-note.readthedocs.io/en/latest/Part%202%20(Modern%20Practical%20Deep%20Networks)/12%20Applications/Computer%20Vision%20External/YOLO.html Bounding Boxes https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/ Anchor Boxes https://towardsdatascience.com/anchor-boxes-the-key-to-quality-object-detection-ddf9d612d4f9 Yolo Insanely well written article by Oracle detailing with examples of Yolo v1-v3 https://datascience.stackexchange.com/questions/85306/yolov1-algorithm-how-to-determine-predictor-responsibility Quick walkthrough of Yolo v1-v5 https://www.harrysprojects.com/articles/yolov1.html#:~:text=The%20architecture%20of%20YOLO%20v1,of%20those%20fully%20connected%20layers. https://chowdera.com/2022/02/202202130021493704.html In Depth: https://www.codetd.com/en/article/11916630 https://towardsdatascience.com/yolov1-you-only-look-once-object-detection-e1f3ffec8a89 https://jonathan-hui.medium.com/real-time-object-detection-with-yolo-yolov2-28b1b93e2088 https://www.analyticsvidhya.com/blog/2018/12/practical-guide-object-detection-yolo-framewor-python/ Yolo Implementations https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/amp/ There is a distinct difference in how anchor boxes are decided in YOLOv1 versus the later versions. https://stackoverflow.com/questions/52710248/anchor-boxes-in-yolo-how-are-they-decided https://leimao.github.io/blog/YOLOs/ https://stats.stackexchange.com/questions/287486/yolo-loss-function-explanation/287497 https://dkharazi.github.io/notes/ml/cnn/yolo How to use Yolo Library https://www.kaggle.com/code/andradaolteanu/greatbarrierreef-yolo-full-guide-train-infer#Step-4.-YOLOv5-Training Train Custom Data provides a solid introduction. There are other detailed tutorials in their documentation page. Weights & Biases Logging also provides how to log metrics to Weights & Biases. Training Colab Notebook can also be found inside the tutorials. Be sure to look out for them as they are scattered around. YOLOv5 Inference Documentation https://colab.research.google.com/github/bala-codes/Yolo-v5_Object_Detection_Blood_Cell_Count_and_Detection/blob/master/codes/1.%20Yolo-V5%20BCC%20Training%20%26%20Testing.ipynb#scrollTo=k3Tc61Qzd4lY https://www.kaggle.com/code/andradaolteanu/greatbarrierreef-yolo-full-guide-train-infer/notebook#Step-3.-YOLO-Configuration https://github.com/awsaf49/bbox/blob/main/bbox/utils.py https://www.kaggle.com/code/andradaolteanu/whales-dolphins-effnet-embedding-cos-distance#6.-Get-Image-Embeddings https://www.kaggle.com/code/awsaf49/happywhale-boundingbox-yolov5/notebook#%F0%9F%9B%A0-Install-Libraries https://www.kaggle.com/code/awsaf49/happywhale-cropped-dataset-yolov5/notebook#Crop-Utility https://towardsai.net/p/computer-vision/yolo-v5-object-detection-on-a-custom-dataset https://docs.ultralytics.com/tutorials/train-custom-datasets/ https://www.kaggle.com/code/vbookshelf/basics-of-yolo-v5-balloon-detection/notebook#Create-the-yaml-file https://towardsdatascience.com/the-practical-guide-for-object-detection-with-yolov5-algorithm-74c04aac4843 https://docs.ultralytics.com/tutorials/train-custom-datasets/#5-train https://docs.ultralytics.com/tutorials/pytorch-hub/ https://github.com/ultralytics/yolov5/issues/36 RCNN Follow This for basic implementation Simple RCNN Implementation https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9 https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection https://tryolabs.com/blog/2017/08/30/object-detection-an-overview-in-the-age-of-deep-learning#deep-learning-approach https://blog.paperspace.com/faster-r-cnn-explained-object-detection/","title":"README"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/#object-detection","text":"https://everitt257.github.io/post/2018/08/10/object_detection.html https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9 https://viso.ai/deep-learning/yolov3-overview/#:~:text=YOLOv3%20(You%20Only%20Look%20Once%2C%20Version%203)%20is%20a,network%20to%20detect%20an%20object. https://sheng-fang.github.io/2020-04-25-review_yolo/ https://deep-learning-study-note.readthedocs.io/en/latest/Part%202%20(Modern%20Practical%20Deep%20Networks)/12%20Applications/Computer%20Vision%20External/YOLO.html","title":"Object Detection"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/#bounding-boxes","text":"https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/","title":"Bounding Boxes"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/#anchor-boxes","text":"https://towardsdatascience.com/anchor-boxes-the-key-to-quality-object-detection-ddf9d612d4f9","title":"Anchor Boxes"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/#yolo","text":"Insanely well written article by Oracle detailing with examples of Yolo v1-v3 https://datascience.stackexchange.com/questions/85306/yolov1-algorithm-how-to-determine-predictor-responsibility Quick walkthrough of Yolo v1-v5 https://www.harrysprojects.com/articles/yolov1.html#:~:text=The%20architecture%20of%20YOLO%20v1,of%20those%20fully%20connected%20layers. https://chowdera.com/2022/02/202202130021493704.html In Depth: https://www.codetd.com/en/article/11916630 https://towardsdatascience.com/yolov1-you-only-look-once-object-detection-e1f3ffec8a89 https://jonathan-hui.medium.com/real-time-object-detection-with-yolo-yolov2-28b1b93e2088 https://www.analyticsvidhya.com/blog/2018/12/practical-guide-object-detection-yolo-framewor-python/ Yolo Implementations https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/amp/ There is a distinct difference in how anchor boxes are decided in YOLOv1 versus the later versions. https://stackoverflow.com/questions/52710248/anchor-boxes-in-yolo-how-are-they-decided https://leimao.github.io/blog/YOLOs/ https://stats.stackexchange.com/questions/287486/yolo-loss-function-explanation/287497 https://dkharazi.github.io/notes/ml/cnn/yolo How to use Yolo Library https://www.kaggle.com/code/andradaolteanu/greatbarrierreef-yolo-full-guide-train-infer#Step-4.-YOLOv5-Training Train Custom Data provides a solid introduction. There are other detailed tutorials in their documentation page. Weights & Biases Logging also provides how to log metrics to Weights & Biases. Training Colab Notebook can also be found inside the tutorials. Be sure to look out for them as they are scattered around. YOLOv5 Inference Documentation https://colab.research.google.com/github/bala-codes/Yolo-v5_Object_Detection_Blood_Cell_Count_and_Detection/blob/master/codes/1.%20Yolo-V5%20BCC%20Training%20%26%20Testing.ipynb#scrollTo=k3Tc61Qzd4lY https://www.kaggle.com/code/andradaolteanu/greatbarrierreef-yolo-full-guide-train-infer/notebook#Step-3.-YOLO-Configuration https://github.com/awsaf49/bbox/blob/main/bbox/utils.py https://www.kaggle.com/code/andradaolteanu/whales-dolphins-effnet-embedding-cos-distance#6.-Get-Image-Embeddings https://www.kaggle.com/code/awsaf49/happywhale-boundingbox-yolov5/notebook#%F0%9F%9B%A0-Install-Libraries https://www.kaggle.com/code/awsaf49/happywhale-cropped-dataset-yolov5/notebook#Crop-Utility https://towardsai.net/p/computer-vision/yolo-v5-object-detection-on-a-custom-dataset https://docs.ultralytics.com/tutorials/train-custom-datasets/ https://www.kaggle.com/code/vbookshelf/basics-of-yolo-v5-balloon-detection/notebook#Create-the-yaml-file https://towardsdatascience.com/the-practical-guide-for-object-detection-with-yolov5-algorithm-74c04aac4843 https://docs.ultralytics.com/tutorials/train-custom-datasets/#5-train https://docs.ultralytics.com/tutorials/pytorch-hub/ https://github.com/ultralytics/yolov5/issues/36","title":"Yolo"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/#rcnn","text":"Follow This for basic implementation Simple RCNN Implementation https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9 https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection https://tryolabs.com/blog/2017/08/30/object-detection-an-overview-in-the-age-of-deep-learning#deep-learning-approach https://blog.paperspace.com/faster-r-cnn-explained-object-detection/","title":"RCNN"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/fundamentals/multi_heads/","text":"https://stackoverflow.com/questions/56004483/what-is-a-multi-headed-model-and-what-exactly-is-a-head-in-a-model/56004582 The explanation you found is accurate. Depending on what you want to predict on your data you require an adequate backbone network and a certain amount of prediction heads . For a basic classification network for example you can view ResNet, AlexNet, VGGNet, Inception,... as the backbone and the fully connected layer as the sole prediction head. A good example for a problem where you need multiple-heads is localization, where you not only want to classify what is in the image but also want to localize the object (find the coordinates of the bounding box around it). The image below shows the general architecture The backbone network (\"convolution and pooling\") is responsible for extracting a feature map from the image that contains higher level summarized information. Each head uses this feature map as input to predict its desired outcome. The loss that you optimize for during training is usually a weighted sum of the individual losses for each prediction head.","title":"Multi heads"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/image_normalization/Image_Normalization_and_Standardization/","text":"Imports and Utils import os import random from typing import Dict , Tuple import cv2 import numpy as np import pandas as pd import torch import torchvision.datasets as datasets from PIL import Image , ImageFile from torchvision import transforms from tqdm import tqdm def seed_all ( seed : int = 1930 ): \"\"\"Seed all random number generators.\"\"\" print ( \"Using Seed Number {} \" . format ( seed )) # set PYTHONHASHSEED env var at fixed value os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) torch . cuda . manual_seed ( seed ) # pytorch (both CPU and CUDA) np . random . seed ( seed ) # for numpy pseudo-random generator random . seed ( seed ) # set fixed value for python built-in pseudo-random generator torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = False torch . backends . cudnn . enabled = False seed_all () Using Seed Number 1930 Disclaimer Note that the following method is not the most efficient way , but it is good for learning as the steps in the codes are laid out sequentially so that it is easy to follow. Info There are a few pre-preprocessing techniques for image data. Here we discuss the most common one that I encounter, Normalization across channels. [1^]: Extracted from CS231n Warning Data leakage will occur if you apply this pre-processing step prior to your train-valid-test split. We should apply normalization on the training step, obtaining the mean and std metrics for \\(X_{\\text{train}}\\) and apply them to validation set during model selection, and to test set during model evaluation. In our examples below, I apply mean and std calculation on the training set (which includes the validation set), in reality, we should further split the training set into training and validation sets. General Steps to Normalize Important Warning Important: Most of the times we resize the images, so different image size may result in different mean and std. So remember to resize first then calcuate. RGB image with 3 channels. We assume it is channels first, if not convert from channels last to channels first. As I am using PyTorch primarily, this is more natural to me. See CIFAR-10 for such example. Load the data into disk using either cv2 or PIL . Divide by 255 across all images first to normalize it. Then find the image's mean and std per channel. For example, if we want to find the mean of the red channel of a batch of images, and assume we have 10 images of size \\((100, 100, 3)\\) each. Then each image has 3 channels, each channel has \\(100 \\times 100\\) pixels, and therefore 10 such images will have \\(10 \\times 100 \\times 100 = 100000\\) pixels. We flatten() all these 10 images' red channel and take the average (i.e. sum all \\(100000\\) red pixels, and divide by \\(1000000\\) ). We do the same for all the other channels. Grayscale image with 1 channel. This is just average the values in one channel. Audio/Spectrograms like SETI etc. CIFAR-10 (RGB) We first see an example of calculating the mean and standard deviation of cifar10, which is of RGB channels. Mean : { \"R\" : 0.49139968 \"G\" : 0.48215827 \"B\" : 0.44653124 } Standard Deviation : { \"R\" : 0.24703233 \"G\" : 0.24348505 \"B\" : 0.26158768 } We will code a function to calculate the mean and standard deviation of a batch of images. TRANSFORMS = transforms . Compose ([ transforms . ToTensor ()]) trainset_cifar10 = datasets . CIFAR10 ( root = \"./data\" , train = True , download = True , transform = TRANSFORMS ) testset_cifar10 = datasets . CIFAR10 ( root = \"./data\" , train = False , download = True , transform = TRANSFORMS ) train_images_cifar10 = np . asarray ( trainset_cifar10 . data ) # (50000, 32, 32, 3) test_images_cifar10 = np . asarray ( testset_cifar10 . data ) # (10000, 32, 32, 3) Files already downloaded and verified Files already downloaded and verified def calcMeanStd ( images : np . ndarray ) -> Dict [ str , Tuple [ float ]]: \"\"\"Take in an numpy array of images and returns mean and std per channel. This function assumes for a start, your array is loaded into disk. Args: images (np.ndarray): [num_images, channel, height, width] or [num_images, height, width, channel] Returns: Dict[str, Tuple[float]]: {\"mean\": (mean_r, mean_g, mean_b), \"std\": (std_r, std_g, std_b)} \"\"\" images = np . asarray ( images ) # good way to test if images is passed in the correct dtype images = images / 255. # min-max and divide by 255 if images . ndim == 4 : # RGB if images . shape [ 1 ] != 3 : # if channel is not first, make it so, assume channels last images = images . transpose ( 0 , 3 , 1 , 2 ) # if tensor use permute instead # permutation applies the following mapping # axis0 -> axis0 # axis1 -> axis3 # axis2 -> axis1 # axis3 -> axis2 b , c , w , h = images . shape r_channel , g_channel , b_channel = images [:, 0 , :, :], images [:, 1 , :, :], images [:, 2 , :, :] # get rgb channels individually r_channel , g_channel , b_channel = r_channel . flatten (), g_channel . flatten (), b_channel . flatten () # flatten each channel into one array mean_r = r_channel . mean ( axis = None ) # since we are averaging per channel, we get the first channel's mean by r_channel.mean mean_g = g_channel . mean ( axis = None ) # same as above mean_b = b_channel . mean ( axis = None ) # same as above # calculate std over each channel (r,g,b) std_r = r_channel . std ( axis = None ) std_g = g_channel . std ( axis = None ) std_b = b_channel . std ( axis = None ) return { 'mean' : ( mean_r , mean_g , mean_b ), 'std' : ( std_r , std_g , std_b )} elif images . ndim == 3 : # grayscale gray_channel = images . flatten () # flatten directly since only 1 channel mean = gray_channel . mean ( axis = None ) std = gray_channel . std ( axis = None ) return { \"mean\" : ( mean ,), \"std\" : ( std , )} else : raise ValueError ( \"passed error is not of the right shape!\" ) mean_std_cifar = calcMeanStd ( train_images_cifar10 ) print ( mean_std_cifar ) {'mean': (0.49139967861519745, 0.4821584083946076, 0.44653091444546616), 'std': (0.2470322324632823, 0.24348512800005553, 0.2615878417279641)} # alternate way to do this. print ( trainset_cifar10 . data . shape ) print ( trainset_cifar10 . data . mean ( axis = ( 0 , 1 , 2 )) / 255 ) print ( trainset_cifar10 . data . std ( axis = ( 0 , 1 , 2 )) / 255 ) (50000, 32, 32, 3) [0.49139968 0.48215841 0.44653091] [0.24703223 0.24348513 0.26158784] Depending on your use case, we can normalize the test/validation set with the parameters found on the train set, though in practice, for image recognition problems, we use the same normalization parameters on both the train and validation set, and apply it to test set. The steps are: Calculate the mean and std using the method above. Divide the training/validation/test set by 255. Normalize it using the values found. Note step 2 can be skipped if the normalization method in the library does a division of 255 internally. TRANSFORMS_with_normalization = transforms . Compose ( [ transforms . Normalize ( mean = mean_std_cifar [ \"mean\" ], std = mean_std_cifar [ \"std\" ] ), transforms . ToTensor (), ] ) trainset_cifar10 = datasets . CIFAR10 ( root = \"./data\" , train = True , download = True , transform = TRANSFORMS_with_normalization ) testset_cifar10 = datasets . CIFAR10 ( root = \"./data\" , train = False , download = True , transform = TRANSFORMS_with_normalization ) train_images_cifar10 = np . asarray ( trainset_cifar10 . data ) # (50000, 32, 32, 3) test_images_cifar10 = np . asarray ( testset_cifar10 . data ) # (10000, 32, 32, 3) Files already downloaded and verified Files already downloaded and verified MNIST (Grayscale) We next see an example of calculating the mean and standard deviation of MNIST, which is of one channel (grayscale). Mean : 0.1307 Standard Deviation : 0.3081 We will code a function to calculate the mean and standard deviation of a batch of images. # mnist trainset_mnist = datasets . MNIST ( root = \"./data/\" , train = True , download = True , transform = TRANSFORMS ) testset_mnist = datasets . MNIST ( root = \"./data\" , train = False , download = True , transform = TRANSFORMS ) train_images_mnist = np . asarray ( trainset_mnist . data ) # (60000, 28, 28) test_images_mnist = np . asarray ( testset_mnist . data ) # (10000, 28, 28) mean_std_mnist = calcMeanStd ( train_images_mnist ) print ( mean_std_mnist ) {'mean': (0.1306604762738429,), 'std': (0.3081078038564622,)} print ( trainset_mnist . data . float () . mean () / 255 ) print ( trainset_mnist . data . float () . std () / 255 ) tensor(0.1307) tensor(0.3081) References To read up more on how others do it efficiently , please have a read below. https://www.kaggle.com/kozodoi/seti-mean-and-std-of-new-data/notebook https://www.kaggle.com/kozodoi/computing-dataset-mean-and-std https://forums.fast.ai/t/calculating-our-own-image-stats-imagenet-stats-cifar-stats-etc/40355/3 https://github.com/JoshVarty/CancerDetection/blob/master/01_ImageStats.ipynb https://forums.fast.ai/t/calculating-new-stats/31214 https://forums.fast.ai/t/calcuating-the-mean-and-standard-deviation-for-normalize/62883/13 https://www.kaggle.com/c/ranzcr-clip-catheter-line-classification/discussion/211039 https://stackoverflow.com/questions/58151507/why-pytorch-officially-use-mean-0-485-0-456-0-406-and-std-0-229-0-224-0-2 https://stackoverflow.com/questions/65699020/calculate-standard-deviation-for-grayscale-imagenet-pixel-values-with-rotation-m/65717887#65717887 https://stackoverflow.com/questions/66678052/how-to-calculate-the-mean-and-the-std-of-cifar10-data https://drive.google.com/drive/u/1/folders/1Gum3vsRsKKRSFZ1hyKaPTiVs1AUAmdKD https://stackoverflow.com/questions/50710493/cifar-10-meaningless-normalization-values https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/6 https://github.com/kuangliu/pytorch-cifar/issues/19 https://github.com/Armour/pytorch-nn-practice/blob/master/utils/meanstd.py","title":"Image Normalization"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/image_normalization/Image_Normalization_and_Standardization/#imports-and-utils","text":"import os import random from typing import Dict , Tuple import cv2 import numpy as np import pandas as pd import torch import torchvision.datasets as datasets from PIL import Image , ImageFile from torchvision import transforms from tqdm import tqdm def seed_all ( seed : int = 1930 ): \"\"\"Seed all random number generators.\"\"\" print ( \"Using Seed Number {} \" . format ( seed )) # set PYTHONHASHSEED env var at fixed value os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) torch . cuda . manual_seed ( seed ) # pytorch (both CPU and CUDA) np . random . seed ( seed ) # for numpy pseudo-random generator random . seed ( seed ) # set fixed value for python built-in pseudo-random generator torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = False torch . backends . cudnn . enabled = False seed_all () Using Seed Number 1930","title":"Imports and Utils"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/image_normalization/Image_Normalization_and_Standardization/#disclaimer","text":"Note that the following method is not the most efficient way , but it is good for learning as the steps in the codes are laid out sequentially so that it is easy to follow. Info There are a few pre-preprocessing techniques for image data. Here we discuss the most common one that I encounter, Normalization across channels. [1^]: Extracted from CS231n Warning Data leakage will occur if you apply this pre-processing step prior to your train-valid-test split. We should apply normalization on the training step, obtaining the mean and std metrics for \\(X_{\\text{train}}\\) and apply them to validation set during model selection, and to test set during model evaluation. In our examples below, I apply mean and std calculation on the training set (which includes the validation set), in reality, we should further split the training set into training and validation sets.","title":"Disclaimer"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/image_normalization/Image_Normalization_and_Standardization/#general-steps-to-normalize","text":"Important Warning Important: Most of the times we resize the images, so different image size may result in different mean and std. So remember to resize first then calcuate. RGB image with 3 channels. We assume it is channels first, if not convert from channels last to channels first. As I am using PyTorch primarily, this is more natural to me. See CIFAR-10 for such example. Load the data into disk using either cv2 or PIL . Divide by 255 across all images first to normalize it. Then find the image's mean and std per channel. For example, if we want to find the mean of the red channel of a batch of images, and assume we have 10 images of size \\((100, 100, 3)\\) each. Then each image has 3 channels, each channel has \\(100 \\times 100\\) pixels, and therefore 10 such images will have \\(10 \\times 100 \\times 100 = 100000\\) pixels. We flatten() all these 10 images' red channel and take the average (i.e. sum all \\(100000\\) red pixels, and divide by \\(1000000\\) ). We do the same for all the other channels. Grayscale image with 1 channel. This is just average the values in one channel. Audio/Spectrograms like SETI etc.","title":"General Steps to Normalize"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/image_normalization/Image_Normalization_and_Standardization/#cifar-10-rgb","text":"We first see an example of calculating the mean and standard deviation of cifar10, which is of RGB channels. Mean : { \"R\" : 0.49139968 \"G\" : 0.48215827 \"B\" : 0.44653124 } Standard Deviation : { \"R\" : 0.24703233 \"G\" : 0.24348505 \"B\" : 0.26158768 } We will code a function to calculate the mean and standard deviation of a batch of images. TRANSFORMS = transforms . Compose ([ transforms . ToTensor ()]) trainset_cifar10 = datasets . CIFAR10 ( root = \"./data\" , train = True , download = True , transform = TRANSFORMS ) testset_cifar10 = datasets . CIFAR10 ( root = \"./data\" , train = False , download = True , transform = TRANSFORMS ) train_images_cifar10 = np . asarray ( trainset_cifar10 . data ) # (50000, 32, 32, 3) test_images_cifar10 = np . asarray ( testset_cifar10 . data ) # (10000, 32, 32, 3) Files already downloaded and verified Files already downloaded and verified def calcMeanStd ( images : np . ndarray ) -> Dict [ str , Tuple [ float ]]: \"\"\"Take in an numpy array of images and returns mean and std per channel. This function assumes for a start, your array is loaded into disk. Args: images (np.ndarray): [num_images, channel, height, width] or [num_images, height, width, channel] Returns: Dict[str, Tuple[float]]: {\"mean\": (mean_r, mean_g, mean_b), \"std\": (std_r, std_g, std_b)} \"\"\" images = np . asarray ( images ) # good way to test if images is passed in the correct dtype images = images / 255. # min-max and divide by 255 if images . ndim == 4 : # RGB if images . shape [ 1 ] != 3 : # if channel is not first, make it so, assume channels last images = images . transpose ( 0 , 3 , 1 , 2 ) # if tensor use permute instead # permutation applies the following mapping # axis0 -> axis0 # axis1 -> axis3 # axis2 -> axis1 # axis3 -> axis2 b , c , w , h = images . shape r_channel , g_channel , b_channel = images [:, 0 , :, :], images [:, 1 , :, :], images [:, 2 , :, :] # get rgb channels individually r_channel , g_channel , b_channel = r_channel . flatten (), g_channel . flatten (), b_channel . flatten () # flatten each channel into one array mean_r = r_channel . mean ( axis = None ) # since we are averaging per channel, we get the first channel's mean by r_channel.mean mean_g = g_channel . mean ( axis = None ) # same as above mean_b = b_channel . mean ( axis = None ) # same as above # calculate std over each channel (r,g,b) std_r = r_channel . std ( axis = None ) std_g = g_channel . std ( axis = None ) std_b = b_channel . std ( axis = None ) return { 'mean' : ( mean_r , mean_g , mean_b ), 'std' : ( std_r , std_g , std_b )} elif images . ndim == 3 : # grayscale gray_channel = images . flatten () # flatten directly since only 1 channel mean = gray_channel . mean ( axis = None ) std = gray_channel . std ( axis = None ) return { \"mean\" : ( mean ,), \"std\" : ( std , )} else : raise ValueError ( \"passed error is not of the right shape!\" ) mean_std_cifar = calcMeanStd ( train_images_cifar10 ) print ( mean_std_cifar ) {'mean': (0.49139967861519745, 0.4821584083946076, 0.44653091444546616), 'std': (0.2470322324632823, 0.24348512800005553, 0.2615878417279641)} # alternate way to do this. print ( trainset_cifar10 . data . shape ) print ( trainset_cifar10 . data . mean ( axis = ( 0 , 1 , 2 )) / 255 ) print ( trainset_cifar10 . data . std ( axis = ( 0 , 1 , 2 )) / 255 ) (50000, 32, 32, 3) [0.49139968 0.48215841 0.44653091] [0.24703223 0.24348513 0.26158784] Depending on your use case, we can normalize the test/validation set with the parameters found on the train set, though in practice, for image recognition problems, we use the same normalization parameters on both the train and validation set, and apply it to test set. The steps are: Calculate the mean and std using the method above. Divide the training/validation/test set by 255. Normalize it using the values found. Note step 2 can be skipped if the normalization method in the library does a division of 255 internally. TRANSFORMS_with_normalization = transforms . Compose ( [ transforms . Normalize ( mean = mean_std_cifar [ \"mean\" ], std = mean_std_cifar [ \"std\" ] ), transforms . ToTensor (), ] ) trainset_cifar10 = datasets . CIFAR10 ( root = \"./data\" , train = True , download = True , transform = TRANSFORMS_with_normalization ) testset_cifar10 = datasets . CIFAR10 ( root = \"./data\" , train = False , download = True , transform = TRANSFORMS_with_normalization ) train_images_cifar10 = np . asarray ( trainset_cifar10 . data ) # (50000, 32, 32, 3) test_images_cifar10 = np . asarray ( testset_cifar10 . data ) # (10000, 32, 32, 3) Files already downloaded and verified Files already downloaded and verified","title":"CIFAR-10 (RGB)"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/image_normalization/Image_Normalization_and_Standardization/#mnist-grayscale","text":"We next see an example of calculating the mean and standard deviation of MNIST, which is of one channel (grayscale). Mean : 0.1307 Standard Deviation : 0.3081 We will code a function to calculate the mean and standard deviation of a batch of images. # mnist trainset_mnist = datasets . MNIST ( root = \"./data/\" , train = True , download = True , transform = TRANSFORMS ) testset_mnist = datasets . MNIST ( root = \"./data\" , train = False , download = True , transform = TRANSFORMS ) train_images_mnist = np . asarray ( trainset_mnist . data ) # (60000, 28, 28) test_images_mnist = np . asarray ( testset_mnist . data ) # (10000, 28, 28) mean_std_mnist = calcMeanStd ( train_images_mnist ) print ( mean_std_mnist ) {'mean': (0.1306604762738429,), 'std': (0.3081078038564622,)} print ( trainset_mnist . data . float () . mean () / 255 ) print ( trainset_mnist . data . float () . std () / 255 ) tensor(0.1307) tensor(0.3081)","title":"MNIST (Grayscale)"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/image_normalization/Image_Normalization_and_Standardization/#references","text":"To read up more on how others do it efficiently , please have a read below. https://www.kaggle.com/kozodoi/seti-mean-and-std-of-new-data/notebook https://www.kaggle.com/kozodoi/computing-dataset-mean-and-std https://forums.fast.ai/t/calculating-our-own-image-stats-imagenet-stats-cifar-stats-etc/40355/3 https://github.com/JoshVarty/CancerDetection/blob/master/01_ImageStats.ipynb https://forums.fast.ai/t/calculating-new-stats/31214 https://forums.fast.ai/t/calcuating-the-mean-and-standard-deviation-for-normalize/62883/13 https://www.kaggle.com/c/ranzcr-clip-catheter-line-classification/discussion/211039 https://stackoverflow.com/questions/58151507/why-pytorch-officially-use-mean-0-485-0-456-0-406-and-std-0-229-0-224-0-2 https://stackoverflow.com/questions/65699020/calculate-standard-deviation-for-grayscale-imagenet-pixel-values-with-rotation-m/65717887#65717887 https://stackoverflow.com/questions/66678052/how-to-calculate-the-mean-and-the-std-of-cifar10-data https://drive.google.com/drive/u/1/folders/1Gum3vsRsKKRSFZ1hyKaPTiVs1AUAmdKD https://stackoverflow.com/questions/50710493/cifar-10-meaningless-normalization-values https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/6 https://github.com/kuangliu/pytorch-cifar/issues/19 https://github.com/Armour/pytorch-nn-practice/blob/master/utils/meanstd.py","title":"References"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/importance_of_model_calibration_and_interpretability_in_healthcare/","text":"Model Calibration Model Interpretation","title":"Importance of model calibration and interpretability in healthcare"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/model_interpretation/","text":"Interpretating what Convolutional Neural Networks Learn On the Importance of Model Interpretability in Healthcare When we started out on our Machine Learning Journey, most of us have seen authors describe the conundrum of the trade off between prediction performance and a model (hypothesis)'s interpretability 1 . The general consensus, though not always correct, is that more complex models will have high performance measures but low interpretability and less complex models, the opposite. One might ask, when should one consider the usage of a complex or a simpler one? Here is a rule of thumb for you to follow. If the only interest is in prediction , for example, one company seeks to develop an algorithm that predicts the crypto price , then it is likely that the performance measure should be maximized over interpretability. On the other hand, if our interest is in inference 2 , for example in a healthcare setting where our medical use case is to predict whether a patient has melanoma (skin cancer) , then interpretability is more important that high performance . This is because the models are really not the only one making the decision, healthcare practitioners will need to understand the cause of the decision made by the model. In short, doctors wanted to know and agree on which particular area of the skin cell propels the model to make the decision that it is cancerous or not. One more point, can help to do error analysis on where the model did wrong. For example, a beginner trained a model to classify red and white blood cells, started from MNIST, he naturally greyed out the blood cells and received poor results. Doing some error analysis to interpret what went wrong easily tells us that by greying out the cells, the model get confused because they looked very similar, one important feature (i.e. color) is missing. For more understanding in this section, the book 3 Interpretable Machine Learning by Christoph Molnar is a good read. Computer Vision and Convolutional Neural Networks The computer vision has seen a bloom in the recent decade, empowering many use cases, including but not limited to the healthcare, automobile and facial recoginition industry . Most computer vision problems uses a type of neural networks Convolutional Neural Networks (CNN) , though recent breakthroughs show more promising results in vision transformers 4 . We will however, focus more on CNN in this section. For the longest time, CNNs are regarded as black box model simply because it is very complex and hence difficult to interpret . However, in recent times, as the need to interpret models grows, there are quite some methods to give a glimpse of what your CNN is looking at. CNN is a black box? Neural Networks are well defined mathematically, so why we cannot use the same way we do to say Logistic Regression to interpret the model? Well ... the feature importance is not so easy to decode as neural networks in computer vision are interpreted pixel level and it is not logical to derive feature importance at a pixel level. We need something at a spatial level. For example, we plot a grayscale image of a number 3 taken from MNIST by pixel level (i.e. \\(28 \\times 28\\) ), then it is not immediately obvious how we should recover the \"importance\" of any single pixel. MNIST pixel level plot; By Hongnan G. High level understanding of CNN The below is referenced from the Interpretable Machine Learning . We start off with the question, why cannot we use a normal model say, SVM or Logistic Regression to predict a cat image? Can't we just flatten the pixels and feed it as input to the models? Flattened pixels lose spatial level information , it only encodes information sequentially and high level features are not captured. Intuition is that a cat is a cat because of its eyes and ears (example) but when flattened these features may not be clustered together. We may need insane Feature Engineering to make classical ML models work (i.e. if you can re-construct the features of a cat's eyes using the flattened pixels then you may be successful). The power of CNN is that it learns high-level spatial features such as colors, edges and patterns that is unique to the image. This is enabled by the number of hidden layers that did many transformations to the inputs. So in a sense, the hidden layers of CNN are implicitly performing feature engineering . Let us detail a high level outline of a \"life cycle of a CNN\". The input (image) is usually of size \\((C, H, W)\\) is fed into the CNN. Note we do not flatten the image . In these CNN layers , the network first learns simple features such as the cat's edges and shapes, then as it progress to later layers, it learns highly abstract features such as more complex textures and patterns . After propagating to the last layer of the Convolutional Layer , we will then use a type of pooling and flatten the learned features and connect it to fully connected layers and predict the classes. Cat and CNN; By Hongnan G. CNN Interpretation Feature Visualization Through Convolutional Layers As mentioned in the previous section on the high level overview of CNN, an immediate solution is to ask if we can visualize what each layer's output is showing. This method is useful for understanding: How successive CNN layers transform their inputs. What each combination of filters does, good for having an intuition of whether the filter detects edges, shapes or more. Visualizing these filters can give us an understanding of the visual pattern that the CNN is capturing. Visualization Here is a visual of the conv layers' outputs. VGG16 Conv layers on a Cat; By Hongnan G. Takeaways Our takeaways are: The first few layers act as a collection of various edge and shape detectors and the activations retain almost all of the information from the original input image. As you go deeper, the layers begin to encode high and abstract features, the features become less \"informative and obvious\" and more \"generic\" (more on this intuition later). The sparsity of the activations increases with the depth of the layer: in the first layer, almost all filters are activated by the input image, but in the following layers, more and more filters are blank. This means the pattern encoded by the filter isn't found in the input image. Intuition As Francis Chollet mentioned, a deep CNN acts as an information distillation pipeline , but what is it? Why is it that as you go deeper, the image of a cat looks less precise and less like a real cat ? The analogy I use (similar to hise) gives you intuition: Imagine you were tasked to recognize a cat, you will do so instantly. Now you are also tasked to draw a cat, surely you can do so if you can recognize it? You started to draw a cat and compare it with a real cat. You realize that we cannot really remember the specific details of a cat but we can draw the abstract (generic) cat. The neural networks is like our brain, where we manage to recognize an image by filtering out the irrelevant details and transform it to high-level abstract features . This is what happens to a CNN and this is ideal! We do not want the model to remember too specific details of an image in fear of it not being able to generalize. A cat image from me A cat image from Unsplash Readings Readings over this section: Fran\u00e7ois Chollet: Deep Learning With Python pp.262-267 Christoph Molnar: Interpretable Machine Learning section. 10.1 Gradcam Interpret decision by determining which feature in our inputs had the highest contribution. If model predicted a cat, is it the eyes, ears or body shape that defined the class? Gradients can be very useful in this. Intuition is that gradients measure the effect on the outputs caused by some inputs. i.e. \\(y = f(x) = 2x\\) , then every change of 1 unit of change in \\(x\\) causes \\(2\\) units of change in our output \\(y\\) . The gradient is 2 here and measures the rate of change of \\(y\\) with respect to \\(x\\) . The same analogy applies here, can we find the pixels \\(x\\) in the image that contributes to the target \\(y\\) ? In practice however, we often denote a loss function \\(\\mathcal{L}(\\hat{f(\\mathbf{x})}, \\mathbf{y})\\) to minimize it and we can compute the gradient of this loss function with respect to the inputs. That is to say, if our loss \\(\\mathcal{L}\\) is low, then our model is doing something right and we can examine the gradients of the loss with respect to \\(x\\) to find out more. If we are interested in the cat class, we focus only on the \\(y_{cat}\\) . Feature maps of a cat: \\(A_1, A_2, ..., A_k\\) , each contributes in making the final decision in what \\(\\hat{y}\\) is. Computing gradient of \\(y_{cat}\\) with respect to \\(A_k\\) will give us the rate of change of the feature maps with respect to the target. For each feature map, we compute the gradients. For example if we have 8 32 by 32 feature maps, we compute gradients for all \\(8 x 32 x 32\\) pixels and average them channel wise. Now we have a single 32 by 32 feature map with all the gradient info, we then apply ReLU to it, where we set negative values to 0 because we are only interested in the class cat. i.e. negative values in gradient does not mean non-importance, it only pulls the prediction to the other classes. Now we have a 32 by 32 feature map with only positive values at certain areas, we need to map it back to the original image. For example if the original image of the cat is 320 x 320, then we need to scale it back to overlay back to the original image. The overlayed image will show a heatmap on where the gradients are non-negative, highlighting areas of focus. Gradcam 1 Gradcam 2 References https://christophm.github.io/interpretable-ml-book/ https://distill.pub/2017/feature-visualization/ Deep Learning with Python by Francois Chollet An Introduction to Statistical Learning, James, G., Witten, D., Hastie, T., & Tibshirani, R. pp.24-25 \u21a9 Note that the difference between inference and prediction is that inference can be thought of as one step beyond prediction, where we are not only concerned with the outputs of our model but also want to be able to extract a meaningful relationship between our input features and our predictions. Inference may be understood as prediction in the ML community, but not in the statistics community. \u21a9 https://christophm.github.io/interpretable-ml-book/ \u21a9 https://en.wikipedia.org/wiki/Vision_transformer \u21a9","title":"Model interpretation"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/model_interpretation/#interpretating-what-convolutional-neural-networks-learn","text":"","title":"Interpretating what Convolutional Neural Networks Learn"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/model_interpretation/#on-the-importance-of-model-interpretability-in-healthcare","text":"When we started out on our Machine Learning Journey, most of us have seen authors describe the conundrum of the trade off between prediction performance and a model (hypothesis)'s interpretability 1 . The general consensus, though not always correct, is that more complex models will have high performance measures but low interpretability and less complex models, the opposite. One might ask, when should one consider the usage of a complex or a simpler one? Here is a rule of thumb for you to follow. If the only interest is in prediction , for example, one company seeks to develop an algorithm that predicts the crypto price , then it is likely that the performance measure should be maximized over interpretability. On the other hand, if our interest is in inference 2 , for example in a healthcare setting where our medical use case is to predict whether a patient has melanoma (skin cancer) , then interpretability is more important that high performance . This is because the models are really not the only one making the decision, healthcare practitioners will need to understand the cause of the decision made by the model. In short, doctors wanted to know and agree on which particular area of the skin cell propels the model to make the decision that it is cancerous or not. One more point, can help to do error analysis on where the model did wrong. For example, a beginner trained a model to classify red and white blood cells, started from MNIST, he naturally greyed out the blood cells and received poor results. Doing some error analysis to interpret what went wrong easily tells us that by greying out the cells, the model get confused because they looked very similar, one important feature (i.e. color) is missing. For more understanding in this section, the book 3 Interpretable Machine Learning by Christoph Molnar is a good read.","title":"On the Importance of Model Interpretability in Healthcare"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/model_interpretation/#computer-vision-and-convolutional-neural-networks","text":"The computer vision has seen a bloom in the recent decade, empowering many use cases, including but not limited to the healthcare, automobile and facial recoginition industry . Most computer vision problems uses a type of neural networks Convolutional Neural Networks (CNN) , though recent breakthroughs show more promising results in vision transformers 4 . We will however, focus more on CNN in this section. For the longest time, CNNs are regarded as black box model simply because it is very complex and hence difficult to interpret . However, in recent times, as the need to interpret models grows, there are quite some methods to give a glimpse of what your CNN is looking at.","title":"Computer Vision and Convolutional Neural Networks"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/model_interpretation/#cnn-is-a-black-box","text":"Neural Networks are well defined mathematically, so why we cannot use the same way we do to say Logistic Regression to interpret the model? Well ... the feature importance is not so easy to decode as neural networks in computer vision are interpreted pixel level and it is not logical to derive feature importance at a pixel level. We need something at a spatial level. For example, we plot a grayscale image of a number 3 taken from MNIST by pixel level (i.e. \\(28 \\times 28\\) ), then it is not immediately obvious how we should recover the \"importance\" of any single pixel. MNIST pixel level plot; By Hongnan G.","title":"CNN is a black box?"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/model_interpretation/#high-level-understanding-of-cnn","text":"The below is referenced from the Interpretable Machine Learning . We start off with the question, why cannot we use a normal model say, SVM or Logistic Regression to predict a cat image? Can't we just flatten the pixels and feed it as input to the models? Flattened pixels lose spatial level information , it only encodes information sequentially and high level features are not captured. Intuition is that a cat is a cat because of its eyes and ears (example) but when flattened these features may not be clustered together. We may need insane Feature Engineering to make classical ML models work (i.e. if you can re-construct the features of a cat's eyes using the flattened pixels then you may be successful). The power of CNN is that it learns high-level spatial features such as colors, edges and patterns that is unique to the image. This is enabled by the number of hidden layers that did many transformations to the inputs. So in a sense, the hidden layers of CNN are implicitly performing feature engineering . Let us detail a high level outline of a \"life cycle of a CNN\". The input (image) is usually of size \\((C, H, W)\\) is fed into the CNN. Note we do not flatten the image . In these CNN layers , the network first learns simple features such as the cat's edges and shapes, then as it progress to later layers, it learns highly abstract features such as more complex textures and patterns . After propagating to the last layer of the Convolutional Layer , we will then use a type of pooling and flatten the learned features and connect it to fully connected layers and predict the classes. Cat and CNN; By Hongnan G.","title":"High level understanding of CNN"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/model_interpretation/#cnn-interpretation","text":"","title":"CNN Interpretation"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/model_interpretation/#feature-visualization-through-convolutional-layers","text":"As mentioned in the previous section on the high level overview of CNN, an immediate solution is to ask if we can visualize what each layer's output is showing. This method is useful for understanding: How successive CNN layers transform their inputs. What each combination of filters does, good for having an intuition of whether the filter detects edges, shapes or more. Visualizing these filters can give us an understanding of the visual pattern that the CNN is capturing.","title":"Feature Visualization Through Convolutional Layers"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/model_interpretation/#visualization","text":"Here is a visual of the conv layers' outputs. VGG16 Conv layers on a Cat; By Hongnan G.","title":"Visualization"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/model_interpretation/#takeaways","text":"Our takeaways are: The first few layers act as a collection of various edge and shape detectors and the activations retain almost all of the information from the original input image. As you go deeper, the layers begin to encode high and abstract features, the features become less \"informative and obvious\" and more \"generic\" (more on this intuition later). The sparsity of the activations increases with the depth of the layer: in the first layer, almost all filters are activated by the input image, but in the following layers, more and more filters are blank. This means the pattern encoded by the filter isn't found in the input image.","title":"Takeaways"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/model_interpretation/#intuition","text":"As Francis Chollet mentioned, a deep CNN acts as an information distillation pipeline , but what is it? Why is it that as you go deeper, the image of a cat looks less precise and less like a real cat ? The analogy I use (similar to hise) gives you intuition: Imagine you were tasked to recognize a cat, you will do so instantly. Now you are also tasked to draw a cat, surely you can do so if you can recognize it? You started to draw a cat and compare it with a real cat. You realize that we cannot really remember the specific details of a cat but we can draw the abstract (generic) cat. The neural networks is like our brain, where we manage to recognize an image by filtering out the irrelevant details and transform it to high-level abstract features . This is what happens to a CNN and this is ideal! We do not want the model to remember too specific details of an image in fear of it not being able to generalize. A cat image from me A cat image from Unsplash","title":"Intuition"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/model_interpretation/#readings","text":"Readings over this section: Fran\u00e7ois Chollet: Deep Learning With Python pp.262-267 Christoph Molnar: Interpretable Machine Learning section. 10.1","title":"Readings"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/model_interpretation/#gradcam","text":"Interpret decision by determining which feature in our inputs had the highest contribution. If model predicted a cat, is it the eyes, ears or body shape that defined the class? Gradients can be very useful in this. Intuition is that gradients measure the effect on the outputs caused by some inputs. i.e. \\(y = f(x) = 2x\\) , then every change of 1 unit of change in \\(x\\) causes \\(2\\) units of change in our output \\(y\\) . The gradient is 2 here and measures the rate of change of \\(y\\) with respect to \\(x\\) . The same analogy applies here, can we find the pixels \\(x\\) in the image that contributes to the target \\(y\\) ? In practice however, we often denote a loss function \\(\\mathcal{L}(\\hat{f(\\mathbf{x})}, \\mathbf{y})\\) to minimize it and we can compute the gradient of this loss function with respect to the inputs. That is to say, if our loss \\(\\mathcal{L}\\) is low, then our model is doing something right and we can examine the gradients of the loss with respect to \\(x\\) to find out more. If we are interested in the cat class, we focus only on the \\(y_{cat}\\) . Feature maps of a cat: \\(A_1, A_2, ..., A_k\\) , each contributes in making the final decision in what \\(\\hat{y}\\) is. Computing gradient of \\(y_{cat}\\) with respect to \\(A_k\\) will give us the rate of change of the feature maps with respect to the target. For each feature map, we compute the gradients. For example if we have 8 32 by 32 feature maps, we compute gradients for all \\(8 x 32 x 32\\) pixels and average them channel wise. Now we have a single 32 by 32 feature map with all the gradient info, we then apply ReLU to it, where we set negative values to 0 because we are only interested in the class cat. i.e. negative values in gradient does not mean non-importance, it only pulls the prediction to the other classes. Now we have a 32 by 32 feature map with only positive values at certain areas, we need to map it back to the original image. For example if the original image of the cat is 320 x 320, then we need to scale it back to overlay back to the original image. The overlayed image will show a heatmap on where the gradients are non-negative, highlighting areas of focus. Gradcam 1 Gradcam 2","title":"Gradcam"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/model_interpretation/#references","text":"https://christophm.github.io/interpretable-ml-book/ https://distill.pub/2017/feature-visualization/ Deep Learning with Python by Francois Chollet An Introduction to Statistical Learning, James, G., Witten, D., Hastie, T., & Tibshirani, R. pp.24-25 \u21a9 Note that the difference between inference and prediction is that inference can be thought of as one step beyond prediction, where we are not only concerned with the outputs of our model but also want to be able to extract a meaningful relationship between our input features and our predictions. Inference may be understood as prediction in the ML community, but not in the statistics community. \u21a9 https://christophm.github.io/interpretable-ml-book/ \u21a9 https://en.wikipedia.org/wiki/Vision_transformer \u21a9","title":"References"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/01_feature_map_activation/feature_map_activation/","text":"EDA Visualizations for Image Recognition (Feature Map Activation) Dependencies and Imports from typing import Dict import matplotlib.pyplot as plt import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import timm import torch import torchvision from torchvision.models.feature_extraction import ( create_feature_extractor , get_graph_node_names ) % matplotlib inline import glob import os import random from math import ceil from PIL import Image plt . rcParams [ \"figure.figsize\" ] = 16 , 8 import glob from torchvision.models import * import cv2 import matplotlib.pyplot as plt import numpy as np import PIL from IPython.core.interactiveshell import InteractiveShell from PIL import Image InteractiveShell . ast_node_interactivity = \"all\" # importing modules import urllib.request from typing import * from PIL import Image Import Custom Utils Import utils function as script. % cd .. import utils C:\\Users\\reighns\\reighns_ml\\reighns_ml_blog\\docs\\reighns_ml_journey\\deep_learning\\computer_vision\\general\\neural_network_interpretation Call Config from Utils device = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) logger = utils . init_logger () utils . seed_all () Using Seed Number 1992 Load Images We load the images and plot the original image. image_paths = glob . glob ( \"./images/animals/*.*\" ) # elephant has RGBA idk why so need convert images = list ( map ( lambda x : Image . open ( x ) . convert ( \"RGB\" ), image_paths )) utils . subplot ( images , title = \"inputs\" , rows_titles = [ \"cat\" , \"dog_and_cat\" , \"african_elephant\" ], nrows = 1 , ncols = 3 , ) Transforms Params (ImageNet) class NormalizeInverse ( torchvision . transforms . Normalize ): \"\"\" Undoes the normalization and returns the reconstructed images in the input domain. https://github.com/FrancescoSaverioZuppichini/A-journey-into-Convolutional-Neural-Network-visualization- \"\"\" def __init__ ( self , mean , std ): mean = torch . Tensor ( mean ) std = torch . Tensor ( std ) std_inv = 1 / ( std + 1e-7 ) mean_inv = - mean * std_inv super () . __init__ ( mean = mean_inv , std = std_inv ) def __call__ ( self , tensor ): return super () . __call__ ( tensor . clone ()) imagenet_mean : List [ float ] = [ 0.485 , 0.456 , 0.406 ] imagenet_std : List [ float ] = [ 0.229 , 0.224 , 0.225 ] image_size : int = 224 normalized_transform = torchvision . transforms . Compose ( [ torchvision . transforms . Resize (( image_size , image_size )), torchvision . transforms . ToTensor (), torchvision . transforms . Normalize ( mean = imagenet_mean , std = imagenet_std ), ] ) inverse_normalized_transform = torchvision . transforms . Compose ( [ torchvision . transforms . Resize (( image_size , image_size )), torchvision . transforms . ToTensor (), NormalizeInverse ( mean = imagenet_mean , std = imagenet_std ), ] ) # We use torchvision's transform to transform the cat image with resize and normalization. # Conveniently, also making it channel first! cat_tensor = normalized_transform ( images [ 0 ]) cat_and_dog_tensor = normalized_transform ( images [ 1 ]) elephant_tensor = normalized_transform ( images [ 2 ]) # assert cat_tensor.shape[0] == cat_and_dog_tensor.shape[0] == 3, \"PyTorch expects Channel First!\" # Now feature_extractor expects batch_size x C x H x W, so we expand one dimension in the 0th dim # and put them on device cat_tensor = cat_tensor . unsqueeze ( dim = 0 ) . to ( device ) cat_and_dog_tensor = cat_and_dog_tensor . unsqueeze ( dim = 0 ) . to ( device ) elephant_tensor = elephant_tensor . unsqueeze ( dim = 0 ) . to ( device ) # logger.info(f\"\\n\\ncat_tensor's shape:\\n{cat_tensor.shape}\\n\\ndog_tensor's shape:\\n{cat_and_dog_tensor.shape}\") images_dict : Dict [ str , torch . Tensor ] = { \"cat\" : cat_tensor , \"cat_and_dog\" : cat_and_dog_tensor , \"elephant\" : elephant_tensor } Working with Torch Models Load the Models alexnet_ = alexnet ( pretrained = True ) . to ( device ) vgg16_ = vgg16 ( pretrained = True ) . to ( device ) Torch Summary import torchsummary def torchsummary_wrapper ( model , image_size : Tuple [ int , int , int ] ) -> torchsummary . model_statistics . ModelStatistics : \"\"\"A torch wrapper to print out layers of a Model. Args: model (CustomNeuralNet): Model. image_size (Tuple[int, int, int]): Image size as a tuple of (channels, height, width). Returns: model_summary (torchsummary.model_statistics.ModelStatistics): Model summary. \"\"\" model_summary = torchsummary . summary ( model , image_size ) return model_summary alexnet_model_summary = torchsummary_wrapper ( alexnet_ , image_size = ( 3 , 224 , 224 )) ========================================================================================== Layer (type:depth-idx) Output Shape Param # ========================================================================================== \u251c\u2500Sequential: 1-1 [-1, 256, 6, 6] -- | \u2514\u2500Conv2d: 2-1 [-1, 64, 55, 55] 23,296 | \u2514\u2500ReLU: 2-2 [-1, 64, 55, 55] -- | \u2514\u2500MaxPool2d: 2-3 [-1, 64, 27, 27] -- | \u2514\u2500Conv2d: 2-4 [-1, 192, 27, 27] 307,392 | \u2514\u2500ReLU: 2-5 [-1, 192, 27, 27] -- | \u2514\u2500MaxPool2d: 2-6 [-1, 192, 13, 13] -- | \u2514\u2500Conv2d: 2-7 [-1, 384, 13, 13] 663,936 | \u2514\u2500ReLU: 2-8 [-1, 384, 13, 13] -- | \u2514\u2500Conv2d: 2-9 [-1, 256, 13, 13] 884,992 | \u2514\u2500ReLU: 2-10 [-1, 256, 13, 13] -- | \u2514\u2500Conv2d: 2-11 [-1, 256, 13, 13] 590,080 | \u2514\u2500ReLU: 2-12 [-1, 256, 13, 13] -- | \u2514\u2500MaxPool2d: 2-13 [-1, 256, 6, 6] -- \u251c\u2500AdaptiveAvgPool2d: 1-2 [-1, 256, 6, 6] -- \u251c\u2500Sequential: 1-3 [-1, 1000] -- | \u2514\u2500Dropout: 2-14 [-1, 9216] -- | \u2514\u2500Linear: 2-15 [-1, 4096] 37,752,832 | \u2514\u2500ReLU: 2-16 [-1, 4096] -- | \u2514\u2500Dropout: 2-17 [-1, 4096] -- | \u2514\u2500Linear: 2-18 [-1, 4096] 16,781,312 | \u2514\u2500ReLU: 2-19 [-1, 4096] -- | \u2514\u2500Linear: 2-20 [-1, 1000] 4,097,000 ========================================================================================== Total params: 61,100,840 Trainable params: 61,100,840 Non-trainable params: 0 Total mult-adds (M): 775.28 ========================================================================================== Input size (MB): 0.57 Forward/backward pass size (MB): 3.77 Params size (MB): 233.08 Estimated Total Size (MB): 237.43 ========================================================================================== vgg16_model_summary = torchsummary_wrapper ( vgg16_ , image_size = ( 3 , 224 , 224 )) ========================================================================================== Layer (type:depth-idx) Output Shape Param # ========================================================================================== \u251c\u2500Sequential: 1-1 [-1, 512, 7, 7] -- | \u2514\u2500Conv2d: 2-1 [-1, 64, 224, 224] 1,792 | \u2514\u2500ReLU: 2-2 [-1, 64, 224, 224] -- | \u2514\u2500Conv2d: 2-3 [-1, 64, 224, 224] 36,928 | \u2514\u2500ReLU: 2-4 [-1, 64, 224, 224] -- | \u2514\u2500MaxPool2d: 2-5 [-1, 64, 112, 112] -- | \u2514\u2500Conv2d: 2-6 [-1, 128, 112, 112] 73,856 | \u2514\u2500ReLU: 2-7 [-1, 128, 112, 112] -- | \u2514\u2500Conv2d: 2-8 [-1, 128, 112, 112] 147,584 | \u2514\u2500ReLU: 2-9 [-1, 128, 112, 112] -- | \u2514\u2500MaxPool2d: 2-10 [-1, 128, 56, 56] -- | \u2514\u2500Conv2d: 2-11 [-1, 256, 56, 56] 295,168 | \u2514\u2500ReLU: 2-12 [-1, 256, 56, 56] -- | \u2514\u2500Conv2d: 2-13 [-1, 256, 56, 56] 590,080 | \u2514\u2500ReLU: 2-14 [-1, 256, 56, 56] -- | \u2514\u2500Conv2d: 2-15 [-1, 256, 56, 56] 590,080 | \u2514\u2500ReLU: 2-16 [-1, 256, 56, 56] -- | \u2514\u2500MaxPool2d: 2-17 [-1, 256, 28, 28] -- | \u2514\u2500Conv2d: 2-18 [-1, 512, 28, 28] 1,180,160 | \u2514\u2500ReLU: 2-19 [-1, 512, 28, 28] -- | \u2514\u2500Conv2d: 2-20 [-1, 512, 28, 28] 2,359,808 | \u2514\u2500ReLU: 2-21 [-1, 512, 28, 28] -- | \u2514\u2500Conv2d: 2-22 [-1, 512, 28, 28] 2,359,808 | \u2514\u2500ReLU: 2-23 [-1, 512, 28, 28] -- | \u2514\u2500MaxPool2d: 2-24 [-1, 512, 14, 14] -- | \u2514\u2500Conv2d: 2-25 [-1, 512, 14, 14] 2,359,808 | \u2514\u2500ReLU: 2-26 [-1, 512, 14, 14] -- | \u2514\u2500Conv2d: 2-27 [-1, 512, 14, 14] 2,359,808 | \u2514\u2500ReLU: 2-28 [-1, 512, 14, 14] -- | \u2514\u2500Conv2d: 2-29 [-1, 512, 14, 14] 2,359,808 | \u2514\u2500ReLU: 2-30 [-1, 512, 14, 14] -- | \u2514\u2500MaxPool2d: 2-31 [-1, 512, 7, 7] -- \u251c\u2500AdaptiveAvgPool2d: 1-2 [-1, 512, 7, 7] -- \u251c\u2500Sequential: 1-3 [-1, 1000] -- | \u2514\u2500Linear: 2-32 [-1, 4096] 102,764,544 | \u2514\u2500ReLU: 2-33 [-1, 4096] -- | \u2514\u2500Dropout: 2-34 [-1, 4096] -- | \u2514\u2500Linear: 2-35 [-1, 4096] 16,781,312 | \u2514\u2500ReLU: 2-36 [-1, 4096] -- | \u2514\u2500Dropout: 2-37 [-1, 4096] -- | \u2514\u2500Linear: 2-38 [-1, 1000] 4,097,000 ========================================================================================== Total params: 138,357,544 Trainable params: 138,357,544 Non-trainable params: 0 Total mult-adds (G): 15.61 ========================================================================================== Input size (MB): 0.57 Forward/backward pass size (MB): 103.43 Params size (MB): 527.79 Estimated Total Size (MB): 631.80 ========================================================================================== Forward Backward Hooks def forward_hook ( module_name : str , forward_activations : Dict [ str , torch . Tensor ] ) -> Callable : \"\"\"In-place forward hook to save activations of a layer. Args: module_name (str): The name of the layer to hook. forward_activations (Dict[str, torch.Tensor]): The dictionary to save the activations. Returns: forward_hook_: The forward hook function. \"\"\" def forward_hook_ ( module , input , output ): # Save forward feature map activations forward_activations [ module_name ] = output . detach () return forward_hook_ def backward_hook ( module_name : str , backward_gradients : Dict [ str , torch . Tensor ] ) -> Callable : \"\"\"In-place backward hook to save gradients of a layer. Args: module_name (str): The name of the layer to hook. backward_gradients (Dict[str, torch.Tensor]): The dictionary to save the gradients. Returns: Callable: The backward hook function. \"\"\" def backward_hook_ ( module , grad_input , grad_output ): # Save the gradients correspond to the feature maps # This will only be saved when backwards is called. backward_gradients [ module_name ] = grad_output [ 0 ] . detach () return backward_hook_ We define a function to get feature map activations. def get_feature_maps_activations ( model : Callable , image : torch . Tensor ) -> Union [ List [ Callable ], Dict [ str , torch . Tensor ]]: \"\"\"Get feature maps and activations from a model. Args: model (Callable): A model. image (torch.Tensor): The input image. Returns: handlers List[Callable]: A list of handlers. forward_activations Dict[str, torch.Tensor]: A dictionary of forward activations. \"\"\" forward_activations = OrderedDict () handlers = [] for name , module in model . named_modules (): module_name = name + \"_\" + str ( module ) handlers . append ( module . register_forward_hook ( forward_hook ( module_name , forward_activations ) ) ) model = model . eval () y_logits = model ( image ) return handlers , forward_activations _ , alexnet_f = get_feature_maps_activations ( alexnet_ , image = images_dict [ \"cat\" ]) _ , vgg16_f = get_feature_maps_activations ( vgg16_ , image = images_dict [ \"cat\" ]) First Conv Layer Output Alexnet We will get the first convolutional layer's output as follows. We note the following stats: There are 64 kernels. Each kernel is of size 11 by 11. You can assume that each kernel is a 2d-image in grayscale. Therefore, the first conv layer output is also a stack of 64 2d-image, output by the 64 kernels respectively. We traditionally call them feature maps . Each 2d output is of 55 by 55 shape. You can assume that this is a 2d-image with dimensions 55 by 55. Note you can calculate by hand that 55 by 55 is correct. We can squeeze the first dimension as this is not needed since we are dealing with 1 image. first_conv_layer_output = alexnet_f [ \"features.0_Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\" ] print ( first_conv_layer_output . shape ) first_conv_layer_output = first_conv_layer_output . squeeze ( dim = 0 ) print ( first_conv_layer_output . shape ) torch.Size([1, 64, 55, 55]) torch.Size([64, 55, 55]) For plotting, we will use the repo 's code verbatim. Note we set number of columns and rows to be 8 by 8 to see all 64 feature maps. plt . rcParams [ \"figure.figsize\" ] = 32 , 24 utils . subplot ( first_conv_layer_output , utils . tensor2img , title = \"features.0_Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\" , ncols = 8 , nrows = 8 ) # plt.rcParams[\"figure.figsize\"]= 32, 24 # utils.subplot(f[\"features.2_MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\"][0], utils.tensor2img, title=\"eatures.2_MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\", ncols=8, nrows=8) First Conv Layer Output VGG16 first_conv_layer_output = vgg16_f [ \"features.0_Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\" ] print ( first_conv_layer_output . shape ) first_conv_layer_output = first_conv_layer_output . squeeze ( dim = 0 ) print ( first_conv_layer_output . shape ) torch.Size([1, 64, 224, 224]) torch.Size([64, 224, 224]) plt . rcParams [ \"figure.figsize\" ] = 32 , 24 utils . subplot ( first_conv_layer_output , utils . tensor2img , title = \"features.0_Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\" , ncols = 8 , nrows = 8 ) Mean and Sum Reduce of the Feature Maps In the above section, if there are 64 feature maps after a conv layer, we will plot all 64 feauture maps. In the next section, we can either sum or average all 64 feature maps to reduce them to 1 single feature map.","title":"Feature Map Activations (Part I)"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/01_feature_map_activation/feature_map_activation/#eda-visualizations-for-image-recognition-feature-map-activation","text":"","title":"EDA Visualizations for Image Recognition (Feature Map Activation)"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/01_feature_map_activation/feature_map_activation/#dependencies-and-imports","text":"from typing import Dict import matplotlib.pyplot as plt import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import timm import torch import torchvision from torchvision.models.feature_extraction import ( create_feature_extractor , get_graph_node_names ) % matplotlib inline import glob import os import random from math import ceil from PIL import Image plt . rcParams [ \"figure.figsize\" ] = 16 , 8 import glob from torchvision.models import * import cv2 import matplotlib.pyplot as plt import numpy as np import PIL from IPython.core.interactiveshell import InteractiveShell from PIL import Image InteractiveShell . ast_node_interactivity = \"all\" # importing modules import urllib.request from typing import * from PIL import Image","title":"Dependencies and Imports"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/01_feature_map_activation/feature_map_activation/#import-custom-utils","text":"Import utils function as script. % cd .. import utils C:\\Users\\reighns\\reighns_ml\\reighns_ml_blog\\docs\\reighns_ml_journey\\deep_learning\\computer_vision\\general\\neural_network_interpretation","title":"Import Custom Utils"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/01_feature_map_activation/feature_map_activation/#call-config-from-utils","text":"device = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) logger = utils . init_logger () utils . seed_all () Using Seed Number 1992","title":"Call Config from Utils"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/01_feature_map_activation/feature_map_activation/#load-images","text":"We load the images and plot the original image. image_paths = glob . glob ( \"./images/animals/*.*\" ) # elephant has RGBA idk why so need convert images = list ( map ( lambda x : Image . open ( x ) . convert ( \"RGB\" ), image_paths )) utils . subplot ( images , title = \"inputs\" , rows_titles = [ \"cat\" , \"dog_and_cat\" , \"african_elephant\" ], nrows = 1 , ncols = 3 , )","title":"Load Images"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/01_feature_map_activation/feature_map_activation/#transforms-params-imagenet","text":"class NormalizeInverse ( torchvision . transforms . Normalize ): \"\"\" Undoes the normalization and returns the reconstructed images in the input domain. https://github.com/FrancescoSaverioZuppichini/A-journey-into-Convolutional-Neural-Network-visualization- \"\"\" def __init__ ( self , mean , std ): mean = torch . Tensor ( mean ) std = torch . Tensor ( std ) std_inv = 1 / ( std + 1e-7 ) mean_inv = - mean * std_inv super () . __init__ ( mean = mean_inv , std = std_inv ) def __call__ ( self , tensor ): return super () . __call__ ( tensor . clone ()) imagenet_mean : List [ float ] = [ 0.485 , 0.456 , 0.406 ] imagenet_std : List [ float ] = [ 0.229 , 0.224 , 0.225 ] image_size : int = 224 normalized_transform = torchvision . transforms . Compose ( [ torchvision . transforms . Resize (( image_size , image_size )), torchvision . transforms . ToTensor (), torchvision . transforms . Normalize ( mean = imagenet_mean , std = imagenet_std ), ] ) inverse_normalized_transform = torchvision . transforms . Compose ( [ torchvision . transforms . Resize (( image_size , image_size )), torchvision . transforms . ToTensor (), NormalizeInverse ( mean = imagenet_mean , std = imagenet_std ), ] ) # We use torchvision's transform to transform the cat image with resize and normalization. # Conveniently, also making it channel first! cat_tensor = normalized_transform ( images [ 0 ]) cat_and_dog_tensor = normalized_transform ( images [ 1 ]) elephant_tensor = normalized_transform ( images [ 2 ]) # assert cat_tensor.shape[0] == cat_and_dog_tensor.shape[0] == 3, \"PyTorch expects Channel First!\" # Now feature_extractor expects batch_size x C x H x W, so we expand one dimension in the 0th dim # and put them on device cat_tensor = cat_tensor . unsqueeze ( dim = 0 ) . to ( device ) cat_and_dog_tensor = cat_and_dog_tensor . unsqueeze ( dim = 0 ) . to ( device ) elephant_tensor = elephant_tensor . unsqueeze ( dim = 0 ) . to ( device ) # logger.info(f\"\\n\\ncat_tensor's shape:\\n{cat_tensor.shape}\\n\\ndog_tensor's shape:\\n{cat_and_dog_tensor.shape}\") images_dict : Dict [ str , torch . Tensor ] = { \"cat\" : cat_tensor , \"cat_and_dog\" : cat_and_dog_tensor , \"elephant\" : elephant_tensor }","title":"Transforms Params (ImageNet)"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/01_feature_map_activation/feature_map_activation/#working-with-torch-models","text":"","title":"Working with Torch Models"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/01_feature_map_activation/feature_map_activation/#load-the-models","text":"alexnet_ = alexnet ( pretrained = True ) . to ( device ) vgg16_ = vgg16 ( pretrained = True ) . to ( device )","title":"Load the Models"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/01_feature_map_activation/feature_map_activation/#torch-summary","text":"import torchsummary def torchsummary_wrapper ( model , image_size : Tuple [ int , int , int ] ) -> torchsummary . model_statistics . ModelStatistics : \"\"\"A torch wrapper to print out layers of a Model. Args: model (CustomNeuralNet): Model. image_size (Tuple[int, int, int]): Image size as a tuple of (channels, height, width). Returns: model_summary (torchsummary.model_statistics.ModelStatistics): Model summary. \"\"\" model_summary = torchsummary . summary ( model , image_size ) return model_summary alexnet_model_summary = torchsummary_wrapper ( alexnet_ , image_size = ( 3 , 224 , 224 )) ========================================================================================== Layer (type:depth-idx) Output Shape Param # ========================================================================================== \u251c\u2500Sequential: 1-1 [-1, 256, 6, 6] -- | \u2514\u2500Conv2d: 2-1 [-1, 64, 55, 55] 23,296 | \u2514\u2500ReLU: 2-2 [-1, 64, 55, 55] -- | \u2514\u2500MaxPool2d: 2-3 [-1, 64, 27, 27] -- | \u2514\u2500Conv2d: 2-4 [-1, 192, 27, 27] 307,392 | \u2514\u2500ReLU: 2-5 [-1, 192, 27, 27] -- | \u2514\u2500MaxPool2d: 2-6 [-1, 192, 13, 13] -- | \u2514\u2500Conv2d: 2-7 [-1, 384, 13, 13] 663,936 | \u2514\u2500ReLU: 2-8 [-1, 384, 13, 13] -- | \u2514\u2500Conv2d: 2-9 [-1, 256, 13, 13] 884,992 | \u2514\u2500ReLU: 2-10 [-1, 256, 13, 13] -- | \u2514\u2500Conv2d: 2-11 [-1, 256, 13, 13] 590,080 | \u2514\u2500ReLU: 2-12 [-1, 256, 13, 13] -- | \u2514\u2500MaxPool2d: 2-13 [-1, 256, 6, 6] -- \u251c\u2500AdaptiveAvgPool2d: 1-2 [-1, 256, 6, 6] -- \u251c\u2500Sequential: 1-3 [-1, 1000] -- | \u2514\u2500Dropout: 2-14 [-1, 9216] -- | \u2514\u2500Linear: 2-15 [-1, 4096] 37,752,832 | \u2514\u2500ReLU: 2-16 [-1, 4096] -- | \u2514\u2500Dropout: 2-17 [-1, 4096] -- | \u2514\u2500Linear: 2-18 [-1, 4096] 16,781,312 | \u2514\u2500ReLU: 2-19 [-1, 4096] -- | \u2514\u2500Linear: 2-20 [-1, 1000] 4,097,000 ========================================================================================== Total params: 61,100,840 Trainable params: 61,100,840 Non-trainable params: 0 Total mult-adds (M): 775.28 ========================================================================================== Input size (MB): 0.57 Forward/backward pass size (MB): 3.77 Params size (MB): 233.08 Estimated Total Size (MB): 237.43 ========================================================================================== vgg16_model_summary = torchsummary_wrapper ( vgg16_ , image_size = ( 3 , 224 , 224 )) ========================================================================================== Layer (type:depth-idx) Output Shape Param # ========================================================================================== \u251c\u2500Sequential: 1-1 [-1, 512, 7, 7] -- | \u2514\u2500Conv2d: 2-1 [-1, 64, 224, 224] 1,792 | \u2514\u2500ReLU: 2-2 [-1, 64, 224, 224] -- | \u2514\u2500Conv2d: 2-3 [-1, 64, 224, 224] 36,928 | \u2514\u2500ReLU: 2-4 [-1, 64, 224, 224] -- | \u2514\u2500MaxPool2d: 2-5 [-1, 64, 112, 112] -- | \u2514\u2500Conv2d: 2-6 [-1, 128, 112, 112] 73,856 | \u2514\u2500ReLU: 2-7 [-1, 128, 112, 112] -- | \u2514\u2500Conv2d: 2-8 [-1, 128, 112, 112] 147,584 | \u2514\u2500ReLU: 2-9 [-1, 128, 112, 112] -- | \u2514\u2500MaxPool2d: 2-10 [-1, 128, 56, 56] -- | \u2514\u2500Conv2d: 2-11 [-1, 256, 56, 56] 295,168 | \u2514\u2500ReLU: 2-12 [-1, 256, 56, 56] -- | \u2514\u2500Conv2d: 2-13 [-1, 256, 56, 56] 590,080 | \u2514\u2500ReLU: 2-14 [-1, 256, 56, 56] -- | \u2514\u2500Conv2d: 2-15 [-1, 256, 56, 56] 590,080 | \u2514\u2500ReLU: 2-16 [-1, 256, 56, 56] -- | \u2514\u2500MaxPool2d: 2-17 [-1, 256, 28, 28] -- | \u2514\u2500Conv2d: 2-18 [-1, 512, 28, 28] 1,180,160 | \u2514\u2500ReLU: 2-19 [-1, 512, 28, 28] -- | \u2514\u2500Conv2d: 2-20 [-1, 512, 28, 28] 2,359,808 | \u2514\u2500ReLU: 2-21 [-1, 512, 28, 28] -- | \u2514\u2500Conv2d: 2-22 [-1, 512, 28, 28] 2,359,808 | \u2514\u2500ReLU: 2-23 [-1, 512, 28, 28] -- | \u2514\u2500MaxPool2d: 2-24 [-1, 512, 14, 14] -- | \u2514\u2500Conv2d: 2-25 [-1, 512, 14, 14] 2,359,808 | \u2514\u2500ReLU: 2-26 [-1, 512, 14, 14] -- | \u2514\u2500Conv2d: 2-27 [-1, 512, 14, 14] 2,359,808 | \u2514\u2500ReLU: 2-28 [-1, 512, 14, 14] -- | \u2514\u2500Conv2d: 2-29 [-1, 512, 14, 14] 2,359,808 | \u2514\u2500ReLU: 2-30 [-1, 512, 14, 14] -- | \u2514\u2500MaxPool2d: 2-31 [-1, 512, 7, 7] -- \u251c\u2500AdaptiveAvgPool2d: 1-2 [-1, 512, 7, 7] -- \u251c\u2500Sequential: 1-3 [-1, 1000] -- | \u2514\u2500Linear: 2-32 [-1, 4096] 102,764,544 | \u2514\u2500ReLU: 2-33 [-1, 4096] -- | \u2514\u2500Dropout: 2-34 [-1, 4096] -- | \u2514\u2500Linear: 2-35 [-1, 4096] 16,781,312 | \u2514\u2500ReLU: 2-36 [-1, 4096] -- | \u2514\u2500Dropout: 2-37 [-1, 4096] -- | \u2514\u2500Linear: 2-38 [-1, 1000] 4,097,000 ========================================================================================== Total params: 138,357,544 Trainable params: 138,357,544 Non-trainable params: 0 Total mult-adds (G): 15.61 ========================================================================================== Input size (MB): 0.57 Forward/backward pass size (MB): 103.43 Params size (MB): 527.79 Estimated Total Size (MB): 631.80 ==========================================================================================","title":"Torch Summary"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/01_feature_map_activation/feature_map_activation/#forward-backward-hooks","text":"def forward_hook ( module_name : str , forward_activations : Dict [ str , torch . Tensor ] ) -> Callable : \"\"\"In-place forward hook to save activations of a layer. Args: module_name (str): The name of the layer to hook. forward_activations (Dict[str, torch.Tensor]): The dictionary to save the activations. Returns: forward_hook_: The forward hook function. \"\"\" def forward_hook_ ( module , input , output ): # Save forward feature map activations forward_activations [ module_name ] = output . detach () return forward_hook_ def backward_hook ( module_name : str , backward_gradients : Dict [ str , torch . Tensor ] ) -> Callable : \"\"\"In-place backward hook to save gradients of a layer. Args: module_name (str): The name of the layer to hook. backward_gradients (Dict[str, torch.Tensor]): The dictionary to save the gradients. Returns: Callable: The backward hook function. \"\"\" def backward_hook_ ( module , grad_input , grad_output ): # Save the gradients correspond to the feature maps # This will only be saved when backwards is called. backward_gradients [ module_name ] = grad_output [ 0 ] . detach () return backward_hook_ We define a function to get feature map activations. def get_feature_maps_activations ( model : Callable , image : torch . Tensor ) -> Union [ List [ Callable ], Dict [ str , torch . Tensor ]]: \"\"\"Get feature maps and activations from a model. Args: model (Callable): A model. image (torch.Tensor): The input image. Returns: handlers List[Callable]: A list of handlers. forward_activations Dict[str, torch.Tensor]: A dictionary of forward activations. \"\"\" forward_activations = OrderedDict () handlers = [] for name , module in model . named_modules (): module_name = name + \"_\" + str ( module ) handlers . append ( module . register_forward_hook ( forward_hook ( module_name , forward_activations ) ) ) model = model . eval () y_logits = model ( image ) return handlers , forward_activations _ , alexnet_f = get_feature_maps_activations ( alexnet_ , image = images_dict [ \"cat\" ]) _ , vgg16_f = get_feature_maps_activations ( vgg16_ , image = images_dict [ \"cat\" ])","title":"Forward Backward Hooks"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/01_feature_map_activation/feature_map_activation/#first-conv-layer-output-alexnet","text":"We will get the first convolutional layer's output as follows. We note the following stats: There are 64 kernels. Each kernel is of size 11 by 11. You can assume that each kernel is a 2d-image in grayscale. Therefore, the first conv layer output is also a stack of 64 2d-image, output by the 64 kernels respectively. We traditionally call them feature maps . Each 2d output is of 55 by 55 shape. You can assume that this is a 2d-image with dimensions 55 by 55. Note you can calculate by hand that 55 by 55 is correct. We can squeeze the first dimension as this is not needed since we are dealing with 1 image. first_conv_layer_output = alexnet_f [ \"features.0_Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\" ] print ( first_conv_layer_output . shape ) first_conv_layer_output = first_conv_layer_output . squeeze ( dim = 0 ) print ( first_conv_layer_output . shape ) torch.Size([1, 64, 55, 55]) torch.Size([64, 55, 55]) For plotting, we will use the repo 's code verbatim. Note we set number of columns and rows to be 8 by 8 to see all 64 feature maps. plt . rcParams [ \"figure.figsize\" ] = 32 , 24 utils . subplot ( first_conv_layer_output , utils . tensor2img , title = \"features.0_Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\" , ncols = 8 , nrows = 8 ) # plt.rcParams[\"figure.figsize\"]= 32, 24 # utils.subplot(f[\"features.2_MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\"][0], utils.tensor2img, title=\"eatures.2_MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\", ncols=8, nrows=8)","title":"First Conv Layer Output Alexnet"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/01_feature_map_activation/feature_map_activation/#first-conv-layer-output-vgg16","text":"first_conv_layer_output = vgg16_f [ \"features.0_Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\" ] print ( first_conv_layer_output . shape ) first_conv_layer_output = first_conv_layer_output . squeeze ( dim = 0 ) print ( first_conv_layer_output . shape ) torch.Size([1, 64, 224, 224]) torch.Size([64, 224, 224]) plt . rcParams [ \"figure.figsize\" ] = 32 , 24 utils . subplot ( first_conv_layer_output , utils . tensor2img , title = \"features.0_Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\" , ncols = 8 , nrows = 8 )","title":"First Conv Layer Output VGG16"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/01_feature_map_activation/feature_map_activation/#mean-and-sum-reduce-of-the-feature-maps","text":"In the above section, if there are 64 feature maps after a conv layer, we will plot all 64 feauture maps. In the next section, we can either sum or average all 64 feature maps to reduce them to 1 single feature map.","title":"Mean and Sum Reduce of the Feature Maps"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/01_feature_map_activation/feature_map_activation_reduced/","text":"EDA Visualizations for Image Recognition (Conv Filter Edition) Dependencies and Imports ! pip install - q timm from typing import Dict import matplotlib.pyplot as plt import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import timm import torch import torchvision from torchvision.models.feature_extraction import ( create_feature_extractor , get_graph_node_names ) % matplotlib inline import glob import os from math import ceil import random import cv2 import PIL from IPython.core.interactiveshell import InteractiveShell InteractiveShell . ast_node_interactivity = \"all\" from typing import * # importing modules import urllib.request from PIL import Image Config and Logging import logging from logging import INFO , FileHandler , Formatter , StreamHandler , getLogger def init_logger ( log_file : str = \"info.log\" ) -> logging . Logger : \"\"\"Initialize logger and save to file. Consider having more log_file paths to save, eg: debug.log, error.log, etc. Args: log_file (str, optional): [description]. Defaults to Path(LOGS_DIR, \"info.log\"). Returns: logging.Logger: [description] \"\"\" logger = getLogger ( __name__ ) logger . setLevel ( INFO ) stream_handler = StreamHandler () stream_handler . setFormatter ( Formatter ( \" %(asctime)s : %(message)s \" , \"%Y-%m- %d %H:%M:%S\" ) ) file_handler = FileHandler ( filename = log_file ) file_handler . setFormatter ( Formatter ( \" %(asctime)s : %(message)s \" , \"%Y-%m- %d %H:%M:%S\" ) ) logger . addHandler ( stream_handler ) logger . addHandler ( file_handler ) return logger logger = init_logger () Utils def plot_multiple_img ( img_matrix_list , title_list , ncols , main_title = \"\" ): fig , myaxes = plt . subplots ( figsize = ( 20 , 15 ), nrows = ceil ( len ( img_matrix_list ) / ncols ), ncols = ncols , squeeze = False , ) fig . suptitle ( main_title , fontsize = 30 ) fig . subplots_adjust ( wspace = 0.3 ) fig . subplots_adjust ( hspace = 0.3 ) for i , ( img , title ) in enumerate ( zip ( img_matrix_list , title_list )): myaxes [ i // ncols ][ i % ncols ] . imshow ( img ) myaxes [ i // ncols ][ i % ncols ] . set_title ( title , fontsize = 15 ) plt . show () Seeding def seed_all ( seed : int = 1992 ) -> None : \"\"\"Seed all random number generators.\"\"\" print ( f \"Using Seed Number { seed } \" ) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) # set PYTHONHASHSEED env var at fixed value torch . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) torch . cuda . manual_seed ( seed ) # pytorch (both CPU and CUDA) np . random . seed ( seed ) # for numpy pseudo-random generator # set fixed value for python built-in pseudo-random generator random . seed ( seed ) torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = True torch . backends . cudnn . enabled = True def seed_worker ( _worker_id ) -> None : \"\"\"Seed a worker with the given ID.\"\"\" worker_seed = torch . initial_seed () % 2 ** 32 np . random . seed ( worker_seed ) random . seed ( worker_seed ) seed_all () Using Seed Number 1992 Transforms Params mean : List [ float ] = [ 0.485 , 0.456 , 0.406 ] std : List [ float ] = [ 0.229 , 0.224 , 0.225 ] image_size : int = 224 transform = torchvision . transforms . Compose ( [ torchvision . transforms . Resize (( image_size , image_size )), torchvision . transforms . ToTensor (), torchvision . transforms . Normalize ( mean = mean , std = std ), ] ) pre_normalize_transform = torchvision . transforms . Compose ( [ torchvision . transforms . Resize (( image_size , image_size )), torchvision . transforms . ToTensor (), ] ) Visualizations cat_p = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/deep_learning/computer_vision/data/misc/cat.jpg\" dog_p = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/deep_learning/computer_vision/data/misc/dog.jpg\" from urllib.request import urlopen # plot cat and dog with title using PIL plt . figure ( figsize = ( 10 , 10 )) plt . subplot ( 1 , 2 , 1 ) cat = PIL . Image . open ( urlopen ( cat_p )) plt . imshow ( cat . resize (( 1024 , 1024 ))) plt . title ( \"Cat\" ) plt . subplot ( 1 , 2 , 2 ) dog = PIL . Image . open ( urlopen ( dog_p )) plt . imshow ( dog . resize (( 1024 , 1024 ))) plt . title ( \"Dog\" ) plt . show (); Convolution Layers The image and content are referenced with courtesy from [Tarun's notebook](https://www.kaggle.com/tarunpaparaju/plant-pathology-2020-eda-models](https://www.kaggle.com/tarunpaparaju/plant-pathology-2020-eda-models). Convolution is a rather simple algorithm which involves a kernel (a 2D matrix) which moves over the entire image, calculating dot products with each window along the way. The GIF below demonstrates convolution in action. The above process can be summarized with an equation, where f is the image and h is the kernel. The dimensions of f are (m, n) and the kernel is a square matrix with dimensions smaller than f : \\( \\(\\text{conv}(f, h) = \\sum_{j}\\sum_{k}h_{jk} \\cdot f_{(m-j)(n-k)}\\) \\) In the above equation, the kernel h is moving across the length and breadth of the image. The dot product of h with a sub-matrix or window of matrix f is taken at each step, hence the double summation (rows and columns). I have always remembered from the revered Andrew Ng about how he taught us about what convolutional layers do. In the beginning, the conv layers are of low level abstraction, detailing a image's features such as shapes and sizes. In particular, he described to us the horizontal and vertical conv filters. As the conv layers go later, it will pick up on many abstract features, which is not really easily distinguished by human eyes. Below, we see an example of horizontal and vertical filters. def conv_horizontal ( image : np . ndarray ) -> None : \"\"\"Plot the horizontal convolution of the image. Args: image (torch.Tensor): [description] \"\"\" fig , ax = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 20 , 20 )) kernel = np . ones (( 3 , 3 ), np . float32 ) kernel [ 1 ] = np . array ([ 0 , 0 , 0 ], np . float32 ) kernel [ 2 ] = np . array ([ - 1 , - 1 , - 1 ], np . float32 ) conv = cv2 . filter2D ( image , - 1 , kernel ) ax [ 0 ] . imshow ( image ) ax [ 0 ] . set_title ( \"Original Image\" , fontsize = 24 ) ax [ 1 ] . imshow ( conv ) ax [ 1 ] . set_title ( \"Convolved Image with horizontal edges\" , fontsize = 24 ) plt . show () def conv_vertical ( image : np . ndarray ) -> None : \"\"\"Plot the vertical convolution of the image. Args: image (torch.Tensor): [description] \"\"\" fig , ax = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 20 , 20 )) kernel = np . ones (( 3 , 3 ), np . float32 ) kernel [ 0 ] = np . array ([ 1 , 0 , - 1 ]) kernel [ 1 ] = np . array ([ 1 , 0 , - 1 ]) kernel [ 2 ] = np . array ([ 1 , 0 , - 1 ]) conv = cv2 . filter2D ( image , - 1 , kernel ) ax [ 0 ] . imshow ( image ) ax [ 0 ] . set_title ( \"Original Image\" , fontsize = 24 ) ax [ 1 ] . imshow ( conv ) ax [ 1 ] . set_title ( \"Convolved Image with vertical edges\" , fontsize = 24 ) plt . show () Well, I can easily make out the horizontal and vertical edges from the dog image! Actually, not so obvious if you don't look closely! conv_horizontal ( np . asarray ( dog )) conv_vertical ( np . asarray ( dog )) The issue is, I want to visualize what our models' conv layers are seeing, like for example, the first conv layer usually has 64 filters, that is a whooping 64 different combinations of filters, each doing a slightly different thing. A mental model that I have for the first conv layer looks something like the following. conv_1_filters = [ \"vertical edge detector\" , \"horizontal edge detector\" , \"slanted 45 degrees detector\" , \"slanted 180 degrees detector\" , ... ] Feature Extractor using PyTorch's native Feature Extraction Module In order to visualize properly, I made use of PyTorch's newest feature_extraction module to do so. Note that the new feature is still in development, but it does make my life easier and reduces overhead. I no longer need use hooks or what not to plot layer information! We just need to import from torchvision.models.feature_extraction import ( create_feature_extractor , get_graph_node_names ) def get_conv_layers ( model : torchvision . models ) -> Dict [ str , str ]: \"\"\"Create a function that give me the conv layers of PyTorch model. Args: model (Union[torchvision.models, timm.models]): A PyTorch model. Returns: conv_layers (Dict[str, str]): {\"layer1.0.conv1\": layer1.0.conv1, ...} \"\"\" conv_layers = {} for name , layer in model . named_modules (): if isinstance ( layer , torch . nn . Conv2d ): conv_layers [ name ] = name return conv_layers def get_feature_maps ( model_name : str , image : torch . Tensor , reduction : str = \"mean\" , pretrained : bool = True ) -> Union [ Dict [ str , torch . Tensor ], List [ torch . Tensor ], List [ str ]]: \"\"\"Function to plot feature maps from PyTorch models. Args: model_name (str): Name of the model to use. image (torch.Tensor): image should be a tensor of shape (1, 3, H, W) reduction (str, optional): Defaults to \"mean\". One of [\"mean\", \"max\", \"sum\"] pretrained (bool): whether the model is pretrained or not Raises: ValueError: Must use Torchvision models. Returns: model_feature_maps (Dict[str, torch.Tensor]): {\"conv_1\": conv_1_feature_map, ...} processed_feature_maps (List[torch.Tensor]): [conv_1_feature_map, ...] processed using a reduction method. feature_map_names (List[str]): [conv_1, ...] Example: >>> from torchvision.models.vgg import vgg16 >>> model = vgg16(pretrained=True) >>> image = torch.rand(1, 3, 224, 224) >>> feature_maps = get_feature_maps(model, image, reduction=\"mean\") Reduction: If a feature map has 4 filters, in the shape of (4, H, W) = (4, 32, 32), then the reduction can be done as follows: >>> reduction = \"mean\": There are 4 filters in this feature map, you can imagine it as 4 32x32 images. We sum up all 4 filters channel wise and get a single 32x32 image. i.e filter 1-4's (0, 0) coordinate has pixel say 1,2,3,4, respectively, then sum up channel wise means 1+2+3+4 = 10 and Then we take the mean of all 32x32 images by dividing by num of kernels to get a single 32x32 image, which is reduction=\"mean\" which means 10 / 4 = 2.5 for that pixel at (0, 0) \"\"\" try : model = getattr ( torchvision . models , model_name )( pretrained = pretrained ) except AttributeError : raise ValueError ( f \"Model { model_name } not found.\" ) train_nodes , eval_nodes = get_graph_node_names ( model ) logger . info ( f \"The train nodes of the model graph is: \\n\\n { train_nodes } \" ) return_conv_nodes = get_conv_layers ( model ) feature_extractor = create_feature_extractor ( model , return_nodes = return_conv_nodes ) # `model_feature_maps` will be a dict of Tensors, each representing a feature map model_feature_maps = feature_extractor ( image ) processed_feature_maps = [] feature_map_names = [] for conv_name , conv_feature_map in model_feature_maps . items (): conv_feature_map = conv_feature_map . squeeze ( dim = 0 ) num_filters = conv_feature_map . shape [ 0 ] if reduction == \"mean\" : gray_scale = torch . sum ( conv_feature_map , dim = 0 ) / num_filters elif reduction == \"max\" : gray_scale = torch . max ( conv_feature_map , dim = 0 ) elif reduction == \"sum\" : gray_scale = torch . sum ( conv_feature_map , dim = 0 ) processed_feature_maps . append ( gray_scale . data . cpu () . numpy ()) feature_map_names . append ( conv_name ) return model_feature_maps , processed_feature_maps , feature_map_names Visualizing VGG16 and ResNet18 Step 1: Initialize the models. As of now, I recommend using torchvision 's models. Ideally, I will want to use timm library for a more detailed list, but there are some bugs that is not easily integrated with the module. device = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) import torchvision.models as models vgg16_pretrained_true = models . vgg16 ( pretrained = True ) vgg16_pretrained_true = vgg16_pretrained_true . to ( device ) resnet18_pretrained_true = models . resnet18 ( pretrained = True ) resnet18_pretrained_true = resnet18_pretrained_true . to ( device ) # Get node names train_nodes , eval_nodes = get_graph_node_names ( vgg16_pretrained_true ) logger . info ( f \"Train nodes of VGG16: \\n\\n { train_nodes } \" ) train_nodes , eval_nodes = get_graph_node_names ( resnet18_pretrained_true ) logger . info ( f \"Train nodes of ResNet18: \\n\\n { train_nodes } \" ) 2021-12-29 19:06:08: Train nodes of VGG16: ['x', 'features.0', 'features.1', 'features.2', 'features.3', 'features.4', 'features.5', 'features.6', 'features.7', 'features.8', 'features.9', 'features.10', 'features.11', 'features.12', 'features.13', 'features.14', 'features.15', 'features.16', 'features.17', 'features.18', 'features.19', 'features.20', 'features.21', 'features.22', 'features.23', 'features.24', 'features.25', 'features.26', 'features.27', 'features.28', 'features.29', 'features.30', 'avgpool', 'flatten', 'classifier.0', 'classifier.1', 'classifier.2', 'classifier.3', 'classifier.4', 'classifier.5', 'classifier.6'] 2021-12-29 19:06:08: Train nodes of ResNet18: ['x', 'conv1', 'bn1', 'relu', 'maxpool', 'layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu', 'layer1.0.conv2', 'layer1.0.bn2', 'layer1.0.add', 'layer1.0.relu_1', 'layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu', 'layer1.1.conv2', 'layer1.1.bn2', 'layer1.1.add', 'layer1.1.relu_1', 'layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu', 'layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.downsample.0', 'layer2.0.downsample.1', 'layer2.0.add', 'layer2.0.relu_1', 'layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.relu', 'layer2.1.conv2', 'layer2.1.bn2', 'layer2.1.add', 'layer2.1.relu_1', 'layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.relu', 'layer3.0.conv2', 'layer3.0.bn2', 'layer3.0.downsample.0', 'layer3.0.downsample.1', 'layer3.0.add', 'layer3.0.relu_1', 'layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.relu', 'layer3.1.conv2', 'layer3.1.bn2', 'layer3.1.add', 'layer3.1.relu_1', 'layer4.0.conv1', 'layer4.0.bn1', 'layer4.0.relu', 'layer4.0.conv2', 'layer4.0.bn2', 'layer4.0.downsample.0', 'layer4.0.downsample.1', 'layer4.0.add', 'layer4.0.relu_1', 'layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.relu', 'layer4.1.conv2', 'layer4.1.bn2', 'layer4.1.add', 'layer4.1.relu_1', 'avgpool', 'flatten', 'fc'] Good God! When I saw the layer names from vgg16 , I nearly fainted, I see no easy way to know which layer belongs to a Conv layer. I understand that get_graph_node_names will get all the nodes on the model's graph, but it is difficult to map the node names to a layer if it is named as such, seeing resnet18 's node names is much easier for one to identify which is conv layer or not. train_nodes , eval_nodes = get_graph_node_names ( model ) logger . info ( f \"The train nodes of the model graph is: \\n\\n { train_nodes } \" ) Thus I wrote a small function get_conv_layers to get the conv layer names. It is not perfect, as downsample layers (1x1 conv layers) are tagged under Conv2d but we may not really need to use them to visualize our feature maps. One can tweak a bit if need be, but for now, I will get all layers that use the Conv2d blocks. If the feature names in vgg16 are named with conv, then we can simply use a small loop below to find the conv layer names. conv_layers = [] for node in nodes : if \"conv\" in node : conv_layers . append ( node ) I actually thought ResNet18 has 18 conv layers, but even minusing to 3 downsample layers, it's 17 conv layers, wonder why? Step 2: Transform the Tensors The PyTorch feature_extraction expects the image input to be of shape [B,C,H,W] . # We use torchvision's transform to transform the cat image to channels first. cat_tensor = transform ( cat ) # Now feature_extractor expects batch_size x C x H x W, so we expand one dimension in the 0th dim cat_tensor = cat_tensor . unsqueeze ( dim = 0 ) . to ( device ) # We use torchvision's transform to transform the cat image with resize and normalization. # Conveniently, also making it channel first! cat_tensor = transform ( cat ) dog_tensor = transform ( dog ) assert cat_tensor . shape [ 0 ] == dog_tensor . shape [ 0 ] == 3 , \"PyTorch expects Channel First!\" # Now feature_extractor expects batch_size x C x H x W, so we expand one dimension in the 0th dim cat_tensor = cat_tensor . unsqueeze ( dim = 0 ) . to ( device ) dog_tensor = dog_tensor . unsqueeze ( dim = 0 ) . to ( device ) logger . info ( f \" \\n\\n cat_tensor's shape: \\n { cat_tensor . shape } \\n\\n dog_tensor's shape: \\n { dog_tensor . shape } \" ) 2021-12-29 19:06:10: cat_tensor's shape: torch.Size([1, 3, 224, 224]) dog_tensor's shape: torch.Size([1, 3, 224, 224]) Step 3: Plotting the Feature Maps We first walk through get_feature_maps and see what my function is doing. # Get node names train_nodes , eval_nodes = get_graph_node_names ( model ) # Since get node names do not indicate properly which is a conv layer or not, # we use get_conv_layer instead to do the job, which returns a dict {\"conv_layer_name\": \"conv_layer_name\"} return_conv_nodes = get_conv_layers ( model ) # call create_feature_extractor on the model and its corresponding conv layer names. feature_extractor = create_feature_extractor ( model , return_nodes = return_conv_nodes ) # `model_feature_maps` will be a dict of Tensors, each representing a feature map # {\"conv_layer_1\": output filter map,...} model_feature_maps = feature_extractor ( image ) # we need to further process the feature maps processed_feature_maps , feature_map_names = [], [] for conv_name , conv_feature_map in model_feature_maps . items (): # Squeeze the dimension from [1, 64, 32, 32] to [64, 32, 32] # This means we have 64 filters of 32x32 \"images\" or kernels conv_feature_map = conv_feature_map . squeeze ( dim = 0 ) # get number of feature/kernels in this layer num_filters = conv_feature_map . shape [ 0 ] # If a feature map has 4 filters, in the shape of (4, H, W) = (4, 32, 32), then the reduction mean can be done as follows: There are 4 filters in this feature map, you can imagine it as 4 32x32 images. # Step 1: We sum up all 4 filters element-wise and get a single 32x32 image. # Step 2: Then we take the mean of all 32x32 images to get a single 32x32 image, which is reduction=\"mean\". if reduction == \"mean\" : gray_scale = torch . sum ( conv_feature_map , dim = 0 ) / num_filters elif reduction == \"max\" : gray_scale = torch . max ( conv_feature_map , dim = 0 ) elif reduction == \"sum\" : gray_scale = torch . sum ( conv_feature_map , dim = 0 ) processed_feature_maps . append ( gray_scale . data . cpu () . numpy ()) feature_map_names . append ( conv_name ) _ , vgg16_processed_feature_maps , vgg16_feature_map_names = get_feature_maps ( model_name = \"vgg16\" , image = cat_tensor , reduction = \"mean\" , pretrained = True ) _ , resnet18_processed_feature_maps , resnet18_feature_map_names = get_feature_maps ( model_name = \"resnet18\" , image = cat_tensor , reduction = \"mean\" , pretrained = True ) 2021-12-29 19:06:12: The train nodes of the model graph is: ['x', 'features.0', 'features.1', 'features.2', 'features.3', 'features.4', 'features.5', 'features.6', 'features.7', 'features.8', 'features.9', 'features.10', 'features.11', 'features.12', 'features.13', 'features.14', 'features.15', 'features.16', 'features.17', 'features.18', 'features.19', 'features.20', 'features.21', 'features.22', 'features.23', 'features.24', 'features.25', 'features.26', 'features.27', 'features.28', 'features.29', 'features.30', 'avgpool', 'flatten', 'classifier.0', 'classifier.1', 'classifier.2', 'classifier.3', 'classifier.4', 'classifier.5', 'classifier.6'] 2021-12-29 19:06:12: The train nodes of the model graph is: ['x', 'conv1', 'bn1', 'relu', 'maxpool', 'layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu', 'layer1.0.conv2', 'layer1.0.bn2', 'layer1.0.add', 'layer1.0.relu_1', 'layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu', 'layer1.1.conv2', 'layer1.1.bn2', 'layer1.1.add', 'layer1.1.relu_1', 'layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu', 'layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.downsample.0', 'layer2.0.downsample.1', 'layer2.0.add', 'layer2.0.relu_1', 'layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.relu', 'layer2.1.conv2', 'layer2.1.bn2', 'layer2.1.add', 'layer2.1.relu_1', 'layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.relu', 'layer3.0.conv2', 'layer3.0.bn2', 'layer3.0.downsample.0', 'layer3.0.downsample.1', 'layer3.0.add', 'layer3.0.relu_1', 'layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.relu', 'layer3.1.conv2', 'layer3.1.bn2', 'layer3.1.add', 'layer3.1.relu_1', 'layer4.0.conv1', 'layer4.0.bn1', 'layer4.0.relu', 'layer4.0.conv2', 'layer4.0.bn2', 'layer4.0.downsample.0', 'layer4.0.downsample.1', 'layer4.0.add', 'layer4.0.relu_1', 'layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.relu', 'layer4.1.conv2', 'layer4.1.bn2', 'layer4.1.add', 'layer4.1.relu_1', 'avgpool', 'flatten', 'fc'] Then we create a simple plot_feature_maps that take in the processed_feature_maps and feature_map_names to plot them. def plot_feature_maps ( processed_feature_maps : List [ torch . Tensor ], feature_map_names : List [ str ], nrows : int , title : str = None ) -> None : \"\"\"Plot the feature maps. Args: processed_feature_maps (List[torch.Tensor]): [description] feature_map_names (List[str]): [description] nrows (int): [description] \"\"\" fig = plt . figure ( figsize = ( 30 , 50 )) ncols = len ( processed_feature_maps ) // nrows + 1 for i in range ( len ( processed_feature_maps )): a = fig . add_subplot ( nrows , ncols , i + 1 ) imgplot = plt . imshow ( processed_feature_maps [ i ]) a . axis ( \"off\" ) a . set_title ( feature_map_names [ i ] . split ( \"(\" )[ 0 ], fontsize = 30 ) fig . suptitle ( title , fontsize = 50 ) fig . tight_layout () fig . subplots_adjust ( top = 0.95 ) plt . savefig ( title , bbox_inches = 'tight' ) plt . show (); plot_feature_maps ( vgg16_processed_feature_maps , vgg16_feature_map_names , nrows = 5 , title = \"VGG16 Pretrained Feature Maps\" , ) plot_feature_maps ( resnet18_processed_feature_maps , resnet18_feature_map_names , nrows = 5 , title = \"ResNet18 Pretrained Feature Maps\" , ) Comparison with Randomly Initialized Weights We know that if the model is not pretrained, it will initialize with random weights using weight initialization methods such as Kaimin or Xavier. I expect the edges to be not so \"smooth\" as the ones that are pretrained! This is logical, as the filters in the conv layers are mostly random, and we have not trained any epochs yet, so let's see what it gives us. _ , vgg16_processed_feature_maps , vgg16_feature_map_names = get_feature_maps ( model_name = \"vgg16\" , image = cat_tensor , reduction = \"mean\" , pretrained = False ) _ , resnet18_processed_feature_maps , resnet18_feature_map_names = get_feature_maps ( model_name = \"resnet18\" , image = cat_tensor , reduction = \"mean\" , pretrained = False ) 2021-12-29 19:06:21: The train nodes of the model graph is: ['x', 'features.0', 'features.1', 'features.2', 'features.3', 'features.4', 'features.5', 'features.6', 'features.7', 'features.8', 'features.9', 'features.10', 'features.11', 'features.12', 'features.13', 'features.14', 'features.15', 'features.16', 'features.17', 'features.18', 'features.19', 'features.20', 'features.21', 'features.22', 'features.23', 'features.24', 'features.25', 'features.26', 'features.27', 'features.28', 'features.29', 'features.30', 'avgpool', 'flatten', 'classifier.0', 'classifier.1', 'classifier.2', 'classifier.3', 'classifier.4', 'classifier.5', 'classifier.6'] 2021-12-29 19:06:21: The train nodes of the model graph is: ['x', 'conv1', 'bn1', 'relu', 'maxpool', 'layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu', 'layer1.0.conv2', 'layer1.0.bn2', 'layer1.0.add', 'layer1.0.relu_1', 'layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu', 'layer1.1.conv2', 'layer1.1.bn2', 'layer1.1.add', 'layer1.1.relu_1', 'layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu', 'layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.downsample.0', 'layer2.0.downsample.1', 'layer2.0.add', 'layer2.0.relu_1', 'layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.relu', 'layer2.1.conv2', 'layer2.1.bn2', 'layer2.1.add', 'layer2.1.relu_1', 'layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.relu', 'layer3.0.conv2', 'layer3.0.bn2', 'layer3.0.downsample.0', 'layer3.0.downsample.1', 'layer3.0.add', 'layer3.0.relu_1', 'layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.relu', 'layer3.1.conv2', 'layer3.1.bn2', 'layer3.1.add', 'layer3.1.relu_1', 'layer4.0.conv1', 'layer4.0.bn1', 'layer4.0.relu', 'layer4.0.conv2', 'layer4.0.bn2', 'layer4.0.downsample.0', 'layer4.0.downsample.1', 'layer4.0.add', 'layer4.0.relu_1', 'layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.relu', 'layer4.1.conv2', 'layer4.1.bn2', 'layer4.1.add', 'layer4.1.relu_1', 'avgpool', 'flatten', 'fc'] plot_feature_maps ( vgg16_processed_feature_maps , vgg16_feature_map_names , nrows = 5 , title = \"VGG16 NOT Pretrained Feature Maps\" , ) plot_feature_maps ( resnet18_processed_feature_maps , resnet18_feature_map_names , nrows = 5 , title = \"ResNet18 NOT Pretrained Feature Maps\" , ) References: https://pytorch.org/vision/stable/feature_extraction.html https://ravivaishnav20.medium.com/visualizing-feature-maps-using-pytorch https://pytorch.org/blog/FX-feature-extraction-torchvision/ https://www.kaggle.com/tarunpaparaju/plant-pathology-2020-eda-models","title":"Feature Map Activations (Part II)"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/01_feature_map_activation/feature_map_activation_reduced/#eda-visualizations-for-image-recognition-conv-filter-edition","text":"","title":"EDA Visualizations for Image Recognition (Conv Filter Edition)"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/01_feature_map_activation/feature_map_activation_reduced/#dependencies-and-imports","text":"! pip install - q timm from typing import Dict import matplotlib.pyplot as plt import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import timm import torch import torchvision from torchvision.models.feature_extraction import ( create_feature_extractor , get_graph_node_names ) % matplotlib inline import glob import os from math import ceil import random import cv2 import PIL from IPython.core.interactiveshell import InteractiveShell InteractiveShell . ast_node_interactivity = \"all\" from typing import * # importing modules import urllib.request from PIL import Image","title":"Dependencies and Imports"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/01_feature_map_activation/feature_map_activation_reduced/#config-and-logging","text":"import logging from logging import INFO , FileHandler , Formatter , StreamHandler , getLogger def init_logger ( log_file : str = \"info.log\" ) -> logging . Logger : \"\"\"Initialize logger and save to file. Consider having more log_file paths to save, eg: debug.log, error.log, etc. Args: log_file (str, optional): [description]. Defaults to Path(LOGS_DIR, \"info.log\"). Returns: logging.Logger: [description] \"\"\" logger = getLogger ( __name__ ) logger . setLevel ( INFO ) stream_handler = StreamHandler () stream_handler . setFormatter ( Formatter ( \" %(asctime)s : %(message)s \" , \"%Y-%m- %d %H:%M:%S\" ) ) file_handler = FileHandler ( filename = log_file ) file_handler . setFormatter ( Formatter ( \" %(asctime)s : %(message)s \" , \"%Y-%m- %d %H:%M:%S\" ) ) logger . addHandler ( stream_handler ) logger . addHandler ( file_handler ) return logger logger = init_logger ()","title":"Config and Logging"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/01_feature_map_activation/feature_map_activation_reduced/#utils","text":"def plot_multiple_img ( img_matrix_list , title_list , ncols , main_title = \"\" ): fig , myaxes = plt . subplots ( figsize = ( 20 , 15 ), nrows = ceil ( len ( img_matrix_list ) / ncols ), ncols = ncols , squeeze = False , ) fig . suptitle ( main_title , fontsize = 30 ) fig . subplots_adjust ( wspace = 0.3 ) fig . subplots_adjust ( hspace = 0.3 ) for i , ( img , title ) in enumerate ( zip ( img_matrix_list , title_list )): myaxes [ i // ncols ][ i % ncols ] . imshow ( img ) myaxes [ i // ncols ][ i % ncols ] . set_title ( title , fontsize = 15 ) plt . show ()","title":"Utils"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/01_feature_map_activation/feature_map_activation_reduced/#seeding","text":"def seed_all ( seed : int = 1992 ) -> None : \"\"\"Seed all random number generators.\"\"\" print ( f \"Using Seed Number { seed } \" ) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) # set PYTHONHASHSEED env var at fixed value torch . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) torch . cuda . manual_seed ( seed ) # pytorch (both CPU and CUDA) np . random . seed ( seed ) # for numpy pseudo-random generator # set fixed value for python built-in pseudo-random generator random . seed ( seed ) torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = True torch . backends . cudnn . enabled = True def seed_worker ( _worker_id ) -> None : \"\"\"Seed a worker with the given ID.\"\"\" worker_seed = torch . initial_seed () % 2 ** 32 np . random . seed ( worker_seed ) random . seed ( worker_seed ) seed_all () Using Seed Number 1992","title":"Seeding"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/01_feature_map_activation/feature_map_activation_reduced/#transforms-params","text":"mean : List [ float ] = [ 0.485 , 0.456 , 0.406 ] std : List [ float ] = [ 0.229 , 0.224 , 0.225 ] image_size : int = 224 transform = torchvision . transforms . Compose ( [ torchvision . transforms . Resize (( image_size , image_size )), torchvision . transforms . ToTensor (), torchvision . transforms . Normalize ( mean = mean , std = std ), ] ) pre_normalize_transform = torchvision . transforms . Compose ( [ torchvision . transforms . Resize (( image_size , image_size )), torchvision . transforms . ToTensor (), ] )","title":"Transforms Params"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/01_feature_map_activation/feature_map_activation_reduced/#visualizations","text":"cat_p = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/deep_learning/computer_vision/data/misc/cat.jpg\" dog_p = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/deep_learning/computer_vision/data/misc/dog.jpg\" from urllib.request import urlopen # plot cat and dog with title using PIL plt . figure ( figsize = ( 10 , 10 )) plt . subplot ( 1 , 2 , 1 ) cat = PIL . Image . open ( urlopen ( cat_p )) plt . imshow ( cat . resize (( 1024 , 1024 ))) plt . title ( \"Cat\" ) plt . subplot ( 1 , 2 , 2 ) dog = PIL . Image . open ( urlopen ( dog_p )) plt . imshow ( dog . resize (( 1024 , 1024 ))) plt . title ( \"Dog\" ) plt . show ();","title":"Visualizations"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/01_feature_map_activation/feature_map_activation_reduced/#convolution-layers","text":"The image and content are referenced with courtesy from [Tarun's notebook](https://www.kaggle.com/tarunpaparaju/plant-pathology-2020-eda-models](https://www.kaggle.com/tarunpaparaju/plant-pathology-2020-eda-models). Convolution is a rather simple algorithm which involves a kernel (a 2D matrix) which moves over the entire image, calculating dot products with each window along the way. The GIF below demonstrates convolution in action. The above process can be summarized with an equation, where f is the image and h is the kernel. The dimensions of f are (m, n) and the kernel is a square matrix with dimensions smaller than f : \\( \\(\\text{conv}(f, h) = \\sum_{j}\\sum_{k}h_{jk} \\cdot f_{(m-j)(n-k)}\\) \\) In the above equation, the kernel h is moving across the length and breadth of the image. The dot product of h with a sub-matrix or window of matrix f is taken at each step, hence the double summation (rows and columns). I have always remembered from the revered Andrew Ng about how he taught us about what convolutional layers do. In the beginning, the conv layers are of low level abstraction, detailing a image's features such as shapes and sizes. In particular, he described to us the horizontal and vertical conv filters. As the conv layers go later, it will pick up on many abstract features, which is not really easily distinguished by human eyes. Below, we see an example of horizontal and vertical filters. def conv_horizontal ( image : np . ndarray ) -> None : \"\"\"Plot the horizontal convolution of the image. Args: image (torch.Tensor): [description] \"\"\" fig , ax = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 20 , 20 )) kernel = np . ones (( 3 , 3 ), np . float32 ) kernel [ 1 ] = np . array ([ 0 , 0 , 0 ], np . float32 ) kernel [ 2 ] = np . array ([ - 1 , - 1 , - 1 ], np . float32 ) conv = cv2 . filter2D ( image , - 1 , kernel ) ax [ 0 ] . imshow ( image ) ax [ 0 ] . set_title ( \"Original Image\" , fontsize = 24 ) ax [ 1 ] . imshow ( conv ) ax [ 1 ] . set_title ( \"Convolved Image with horizontal edges\" , fontsize = 24 ) plt . show () def conv_vertical ( image : np . ndarray ) -> None : \"\"\"Plot the vertical convolution of the image. Args: image (torch.Tensor): [description] \"\"\" fig , ax = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 20 , 20 )) kernel = np . ones (( 3 , 3 ), np . float32 ) kernel [ 0 ] = np . array ([ 1 , 0 , - 1 ]) kernel [ 1 ] = np . array ([ 1 , 0 , - 1 ]) kernel [ 2 ] = np . array ([ 1 , 0 , - 1 ]) conv = cv2 . filter2D ( image , - 1 , kernel ) ax [ 0 ] . imshow ( image ) ax [ 0 ] . set_title ( \"Original Image\" , fontsize = 24 ) ax [ 1 ] . imshow ( conv ) ax [ 1 ] . set_title ( \"Convolved Image with vertical edges\" , fontsize = 24 ) plt . show () Well, I can easily make out the horizontal and vertical edges from the dog image! Actually, not so obvious if you don't look closely! conv_horizontal ( np . asarray ( dog )) conv_vertical ( np . asarray ( dog )) The issue is, I want to visualize what our models' conv layers are seeing, like for example, the first conv layer usually has 64 filters, that is a whooping 64 different combinations of filters, each doing a slightly different thing. A mental model that I have for the first conv layer looks something like the following. conv_1_filters = [ \"vertical edge detector\" , \"horizontal edge detector\" , \"slanted 45 degrees detector\" , \"slanted 180 degrees detector\" , ... ]","title":"Convolution Layers "},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/01_feature_map_activation/feature_map_activation_reduced/#feature-extractor-using-pytorchs-native-feature-extraction-module","text":"In order to visualize properly, I made use of PyTorch's newest feature_extraction module to do so. Note that the new feature is still in development, but it does make my life easier and reduces overhead. I no longer need use hooks or what not to plot layer information! We just need to import from torchvision.models.feature_extraction import ( create_feature_extractor , get_graph_node_names ) def get_conv_layers ( model : torchvision . models ) -> Dict [ str , str ]: \"\"\"Create a function that give me the conv layers of PyTorch model. Args: model (Union[torchvision.models, timm.models]): A PyTorch model. Returns: conv_layers (Dict[str, str]): {\"layer1.0.conv1\": layer1.0.conv1, ...} \"\"\" conv_layers = {} for name , layer in model . named_modules (): if isinstance ( layer , torch . nn . Conv2d ): conv_layers [ name ] = name return conv_layers def get_feature_maps ( model_name : str , image : torch . Tensor , reduction : str = \"mean\" , pretrained : bool = True ) -> Union [ Dict [ str , torch . Tensor ], List [ torch . Tensor ], List [ str ]]: \"\"\"Function to plot feature maps from PyTorch models. Args: model_name (str): Name of the model to use. image (torch.Tensor): image should be a tensor of shape (1, 3, H, W) reduction (str, optional): Defaults to \"mean\". One of [\"mean\", \"max\", \"sum\"] pretrained (bool): whether the model is pretrained or not Raises: ValueError: Must use Torchvision models. Returns: model_feature_maps (Dict[str, torch.Tensor]): {\"conv_1\": conv_1_feature_map, ...} processed_feature_maps (List[torch.Tensor]): [conv_1_feature_map, ...] processed using a reduction method. feature_map_names (List[str]): [conv_1, ...] Example: >>> from torchvision.models.vgg import vgg16 >>> model = vgg16(pretrained=True) >>> image = torch.rand(1, 3, 224, 224) >>> feature_maps = get_feature_maps(model, image, reduction=\"mean\") Reduction: If a feature map has 4 filters, in the shape of (4, H, W) = (4, 32, 32), then the reduction can be done as follows: >>> reduction = \"mean\": There are 4 filters in this feature map, you can imagine it as 4 32x32 images. We sum up all 4 filters channel wise and get a single 32x32 image. i.e filter 1-4's (0, 0) coordinate has pixel say 1,2,3,4, respectively, then sum up channel wise means 1+2+3+4 = 10 and Then we take the mean of all 32x32 images by dividing by num of kernels to get a single 32x32 image, which is reduction=\"mean\" which means 10 / 4 = 2.5 for that pixel at (0, 0) \"\"\" try : model = getattr ( torchvision . models , model_name )( pretrained = pretrained ) except AttributeError : raise ValueError ( f \"Model { model_name } not found.\" ) train_nodes , eval_nodes = get_graph_node_names ( model ) logger . info ( f \"The train nodes of the model graph is: \\n\\n { train_nodes } \" ) return_conv_nodes = get_conv_layers ( model ) feature_extractor = create_feature_extractor ( model , return_nodes = return_conv_nodes ) # `model_feature_maps` will be a dict of Tensors, each representing a feature map model_feature_maps = feature_extractor ( image ) processed_feature_maps = [] feature_map_names = [] for conv_name , conv_feature_map in model_feature_maps . items (): conv_feature_map = conv_feature_map . squeeze ( dim = 0 ) num_filters = conv_feature_map . shape [ 0 ] if reduction == \"mean\" : gray_scale = torch . sum ( conv_feature_map , dim = 0 ) / num_filters elif reduction == \"max\" : gray_scale = torch . max ( conv_feature_map , dim = 0 ) elif reduction == \"sum\" : gray_scale = torch . sum ( conv_feature_map , dim = 0 ) processed_feature_maps . append ( gray_scale . data . cpu () . numpy ()) feature_map_names . append ( conv_name ) return model_feature_maps , processed_feature_maps , feature_map_names","title":"Feature Extractor using PyTorch's native Feature Extraction Module"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/01_feature_map_activation/feature_map_activation_reduced/#visualizing-vgg16-and-resnet18","text":"","title":"Visualizing VGG16 and ResNet18"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/01_feature_map_activation/feature_map_activation_reduced/#step-1-initialize-the-models","text":"As of now, I recommend using torchvision 's models. Ideally, I will want to use timm library for a more detailed list, but there are some bugs that is not easily integrated with the module. device = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) import torchvision.models as models vgg16_pretrained_true = models . vgg16 ( pretrained = True ) vgg16_pretrained_true = vgg16_pretrained_true . to ( device ) resnet18_pretrained_true = models . resnet18 ( pretrained = True ) resnet18_pretrained_true = resnet18_pretrained_true . to ( device ) # Get node names train_nodes , eval_nodes = get_graph_node_names ( vgg16_pretrained_true ) logger . info ( f \"Train nodes of VGG16: \\n\\n { train_nodes } \" ) train_nodes , eval_nodes = get_graph_node_names ( resnet18_pretrained_true ) logger . info ( f \"Train nodes of ResNet18: \\n\\n { train_nodes } \" ) 2021-12-29 19:06:08: Train nodes of VGG16: ['x', 'features.0', 'features.1', 'features.2', 'features.3', 'features.4', 'features.5', 'features.6', 'features.7', 'features.8', 'features.9', 'features.10', 'features.11', 'features.12', 'features.13', 'features.14', 'features.15', 'features.16', 'features.17', 'features.18', 'features.19', 'features.20', 'features.21', 'features.22', 'features.23', 'features.24', 'features.25', 'features.26', 'features.27', 'features.28', 'features.29', 'features.30', 'avgpool', 'flatten', 'classifier.0', 'classifier.1', 'classifier.2', 'classifier.3', 'classifier.4', 'classifier.5', 'classifier.6'] 2021-12-29 19:06:08: Train nodes of ResNet18: ['x', 'conv1', 'bn1', 'relu', 'maxpool', 'layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu', 'layer1.0.conv2', 'layer1.0.bn2', 'layer1.0.add', 'layer1.0.relu_1', 'layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu', 'layer1.1.conv2', 'layer1.1.bn2', 'layer1.1.add', 'layer1.1.relu_1', 'layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu', 'layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.downsample.0', 'layer2.0.downsample.1', 'layer2.0.add', 'layer2.0.relu_1', 'layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.relu', 'layer2.1.conv2', 'layer2.1.bn2', 'layer2.1.add', 'layer2.1.relu_1', 'layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.relu', 'layer3.0.conv2', 'layer3.0.bn2', 'layer3.0.downsample.0', 'layer3.0.downsample.1', 'layer3.0.add', 'layer3.0.relu_1', 'layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.relu', 'layer3.1.conv2', 'layer3.1.bn2', 'layer3.1.add', 'layer3.1.relu_1', 'layer4.0.conv1', 'layer4.0.bn1', 'layer4.0.relu', 'layer4.0.conv2', 'layer4.0.bn2', 'layer4.0.downsample.0', 'layer4.0.downsample.1', 'layer4.0.add', 'layer4.0.relu_1', 'layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.relu', 'layer4.1.conv2', 'layer4.1.bn2', 'layer4.1.add', 'layer4.1.relu_1', 'avgpool', 'flatten', 'fc'] Good God! When I saw the layer names from vgg16 , I nearly fainted, I see no easy way to know which layer belongs to a Conv layer. I understand that get_graph_node_names will get all the nodes on the model's graph, but it is difficult to map the node names to a layer if it is named as such, seeing resnet18 's node names is much easier for one to identify which is conv layer or not. train_nodes , eval_nodes = get_graph_node_names ( model ) logger . info ( f \"The train nodes of the model graph is: \\n\\n { train_nodes } \" ) Thus I wrote a small function get_conv_layers to get the conv layer names. It is not perfect, as downsample layers (1x1 conv layers) are tagged under Conv2d but we may not really need to use them to visualize our feature maps. One can tweak a bit if need be, but for now, I will get all layers that use the Conv2d blocks. If the feature names in vgg16 are named with conv, then we can simply use a small loop below to find the conv layer names. conv_layers = [] for node in nodes : if \"conv\" in node : conv_layers . append ( node ) I actually thought ResNet18 has 18 conv layers, but even minusing to 3 downsample layers, it's 17 conv layers, wonder why?","title":"Step 1: Initialize the models."},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/01_feature_map_activation/feature_map_activation_reduced/#step-2-transform-the-tensors","text":"The PyTorch feature_extraction expects the image input to be of shape [B,C,H,W] . # We use torchvision's transform to transform the cat image to channels first. cat_tensor = transform ( cat ) # Now feature_extractor expects batch_size x C x H x W, so we expand one dimension in the 0th dim cat_tensor = cat_tensor . unsqueeze ( dim = 0 ) . to ( device ) # We use torchvision's transform to transform the cat image with resize and normalization. # Conveniently, also making it channel first! cat_tensor = transform ( cat ) dog_tensor = transform ( dog ) assert cat_tensor . shape [ 0 ] == dog_tensor . shape [ 0 ] == 3 , \"PyTorch expects Channel First!\" # Now feature_extractor expects batch_size x C x H x W, so we expand one dimension in the 0th dim cat_tensor = cat_tensor . unsqueeze ( dim = 0 ) . to ( device ) dog_tensor = dog_tensor . unsqueeze ( dim = 0 ) . to ( device ) logger . info ( f \" \\n\\n cat_tensor's shape: \\n { cat_tensor . shape } \\n\\n dog_tensor's shape: \\n { dog_tensor . shape } \" ) 2021-12-29 19:06:10: cat_tensor's shape: torch.Size([1, 3, 224, 224]) dog_tensor's shape: torch.Size([1, 3, 224, 224])","title":"Step 2: Transform the Tensors"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/01_feature_map_activation/feature_map_activation_reduced/#step-3-plotting-the-feature-maps","text":"We first walk through get_feature_maps and see what my function is doing. # Get node names train_nodes , eval_nodes = get_graph_node_names ( model ) # Since get node names do not indicate properly which is a conv layer or not, # we use get_conv_layer instead to do the job, which returns a dict {\"conv_layer_name\": \"conv_layer_name\"} return_conv_nodes = get_conv_layers ( model ) # call create_feature_extractor on the model and its corresponding conv layer names. feature_extractor = create_feature_extractor ( model , return_nodes = return_conv_nodes ) # `model_feature_maps` will be a dict of Tensors, each representing a feature map # {\"conv_layer_1\": output filter map,...} model_feature_maps = feature_extractor ( image ) # we need to further process the feature maps processed_feature_maps , feature_map_names = [], [] for conv_name , conv_feature_map in model_feature_maps . items (): # Squeeze the dimension from [1, 64, 32, 32] to [64, 32, 32] # This means we have 64 filters of 32x32 \"images\" or kernels conv_feature_map = conv_feature_map . squeeze ( dim = 0 ) # get number of feature/kernels in this layer num_filters = conv_feature_map . shape [ 0 ] # If a feature map has 4 filters, in the shape of (4, H, W) = (4, 32, 32), then the reduction mean can be done as follows: There are 4 filters in this feature map, you can imagine it as 4 32x32 images. # Step 1: We sum up all 4 filters element-wise and get a single 32x32 image. # Step 2: Then we take the mean of all 32x32 images to get a single 32x32 image, which is reduction=\"mean\". if reduction == \"mean\" : gray_scale = torch . sum ( conv_feature_map , dim = 0 ) / num_filters elif reduction == \"max\" : gray_scale = torch . max ( conv_feature_map , dim = 0 ) elif reduction == \"sum\" : gray_scale = torch . sum ( conv_feature_map , dim = 0 ) processed_feature_maps . append ( gray_scale . data . cpu () . numpy ()) feature_map_names . append ( conv_name ) _ , vgg16_processed_feature_maps , vgg16_feature_map_names = get_feature_maps ( model_name = \"vgg16\" , image = cat_tensor , reduction = \"mean\" , pretrained = True ) _ , resnet18_processed_feature_maps , resnet18_feature_map_names = get_feature_maps ( model_name = \"resnet18\" , image = cat_tensor , reduction = \"mean\" , pretrained = True ) 2021-12-29 19:06:12: The train nodes of the model graph is: ['x', 'features.0', 'features.1', 'features.2', 'features.3', 'features.4', 'features.5', 'features.6', 'features.7', 'features.8', 'features.9', 'features.10', 'features.11', 'features.12', 'features.13', 'features.14', 'features.15', 'features.16', 'features.17', 'features.18', 'features.19', 'features.20', 'features.21', 'features.22', 'features.23', 'features.24', 'features.25', 'features.26', 'features.27', 'features.28', 'features.29', 'features.30', 'avgpool', 'flatten', 'classifier.0', 'classifier.1', 'classifier.2', 'classifier.3', 'classifier.4', 'classifier.5', 'classifier.6'] 2021-12-29 19:06:12: The train nodes of the model graph is: ['x', 'conv1', 'bn1', 'relu', 'maxpool', 'layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu', 'layer1.0.conv2', 'layer1.0.bn2', 'layer1.0.add', 'layer1.0.relu_1', 'layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu', 'layer1.1.conv2', 'layer1.1.bn2', 'layer1.1.add', 'layer1.1.relu_1', 'layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu', 'layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.downsample.0', 'layer2.0.downsample.1', 'layer2.0.add', 'layer2.0.relu_1', 'layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.relu', 'layer2.1.conv2', 'layer2.1.bn2', 'layer2.1.add', 'layer2.1.relu_1', 'layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.relu', 'layer3.0.conv2', 'layer3.0.bn2', 'layer3.0.downsample.0', 'layer3.0.downsample.1', 'layer3.0.add', 'layer3.0.relu_1', 'layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.relu', 'layer3.1.conv2', 'layer3.1.bn2', 'layer3.1.add', 'layer3.1.relu_1', 'layer4.0.conv1', 'layer4.0.bn1', 'layer4.0.relu', 'layer4.0.conv2', 'layer4.0.bn2', 'layer4.0.downsample.0', 'layer4.0.downsample.1', 'layer4.0.add', 'layer4.0.relu_1', 'layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.relu', 'layer4.1.conv2', 'layer4.1.bn2', 'layer4.1.add', 'layer4.1.relu_1', 'avgpool', 'flatten', 'fc'] Then we create a simple plot_feature_maps that take in the processed_feature_maps and feature_map_names to plot them. def plot_feature_maps ( processed_feature_maps : List [ torch . Tensor ], feature_map_names : List [ str ], nrows : int , title : str = None ) -> None : \"\"\"Plot the feature maps. Args: processed_feature_maps (List[torch.Tensor]): [description] feature_map_names (List[str]): [description] nrows (int): [description] \"\"\" fig = plt . figure ( figsize = ( 30 , 50 )) ncols = len ( processed_feature_maps ) // nrows + 1 for i in range ( len ( processed_feature_maps )): a = fig . add_subplot ( nrows , ncols , i + 1 ) imgplot = plt . imshow ( processed_feature_maps [ i ]) a . axis ( \"off\" ) a . set_title ( feature_map_names [ i ] . split ( \"(\" )[ 0 ], fontsize = 30 ) fig . suptitle ( title , fontsize = 50 ) fig . tight_layout () fig . subplots_adjust ( top = 0.95 ) plt . savefig ( title , bbox_inches = 'tight' ) plt . show (); plot_feature_maps ( vgg16_processed_feature_maps , vgg16_feature_map_names , nrows = 5 , title = \"VGG16 Pretrained Feature Maps\" , ) plot_feature_maps ( resnet18_processed_feature_maps , resnet18_feature_map_names , nrows = 5 , title = \"ResNet18 Pretrained Feature Maps\" , )","title":"Step 3: Plotting the Feature Maps"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/01_feature_map_activation/feature_map_activation_reduced/#comparison-with-randomly-initialized-weights","text":"We know that if the model is not pretrained, it will initialize with random weights using weight initialization methods such as Kaimin or Xavier. I expect the edges to be not so \"smooth\" as the ones that are pretrained! This is logical, as the filters in the conv layers are mostly random, and we have not trained any epochs yet, so let's see what it gives us. _ , vgg16_processed_feature_maps , vgg16_feature_map_names = get_feature_maps ( model_name = \"vgg16\" , image = cat_tensor , reduction = \"mean\" , pretrained = False ) _ , resnet18_processed_feature_maps , resnet18_feature_map_names = get_feature_maps ( model_name = \"resnet18\" , image = cat_tensor , reduction = \"mean\" , pretrained = False ) 2021-12-29 19:06:21: The train nodes of the model graph is: ['x', 'features.0', 'features.1', 'features.2', 'features.3', 'features.4', 'features.5', 'features.6', 'features.7', 'features.8', 'features.9', 'features.10', 'features.11', 'features.12', 'features.13', 'features.14', 'features.15', 'features.16', 'features.17', 'features.18', 'features.19', 'features.20', 'features.21', 'features.22', 'features.23', 'features.24', 'features.25', 'features.26', 'features.27', 'features.28', 'features.29', 'features.30', 'avgpool', 'flatten', 'classifier.0', 'classifier.1', 'classifier.2', 'classifier.3', 'classifier.4', 'classifier.5', 'classifier.6'] 2021-12-29 19:06:21: The train nodes of the model graph is: ['x', 'conv1', 'bn1', 'relu', 'maxpool', 'layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu', 'layer1.0.conv2', 'layer1.0.bn2', 'layer1.0.add', 'layer1.0.relu_1', 'layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu', 'layer1.1.conv2', 'layer1.1.bn2', 'layer1.1.add', 'layer1.1.relu_1', 'layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu', 'layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.downsample.0', 'layer2.0.downsample.1', 'layer2.0.add', 'layer2.0.relu_1', 'layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.relu', 'layer2.1.conv2', 'layer2.1.bn2', 'layer2.1.add', 'layer2.1.relu_1', 'layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.relu', 'layer3.0.conv2', 'layer3.0.bn2', 'layer3.0.downsample.0', 'layer3.0.downsample.1', 'layer3.0.add', 'layer3.0.relu_1', 'layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.relu', 'layer3.1.conv2', 'layer3.1.bn2', 'layer3.1.add', 'layer3.1.relu_1', 'layer4.0.conv1', 'layer4.0.bn1', 'layer4.0.relu', 'layer4.0.conv2', 'layer4.0.bn2', 'layer4.0.downsample.0', 'layer4.0.downsample.1', 'layer4.0.add', 'layer4.0.relu_1', 'layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.relu', 'layer4.1.conv2', 'layer4.1.bn2', 'layer4.1.add', 'layer4.1.relu_1', 'avgpool', 'flatten', 'fc'] plot_feature_maps ( vgg16_processed_feature_maps , vgg16_feature_map_names , nrows = 5 , title = \"VGG16 NOT Pretrained Feature Maps\" , ) plot_feature_maps ( resnet18_processed_feature_maps , resnet18_feature_map_names , nrows = 5 , title = \"ResNet18 NOT Pretrained Feature Maps\" , ) References: https://pytorch.org/vision/stable/feature_extraction.html https://ravivaishnav20.medium.com/visualizing-feature-maps-using-pytorch https://pytorch.org/blog/FX-feature-extraction-torchvision/ https://www.kaggle.com/tarunpaparaju/plant-pathology-2020-eda-models","title":"Comparison with Randomly Initialized Weights"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/02_conv_filters/Visualizing%20Convolutional%20Filters/","text":"EDA Visualizations for Image Recognition (Conv Filter Edition) Dependencies and Imports ! pip install - q timm from typing import Dict import matplotlib.pyplot as plt import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import timm import torch import torchvision from torchvision.models.feature_extraction import ( create_feature_extractor , get_graph_node_names ) % matplotlib inline import glob import os from math import ceil import random import cv2 import PIL from IPython.core.interactiveshell import InteractiveShell InteractiveShell . ast_node_interactivity = \"all\" from typing import * # importing modules import urllib.request from PIL import Image Config and Logging import logging from logging import INFO , FileHandler , Formatter , StreamHandler , getLogger def init_logger ( log_file : str = \"info.log\" ) -> logging . Logger : \"\"\"Initialize logger and save to file. Consider having more log_file paths to save, eg: debug.log, error.log, etc. Args: log_file (str, optional): [description]. Defaults to Path(LOGS_DIR, \"info.log\"). Returns: logging.Logger: [description] \"\"\" logger = getLogger ( __name__ ) logger . setLevel ( INFO ) stream_handler = StreamHandler () stream_handler . setFormatter ( Formatter ( \" %(asctime)s : %(message)s \" , \"%Y-%m- %d %H:%M:%S\" ) ) file_handler = FileHandler ( filename = log_file ) file_handler . setFormatter ( Formatter ( \" %(asctime)s : %(message)s \" , \"%Y-%m- %d %H:%M:%S\" ) ) logger . addHandler ( stream_handler ) logger . addHandler ( file_handler ) return logger logger = init_logger () Utils def plot_multiple_img ( img_matrix_list , title_list , ncols , main_title = \"\" ): fig , myaxes = plt . subplots ( figsize = ( 20 , 15 ), nrows = ceil ( len ( img_matrix_list ) / ncols ), ncols = ncols , squeeze = False , ) fig . suptitle ( main_title , fontsize = 30 ) fig . subplots_adjust ( wspace = 0.3 ) fig . subplots_adjust ( hspace = 0.3 ) for i , ( img , title ) in enumerate ( zip ( img_matrix_list , title_list )): myaxes [ i // ncols ][ i % ncols ] . imshow ( img ) myaxes [ i // ncols ][ i % ncols ] . set_title ( title , fontsize = 15 ) plt . show () Seeding def seed_all ( seed : int = 1992 ) -> None : \"\"\"Seed all random number generators.\"\"\" print ( f \"Using Seed Number { seed } \" ) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) # set PYTHONHASHSEED env var at fixed value torch . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) torch . cuda . manual_seed ( seed ) # pytorch (both CPU and CUDA) np . random . seed ( seed ) # for numpy pseudo-random generator # set fixed value for python built-in pseudo-random generator random . seed ( seed ) torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = True torch . backends . cudnn . enabled = True def seed_worker ( _worker_id ) -> None : \"\"\"Seed a worker with the given ID.\"\"\" worker_seed = torch . initial_seed () % 2 ** 32 np . random . seed ( worker_seed ) random . seed ( worker_seed ) seed_all () Using Seed Number 1992 Transforms Params mean : List [ float ] = [ 0.485 , 0.456 , 0.406 ] std : List [ float ] = [ 0.229 , 0.224 , 0.225 ] image_size : int = 224 transform = torchvision . transforms . Compose ( [ torchvision . transforms . Resize (( image_size , image_size )), torchvision . transforms . ToTensor (), torchvision . transforms . Normalize ( mean = mean , std = std ), ] ) pre_normalize_transform = torchvision . transforms . Compose ( [ torchvision . transforms . Resize (( image_size , image_size )), torchvision . transforms . ToTensor (), ] ) Visualizations cat_p = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/deep_learning/computer_vision/data/misc/cat.jpg\" dog_p = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/deep_learning/computer_vision/data/misc/dog.jpg\" from urllib.request import urlopen # plot cat and dog with title using PIL plt . figure ( figsize = ( 10 , 10 )) plt . subplot ( 1 , 2 , 1 ) cat = PIL . Image . open ( urlopen ( cat_p )) plt . imshow ( cat . resize (( 1024 , 1024 ))) plt . title ( \"Cat\" ) plt . subplot ( 1 , 2 , 2 ) dog = PIL . Image . open ( urlopen ( dog_p )) plt . imshow ( dog . resize (( 1024 , 1024 ))) plt . title ( \"Dog\" ) plt . show (); Convolution Layers The image and content are referenced with courtesy from [Tarun's notebook](https://www.kaggle.com/tarunpaparaju/plant-pathology-2020-eda-models](https://www.kaggle.com/tarunpaparaju/plant-pathology-2020-eda-models). Convolution is a rather simple algorithm which involves a kernel (a 2D matrix) which moves over the entire image, calculating dot products with each window along the way. The GIF below demonstrates convolution in action. The above process can be summarized with an equation, where f is the image and h is the kernel. The dimensions of f are (m, n) and the kernel is a square matrix with dimensions smaller than f : \\( \\(\\text{conv}(f, h) = \\sum_{j}\\sum_{k}h_{jk} \\cdot f_{(m-j)(n-k)}\\) \\) In the above equation, the kernel h is moving across the length and breadth of the image. The dot product of h with a sub-matrix or window of matrix f is taken at each step, hence the double summation (rows and columns). I have always remembered from the revered Andrew Ng about how he taught us about what convolutional layers do. In the beginning, the conv layers are of low level abstraction, detailing a image's features such as shapes and sizes. In particular, he described to us the horizontal and vertical conv filters. As the conv layers go later, it will pick up on many abstract features, which is not really easily distinguished by human eyes. Below, we see an example of horizontal and vertical filters. def conv_horizontal ( image : np . ndarray ) -> None : \"\"\"Plot the horizontal convolution of the image. Args: image (torch.Tensor): [description] \"\"\" fig , ax = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 20 , 20 )) kernel = np . ones (( 3 , 3 ), np . float32 ) kernel [ 1 ] = np . array ([ 0 , 0 , 0 ], np . float32 ) kernel [ 2 ] = np . array ([ - 1 , - 1 , - 1 ], np . float32 ) conv = cv2 . filter2D ( image , - 1 , kernel ) ax [ 0 ] . imshow ( image ) ax [ 0 ] . set_title ( \"Original Image\" , fontsize = 24 ) ax [ 1 ] . imshow ( conv ) ax [ 1 ] . set_title ( \"Convolved Image with horizontal edges\" , fontsize = 24 ) plt . show () def conv_vertical ( image : np . ndarray ) -> None : \"\"\"Plot the vertical convolution of the image. Args: image (torch.Tensor): [description] \"\"\" fig , ax = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 20 , 20 )) kernel = np . ones (( 3 , 3 ), np . float32 ) kernel [ 0 ] = np . array ([ 1 , 0 , - 1 ]) kernel [ 1 ] = np . array ([ 1 , 0 , - 1 ]) kernel [ 2 ] = np . array ([ 1 , 0 , - 1 ]) conv = cv2 . filter2D ( image , - 1 , kernel ) ax [ 0 ] . imshow ( image ) ax [ 0 ] . set_title ( \"Original Image\" , fontsize = 24 ) ax [ 1 ] . imshow ( conv ) ax [ 1 ] . set_title ( \"Convolved Image with vertical edges\" , fontsize = 24 ) plt . show () Well, I can easily make out the horizontal and vertical edges from the dog image! Actually, not so obvious if you don't look closely! conv_horizontal ( np . asarray ( dog )) conv_vertical ( np . asarray ( dog )) The issue is, I want to visualize what our models' conv layers are seeing, like for example, the first conv layer usually has 64 filters, that is a whooping 64 different combinations of filters, each doing a slightly different thing. A mental model that I have for the first conv layer looks something like the following. conv_1_filters = [ \"vertical edge detector\" , \"horizontal edge detector\" , \"slanted 45 degrees detector\" , \"slanted 180 degrees detector\" , ... ] Feature Extractor using PyTorch's native Feature Extraction Module In order to visualize properly, I made use of PyTorch's newest feature_extraction module to do so. Note that the new feature is still in development, but it does make my life easier and reduces overhead. I no longer need use hooks or what not to plot layer information! We just need to import from torchvision.models.feature_extraction import ( create_feature_extractor , get_graph_node_names ) def get_conv_layers ( model : torchvision . models ) -> Dict [ str , str ]: \"\"\"Create a function that give me the conv layers of PyTorch model. Args: model (Union[torchvision.models, timm.models]): A PyTorch model. Returns: conv_layers (Dict[str, str]): {\"layer1.0.conv1\": layer1.0.conv1, ...} \"\"\" conv_layers = {} for name , layer in model . named_modules (): if isinstance ( layer , torch . nn . Conv2d ): conv_layers [ name ] = name return conv_layers def get_feature_maps ( model_name : str , image : torch . Tensor , reduction : str = \"mean\" , pretrained : bool = True ) -> Union [ Dict [ str , torch . Tensor ], List [ torch . Tensor ], List [ str ]]: \"\"\"Function to plot feature maps from PyTorch models. Args: model_name (str): Name of the model to use. image (torch.Tensor): image should be a tensor of shape (1, 3, H, W) reduction (str, optional): Defaults to \"mean\". One of [\"mean\", \"max\", \"sum\"] pretrained (bool): whether the model is pretrained or not Raises: ValueError: Must use Torchvision models. Returns: model_feature_maps (Dict[str, torch.Tensor]): {\"conv_1\": conv_1_feature_map, ...} processed_feature_maps (List[torch.Tensor]): [conv_1_feature_map, ...] processed using a reduction method. feature_map_names (List[str]): [conv_1, ...] Example: >>> from torchvision.models.vgg import vgg16 >>> model = vgg16(pretrained=True) >>> image = torch.rand(1, 3, 224, 224) >>> feature_maps = get_feature_maps(model, image, reduction=\"mean\") Reduction: If a feature map has 4 filters, in the shape of (4, H, W) = (4, 32, 32), then the reduction can be done as follows: >>> reduction = \"mean\": There are 4 filters in this feature map, you can imagine it as 4 32x32 images. We sum up all 4 filters element-wise and get a single 32x32 image. Then we take the mean of all 32x32 images by dividing by num of kernels to get a single 32x32 image, which is reduction=\"mean\". \"\"\" try : model = getattr ( torchvision . models , model_name )( pretrained = pretrained ) except AttributeError : raise ValueError ( f \"Model { model_name } not found.\" ) train_nodes , eval_nodes = get_graph_node_names ( model ) logger . info ( f \"The train nodes of the model graph is: \\n\\n { train_nodes } \" ) return_conv_nodes = get_conv_layers ( model ) feature_extractor = create_feature_extractor ( model , return_nodes = return_conv_nodes ) # `model_feature_maps` will be a dict of Tensors, each representing a feature map model_feature_maps = feature_extractor ( image ) processed_feature_maps = [] feature_map_names = [] for conv_name , conv_feature_map in model_feature_maps . items (): conv_feature_map = conv_feature_map . squeeze ( dim = 0 ) num_filters = conv_feature_map . shape [ 0 ] if reduction == \"mean\" : gray_scale = torch . sum ( conv_feature_map , dim = 0 ) / num_filters elif reduction == \"max\" : gray_scale = torch . max ( conv_feature_map , dim = 0 ) elif reduction == \"sum\" : gray_scale = torch . sum ( conv_feature_map , dim = 0 ) processed_feature_maps . append ( gray_scale . data . cpu () . numpy ()) feature_map_names . append ( conv_name ) return model_feature_maps , processed_feature_maps , feature_map_names Visualizing VGG16 and ResNet18 Step 1: Initialize the models. As of now, I recommend using torchvision 's models. Ideally, I will want to use timm library for a more detailed list, but there are some bugs that is not easily integrated with the module. device = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) import torchvision.models as models vgg16_pretrained_true = models . vgg16 ( pretrained = True ) vgg16_pretrained_true = vgg16_pretrained_true . to ( device ) resnet18_pretrained_true = models . resnet18 ( pretrained = True ) resnet18_pretrained_true = resnet18_pretrained_true . to ( device ) # Get node names train_nodes , eval_nodes = get_graph_node_names ( vgg16_pretrained_true ) logger . info ( f \"Train nodes of VGG16: \\n\\n { train_nodes } \" ) train_nodes , eval_nodes = get_graph_node_names ( resnet18_pretrained_true ) logger . info ( f \"Train nodes of ResNet18: \\n\\n { train_nodes } \" ) 2021-12-29 19:06:08: Train nodes of VGG16: ['x', 'features.0', 'features.1', 'features.2', 'features.3', 'features.4', 'features.5', 'features.6', 'features.7', 'features.8', 'features.9', 'features.10', 'features.11', 'features.12', 'features.13', 'features.14', 'features.15', 'features.16', 'features.17', 'features.18', 'features.19', 'features.20', 'features.21', 'features.22', 'features.23', 'features.24', 'features.25', 'features.26', 'features.27', 'features.28', 'features.29', 'features.30', 'avgpool', 'flatten', 'classifier.0', 'classifier.1', 'classifier.2', 'classifier.3', 'classifier.4', 'classifier.5', 'classifier.6'] 2021-12-29 19:06:08: Train nodes of ResNet18: ['x', 'conv1', 'bn1', 'relu', 'maxpool', 'layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu', 'layer1.0.conv2', 'layer1.0.bn2', 'layer1.0.add', 'layer1.0.relu_1', 'layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu', 'layer1.1.conv2', 'layer1.1.bn2', 'layer1.1.add', 'layer1.1.relu_1', 'layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu', 'layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.downsample.0', 'layer2.0.downsample.1', 'layer2.0.add', 'layer2.0.relu_1', 'layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.relu', 'layer2.1.conv2', 'layer2.1.bn2', 'layer2.1.add', 'layer2.1.relu_1', 'layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.relu', 'layer3.0.conv2', 'layer3.0.bn2', 'layer3.0.downsample.0', 'layer3.0.downsample.1', 'layer3.0.add', 'layer3.0.relu_1', 'layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.relu', 'layer3.1.conv2', 'layer3.1.bn2', 'layer3.1.add', 'layer3.1.relu_1', 'layer4.0.conv1', 'layer4.0.bn1', 'layer4.0.relu', 'layer4.0.conv2', 'layer4.0.bn2', 'layer4.0.downsample.0', 'layer4.0.downsample.1', 'layer4.0.add', 'layer4.0.relu_1', 'layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.relu', 'layer4.1.conv2', 'layer4.1.bn2', 'layer4.1.add', 'layer4.1.relu_1', 'avgpool', 'flatten', 'fc'] Good God! When I saw the layer names from vgg16 , I nearly fainted, I see no easy way to know which layer belongs to a Conv layer. I understand that get_graph_node_names will get all the nodes on the model's graph, but it is difficult to map the node names to a layer if it is named as such, seeing resnet18 's node names is much easier for one to identify which is conv layer or not. train_nodes , eval_nodes = get_graph_node_names ( model ) logger . info ( f \"The train nodes of the model graph is: \\n\\n { train_nodes } \" ) Thus I wrote a small function get_conv_layers to get the conv layer names. It is not perfect, as downsample layers (1x1 conv layers) are tagged under Conv2d but we may not really need to use them to visualize our feature maps. One can tweak a bit if need be, but for now, I will get all layers that use the Conv2d blocks. If the feature names in vgg16 are named with conv, then we can simply use a small loop below to find the conv layer names. conv_layers = [] for node in nodes : if \"conv\" in node : conv_layers . append ( node ) I actually thought ResNet18 has 18 conv layers, but even minusing to 3 downsample layers, it's 17 conv layers, wonder why? Step 2: Transform the Tensors The PyTorch feature_extraction expects the image input to be of shape [B,C,H,W] . # We use torchvision's transform to transform the cat image to channels first. cat_tensor = transform ( cat ) # Now feature_extractor expects batch_size x C x H x W, so we expand one dimension in the 0th dim cat_tensor = cat_tensor . unsqueeze ( dim = 0 ) . to ( device ) # We use torchvision's transform to transform the cat image with resize and normalization. # Conveniently, also making it channel first! cat_tensor = transform ( cat ) dog_tensor = transform ( dog ) assert cat_tensor . shape [ 0 ] == dog_tensor . shape [ 0 ] == 3 , \"PyTorch expects Channel First!\" # Now feature_extractor expects batch_size x C x H x W, so we expand one dimension in the 0th dim cat_tensor = cat_tensor . unsqueeze ( dim = 0 ) . to ( device ) dog_tensor = dog_tensor . unsqueeze ( dim = 0 ) . to ( device ) logger . info ( f \" \\n\\n cat_tensor's shape: \\n { cat_tensor . shape } \\n\\n dog_tensor's shape: \\n { dog_tensor . shape } \" ) 2021-12-29 19:06:10: cat_tensor's shape: torch.Size([1, 3, 224, 224]) dog_tensor's shape: torch.Size([1, 3, 224, 224]) Step 3: Plotting the Feature Maps We first walk through get_feature_maps and see what my function is doing. # Get node names train_nodes , eval_nodes = get_graph_node_names ( model ) # Since get node names do not indicate properly which is a conv layer or not, # we use get_conv_layer instead to do the job, which returns a dict {\"conv_layer_name\": \"conv_layer_name\"} return_conv_nodes = get_conv_layers ( model ) # call create_feature_extractor on the model and its corresponding conv layer names. feature_extractor = create_feature_extractor ( model , return_nodes = return_conv_nodes ) # `model_feature_maps` will be a dict of Tensors, each representing a feature map # {\"conv_layer_1\": output filter map,...} model_feature_maps = feature_extractor ( image ) # we need to further process the feature maps processed_feature_maps , feature_map_names = [], [] for conv_name , conv_feature_map in model_feature_maps . items (): # Squeeze the dimension from [1, 64, 32, 32] to [64, 32, 32] # This means we have 64 filters of 32x32 \"images\" or kernels conv_feature_map = conv_feature_map . squeeze ( dim = 0 ) # get number of feature/kernels in this layer num_filters = conv_feature_map . shape [ 0 ] # If a feature map has 4 filters, in the shape of (4, H, W) = (4, 32, 32), then the reduction mean can be done as follows: There are 4 filters in this feature map, you can imagine it as 4 32x32 images. # Step 1: We sum up all 4 filters element-wise and get a single 32x32 image. # Step 2: Then we take the mean of all 32x32 images to get a single 32x32 image, which is reduction=\"mean\". if reduction == \"mean\" : gray_scale = torch . sum ( conv_feature_map , dim = 0 ) / num_filters elif reduction == \"max\" : gray_scale = torch . max ( conv_feature_map , dim = 0 ) elif reduction == \"sum\" : gray_scale = torch . sum ( conv_feature_map , dim = 0 ) processed_feature_maps . append ( gray_scale . data . cpu () . numpy ()) feature_map_names . append ( conv_name ) _ , vgg16_processed_feature_maps , vgg16_feature_map_names = get_feature_maps ( model_name = \"vgg16\" , image = cat_tensor , reduction = \"mean\" , pretrained = True ) _ , resnet18_processed_feature_maps , resnet18_feature_map_names = get_feature_maps ( model_name = \"resnet18\" , image = cat_tensor , reduction = \"mean\" , pretrained = True ) 2021-12-29 19:06:12: The train nodes of the model graph is: ['x', 'features.0', 'features.1', 'features.2', 'features.3', 'features.4', 'features.5', 'features.6', 'features.7', 'features.8', 'features.9', 'features.10', 'features.11', 'features.12', 'features.13', 'features.14', 'features.15', 'features.16', 'features.17', 'features.18', 'features.19', 'features.20', 'features.21', 'features.22', 'features.23', 'features.24', 'features.25', 'features.26', 'features.27', 'features.28', 'features.29', 'features.30', 'avgpool', 'flatten', 'classifier.0', 'classifier.1', 'classifier.2', 'classifier.3', 'classifier.4', 'classifier.5', 'classifier.6'] 2021-12-29 19:06:12: The train nodes of the model graph is: ['x', 'conv1', 'bn1', 'relu', 'maxpool', 'layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu', 'layer1.0.conv2', 'layer1.0.bn2', 'layer1.0.add', 'layer1.0.relu_1', 'layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu', 'layer1.1.conv2', 'layer1.1.bn2', 'layer1.1.add', 'layer1.1.relu_1', 'layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu', 'layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.downsample.0', 'layer2.0.downsample.1', 'layer2.0.add', 'layer2.0.relu_1', 'layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.relu', 'layer2.1.conv2', 'layer2.1.bn2', 'layer2.1.add', 'layer2.1.relu_1', 'layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.relu', 'layer3.0.conv2', 'layer3.0.bn2', 'layer3.0.downsample.0', 'layer3.0.downsample.1', 'layer3.0.add', 'layer3.0.relu_1', 'layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.relu', 'layer3.1.conv2', 'layer3.1.bn2', 'layer3.1.add', 'layer3.1.relu_1', 'layer4.0.conv1', 'layer4.0.bn1', 'layer4.0.relu', 'layer4.0.conv2', 'layer4.0.bn2', 'layer4.0.downsample.0', 'layer4.0.downsample.1', 'layer4.0.add', 'layer4.0.relu_1', 'layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.relu', 'layer4.1.conv2', 'layer4.1.bn2', 'layer4.1.add', 'layer4.1.relu_1', 'avgpool', 'flatten', 'fc'] Then we create a simple plot_feature_maps that take in the processed_feature_maps and feature_map_names to plot them. def plot_feature_maps ( processed_feature_maps : List [ torch . Tensor ], feature_map_names : List [ str ], nrows : int , title : str = None ) -> None : \"\"\"Plot the feature maps. Args: processed_feature_maps (List[torch.Tensor]): [description] feature_map_names (List[str]): [description] nrows (int): [description] \"\"\" fig = plt . figure ( figsize = ( 30 , 50 )) ncols = len ( processed_feature_maps ) // nrows + 1 for i in range ( len ( processed_feature_maps )): a = fig . add_subplot ( nrows , ncols , i + 1 ) imgplot = plt . imshow ( processed_feature_maps [ i ]) a . axis ( \"off\" ) a . set_title ( feature_map_names [ i ] . split ( \"(\" )[ 0 ], fontsize = 30 ) fig . suptitle ( title , fontsize = 50 ) fig . tight_layout () fig . subplots_adjust ( top = 0.95 ) plt . savefig ( title , bbox_inches = 'tight' ) plt . show (); plot_feature_maps ( vgg16_processed_feature_maps , vgg16_feature_map_names , nrows = 5 , title = \"VGG16 Pretrained Feature Maps\" , ) plot_feature_maps ( resnet18_processed_feature_maps , resnet18_feature_map_names , nrows = 5 , title = \"ResNet18 Pretrained Feature Maps\" , ) Comparison with Randomly Initialized Weights We know that if the model is not pretrained, it will initialize with random weights using weight initialization methods such as Kaimin or Xavier. I expect the edges to be not so \"smooth\" as the ones that are pretrained! This is logical, as the filters in the conv layers are mostly random, and we have not trained any epochs yet, so let's see what it gives us. _ , vgg16_processed_feature_maps , vgg16_feature_map_names = get_feature_maps ( model_name = \"vgg16\" , image = cat_tensor , reduction = \"mean\" , pretrained = False ) _ , resnet18_processed_feature_maps , resnet18_feature_map_names = get_feature_maps ( model_name = \"resnet18\" , image = cat_tensor , reduction = \"mean\" , pretrained = False ) 2021-12-29 19:06:21: The train nodes of the model graph is: ['x', 'features.0', 'features.1', 'features.2', 'features.3', 'features.4', 'features.5', 'features.6', 'features.7', 'features.8', 'features.9', 'features.10', 'features.11', 'features.12', 'features.13', 'features.14', 'features.15', 'features.16', 'features.17', 'features.18', 'features.19', 'features.20', 'features.21', 'features.22', 'features.23', 'features.24', 'features.25', 'features.26', 'features.27', 'features.28', 'features.29', 'features.30', 'avgpool', 'flatten', 'classifier.0', 'classifier.1', 'classifier.2', 'classifier.3', 'classifier.4', 'classifier.5', 'classifier.6'] 2021-12-29 19:06:21: The train nodes of the model graph is: ['x', 'conv1', 'bn1', 'relu', 'maxpool', 'layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu', 'layer1.0.conv2', 'layer1.0.bn2', 'layer1.0.add', 'layer1.0.relu_1', 'layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu', 'layer1.1.conv2', 'layer1.1.bn2', 'layer1.1.add', 'layer1.1.relu_1', 'layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu', 'layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.downsample.0', 'layer2.0.downsample.1', 'layer2.0.add', 'layer2.0.relu_1', 'layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.relu', 'layer2.1.conv2', 'layer2.1.bn2', 'layer2.1.add', 'layer2.1.relu_1', 'layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.relu', 'layer3.0.conv2', 'layer3.0.bn2', 'layer3.0.downsample.0', 'layer3.0.downsample.1', 'layer3.0.add', 'layer3.0.relu_1', 'layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.relu', 'layer3.1.conv2', 'layer3.1.bn2', 'layer3.1.add', 'layer3.1.relu_1', 'layer4.0.conv1', 'layer4.0.bn1', 'layer4.0.relu', 'layer4.0.conv2', 'layer4.0.bn2', 'layer4.0.downsample.0', 'layer4.0.downsample.1', 'layer4.0.add', 'layer4.0.relu_1', 'layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.relu', 'layer4.1.conv2', 'layer4.1.bn2', 'layer4.1.add', 'layer4.1.relu_1', 'avgpool', 'flatten', 'fc'] plot_feature_maps ( vgg16_processed_feature_maps , vgg16_feature_map_names , nrows = 5 , title = \"VGG16 NOT Pretrained Feature Maps\" , ) plot_feature_maps ( resnet18_processed_feature_maps , resnet18_feature_map_names , nrows = 5 , title = \"ResNet18 NOT Pretrained Feature Maps\" , ) References: https://pytorch.org/vision/stable/feature_extraction.html https://ravivaishnav20.medium.com/visualizing-feature-maps-using-pytorch https://pytorch.org/blog/FX-feature-extraction-torchvision/ https://www.kaggle.com/tarunpaparaju/plant-pathology-2020-eda-models","title":"EDA Visualizations for Image Recognition (Conv Filter Edition)"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/02_conv_filters/Visualizing%20Convolutional%20Filters/#eda-visualizations-for-image-recognition-conv-filter-edition","text":"","title":"EDA Visualizations for Image Recognition (Conv Filter Edition)"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/02_conv_filters/Visualizing%20Convolutional%20Filters/#dependencies-and-imports","text":"! pip install - q timm from typing import Dict import matplotlib.pyplot as plt import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import timm import torch import torchvision from torchvision.models.feature_extraction import ( create_feature_extractor , get_graph_node_names ) % matplotlib inline import glob import os from math import ceil import random import cv2 import PIL from IPython.core.interactiveshell import InteractiveShell InteractiveShell . ast_node_interactivity = \"all\" from typing import * # importing modules import urllib.request from PIL import Image","title":"Dependencies and Imports"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/02_conv_filters/Visualizing%20Convolutional%20Filters/#config-and-logging","text":"import logging from logging import INFO , FileHandler , Formatter , StreamHandler , getLogger def init_logger ( log_file : str = \"info.log\" ) -> logging . Logger : \"\"\"Initialize logger and save to file. Consider having more log_file paths to save, eg: debug.log, error.log, etc. Args: log_file (str, optional): [description]. Defaults to Path(LOGS_DIR, \"info.log\"). Returns: logging.Logger: [description] \"\"\" logger = getLogger ( __name__ ) logger . setLevel ( INFO ) stream_handler = StreamHandler () stream_handler . setFormatter ( Formatter ( \" %(asctime)s : %(message)s \" , \"%Y-%m- %d %H:%M:%S\" ) ) file_handler = FileHandler ( filename = log_file ) file_handler . setFormatter ( Formatter ( \" %(asctime)s : %(message)s \" , \"%Y-%m- %d %H:%M:%S\" ) ) logger . addHandler ( stream_handler ) logger . addHandler ( file_handler ) return logger logger = init_logger ()","title":"Config and Logging"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/02_conv_filters/Visualizing%20Convolutional%20Filters/#utils","text":"def plot_multiple_img ( img_matrix_list , title_list , ncols , main_title = \"\" ): fig , myaxes = plt . subplots ( figsize = ( 20 , 15 ), nrows = ceil ( len ( img_matrix_list ) / ncols ), ncols = ncols , squeeze = False , ) fig . suptitle ( main_title , fontsize = 30 ) fig . subplots_adjust ( wspace = 0.3 ) fig . subplots_adjust ( hspace = 0.3 ) for i , ( img , title ) in enumerate ( zip ( img_matrix_list , title_list )): myaxes [ i // ncols ][ i % ncols ] . imshow ( img ) myaxes [ i // ncols ][ i % ncols ] . set_title ( title , fontsize = 15 ) plt . show ()","title":"Utils"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/02_conv_filters/Visualizing%20Convolutional%20Filters/#seeding","text":"def seed_all ( seed : int = 1992 ) -> None : \"\"\"Seed all random number generators.\"\"\" print ( f \"Using Seed Number { seed } \" ) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) # set PYTHONHASHSEED env var at fixed value torch . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) torch . cuda . manual_seed ( seed ) # pytorch (both CPU and CUDA) np . random . seed ( seed ) # for numpy pseudo-random generator # set fixed value for python built-in pseudo-random generator random . seed ( seed ) torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = True torch . backends . cudnn . enabled = True def seed_worker ( _worker_id ) -> None : \"\"\"Seed a worker with the given ID.\"\"\" worker_seed = torch . initial_seed () % 2 ** 32 np . random . seed ( worker_seed ) random . seed ( worker_seed ) seed_all () Using Seed Number 1992","title":"Seeding"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/02_conv_filters/Visualizing%20Convolutional%20Filters/#transforms-params","text":"mean : List [ float ] = [ 0.485 , 0.456 , 0.406 ] std : List [ float ] = [ 0.229 , 0.224 , 0.225 ] image_size : int = 224 transform = torchvision . transforms . Compose ( [ torchvision . transforms . Resize (( image_size , image_size )), torchvision . transforms . ToTensor (), torchvision . transforms . Normalize ( mean = mean , std = std ), ] ) pre_normalize_transform = torchvision . transforms . Compose ( [ torchvision . transforms . Resize (( image_size , image_size )), torchvision . transforms . ToTensor (), ] )","title":"Transforms Params"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/02_conv_filters/Visualizing%20Convolutional%20Filters/#visualizations","text":"cat_p = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/deep_learning/computer_vision/data/misc/cat.jpg\" dog_p = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/deep_learning/computer_vision/data/misc/dog.jpg\" from urllib.request import urlopen # plot cat and dog with title using PIL plt . figure ( figsize = ( 10 , 10 )) plt . subplot ( 1 , 2 , 1 ) cat = PIL . Image . open ( urlopen ( cat_p )) plt . imshow ( cat . resize (( 1024 , 1024 ))) plt . title ( \"Cat\" ) plt . subplot ( 1 , 2 , 2 ) dog = PIL . Image . open ( urlopen ( dog_p )) plt . imshow ( dog . resize (( 1024 , 1024 ))) plt . title ( \"Dog\" ) plt . show ();","title":"Visualizations"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/02_conv_filters/Visualizing%20Convolutional%20Filters/#convolution-layers","text":"The image and content are referenced with courtesy from [Tarun's notebook](https://www.kaggle.com/tarunpaparaju/plant-pathology-2020-eda-models](https://www.kaggle.com/tarunpaparaju/plant-pathology-2020-eda-models). Convolution is a rather simple algorithm which involves a kernel (a 2D matrix) which moves over the entire image, calculating dot products with each window along the way. The GIF below demonstrates convolution in action. The above process can be summarized with an equation, where f is the image and h is the kernel. The dimensions of f are (m, n) and the kernel is a square matrix with dimensions smaller than f : \\( \\(\\text{conv}(f, h) = \\sum_{j}\\sum_{k}h_{jk} \\cdot f_{(m-j)(n-k)}\\) \\) In the above equation, the kernel h is moving across the length and breadth of the image. The dot product of h with a sub-matrix or window of matrix f is taken at each step, hence the double summation (rows and columns). I have always remembered from the revered Andrew Ng about how he taught us about what convolutional layers do. In the beginning, the conv layers are of low level abstraction, detailing a image's features such as shapes and sizes. In particular, he described to us the horizontal and vertical conv filters. As the conv layers go later, it will pick up on many abstract features, which is not really easily distinguished by human eyes. Below, we see an example of horizontal and vertical filters. def conv_horizontal ( image : np . ndarray ) -> None : \"\"\"Plot the horizontal convolution of the image. Args: image (torch.Tensor): [description] \"\"\" fig , ax = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 20 , 20 )) kernel = np . ones (( 3 , 3 ), np . float32 ) kernel [ 1 ] = np . array ([ 0 , 0 , 0 ], np . float32 ) kernel [ 2 ] = np . array ([ - 1 , - 1 , - 1 ], np . float32 ) conv = cv2 . filter2D ( image , - 1 , kernel ) ax [ 0 ] . imshow ( image ) ax [ 0 ] . set_title ( \"Original Image\" , fontsize = 24 ) ax [ 1 ] . imshow ( conv ) ax [ 1 ] . set_title ( \"Convolved Image with horizontal edges\" , fontsize = 24 ) plt . show () def conv_vertical ( image : np . ndarray ) -> None : \"\"\"Plot the vertical convolution of the image. Args: image (torch.Tensor): [description] \"\"\" fig , ax = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 20 , 20 )) kernel = np . ones (( 3 , 3 ), np . float32 ) kernel [ 0 ] = np . array ([ 1 , 0 , - 1 ]) kernel [ 1 ] = np . array ([ 1 , 0 , - 1 ]) kernel [ 2 ] = np . array ([ 1 , 0 , - 1 ]) conv = cv2 . filter2D ( image , - 1 , kernel ) ax [ 0 ] . imshow ( image ) ax [ 0 ] . set_title ( \"Original Image\" , fontsize = 24 ) ax [ 1 ] . imshow ( conv ) ax [ 1 ] . set_title ( \"Convolved Image with vertical edges\" , fontsize = 24 ) plt . show () Well, I can easily make out the horizontal and vertical edges from the dog image! Actually, not so obvious if you don't look closely! conv_horizontal ( np . asarray ( dog )) conv_vertical ( np . asarray ( dog )) The issue is, I want to visualize what our models' conv layers are seeing, like for example, the first conv layer usually has 64 filters, that is a whooping 64 different combinations of filters, each doing a slightly different thing. A mental model that I have for the first conv layer looks something like the following. conv_1_filters = [ \"vertical edge detector\" , \"horizontal edge detector\" , \"slanted 45 degrees detector\" , \"slanted 180 degrees detector\" , ... ]","title":"Convolution Layers "},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/02_conv_filters/Visualizing%20Convolutional%20Filters/#feature-extractor-using-pytorchs-native-feature-extraction-module","text":"In order to visualize properly, I made use of PyTorch's newest feature_extraction module to do so. Note that the new feature is still in development, but it does make my life easier and reduces overhead. I no longer need use hooks or what not to plot layer information! We just need to import from torchvision.models.feature_extraction import ( create_feature_extractor , get_graph_node_names ) def get_conv_layers ( model : torchvision . models ) -> Dict [ str , str ]: \"\"\"Create a function that give me the conv layers of PyTorch model. Args: model (Union[torchvision.models, timm.models]): A PyTorch model. Returns: conv_layers (Dict[str, str]): {\"layer1.0.conv1\": layer1.0.conv1, ...} \"\"\" conv_layers = {} for name , layer in model . named_modules (): if isinstance ( layer , torch . nn . Conv2d ): conv_layers [ name ] = name return conv_layers def get_feature_maps ( model_name : str , image : torch . Tensor , reduction : str = \"mean\" , pretrained : bool = True ) -> Union [ Dict [ str , torch . Tensor ], List [ torch . Tensor ], List [ str ]]: \"\"\"Function to plot feature maps from PyTorch models. Args: model_name (str): Name of the model to use. image (torch.Tensor): image should be a tensor of shape (1, 3, H, W) reduction (str, optional): Defaults to \"mean\". One of [\"mean\", \"max\", \"sum\"] pretrained (bool): whether the model is pretrained or not Raises: ValueError: Must use Torchvision models. Returns: model_feature_maps (Dict[str, torch.Tensor]): {\"conv_1\": conv_1_feature_map, ...} processed_feature_maps (List[torch.Tensor]): [conv_1_feature_map, ...] processed using a reduction method. feature_map_names (List[str]): [conv_1, ...] Example: >>> from torchvision.models.vgg import vgg16 >>> model = vgg16(pretrained=True) >>> image = torch.rand(1, 3, 224, 224) >>> feature_maps = get_feature_maps(model, image, reduction=\"mean\") Reduction: If a feature map has 4 filters, in the shape of (4, H, W) = (4, 32, 32), then the reduction can be done as follows: >>> reduction = \"mean\": There are 4 filters in this feature map, you can imagine it as 4 32x32 images. We sum up all 4 filters element-wise and get a single 32x32 image. Then we take the mean of all 32x32 images by dividing by num of kernels to get a single 32x32 image, which is reduction=\"mean\". \"\"\" try : model = getattr ( torchvision . models , model_name )( pretrained = pretrained ) except AttributeError : raise ValueError ( f \"Model { model_name } not found.\" ) train_nodes , eval_nodes = get_graph_node_names ( model ) logger . info ( f \"The train nodes of the model graph is: \\n\\n { train_nodes } \" ) return_conv_nodes = get_conv_layers ( model ) feature_extractor = create_feature_extractor ( model , return_nodes = return_conv_nodes ) # `model_feature_maps` will be a dict of Tensors, each representing a feature map model_feature_maps = feature_extractor ( image ) processed_feature_maps = [] feature_map_names = [] for conv_name , conv_feature_map in model_feature_maps . items (): conv_feature_map = conv_feature_map . squeeze ( dim = 0 ) num_filters = conv_feature_map . shape [ 0 ] if reduction == \"mean\" : gray_scale = torch . sum ( conv_feature_map , dim = 0 ) / num_filters elif reduction == \"max\" : gray_scale = torch . max ( conv_feature_map , dim = 0 ) elif reduction == \"sum\" : gray_scale = torch . sum ( conv_feature_map , dim = 0 ) processed_feature_maps . append ( gray_scale . data . cpu () . numpy ()) feature_map_names . append ( conv_name ) return model_feature_maps , processed_feature_maps , feature_map_names","title":"Feature Extractor using PyTorch's native Feature Extraction Module"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/02_conv_filters/Visualizing%20Convolutional%20Filters/#visualizing-vgg16-and-resnet18","text":"","title":"Visualizing VGG16 and ResNet18"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/02_conv_filters/Visualizing%20Convolutional%20Filters/#step-1-initialize-the-models","text":"As of now, I recommend using torchvision 's models. Ideally, I will want to use timm library for a more detailed list, but there are some bugs that is not easily integrated with the module. device = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) import torchvision.models as models vgg16_pretrained_true = models . vgg16 ( pretrained = True ) vgg16_pretrained_true = vgg16_pretrained_true . to ( device ) resnet18_pretrained_true = models . resnet18 ( pretrained = True ) resnet18_pretrained_true = resnet18_pretrained_true . to ( device ) # Get node names train_nodes , eval_nodes = get_graph_node_names ( vgg16_pretrained_true ) logger . info ( f \"Train nodes of VGG16: \\n\\n { train_nodes } \" ) train_nodes , eval_nodes = get_graph_node_names ( resnet18_pretrained_true ) logger . info ( f \"Train nodes of ResNet18: \\n\\n { train_nodes } \" ) 2021-12-29 19:06:08: Train nodes of VGG16: ['x', 'features.0', 'features.1', 'features.2', 'features.3', 'features.4', 'features.5', 'features.6', 'features.7', 'features.8', 'features.9', 'features.10', 'features.11', 'features.12', 'features.13', 'features.14', 'features.15', 'features.16', 'features.17', 'features.18', 'features.19', 'features.20', 'features.21', 'features.22', 'features.23', 'features.24', 'features.25', 'features.26', 'features.27', 'features.28', 'features.29', 'features.30', 'avgpool', 'flatten', 'classifier.0', 'classifier.1', 'classifier.2', 'classifier.3', 'classifier.4', 'classifier.5', 'classifier.6'] 2021-12-29 19:06:08: Train nodes of ResNet18: ['x', 'conv1', 'bn1', 'relu', 'maxpool', 'layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu', 'layer1.0.conv2', 'layer1.0.bn2', 'layer1.0.add', 'layer1.0.relu_1', 'layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu', 'layer1.1.conv2', 'layer1.1.bn2', 'layer1.1.add', 'layer1.1.relu_1', 'layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu', 'layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.downsample.0', 'layer2.0.downsample.1', 'layer2.0.add', 'layer2.0.relu_1', 'layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.relu', 'layer2.1.conv2', 'layer2.1.bn2', 'layer2.1.add', 'layer2.1.relu_1', 'layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.relu', 'layer3.0.conv2', 'layer3.0.bn2', 'layer3.0.downsample.0', 'layer3.0.downsample.1', 'layer3.0.add', 'layer3.0.relu_1', 'layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.relu', 'layer3.1.conv2', 'layer3.1.bn2', 'layer3.1.add', 'layer3.1.relu_1', 'layer4.0.conv1', 'layer4.0.bn1', 'layer4.0.relu', 'layer4.0.conv2', 'layer4.0.bn2', 'layer4.0.downsample.0', 'layer4.0.downsample.1', 'layer4.0.add', 'layer4.0.relu_1', 'layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.relu', 'layer4.1.conv2', 'layer4.1.bn2', 'layer4.1.add', 'layer4.1.relu_1', 'avgpool', 'flatten', 'fc'] Good God! When I saw the layer names from vgg16 , I nearly fainted, I see no easy way to know which layer belongs to a Conv layer. I understand that get_graph_node_names will get all the nodes on the model's graph, but it is difficult to map the node names to a layer if it is named as such, seeing resnet18 's node names is much easier for one to identify which is conv layer or not. train_nodes , eval_nodes = get_graph_node_names ( model ) logger . info ( f \"The train nodes of the model graph is: \\n\\n { train_nodes } \" ) Thus I wrote a small function get_conv_layers to get the conv layer names. It is not perfect, as downsample layers (1x1 conv layers) are tagged under Conv2d but we may not really need to use them to visualize our feature maps. One can tweak a bit if need be, but for now, I will get all layers that use the Conv2d blocks. If the feature names in vgg16 are named with conv, then we can simply use a small loop below to find the conv layer names. conv_layers = [] for node in nodes : if \"conv\" in node : conv_layers . append ( node ) I actually thought ResNet18 has 18 conv layers, but even minusing to 3 downsample layers, it's 17 conv layers, wonder why?","title":"Step 1: Initialize the models."},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/02_conv_filters/Visualizing%20Convolutional%20Filters/#step-2-transform-the-tensors","text":"The PyTorch feature_extraction expects the image input to be of shape [B,C,H,W] . # We use torchvision's transform to transform the cat image to channels first. cat_tensor = transform ( cat ) # Now feature_extractor expects batch_size x C x H x W, so we expand one dimension in the 0th dim cat_tensor = cat_tensor . unsqueeze ( dim = 0 ) . to ( device ) # We use torchvision's transform to transform the cat image with resize and normalization. # Conveniently, also making it channel first! cat_tensor = transform ( cat ) dog_tensor = transform ( dog ) assert cat_tensor . shape [ 0 ] == dog_tensor . shape [ 0 ] == 3 , \"PyTorch expects Channel First!\" # Now feature_extractor expects batch_size x C x H x W, so we expand one dimension in the 0th dim cat_tensor = cat_tensor . unsqueeze ( dim = 0 ) . to ( device ) dog_tensor = dog_tensor . unsqueeze ( dim = 0 ) . to ( device ) logger . info ( f \" \\n\\n cat_tensor's shape: \\n { cat_tensor . shape } \\n\\n dog_tensor's shape: \\n { dog_tensor . shape } \" ) 2021-12-29 19:06:10: cat_tensor's shape: torch.Size([1, 3, 224, 224]) dog_tensor's shape: torch.Size([1, 3, 224, 224])","title":"Step 2: Transform the Tensors"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/02_conv_filters/Visualizing%20Convolutional%20Filters/#step-3-plotting-the-feature-maps","text":"We first walk through get_feature_maps and see what my function is doing. # Get node names train_nodes , eval_nodes = get_graph_node_names ( model ) # Since get node names do not indicate properly which is a conv layer or not, # we use get_conv_layer instead to do the job, which returns a dict {\"conv_layer_name\": \"conv_layer_name\"} return_conv_nodes = get_conv_layers ( model ) # call create_feature_extractor on the model and its corresponding conv layer names. feature_extractor = create_feature_extractor ( model , return_nodes = return_conv_nodes ) # `model_feature_maps` will be a dict of Tensors, each representing a feature map # {\"conv_layer_1\": output filter map,...} model_feature_maps = feature_extractor ( image ) # we need to further process the feature maps processed_feature_maps , feature_map_names = [], [] for conv_name , conv_feature_map in model_feature_maps . items (): # Squeeze the dimension from [1, 64, 32, 32] to [64, 32, 32] # This means we have 64 filters of 32x32 \"images\" or kernels conv_feature_map = conv_feature_map . squeeze ( dim = 0 ) # get number of feature/kernels in this layer num_filters = conv_feature_map . shape [ 0 ] # If a feature map has 4 filters, in the shape of (4, H, W) = (4, 32, 32), then the reduction mean can be done as follows: There are 4 filters in this feature map, you can imagine it as 4 32x32 images. # Step 1: We sum up all 4 filters element-wise and get a single 32x32 image. # Step 2: Then we take the mean of all 32x32 images to get a single 32x32 image, which is reduction=\"mean\". if reduction == \"mean\" : gray_scale = torch . sum ( conv_feature_map , dim = 0 ) / num_filters elif reduction == \"max\" : gray_scale = torch . max ( conv_feature_map , dim = 0 ) elif reduction == \"sum\" : gray_scale = torch . sum ( conv_feature_map , dim = 0 ) processed_feature_maps . append ( gray_scale . data . cpu () . numpy ()) feature_map_names . append ( conv_name ) _ , vgg16_processed_feature_maps , vgg16_feature_map_names = get_feature_maps ( model_name = \"vgg16\" , image = cat_tensor , reduction = \"mean\" , pretrained = True ) _ , resnet18_processed_feature_maps , resnet18_feature_map_names = get_feature_maps ( model_name = \"resnet18\" , image = cat_tensor , reduction = \"mean\" , pretrained = True ) 2021-12-29 19:06:12: The train nodes of the model graph is: ['x', 'features.0', 'features.1', 'features.2', 'features.3', 'features.4', 'features.5', 'features.6', 'features.7', 'features.8', 'features.9', 'features.10', 'features.11', 'features.12', 'features.13', 'features.14', 'features.15', 'features.16', 'features.17', 'features.18', 'features.19', 'features.20', 'features.21', 'features.22', 'features.23', 'features.24', 'features.25', 'features.26', 'features.27', 'features.28', 'features.29', 'features.30', 'avgpool', 'flatten', 'classifier.0', 'classifier.1', 'classifier.2', 'classifier.3', 'classifier.4', 'classifier.5', 'classifier.6'] 2021-12-29 19:06:12: The train nodes of the model graph is: ['x', 'conv1', 'bn1', 'relu', 'maxpool', 'layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu', 'layer1.0.conv2', 'layer1.0.bn2', 'layer1.0.add', 'layer1.0.relu_1', 'layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu', 'layer1.1.conv2', 'layer1.1.bn2', 'layer1.1.add', 'layer1.1.relu_1', 'layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu', 'layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.downsample.0', 'layer2.0.downsample.1', 'layer2.0.add', 'layer2.0.relu_1', 'layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.relu', 'layer2.1.conv2', 'layer2.1.bn2', 'layer2.1.add', 'layer2.1.relu_1', 'layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.relu', 'layer3.0.conv2', 'layer3.0.bn2', 'layer3.0.downsample.0', 'layer3.0.downsample.1', 'layer3.0.add', 'layer3.0.relu_1', 'layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.relu', 'layer3.1.conv2', 'layer3.1.bn2', 'layer3.1.add', 'layer3.1.relu_1', 'layer4.0.conv1', 'layer4.0.bn1', 'layer4.0.relu', 'layer4.0.conv2', 'layer4.0.bn2', 'layer4.0.downsample.0', 'layer4.0.downsample.1', 'layer4.0.add', 'layer4.0.relu_1', 'layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.relu', 'layer4.1.conv2', 'layer4.1.bn2', 'layer4.1.add', 'layer4.1.relu_1', 'avgpool', 'flatten', 'fc'] Then we create a simple plot_feature_maps that take in the processed_feature_maps and feature_map_names to plot them. def plot_feature_maps ( processed_feature_maps : List [ torch . Tensor ], feature_map_names : List [ str ], nrows : int , title : str = None ) -> None : \"\"\"Plot the feature maps. Args: processed_feature_maps (List[torch.Tensor]): [description] feature_map_names (List[str]): [description] nrows (int): [description] \"\"\" fig = plt . figure ( figsize = ( 30 , 50 )) ncols = len ( processed_feature_maps ) // nrows + 1 for i in range ( len ( processed_feature_maps )): a = fig . add_subplot ( nrows , ncols , i + 1 ) imgplot = plt . imshow ( processed_feature_maps [ i ]) a . axis ( \"off\" ) a . set_title ( feature_map_names [ i ] . split ( \"(\" )[ 0 ], fontsize = 30 ) fig . suptitle ( title , fontsize = 50 ) fig . tight_layout () fig . subplots_adjust ( top = 0.95 ) plt . savefig ( title , bbox_inches = 'tight' ) plt . show (); plot_feature_maps ( vgg16_processed_feature_maps , vgg16_feature_map_names , nrows = 5 , title = \"VGG16 Pretrained Feature Maps\" , ) plot_feature_maps ( resnet18_processed_feature_maps , resnet18_feature_map_names , nrows = 5 , title = \"ResNet18 Pretrained Feature Maps\" , )","title":"Step 3: Plotting the Feature Maps"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/02_conv_filters/Visualizing%20Convolutional%20Filters/#comparison-with-randomly-initialized-weights","text":"We know that if the model is not pretrained, it will initialize with random weights using weight initialization methods such as Kaimin or Xavier. I expect the edges to be not so \"smooth\" as the ones that are pretrained! This is logical, as the filters in the conv layers are mostly random, and we have not trained any epochs yet, so let's see what it gives us. _ , vgg16_processed_feature_maps , vgg16_feature_map_names = get_feature_maps ( model_name = \"vgg16\" , image = cat_tensor , reduction = \"mean\" , pretrained = False ) _ , resnet18_processed_feature_maps , resnet18_feature_map_names = get_feature_maps ( model_name = \"resnet18\" , image = cat_tensor , reduction = \"mean\" , pretrained = False ) 2021-12-29 19:06:21: The train nodes of the model graph is: ['x', 'features.0', 'features.1', 'features.2', 'features.3', 'features.4', 'features.5', 'features.6', 'features.7', 'features.8', 'features.9', 'features.10', 'features.11', 'features.12', 'features.13', 'features.14', 'features.15', 'features.16', 'features.17', 'features.18', 'features.19', 'features.20', 'features.21', 'features.22', 'features.23', 'features.24', 'features.25', 'features.26', 'features.27', 'features.28', 'features.29', 'features.30', 'avgpool', 'flatten', 'classifier.0', 'classifier.1', 'classifier.2', 'classifier.3', 'classifier.4', 'classifier.5', 'classifier.6'] 2021-12-29 19:06:21: The train nodes of the model graph is: ['x', 'conv1', 'bn1', 'relu', 'maxpool', 'layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu', 'layer1.0.conv2', 'layer1.0.bn2', 'layer1.0.add', 'layer1.0.relu_1', 'layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu', 'layer1.1.conv2', 'layer1.1.bn2', 'layer1.1.add', 'layer1.1.relu_1', 'layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu', 'layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.downsample.0', 'layer2.0.downsample.1', 'layer2.0.add', 'layer2.0.relu_1', 'layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.relu', 'layer2.1.conv2', 'layer2.1.bn2', 'layer2.1.add', 'layer2.1.relu_1', 'layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.relu', 'layer3.0.conv2', 'layer3.0.bn2', 'layer3.0.downsample.0', 'layer3.0.downsample.1', 'layer3.0.add', 'layer3.0.relu_1', 'layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.relu', 'layer3.1.conv2', 'layer3.1.bn2', 'layer3.1.add', 'layer3.1.relu_1', 'layer4.0.conv1', 'layer4.0.bn1', 'layer4.0.relu', 'layer4.0.conv2', 'layer4.0.bn2', 'layer4.0.downsample.0', 'layer4.0.downsample.1', 'layer4.0.add', 'layer4.0.relu_1', 'layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.relu', 'layer4.1.conv2', 'layer4.1.bn2', 'layer4.1.add', 'layer4.1.relu_1', 'avgpool', 'flatten', 'fc'] plot_feature_maps ( vgg16_processed_feature_maps , vgg16_feature_map_names , nrows = 5 , title = \"VGG16 NOT Pretrained Feature Maps\" , ) plot_feature_maps ( resnet18_processed_feature_maps , resnet18_feature_map_names , nrows = 5 , title = \"ResNet18 NOT Pretrained Feature Maps\" , ) References: https://pytorch.org/vision/stable/feature_extraction.html https://ravivaishnav20.medium.com/visualizing-feature-maps-using-pytorch https://pytorch.org/blog/FX-feature-extraction-torchvision/ https://www.kaggle.com/tarunpaparaju/plant-pathology-2020-eda-models","title":"Comparison with Randomly Initialized Weights"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/%28archived%29%20gradcam_from_scratch/","text":"\\[ \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\L}{\\mathbf{L}} \\newcommand{\\X}{\\mathbf{X}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\] Dependencies and Config from typing import * import cv2 import matplotlib.pyplot as plt import numpy as np import torch import torch.nn as nn import torchsummary import torchvision from torch.utils import data from torchvision import datasets , transforms from torchvision.models import vgg19 , resnet34 import os import random # Display from IPython.display import Image , display from pytorch_grad_cam import GradCAM from pytorch_grad_cam.utils.image import show_cam_on_image % cd .. import utils device = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) utils . seed_all () We will be using three sample images from ImageNet dataset. elephant_path = \"../images/animals/elephant.png\" single_cat_path = \"../images/animals/cat.jpg\" dog_and_cat_path = \"../images/animals/dog_and_cat.jpg\" Dataset and Dataloader We create a simple PyTorch Dataset that reads and return one single image, this is to illustrate our example easily. from torch.utils.data import Dataset class CustomDataset ( Dataset ): def __init__ ( self , image_path , target = None , transform = None ): \"\"\"This Dataset reads 1 image at a time. Args: image_path (_type_): The path to the image target (_type_): The target class of the image transform (_type_, optional): The transform to apply to the image \"\"\" self . image_path = image_path self . target = target self . transform = transform def __len__ ( self ): \"\"\"This loader only returns 1 image at a time Returns: The length of the dataset \"\"\" return 1 def __getitem__ ( self , x ): # read 1 image only so no need subset idx image_path = self . image_path image = cv2 . imread ( image_path ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) if self . target is None : # force assign 0 tensor to be compatible with loader self . target = torch . zeros ( 0 ) if self . transform is not None : image = self . transform ( image ) return image , self . target Transforms We use some default transforms of image size 224 with ImageNet mean and standard deviation. # use the ImageNet transformation transform = transforms . Compose ( [ transforms . ToPILImage (), transforms . Resize (( 224 , 224 )), transforms . ToTensor (), transforms . Normalize ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ] ), ] ) Create the DataLoaders We create the three datasets and dataloaders below. # define a 1 image dataset elephant_dataset = CustomDataset ( image_path = elephant_path , transform = transform ) cat_dataset = CustomDataset ( image_path = single_cat_path , transform = transform ) dog_and_cat_dataset = CustomDataset ( image_path = dog_and_cat_path , transform = transform ) # define the dataloader to load that single image elephant_dataloader = torch . utils . data . DataLoader ( dataset = elephant_dataset , shuffle = False , batch_size = 1 ) cat_dataloader = torch . utils . data . DataLoader ( dataset = cat_dataset , shuffle = False , batch_size = 1 ) dog_and_cat_dataloader = torch . utils . data . DataLoader ( dataset = dog_and_cat_dataset , shuffle = False , batch_size = 1 ) Sanity Check We get a feel if our dataloader is working by plotting the image in the loader. # get some random training images dataiter = iter ( dog_and_cat_dataloader ) images , targets = dataiter . next () # show images plt . imshow ( np . transpose ( torchvision . utils . make_grid ( images ), ( 1 , 2 , 0 ))) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). <matplotlib.image.AxesImage at 0x17f73646640> Models Checking the Layers We use a library to output the layer information like Keras's model.summary() . We need to identify the last convolutional layer. def torchsummary_wrapper ( model , image_size : Tuple [ int , int , int ] ) -> torchsummary . model_statistics . ModelStatistics : \"\"\"A torch wrapper to print out layers of a Model. Args: model (CustomNeuralNet): Model. image_size (Tuple[int, int, int]): Image size as a tuple of (channels, height, width). Returns: model_summary (torchsummary.model_statistics.ModelStatistics): Model summary. \"\"\" model_summary = torchsummary . summary ( model , image_size ) return model_summary vgg19_ = vgg19 ( pretrained = True ) resnet34_ = resnet34 ( pretrained = True ) Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to C:\\Users\\reighns/.cache\\torch\\hub\\checkpoints\\resnet34-b627a593.pth 0%| | 0.00/83.3M [00:00<?, ?B/s] vgg19_model_summary = torchsummary_wrapper ( vgg19_ , image_size = ( 3 , 224 , 224 )) ========================================================================================== Layer (type:depth-idx) Output Shape Param # ========================================================================================== \u251c\u2500Sequential: 1-1 [-1, 512, 7, 7] -- | \u2514\u2500Conv2d: 2-1 [-1, 64, 224, 224] 1,792 | \u2514\u2500ReLU: 2-2 [-1, 64, 224, 224] -- | \u2514\u2500Conv2d: 2-3 [-1, 64, 224, 224] 36,928 | \u2514\u2500ReLU: 2-4 [-1, 64, 224, 224] -- | \u2514\u2500MaxPool2d: 2-5 [-1, 64, 112, 112] -- | \u2514\u2500Conv2d: 2-6 [-1, 128, 112, 112] 73,856 | \u2514\u2500ReLU: 2-7 [-1, 128, 112, 112] -- | \u2514\u2500Conv2d: 2-8 [-1, 128, 112, 112] 147,584 | \u2514\u2500ReLU: 2-9 [-1, 128, 112, 112] -- | \u2514\u2500MaxPool2d: 2-10 [-1, 128, 56, 56] -- | \u2514\u2500Conv2d: 2-11 [-1, 256, 56, 56] 295,168 | \u2514\u2500ReLU: 2-12 [-1, 256, 56, 56] -- | \u2514\u2500Conv2d: 2-13 [-1, 256, 56, 56] 590,080 | \u2514\u2500ReLU: 2-14 [-1, 256, 56, 56] -- | \u2514\u2500Conv2d: 2-15 [-1, 256, 56, 56] 590,080 | \u2514\u2500ReLU: 2-16 [-1, 256, 56, 56] -- | \u2514\u2500Conv2d: 2-17 [-1, 256, 56, 56] 590,080 | \u2514\u2500ReLU: 2-18 [-1, 256, 56, 56] -- | \u2514\u2500MaxPool2d: 2-19 [-1, 256, 28, 28] -- | \u2514\u2500Conv2d: 2-20 [-1, 512, 28, 28] 1,180,160 | \u2514\u2500ReLU: 2-21 [-1, 512, 28, 28] -- | \u2514\u2500Conv2d: 2-22 [-1, 512, 28, 28] 2,359,808 | \u2514\u2500ReLU: 2-23 [-1, 512, 28, 28] -- | \u2514\u2500Conv2d: 2-24 [-1, 512, 28, 28] 2,359,808 | \u2514\u2500ReLU: 2-25 [-1, 512, 28, 28] -- | \u2514\u2500Conv2d: 2-26 [-1, 512, 28, 28] 2,359,808 | \u2514\u2500ReLU: 2-27 [-1, 512, 28, 28] -- | \u2514\u2500MaxPool2d: 2-28 [-1, 512, 14, 14] -- | \u2514\u2500Conv2d: 2-29 [-1, 512, 14, 14] 2,359,808 | \u2514\u2500ReLU: 2-30 [-1, 512, 14, 14] -- | \u2514\u2500Conv2d: 2-31 [-1, 512, 14, 14] 2,359,808 | \u2514\u2500ReLU: 2-32 [-1, 512, 14, 14] -- | \u2514\u2500Conv2d: 2-33 [-1, 512, 14, 14] 2,359,808 | \u2514\u2500ReLU: 2-34 [-1, 512, 14, 14] -- | \u2514\u2500Conv2d: 2-35 [-1, 512, 14, 14] 2,359,808 | \u2514\u2500ReLU: 2-36 [-1, 512, 14, 14] -- | \u2514\u2500MaxPool2d: 2-37 [-1, 512, 7, 7] -- \u251c\u2500AdaptiveAvgPool2d: 1-2 [-1, 512, 7, 7] -- \u251c\u2500Sequential: 1-3 [-1, 1000] -- | \u2514\u2500Linear: 2-38 [-1, 4096] 102,764,544 | \u2514\u2500ReLU: 2-39 [-1, 4096] -- | \u2514\u2500Dropout: 2-40 [-1, 4096] -- | \u2514\u2500Linear: 2-41 [-1, 4096] 16,781,312 | \u2514\u2500ReLU: 2-42 [-1, 4096] -- | \u2514\u2500Dropout: 2-43 [-1, 4096] -- | \u2514\u2500Linear: 2-44 [-1, 1000] 4,097,000 ========================================================================================== Total params: 143,667,240 Trainable params: 143,667,240 Non-trainable params: 0 Total mult-adds (G): 19.78 ========================================================================================== Input size (MB): 0.57 Forward/backward pass size (MB): 113.38 Params size (MB): 548.05 Estimated Total Size (MB): 662.00 ========================================================================================== # resnet34_model_summary = torchsummary_wrapper(resnet34_, image_size = (3, 224, 224)) Define our Target Layers We define what our target layers would be for the respective models. # this is the last conv layer layer 35 from Francis's book right after conv2d + relu, but https://github.com/jacobgil/pytorch-grad-cam suggests [-1] where it is max pooled. Can try both. vgg19_last_conv_layer = vgg19_ . features [ - 2 ] print ( vgg19_last_conv_layer ) resnet34_last_conv_layer = resnet34_ . layer4 [ - 1 ] print ( resnet34_last_conv_layer ) ReLU(inplace=True) BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) Store Forward and Backward Hooks These function help us to get forward activations and backward gradients. forward_activations = OrderedDict () backward_gradients = OrderedDict () def get_forward_hook ( name : str ) -> Callable : \"\"\"Get the intermediate features of a model. Forward Hook. This is using forward hook with reference https://discuss.pytorch.org/t/how-can-l-load-my-best-model-as-a-feature-extractor-evaluator/17254/5 Args: name (str): name of the layer. Returns: Callable: [description] \"\"\" def forward_hook ( model , input , output ): forward_activations [ name ] = output . detach () return forward_hook def get_backward_hook ( name : str ) -> Callable : \"\"\"Get the intermediate features of a model. Backward Hook. This is using backward hook with reference https://discuss.pytorch.org/t/how-can-l-load-my-best-model-as-a-feature-extractor-evaluator/17254/5 Args: name (str): name of the layer. Returns: Callable: [description] \"\"\" def backward_hook ( model , grad_input , grad_output ): backward_gradients [ name ] = grad_output [ 0 ] . detach () return backward_hook Step by Step Walkthrough Skip to the next section if you are familiar with the Grad-CAM process. vgg19_last_conv_layer . register_forward_hook ( get_forward_hook ( \"features.35\" )) vgg19_last_conv_layer . register_backward_hook ( get_backward_hook ( \"features.35\" )) # register_backward_hook deprecated but for now we use it <torch.utils.hooks.RemovableHandle at 0x1a39f0623a0> # set the evaluation mode vgg19_ . eval () # get the image from the dataloader image , _ = next ( iter ( elephant_dataloader )) # the y_logits before softmax y_logits = vgg19_ ( image ) target_category = None if target_category is None : # get the most likely prediction of the model target_category = vgg19_ ( image ) . argmax ( dim = 1 ) target_category = target_category . item () # convert to scalar # get the activations of the last convolutional layer forward_conv_activations = forward_activations [ \"features.35\" ] y_logits [:, target_category ] . backward () dyc_dA = backward_gradients [ \"features.35\" ] # average pool the gradients across the channels global_average_pooled_gradients = torch . mean ( dyc_dA , dim = [ 0 , 2 , 3 ]) num_feature_maps = 512 weighted_localization_map = torch . clone ( forward_conv_activations ) # weight the channels by corresponding gradients for i in range ( num_feature_maps ): weighted_localization_map [:, i , :, :] *= global_average_pooled_gradients [ i ] # sum the channels of the activations weighted_localization_map = torch . sum ( weighted_localization_map , dim = 1 ) . squeeze () assert weighted_localization_map . shape == ( 14 , 14 ) # relu on top of the heatmap expression (2) in https://arxiv.org/pdf/1610.02391.pdf relu_weighted_localization_map = torch . nn . ReLU ( inplace = False )( weighted_localization_map ) # normalize the heatmap, scale features to between 0 and 1 to plot heatmap = relu_weighted_localization_map / torch . max ( relu_weighted_localization_map ) # draw the heatmap plt . matshow ( heatmap . squeeze ()) tensor([[0.1695, 0.1367, 0.1292, 0.1435, 0.1037, 0.0376, 0.0048, 0.0520, 0.1114, 0.1495, 0.1856, 0.1638, 0.0733, 0.2405], [0.1262, 0.0933, 0.1134, 0.1632, 0.0805, 0.0000, 0.0000, 0.0254, 0.1456, 0.2702, 0.2565, 0.1883, 0.0819, 0.1331], [0.1508, 0.1267, 0.1541, 0.2010, 0.0570, 0.0000, 0.0000, 0.0973, 0.5866, 0.8733, 0.8954, 0.7577, 0.2693, 0.2221], [0.1884, 0.1376, 0.0959, 0.0888, 0.0000, 0.0000, 0.0715, 0.3818, 0.8004, 1.0000, 0.9425, 0.7478, 0.2823, 0.1800], [0.1931, 0.1158, 0.0351, 0.1330, 0.1786, 0.1146, 0.1243, 0.3891, 0.6943, 0.7773, 0.6895, 0.5610, 0.2427, 0.1264], [0.1552, 0.1056, 0.0473, 0.2595, 0.4205, 0.4517, 0.3047, 0.2987, 0.3538, 0.3354, 0.3093, 0.3014, 0.2003, 0.1028], [0.1290, 0.0747, 0.0794, 0.2144, 0.3984, 0.4928, 0.3979, 0.2380, 0.1140, 0.0156, 0.0354, 0.1017, 0.0838, 0.0565], [0.1351, 0.0931, 0.0892, 0.2174, 0.2635, 0.2547, 0.2120, 0.0853, 0.0532, 0.0892, 0.0746, 0.0637, 0.0590, 0.0669], [0.1265, 0.1018, 0.1014, 0.1031, 0.0350, 0.0000, 0.0000, 0.0000, 0.0760, 0.1283, 0.0963, 0.0719, 0.0648, 0.1028], [0.1022, 0.0842, 0.0833, 0.0784, 0.0661, 0.0453, 0.0620, 0.0855, 0.0810, 0.0651, 0.0514, 0.0575, 0.0807, 0.0874], [0.0744, 0.0565, 0.0446, 0.0513, 0.0579, 0.0656, 0.0686, 0.0619, 0.0518, 0.0432, 0.0352, 0.0422, 0.0603, 0.0671], [0.0691, 0.0449, 0.0282, 0.0311, 0.0371, 0.0427, 0.0493, 0.0538, 0.0555, 0.0554, 0.0536, 0.0601, 0.0876, 0.1001], [0.0985, 0.0838, 0.0768, 0.0767, 0.0800, 0.0848, 0.0896, 0.0934, 0.0930, 0.0902, 0.0896, 0.0985, 0.1177, 0.1245], [0.1639, 0.1344, 0.1271, 0.1290, 0.1266, 0.1226, 0.1187, 0.1174, 0.1148, 0.1075, 0.1006, 0.0996, 0.1054, 0.1203]]) <matplotlib.image.AxesImage at 0x1a39f0dc0d0> img = cv2 . imread ( elephant_path ) heatmap = cv2 . resize ( heatmap . cpu () . detach () . numpy (), ( img . shape [ 1 ], img . shape [ 0 ])) heatmap = np . uint8 ( 255 * heatmap ) heatmap = cv2 . applyColorMap ( heatmap , cv2 . COLORMAP_JET ) superimposed_img = heatmap * 0.4 + img cv2 . imwrite ( './elephant_gradcam.jpg' , superimposed_img ) True Function for Grad-CAM Heatmap def make_gradcam_heatmap ( target_layer : torch . nn . Module , target_layer_name : str , model : torch . nn . Module , image : torch . Tensor , target_category : Optional [ int ] = None , ): \"\"\"_summary_ Args: target_layer (str): _description_ model (torch.nn.Module): _description_ image (torch.Tensor): _description_ target_category (Optional[int], optional): _description_. Defaults to None. \"\"\" target_layer . register_forward_hook ( get_forward_hook ( target_layer_name ) ) target_layer . register_backward_hook ( get_backward_hook ( target_layer_name ) ) # register_backward_hook deprecated but for now we use it model . eval () # the y_logits before softmax y_logits = model ( image ) if target_category is None : # get the most likely prediction of the model target_category = model ( image ) . argmax ( dim = 1 ) target_category = target_category . item () # convert to scalar # get the activations of the last convolutional layer forward_conv_activations = forward_activations [ target_layer_name ] y_logits [:, target_category ] . backward () dyc_dA = backward_gradients [ target_layer_name ] # average pool the gradients across the channels global_average_pooled_gradients = torch . mean ( dyc_dA , dim = [ 0 , 2 , 3 ]) num_feature_maps = forward_conv_activations . squeeze () . shape [ 0 ] weighted_localization_map = torch . clone ( forward_conv_activations ) # weight the channels by corresponding gradients for i in range ( num_feature_maps ): weighted_localization_map [ :, i , :, : ] *= global_average_pooled_gradients [ i ] # sum the channels of the activations weighted_localization_map = torch . sum ( weighted_localization_map , dim = 1 ) . squeeze () # relu on top of the heatmap expression (2) in https://arxiv.org/pdf/1610.02391.pdf relu_weighted_localization_map = torch . nn . ReLU ( inplace = False )( weighted_localization_map ) # normalize the heatmap, scale features to between 0 and 1 to plot heatmap = relu_weighted_localization_map / torch . max ( relu_weighted_localization_map ) # draw the heatmap plt . matshow ( heatmap . squeeze ()) return heatmap cat_image , _ = next ( iter ( cat_dataloader )) dog_and_cat_image , _ = next ( iter ( dog_and_cat_dataloader )) elephant_image , _ = next ( iter ( elephant_dataloader )) dog_and_cat_heatmap = make_gradcam_heatmap ( target_layer = vgg19_last_conv_layer , target_layer_name = \"vgg19.feature.35\" , model = vgg19_ , image = dog_and_cat_image , target_category = None ) dog_and_cat_heatmap_resnet34 = make_gradcam_heatmap ( target_layer = resnet34_last_conv_layer , target_layer_name = \"resnet34.layer4[-1]\" , model = resnet34_ , image = dog_and_cat_image , target_category = None ) dog_and_cat_heatmap_resnet34_target_dog = make_gradcam_heatmap ( target_layer = resnet34_last_conv_layer , target_layer_name = \"resnet34.layer4[-1]\" , model = resnet34_ , image = dog_and_cat_image , target_category = 260 ) dog_and_cat_heatmap_resnet34_target_cat = make_gradcam_heatmap ( target_layer = resnet34_last_conv_layer , target_layer_name = \"resnet34.layer4[-1]\" , model = resnet34_ , image = dog_and_cat_image , target_category = 285 ) cat_heatmap = make_gradcam_heatmap ( target_layer = vgg19_last_conv_layer , target_layer_name = \"vgg19.feature.35\" , model = vgg19_ , image = cat_image , target_category = None ) cat_heatmap_resnet34 = make_gradcam_heatmap ( target_layer = resnet34_last_conv_layer , target_layer_name = \"resnet34.layer4[-1]\" , model = resnet34_ , image = cat_image , target_category = None ) elephant_heatmap = make_gradcam_heatmap ( target_layer = vgg19_last_conv_layer , target_layer_name = \"vgg19.features.35\" , model = vgg19_ , image = elephant_image , target_category = None ) elephant_heatmap_resnet34 = make_gradcam_heatmap ( target_layer = resnet34_last_conv_layer , target_layer_name = \"resnet34.layer4[-1]\" , model = resnet34_ , image = elephant_image , target_category = None ) # def save_and_display_gradcam( # image_path, heatmap, image_size=None, cam_path=\"./cat_gradcam.jpg\", alpha=0.4 # ): # # cv2 imread auto reads as BGR # image = cv2.imread(image_path) # if image_size is not None: # image = cv2.resize(image, (image_size, image_size)) # if not isinstance(heatmap, np.ndarray): # heatmap = heatmap.cpu().detach().numpy() # heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0])) # heatmap = np.uint8(255 * heatmap) # heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET) # superimposed_image = heatmap * alpha + image # cv2.imwrite(cam_path, superimposed_image) # return np.uint8(superimposed_image) def save_and_display_gradcam ( image_path , heatmap , image_size = None , cam_path = \"./cat_gradcam.jpg\" , alpha = 0.4 , use_rgb = False , ): \"\"\"_summary_ Args: image_path (_type_): _description_ heatmap (_type_): _description_ image_size (_type_, optional): _description_. Defaults to None. cam_path (str, optional): _description_. Defaults to \"./cat_gradcam.jpg\". alpha (float, optional): _description_. Defaults to 0.4. Returns: _type_: _description_ \"\"\" # Load original image, cv2 imread auto reads as BGR image = cv2 . imread ( image_path ) if image_size is not None : image = cv2 . resize ( image , ( image_size , image_size )) if isinstance ( heatmap , torch . Tensor ): heatmap = heatmap . cpu () . detach () . numpy () # Rescale heatmap to a range 0-255 heatmap = np . uint8 ( 255 * heatmap ) # Use Jet colormap to colorize heatmap heatmap = cv2 . applyColorMap ( heatmap , cv2 . COLORMAP_JET ) if use_rgb : image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) heatmap = cv2 . cvtColor ( heatmap , cv2 . COLOR_BGR2RGB ) # resize heatmap to original image size heatmap = cv2 . resize ( heatmap , ( image . shape [ 1 ], image . shape [ 0 ])) superimposed_image = heatmap * alpha + image cv2 . imwrite ( cam_path , superimposed_image ) # Display Grad CAM display ( Image ( cam_path )) return np . uint8 ( superimposed_image ) # def save_and_display_gradcam( # image_path, heatmap, cam_path=\"./cat_gradcam.jpg\", alpha=0.4 # ): # image = cv2.imread(image_path) # heatmap = cv2.resize( # heatmap.cpu().detach().numpy(), (image.shape[1], image.shape[0]) # ) # heatmap = np.uint8(255 * heatmap) # heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET) # superimposed_image = heatmap * alpha + image # cv2.imwrite(cam_path, superimposed_image) # return superimposed_image elephant_gradcam_vgg19 = save_and_display_gradcam ( image_path = elephant_path , heatmap = elephant_heatmap , image_size = 224 , cam_path = \"./elephant_gradcam.jpg\" , alpha = 0.4 , use_rgb = False ) cat_gradcam_vgg19 = save_and_display_gradcam ( image_path = single_cat_path , heatmap = cat_heatmap , image_size = 224 , cam_path = \"./cat_gradcam.jpg\" , alpha = 0.4 , use_rgb = False ) cat_gradcam_resnet34 = save_and_display_gradcam ( image_path = single_cat_path , heatmap = cat_heatmap_resnet34 , image_size = 224 , cam_path = \"./cat_gradcam_resnet34.jpg\" , alpha = 0.4 ) elephant_gradcam_resnet34 = save_and_display_gradcam ( image_path = elephant_path , heatmap = elephant_heatmap_resnet34 , image_size = 224 , cam_path = \"./elephant_gradcam_resnet34.jpg\" , alpha = 0.4 ) dog_and_cat_gradcam_resnet34 = save_and_display_gradcam ( image_path = dog_and_cat_path , heatmap = dog_and_cat_heatmap_resnet34 , image_size = 224 , cam_path = \"./dog_and_cat_gradcam_resnet34.jpg\" , alpha = 0.4 ) dog_and_cat_gradcam_resnet34_target_dog = save_and_display_gradcam ( image_path = dog_and_cat_path , heatmap = dog_and_cat_heatmap_resnet34_target_dog , image_size = 224 , cam_path = \"./dog_and_cat_gradcam_target_dog_resnet34.jpg\" , alpha = 0.4 ) dog_and_cat_gradcam_resnet34_target_dog = save_and_display_gradcam ( image_path = dog_and_cat_path , heatmap = dog_and_cat_heatmap_resnet34_target_cat , image_size = 224 , cam_path = \"./dog_and_cat_gradcam_target_cat_resnet34.jpg\" , alpha = 0.4 ) save_and_display_gradcam ( image_path = dog_and_cat_path , heatmap = dog_and_cat_heatmap , cam_path = \"./dog_and_cat_gradcam.jpg\" , alpha = 0.4 ) image = cv2 . imread ( single_cat_path ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) # needed for gradcam. original_image = cv2 . resize ( image , ( 224 , 224 )) from pytorch_grad_cam import GradCAM , ScoreCAM , GradCAMPlusPlus , AblationCAM , XGradCAM , EigenCAM , FullGrad # from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget from pytorch_grad_cam.utils.image import show_cam_on_image from torchvision.models import resnet50 , vgg19 , resnet34 model = resnet34 ( pretrained = True ) target_layers = [ model . layer4 [ - 1 ]] input_tensor = cat_image # Create an input tensor image for your model.. # Note: input_tensor can be a batch tensor with several images! # Construct the CAM object once, and then re-use it on many images: cam = GradCAM ( model = model , target_layers = target_layers , use_cuda = False ) # You can also use it within a with statement, to make sure it is freed, # In case you need to re-create it inside an outer loop: # with GradCAM(model=model, target_layers=target_layers, use_cuda=args.use_cuda) as cam: # ... # We have to specify the target we want to generate # the Class Activation Maps for. # If targets is None, the highest scoring category # will be used for every image in the batch. # Here we use ClassifierOutputTarget, but you can define your own custom targets # That are, for example, combinations of categories, or specific outputs in a non standard model. target_category = None # You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing. grayscale_cam = cam ( input_tensor = input_tensor , target_category = target_category ) # In this example grayscale_cam has only one image in the batch: grayscale_cam = grayscale_cam [ 0 , :] cat_image_normalized = original_image / 255. cat_image_normalized . shape (224, 224, 3) visualization = show_cam_on_image ( cat_image_normalized , grayscale_cam , use_rgb = True ) _fig , axes = plt . subplots ( figsize = ( 8 , 8 ), ncols = 2 ) axes [ 0 ] . imshow ( cat_image_normalized ) axes [ 1 ] . imshow ( visualization ) plt . show () torch . cuda . empty_cache ()","title":"(archived) gradcam from scratch"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/%28archived%29%20gradcam_from_scratch/#dependencies-and-config","text":"from typing import * import cv2 import matplotlib.pyplot as plt import numpy as np import torch import torch.nn as nn import torchsummary import torchvision from torch.utils import data from torchvision import datasets , transforms from torchvision.models import vgg19 , resnet34 import os import random # Display from IPython.display import Image , display from pytorch_grad_cam import GradCAM from pytorch_grad_cam.utils.image import show_cam_on_image % cd .. import utils device = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) utils . seed_all () We will be using three sample images from ImageNet dataset. elephant_path = \"../images/animals/elephant.png\" single_cat_path = \"../images/animals/cat.jpg\" dog_and_cat_path = \"../images/animals/dog_and_cat.jpg\"","title":"Dependencies and Config"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/%28archived%29%20gradcam_from_scratch/#dataset-and-dataloader","text":"We create a simple PyTorch Dataset that reads and return one single image, this is to illustrate our example easily. from torch.utils.data import Dataset class CustomDataset ( Dataset ): def __init__ ( self , image_path , target = None , transform = None ): \"\"\"This Dataset reads 1 image at a time. Args: image_path (_type_): The path to the image target (_type_): The target class of the image transform (_type_, optional): The transform to apply to the image \"\"\" self . image_path = image_path self . target = target self . transform = transform def __len__ ( self ): \"\"\"This loader only returns 1 image at a time Returns: The length of the dataset \"\"\" return 1 def __getitem__ ( self , x ): # read 1 image only so no need subset idx image_path = self . image_path image = cv2 . imread ( image_path ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) if self . target is None : # force assign 0 tensor to be compatible with loader self . target = torch . zeros ( 0 ) if self . transform is not None : image = self . transform ( image ) return image , self . target","title":"Dataset and Dataloader"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/%28archived%29%20gradcam_from_scratch/#transforms","text":"We use some default transforms of image size 224 with ImageNet mean and standard deviation. # use the ImageNet transformation transform = transforms . Compose ( [ transforms . ToPILImage (), transforms . Resize (( 224 , 224 )), transforms . ToTensor (), transforms . Normalize ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ] ), ] )","title":"Transforms"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/%28archived%29%20gradcam_from_scratch/#create-the-dataloaders","text":"We create the three datasets and dataloaders below. # define a 1 image dataset elephant_dataset = CustomDataset ( image_path = elephant_path , transform = transform ) cat_dataset = CustomDataset ( image_path = single_cat_path , transform = transform ) dog_and_cat_dataset = CustomDataset ( image_path = dog_and_cat_path , transform = transform ) # define the dataloader to load that single image elephant_dataloader = torch . utils . data . DataLoader ( dataset = elephant_dataset , shuffle = False , batch_size = 1 ) cat_dataloader = torch . utils . data . DataLoader ( dataset = cat_dataset , shuffle = False , batch_size = 1 ) dog_and_cat_dataloader = torch . utils . data . DataLoader ( dataset = dog_and_cat_dataset , shuffle = False , batch_size = 1 )","title":"Create the DataLoaders"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/%28archived%29%20gradcam_from_scratch/#sanity-check","text":"We get a feel if our dataloader is working by plotting the image in the loader. # get some random training images dataiter = iter ( dog_and_cat_dataloader ) images , targets = dataiter . next () # show images plt . imshow ( np . transpose ( torchvision . utils . make_grid ( images ), ( 1 , 2 , 0 ))) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). <matplotlib.image.AxesImage at 0x17f73646640>","title":"Sanity Check"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/%28archived%29%20gradcam_from_scratch/#models","text":"","title":"Models"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/%28archived%29%20gradcam_from_scratch/#checking-the-layers","text":"We use a library to output the layer information like Keras's model.summary() . We need to identify the last convolutional layer. def torchsummary_wrapper ( model , image_size : Tuple [ int , int , int ] ) -> torchsummary . model_statistics . ModelStatistics : \"\"\"A torch wrapper to print out layers of a Model. Args: model (CustomNeuralNet): Model. image_size (Tuple[int, int, int]): Image size as a tuple of (channels, height, width). Returns: model_summary (torchsummary.model_statistics.ModelStatistics): Model summary. \"\"\" model_summary = torchsummary . summary ( model , image_size ) return model_summary vgg19_ = vgg19 ( pretrained = True ) resnet34_ = resnet34 ( pretrained = True ) Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to C:\\Users\\reighns/.cache\\torch\\hub\\checkpoints\\resnet34-b627a593.pth 0%| | 0.00/83.3M [00:00<?, ?B/s] vgg19_model_summary = torchsummary_wrapper ( vgg19_ , image_size = ( 3 , 224 , 224 )) ========================================================================================== Layer (type:depth-idx) Output Shape Param # ========================================================================================== \u251c\u2500Sequential: 1-1 [-1, 512, 7, 7] -- | \u2514\u2500Conv2d: 2-1 [-1, 64, 224, 224] 1,792 | \u2514\u2500ReLU: 2-2 [-1, 64, 224, 224] -- | \u2514\u2500Conv2d: 2-3 [-1, 64, 224, 224] 36,928 | \u2514\u2500ReLU: 2-4 [-1, 64, 224, 224] -- | \u2514\u2500MaxPool2d: 2-5 [-1, 64, 112, 112] -- | \u2514\u2500Conv2d: 2-6 [-1, 128, 112, 112] 73,856 | \u2514\u2500ReLU: 2-7 [-1, 128, 112, 112] -- | \u2514\u2500Conv2d: 2-8 [-1, 128, 112, 112] 147,584 | \u2514\u2500ReLU: 2-9 [-1, 128, 112, 112] -- | \u2514\u2500MaxPool2d: 2-10 [-1, 128, 56, 56] -- | \u2514\u2500Conv2d: 2-11 [-1, 256, 56, 56] 295,168 | \u2514\u2500ReLU: 2-12 [-1, 256, 56, 56] -- | \u2514\u2500Conv2d: 2-13 [-1, 256, 56, 56] 590,080 | \u2514\u2500ReLU: 2-14 [-1, 256, 56, 56] -- | \u2514\u2500Conv2d: 2-15 [-1, 256, 56, 56] 590,080 | \u2514\u2500ReLU: 2-16 [-1, 256, 56, 56] -- | \u2514\u2500Conv2d: 2-17 [-1, 256, 56, 56] 590,080 | \u2514\u2500ReLU: 2-18 [-1, 256, 56, 56] -- | \u2514\u2500MaxPool2d: 2-19 [-1, 256, 28, 28] -- | \u2514\u2500Conv2d: 2-20 [-1, 512, 28, 28] 1,180,160 | \u2514\u2500ReLU: 2-21 [-1, 512, 28, 28] -- | \u2514\u2500Conv2d: 2-22 [-1, 512, 28, 28] 2,359,808 | \u2514\u2500ReLU: 2-23 [-1, 512, 28, 28] -- | \u2514\u2500Conv2d: 2-24 [-1, 512, 28, 28] 2,359,808 | \u2514\u2500ReLU: 2-25 [-1, 512, 28, 28] -- | \u2514\u2500Conv2d: 2-26 [-1, 512, 28, 28] 2,359,808 | \u2514\u2500ReLU: 2-27 [-1, 512, 28, 28] -- | \u2514\u2500MaxPool2d: 2-28 [-1, 512, 14, 14] -- | \u2514\u2500Conv2d: 2-29 [-1, 512, 14, 14] 2,359,808 | \u2514\u2500ReLU: 2-30 [-1, 512, 14, 14] -- | \u2514\u2500Conv2d: 2-31 [-1, 512, 14, 14] 2,359,808 | \u2514\u2500ReLU: 2-32 [-1, 512, 14, 14] -- | \u2514\u2500Conv2d: 2-33 [-1, 512, 14, 14] 2,359,808 | \u2514\u2500ReLU: 2-34 [-1, 512, 14, 14] -- | \u2514\u2500Conv2d: 2-35 [-1, 512, 14, 14] 2,359,808 | \u2514\u2500ReLU: 2-36 [-1, 512, 14, 14] -- | \u2514\u2500MaxPool2d: 2-37 [-1, 512, 7, 7] -- \u251c\u2500AdaptiveAvgPool2d: 1-2 [-1, 512, 7, 7] -- \u251c\u2500Sequential: 1-3 [-1, 1000] -- | \u2514\u2500Linear: 2-38 [-1, 4096] 102,764,544 | \u2514\u2500ReLU: 2-39 [-1, 4096] -- | \u2514\u2500Dropout: 2-40 [-1, 4096] -- | \u2514\u2500Linear: 2-41 [-1, 4096] 16,781,312 | \u2514\u2500ReLU: 2-42 [-1, 4096] -- | \u2514\u2500Dropout: 2-43 [-1, 4096] -- | \u2514\u2500Linear: 2-44 [-1, 1000] 4,097,000 ========================================================================================== Total params: 143,667,240 Trainable params: 143,667,240 Non-trainable params: 0 Total mult-adds (G): 19.78 ========================================================================================== Input size (MB): 0.57 Forward/backward pass size (MB): 113.38 Params size (MB): 548.05 Estimated Total Size (MB): 662.00 ========================================================================================== # resnet34_model_summary = torchsummary_wrapper(resnet34_, image_size = (3, 224, 224))","title":"Checking the Layers"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/%28archived%29%20gradcam_from_scratch/#define-our-target-layers","text":"We define what our target layers would be for the respective models. # this is the last conv layer layer 35 from Francis's book right after conv2d + relu, but https://github.com/jacobgil/pytorch-grad-cam suggests [-1] where it is max pooled. Can try both. vgg19_last_conv_layer = vgg19_ . features [ - 2 ] print ( vgg19_last_conv_layer ) resnet34_last_conv_layer = resnet34_ . layer4 [ - 1 ] print ( resnet34_last_conv_layer ) ReLU(inplace=True) BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) )","title":"Define our Target Layers"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/%28archived%29%20gradcam_from_scratch/#store-forward-and-backward-hooks","text":"These function help us to get forward activations and backward gradients. forward_activations = OrderedDict () backward_gradients = OrderedDict () def get_forward_hook ( name : str ) -> Callable : \"\"\"Get the intermediate features of a model. Forward Hook. This is using forward hook with reference https://discuss.pytorch.org/t/how-can-l-load-my-best-model-as-a-feature-extractor-evaluator/17254/5 Args: name (str): name of the layer. Returns: Callable: [description] \"\"\" def forward_hook ( model , input , output ): forward_activations [ name ] = output . detach () return forward_hook def get_backward_hook ( name : str ) -> Callable : \"\"\"Get the intermediate features of a model. Backward Hook. This is using backward hook with reference https://discuss.pytorch.org/t/how-can-l-load-my-best-model-as-a-feature-extractor-evaluator/17254/5 Args: name (str): name of the layer. Returns: Callable: [description] \"\"\" def backward_hook ( model , grad_input , grad_output ): backward_gradients [ name ] = grad_output [ 0 ] . detach () return backward_hook","title":"Store Forward and Backward Hooks"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/%28archived%29%20gradcam_from_scratch/#step-by-step-walkthrough","text":"Skip to the next section if you are familiar with the Grad-CAM process. vgg19_last_conv_layer . register_forward_hook ( get_forward_hook ( \"features.35\" )) vgg19_last_conv_layer . register_backward_hook ( get_backward_hook ( \"features.35\" )) # register_backward_hook deprecated but for now we use it <torch.utils.hooks.RemovableHandle at 0x1a39f0623a0> # set the evaluation mode vgg19_ . eval () # get the image from the dataloader image , _ = next ( iter ( elephant_dataloader )) # the y_logits before softmax y_logits = vgg19_ ( image ) target_category = None if target_category is None : # get the most likely prediction of the model target_category = vgg19_ ( image ) . argmax ( dim = 1 ) target_category = target_category . item () # convert to scalar # get the activations of the last convolutional layer forward_conv_activations = forward_activations [ \"features.35\" ] y_logits [:, target_category ] . backward () dyc_dA = backward_gradients [ \"features.35\" ] # average pool the gradients across the channels global_average_pooled_gradients = torch . mean ( dyc_dA , dim = [ 0 , 2 , 3 ]) num_feature_maps = 512 weighted_localization_map = torch . clone ( forward_conv_activations ) # weight the channels by corresponding gradients for i in range ( num_feature_maps ): weighted_localization_map [:, i , :, :] *= global_average_pooled_gradients [ i ] # sum the channels of the activations weighted_localization_map = torch . sum ( weighted_localization_map , dim = 1 ) . squeeze () assert weighted_localization_map . shape == ( 14 , 14 ) # relu on top of the heatmap expression (2) in https://arxiv.org/pdf/1610.02391.pdf relu_weighted_localization_map = torch . nn . ReLU ( inplace = False )( weighted_localization_map ) # normalize the heatmap, scale features to between 0 and 1 to plot heatmap = relu_weighted_localization_map / torch . max ( relu_weighted_localization_map ) # draw the heatmap plt . matshow ( heatmap . squeeze ()) tensor([[0.1695, 0.1367, 0.1292, 0.1435, 0.1037, 0.0376, 0.0048, 0.0520, 0.1114, 0.1495, 0.1856, 0.1638, 0.0733, 0.2405], [0.1262, 0.0933, 0.1134, 0.1632, 0.0805, 0.0000, 0.0000, 0.0254, 0.1456, 0.2702, 0.2565, 0.1883, 0.0819, 0.1331], [0.1508, 0.1267, 0.1541, 0.2010, 0.0570, 0.0000, 0.0000, 0.0973, 0.5866, 0.8733, 0.8954, 0.7577, 0.2693, 0.2221], [0.1884, 0.1376, 0.0959, 0.0888, 0.0000, 0.0000, 0.0715, 0.3818, 0.8004, 1.0000, 0.9425, 0.7478, 0.2823, 0.1800], [0.1931, 0.1158, 0.0351, 0.1330, 0.1786, 0.1146, 0.1243, 0.3891, 0.6943, 0.7773, 0.6895, 0.5610, 0.2427, 0.1264], [0.1552, 0.1056, 0.0473, 0.2595, 0.4205, 0.4517, 0.3047, 0.2987, 0.3538, 0.3354, 0.3093, 0.3014, 0.2003, 0.1028], [0.1290, 0.0747, 0.0794, 0.2144, 0.3984, 0.4928, 0.3979, 0.2380, 0.1140, 0.0156, 0.0354, 0.1017, 0.0838, 0.0565], [0.1351, 0.0931, 0.0892, 0.2174, 0.2635, 0.2547, 0.2120, 0.0853, 0.0532, 0.0892, 0.0746, 0.0637, 0.0590, 0.0669], [0.1265, 0.1018, 0.1014, 0.1031, 0.0350, 0.0000, 0.0000, 0.0000, 0.0760, 0.1283, 0.0963, 0.0719, 0.0648, 0.1028], [0.1022, 0.0842, 0.0833, 0.0784, 0.0661, 0.0453, 0.0620, 0.0855, 0.0810, 0.0651, 0.0514, 0.0575, 0.0807, 0.0874], [0.0744, 0.0565, 0.0446, 0.0513, 0.0579, 0.0656, 0.0686, 0.0619, 0.0518, 0.0432, 0.0352, 0.0422, 0.0603, 0.0671], [0.0691, 0.0449, 0.0282, 0.0311, 0.0371, 0.0427, 0.0493, 0.0538, 0.0555, 0.0554, 0.0536, 0.0601, 0.0876, 0.1001], [0.0985, 0.0838, 0.0768, 0.0767, 0.0800, 0.0848, 0.0896, 0.0934, 0.0930, 0.0902, 0.0896, 0.0985, 0.1177, 0.1245], [0.1639, 0.1344, 0.1271, 0.1290, 0.1266, 0.1226, 0.1187, 0.1174, 0.1148, 0.1075, 0.1006, 0.0996, 0.1054, 0.1203]]) <matplotlib.image.AxesImage at 0x1a39f0dc0d0> img = cv2 . imread ( elephant_path ) heatmap = cv2 . resize ( heatmap . cpu () . detach () . numpy (), ( img . shape [ 1 ], img . shape [ 0 ])) heatmap = np . uint8 ( 255 * heatmap ) heatmap = cv2 . applyColorMap ( heatmap , cv2 . COLORMAP_JET ) superimposed_img = heatmap * 0.4 + img cv2 . imwrite ( './elephant_gradcam.jpg' , superimposed_img ) True","title":"Step by Step Walkthrough"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/%28archived%29%20gradcam_from_scratch/#function-for-grad-cam-heatmap","text":"def make_gradcam_heatmap ( target_layer : torch . nn . Module , target_layer_name : str , model : torch . nn . Module , image : torch . Tensor , target_category : Optional [ int ] = None , ): \"\"\"_summary_ Args: target_layer (str): _description_ model (torch.nn.Module): _description_ image (torch.Tensor): _description_ target_category (Optional[int], optional): _description_. Defaults to None. \"\"\" target_layer . register_forward_hook ( get_forward_hook ( target_layer_name ) ) target_layer . register_backward_hook ( get_backward_hook ( target_layer_name ) ) # register_backward_hook deprecated but for now we use it model . eval () # the y_logits before softmax y_logits = model ( image ) if target_category is None : # get the most likely prediction of the model target_category = model ( image ) . argmax ( dim = 1 ) target_category = target_category . item () # convert to scalar # get the activations of the last convolutional layer forward_conv_activations = forward_activations [ target_layer_name ] y_logits [:, target_category ] . backward () dyc_dA = backward_gradients [ target_layer_name ] # average pool the gradients across the channels global_average_pooled_gradients = torch . mean ( dyc_dA , dim = [ 0 , 2 , 3 ]) num_feature_maps = forward_conv_activations . squeeze () . shape [ 0 ] weighted_localization_map = torch . clone ( forward_conv_activations ) # weight the channels by corresponding gradients for i in range ( num_feature_maps ): weighted_localization_map [ :, i , :, : ] *= global_average_pooled_gradients [ i ] # sum the channels of the activations weighted_localization_map = torch . sum ( weighted_localization_map , dim = 1 ) . squeeze () # relu on top of the heatmap expression (2) in https://arxiv.org/pdf/1610.02391.pdf relu_weighted_localization_map = torch . nn . ReLU ( inplace = False )( weighted_localization_map ) # normalize the heatmap, scale features to between 0 and 1 to plot heatmap = relu_weighted_localization_map / torch . max ( relu_weighted_localization_map ) # draw the heatmap plt . matshow ( heatmap . squeeze ()) return heatmap cat_image , _ = next ( iter ( cat_dataloader )) dog_and_cat_image , _ = next ( iter ( dog_and_cat_dataloader )) elephant_image , _ = next ( iter ( elephant_dataloader )) dog_and_cat_heatmap = make_gradcam_heatmap ( target_layer = vgg19_last_conv_layer , target_layer_name = \"vgg19.feature.35\" , model = vgg19_ , image = dog_and_cat_image , target_category = None ) dog_and_cat_heatmap_resnet34 = make_gradcam_heatmap ( target_layer = resnet34_last_conv_layer , target_layer_name = \"resnet34.layer4[-1]\" , model = resnet34_ , image = dog_and_cat_image , target_category = None ) dog_and_cat_heatmap_resnet34_target_dog = make_gradcam_heatmap ( target_layer = resnet34_last_conv_layer , target_layer_name = \"resnet34.layer4[-1]\" , model = resnet34_ , image = dog_and_cat_image , target_category = 260 ) dog_and_cat_heatmap_resnet34_target_cat = make_gradcam_heatmap ( target_layer = resnet34_last_conv_layer , target_layer_name = \"resnet34.layer4[-1]\" , model = resnet34_ , image = dog_and_cat_image , target_category = 285 ) cat_heatmap = make_gradcam_heatmap ( target_layer = vgg19_last_conv_layer , target_layer_name = \"vgg19.feature.35\" , model = vgg19_ , image = cat_image , target_category = None ) cat_heatmap_resnet34 = make_gradcam_heatmap ( target_layer = resnet34_last_conv_layer , target_layer_name = \"resnet34.layer4[-1]\" , model = resnet34_ , image = cat_image , target_category = None ) elephant_heatmap = make_gradcam_heatmap ( target_layer = vgg19_last_conv_layer , target_layer_name = \"vgg19.features.35\" , model = vgg19_ , image = elephant_image , target_category = None ) elephant_heatmap_resnet34 = make_gradcam_heatmap ( target_layer = resnet34_last_conv_layer , target_layer_name = \"resnet34.layer4[-1]\" , model = resnet34_ , image = elephant_image , target_category = None ) # def save_and_display_gradcam( # image_path, heatmap, image_size=None, cam_path=\"./cat_gradcam.jpg\", alpha=0.4 # ): # # cv2 imread auto reads as BGR # image = cv2.imread(image_path) # if image_size is not None: # image = cv2.resize(image, (image_size, image_size)) # if not isinstance(heatmap, np.ndarray): # heatmap = heatmap.cpu().detach().numpy() # heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0])) # heatmap = np.uint8(255 * heatmap) # heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET) # superimposed_image = heatmap * alpha + image # cv2.imwrite(cam_path, superimposed_image) # return np.uint8(superimposed_image) def save_and_display_gradcam ( image_path , heatmap , image_size = None , cam_path = \"./cat_gradcam.jpg\" , alpha = 0.4 , use_rgb = False , ): \"\"\"_summary_ Args: image_path (_type_): _description_ heatmap (_type_): _description_ image_size (_type_, optional): _description_. Defaults to None. cam_path (str, optional): _description_. Defaults to \"./cat_gradcam.jpg\". alpha (float, optional): _description_. Defaults to 0.4. Returns: _type_: _description_ \"\"\" # Load original image, cv2 imread auto reads as BGR image = cv2 . imread ( image_path ) if image_size is not None : image = cv2 . resize ( image , ( image_size , image_size )) if isinstance ( heatmap , torch . Tensor ): heatmap = heatmap . cpu () . detach () . numpy () # Rescale heatmap to a range 0-255 heatmap = np . uint8 ( 255 * heatmap ) # Use Jet colormap to colorize heatmap heatmap = cv2 . applyColorMap ( heatmap , cv2 . COLORMAP_JET ) if use_rgb : image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) heatmap = cv2 . cvtColor ( heatmap , cv2 . COLOR_BGR2RGB ) # resize heatmap to original image size heatmap = cv2 . resize ( heatmap , ( image . shape [ 1 ], image . shape [ 0 ])) superimposed_image = heatmap * alpha + image cv2 . imwrite ( cam_path , superimposed_image ) # Display Grad CAM display ( Image ( cam_path )) return np . uint8 ( superimposed_image ) # def save_and_display_gradcam( # image_path, heatmap, cam_path=\"./cat_gradcam.jpg\", alpha=0.4 # ): # image = cv2.imread(image_path) # heatmap = cv2.resize( # heatmap.cpu().detach().numpy(), (image.shape[1], image.shape[0]) # ) # heatmap = np.uint8(255 * heatmap) # heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET) # superimposed_image = heatmap * alpha + image # cv2.imwrite(cam_path, superimposed_image) # return superimposed_image elephant_gradcam_vgg19 = save_and_display_gradcam ( image_path = elephant_path , heatmap = elephant_heatmap , image_size = 224 , cam_path = \"./elephant_gradcam.jpg\" , alpha = 0.4 , use_rgb = False ) cat_gradcam_vgg19 = save_and_display_gradcam ( image_path = single_cat_path , heatmap = cat_heatmap , image_size = 224 , cam_path = \"./cat_gradcam.jpg\" , alpha = 0.4 , use_rgb = False ) cat_gradcam_resnet34 = save_and_display_gradcam ( image_path = single_cat_path , heatmap = cat_heatmap_resnet34 , image_size = 224 , cam_path = \"./cat_gradcam_resnet34.jpg\" , alpha = 0.4 ) elephant_gradcam_resnet34 = save_and_display_gradcam ( image_path = elephant_path , heatmap = elephant_heatmap_resnet34 , image_size = 224 , cam_path = \"./elephant_gradcam_resnet34.jpg\" , alpha = 0.4 ) dog_and_cat_gradcam_resnet34 = save_and_display_gradcam ( image_path = dog_and_cat_path , heatmap = dog_and_cat_heatmap_resnet34 , image_size = 224 , cam_path = \"./dog_and_cat_gradcam_resnet34.jpg\" , alpha = 0.4 ) dog_and_cat_gradcam_resnet34_target_dog = save_and_display_gradcam ( image_path = dog_and_cat_path , heatmap = dog_and_cat_heatmap_resnet34_target_dog , image_size = 224 , cam_path = \"./dog_and_cat_gradcam_target_dog_resnet34.jpg\" , alpha = 0.4 ) dog_and_cat_gradcam_resnet34_target_dog = save_and_display_gradcam ( image_path = dog_and_cat_path , heatmap = dog_and_cat_heatmap_resnet34_target_cat , image_size = 224 , cam_path = \"./dog_and_cat_gradcam_target_cat_resnet34.jpg\" , alpha = 0.4 ) save_and_display_gradcam ( image_path = dog_and_cat_path , heatmap = dog_and_cat_heatmap , cam_path = \"./dog_and_cat_gradcam.jpg\" , alpha = 0.4 ) image = cv2 . imread ( single_cat_path ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) # needed for gradcam. original_image = cv2 . resize ( image , ( 224 , 224 )) from pytorch_grad_cam import GradCAM , ScoreCAM , GradCAMPlusPlus , AblationCAM , XGradCAM , EigenCAM , FullGrad # from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget from pytorch_grad_cam.utils.image import show_cam_on_image from torchvision.models import resnet50 , vgg19 , resnet34 model = resnet34 ( pretrained = True ) target_layers = [ model . layer4 [ - 1 ]] input_tensor = cat_image # Create an input tensor image for your model.. # Note: input_tensor can be a batch tensor with several images! # Construct the CAM object once, and then re-use it on many images: cam = GradCAM ( model = model , target_layers = target_layers , use_cuda = False ) # You can also use it within a with statement, to make sure it is freed, # In case you need to re-create it inside an outer loop: # with GradCAM(model=model, target_layers=target_layers, use_cuda=args.use_cuda) as cam: # ... # We have to specify the target we want to generate # the Class Activation Maps for. # If targets is None, the highest scoring category # will be used for every image in the batch. # Here we use ClassifierOutputTarget, but you can define your own custom targets # That are, for example, combinations of categories, or specific outputs in a non standard model. target_category = None # You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing. grayscale_cam = cam ( input_tensor = input_tensor , target_category = target_category ) # In this example grayscale_cam has only one image in the batch: grayscale_cam = grayscale_cam [ 0 , :] cat_image_normalized = original_image / 255. cat_image_normalized . shape (224, 224, 3) visualization = show_cam_on_image ( cat_image_normalized , grayscale_cam , use_rgb = True ) _fig , axes = plt . subplots ( figsize = ( 8 , 8 ), ncols = 2 ) axes [ 0 ] . imshow ( cat_image_normalized ) axes [ 1 ] . imshow ( visualization ) plt . show () torch . cuda . empty_cache ()","title":"Function for Grad-CAM Heatmap"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_explained/","text":"\\[ \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\L}{\\mathbf{L}} \\newcommand{\\X}{\\mathbf{X}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\] Grad-CAM, an Introduction We will be discussing the paper written by R.R Selvaraju et al on Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization on how we can interpret large scale Deep Neural Networks (CNN). Terminologies Feature Maps Feature Maps has an intuitive interpretation , understanding it intuitively should be prioritized than knowing the underlying mathematical structure. Imagine a 32 by 32 image, consider that a convolutional layer applied on the input and we assume only 2 kernels are applied, then only 2 feature maps are obtained: The first feature map is a color filter that is responsible to detect color contrast of an image; The second feature map is a sobel's filter where it detects edges. Then assume kernel has size 5 by 5, when convolved on the 32 by 32 image, yields a feature map output of size 28 by 28, although \"downsampled\", this feature map should still capture valueble spatial information of the 32 by 32 image, and in this case, all edges of the input image (say a cat) will be shown in the feature map 1, we call it \\(\\A^1\\) , and the color contrast is displayed on feature map 2, \\(\\A^2\\) . So the full settings is: An input image \\(\\mathcal{X}\\) of size \\((3, 32, 32)\\) . A CNN but we will only look at its very first layer, a conv layer that has: 2 kernels of size 5 by 5 and default paddings and stride such that the output feature map is 28 by 28. Original Cat Image Feature Map A1 and A2; By Hongnan G. Notice in the 2 feature maps above, after we pass in the RGB cat image of 3 channels to the first conv layer, we will receive two outputs, called feature maps , each of them will describe the image in one way or another. One thing to note is that earlier conv layers will retain a lot detailed information of the image such as the edges, color contrast, but as we go on later, the feature maps become more abstract as they capture high level and abstract details that we human cannot comprehend easily. Global Average Pooling (GAP) The GAP is usually applied after the last conv layer. But for the sake of explanation, we will assume that our \"last conv layer\" is our first conv layer. Instead of down sampling patches of the input feature map, global pooling down samples the entire feature map to a single value. In the last few years, experts have turned to global average pooling (GAP) layers to minimize overfitting by reducing the total number of parameters in the model. Similar to max pooling layers, GAP layers are used to reduce the spatial dimensions of a three-dimensional tensor. However, GAP layers perform a more extreme type of dimensionality reduction, where a tensor with dimensions h\u00d7w\u00d7d is reduced in size to have dimensions 1\u00d71\u00d7d. GAP layers reduce each h\u00d7w feature map to a single number by simply taking the average of all hw values. Visualize GAP; Courtesy of Alexis Cook Eventually our task is to classify whether the image is a cat or something else. We are unable to succinctly pass these 2 feature maps directly to a linear classifier to classify the cats (i.e. passing in two 28 by 28 kernels to a linear layer does not work but we need linear layer to give us an output). Therefore, we need to flatten these feature maps generated by the conv layers. One way to do this is Global Average Pooling . A wrong example First, I show you a source of confusion on what some perceive GAP as: For example: \\[ \\A^1 = \\begin{bmatrix} 2 & 3 \\\\ 1 & 5 \\end{bmatrix} \\quad \\A^2 = \\begin{bmatrix} 3 & 1 \\\\ 2 & 8 \\end{bmatrix} \\] Assume for a moment \\(\\A^1\\) and \\(\\A^2\\) are our feature maps (although I mentioned it was 28 by 28 but this illustrates the idea). Then to retain the spatial information, we can perform an averaging across depths/channels of each kernel as such: \\[ \\A_{\\text{average feature maps across depth}} = \\dfrac{\\A^1 + \\A^2}{\\textbf{num_feature_maps}} = \\dfrac{\\A^1 + \\A^2}{2} = \\begin{bmatrix} 2.5 & 2 \\\\ 1.5 & 6.5 \\end{bmatrix} \\] Then we flatten the \\(\\A_{gap}\\) to a single vector and pass it to linear layer. Feature Map A1 and A2 but Averaged over Channels; By Hongnan G. A correct example \\[ \\A^1 = \\begin{bmatrix} 2 & 3 \\\\ 1 & 5 \\end{bmatrix} \\quad \\A^2 = \\begin{bmatrix} 3 & 1 \\\\ 2 & 8 \\end{bmatrix} \\] Assume for a moment \\(\\A^1\\) and \\(\\A^2\\) are our feature maps (although I mentioned it was 28 by 28 but this illustrates the idea). Then to retain the spatial information, we can perform an averaging across each kernel as such: \\[ \\text{mean}\\left(\\A^1\\right) = \\dfrac{2+3+1+5}{2 \\times 2} = 2.75 \\quad \\text{mean}\\left(\\A^2\\right) = \\dfrac{3+1+2+8}{2 \\times 2} = 3.5 \\] where by \\(2 \\times 2\\) means the number of pixels in the feature map; and thus the \\[ \\A_{\\text{gap}} = \\begin{bmatrix}2.75 \\\\ 3.5 \\end{bmatrix} \\] The Grad-CAM paper also applied the GAP idea over each feature map \\(\\A_k\\) . One can refer to here to understand the intuition here. Basically each unique feature map will be reduced to a single value, for eg, \\(2.75\\) , which summarizes the entire feature map. The Intuition This part is referenced from Interpretable Machine Learning by Christoph Molnar . He started off with an intuitive description of Grad-CAM. High level idea: Grad-CAM is to understand at which parts of an image a convolutional layer \"looks\" for a certain classification. As a reminder, the first convolutional layer of a CNN takes as input the images and outputs feature maps that encode learned features (see the chapter on Learned Features ). The higher-level convolutional layers do the same, but take as input the feature maps of the previous convolutional layers. To understand how the CNN makes decisions, Grad-CAM analyzes which regions are activated in the feature maps of the last convolutional layers. There are \\(k\\) feature maps in the last convolutional layer, and I will call them \\(\\A^1, \\A^2, \\ldots, \\A^k\\) . How can we \"see\" from the feature maps how the convolutional neural network has made a certain classification? In the first approach, we could simply visualize the raw values of each feature map, average over the feature maps and overlay this over our image. This is the method of visualization feature map activations. This would not be helpful since the feature maps encode information for all classes , but we are interested in a particular class. Grad-CAM has to decide how important each of the k feature map was to our class c that we are interested in. We have to weight each pixel of each feature map with the gradient before we average over the feature maps. This gives us a heatmap which highlights regions that positively or negatively affect the class of interest. This heatmap is send through the ReLU function, which is a fancy way of saying that we set all negative values to zero. Grad-CAM removes all negative values by using a ReLU function, with the argument that we are only interested in the parts that contribute to the selected class c and not to other classes. The word pixel might be misleading here as the feature map is smaller than the image (because of the pooling units) but is mapped back to the original image. We then scale the Grad-CAM map to the interval [0,1] for visualization purposes and overlay it over the original image. The Big Picture This part can be best understood alongside my drawings. The big picture is when you \"reduce\" the 3 feature maps into 1 single new \"feature map\" (localization map) by way of a linear sum where each feature map \\(\\A^k\\) is multiplied by the global average pooled gradients \\(\\alpha_k^c\\) . In the example on my drawings, it can be seen that feature maps \\(\\A^1\\) and \\(\\A^2\\) are more important for the model to distinguish the elephant class than that of \\(\\A^3\\) . By design, I made \\(\\A^3\\) in itself already look different in terms of values from \\(\\A^1\\) and \\(\\A^2\\) on the get go. But at this point in time, even though we know the values in feature map \\(\\A^3\\) are vastly different, we cannot say for sure whether \\(\\A^3\\) in itself contributed to the prediction of our class elephant or not. It could also be the case that \\(\\A^1\\) and \\(\\A^2\\) are bonkers. Thus, the concept of taking the gradient of the class \\(\\y^c\\) with respect to each of these feature maps \\(\\A^k\\) becomes increasingly important as now we have a mathematical way to assign some importance to each of these feature maps. Now to even further bring out the intuition, imagine the 3 feature maps for the input image elephant as follows: Feature map \\(\\A^1\\) corresponds to Sobel Filter where it detect edges in particular their trunk of the elephants . Feature map \\(\\A^2\\) corresponds to Another Filter where it detects the ear of the elephants . Feature map \\(\\A^3\\) corresponds to a Background Filter where it detect the background and in particular the grass patch under the elephants . Now this is apparent why \\(\\A^3\\) is vastly different in terms of values since it is actually representing the background and grass. We know that grass is a non-factor in determining an elephant since the african grass field can hold many other animals as well. We really do not want the model to assign important to the grass, if any at all. We can then compute the gradient of \\(\\y^c\\) wrt each feature map and we can intuitively understand it as (see my diagrams): Gradient map \\(\\frac{d\\y^{386}}{d\\A^1}\\) has values \\(\\begin{bmatrix} 2 & 3 \\\\ 4 & 2 \\end{bmatrix}\\) These values are the gradients of feature map \\(\\A^1\\) at a pixel level. We may find it difficult to comprehend these gradients and we want a quick summary of how feature map \\(\\A^1\\) changes our model's decision for \\(\\y^{386}\\) . Thus we use GAP. and when we perform GAP on them, we have \\(\\alpha_{1}^{386} = 2.75\\) . This value roughly translates to a pertubation of \\(2.75\\) in feature map \\(\\A^1\\) will result in a unit change in our class \\(\\y^{386}\\) . Gradient map \\(\\frac{d\\y^{386}}{d\\A^2}\\) has values \\(\\begin{bmatrix} 3 & 5 \\\\ 6 & 3 \\end{bmatrix}\\) and when we perform GAP on them, we have \\(4.25\\) . The same logic applies. Gradient map \\(\\frac{d\\y^{386}}{d\\A^3}\\) has values \\(\\begin{bmatrix} 0.1 & -0.1 \\\\ 0.2 & 0.2 \\end{bmatrix}\\) and when we perform GAP on them, we have \\(0.1\\) . The same logic applies and we see that the gradient is small here and hence we can deduce that this feature map is not very important. Ultimately however, we want to know how the change in all feature maps \\(\\A^k\\) affects our final decision. This is where we perform a linear sum of \\(\\left(\\sum_{k} \\alpha_k^c A^k\\right)\\) of all feature maps with the weight coefficient being their global average pooled gradient map. Just like our good old linear regression model, the intuition is the same as in \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...\\) . The coefficients (the gradient values of \\(2.75, 4.25, 0.1\\) decides which feature map are important and gets \"activated and highlighted\"). We get the weighted sum heatmap to be the shape \\((2, 2)\\) of values \\([8.925, -2.025, 16.125, 2.6]\\) . Now this is a very small map and is likely not going to happen in real life. More often the map at this stage is of size \\((10, 10)\\) and beyond. Imagine once more how \\(\\A^3\\) played almost no role. Lastly, an elementwise ReLU operation is applied to the heatmap to get \\([8,925, 0, 16.125, 2.6]\\) and from the paper: We apply a ReLU to the linear combination of maps because we are only interested in the features that have a positive influence on the class of interest, i.e. pixels whose intensity should be increased in order to increase \\(y^{c}\\) . Intuitively, the idea is that negative values contribute to other classes and we should not really care about it. We can scale back the heatmap to overlay on the original image. From here , author said: Another potential question that can arise is why wouldn\u2019t we just compute the gradient of the class logit with respect to the input image. Remember that a convolutional neural network works as a feature extractor and deeper layers of the network operate in increasingly abstract spaces. We want to see which of the features actually influenced the model\u2019s choice of the class rather than just individual image pixels. That is why it is crucial to take the activation maps of deeper convolutional layers. The Algorithm With reference to Interpretable Machine Learning by Christoph Molnar . Notation Input image: \\(\\X\\) Feature Maps of the last convolutional layer in a CNN is denoted as: \\[\\A_1, \\A_2, \\ldots, \\A_k\\] The \\(k\\) is an arbitrary number for the number of feature maps. The Feature Logits of a particular class \\(c\\) is denoted as: \\[\\y^{c}\\] In other words, in ImageNet , the elephant class is indexed \\(386\\) , and is denoted \\(\\y^{386}\\) . This is also the raw feature logits output before the softmax layer. The gradient of the fully-connected logits for class \\(c\\) , \\(\\y^{c}\\) (before the softmax), with respect to feature map activations \\(\\A_k\\) of a convolutional layer : \\[\\dfrac{d\\y^{c}}{d\\A^{k}}\\] It follows that the gradient of the fully-connected for each class \\(c\\) , \\(\\y^{c}\\) , with respect to each pixel on the feature map activations \\(\\A_k\\) is denoted as : \\[\\dfrac{d\\y^{c}}{d\\A^{k}_{ij}}\\] Let us define the gradient of \\(\\y^c\\) with respect to the GAP of each feature map \\(\\A^k\\) to be: \\[\\alpha_k^c = \\overbrace{\\frac{1}{Z}\\sum_{i}\\sum_{j}}^{\\text{global average pooling}} \\underbrace{\\frac{\\delta y^c}{\\delta A_{ij}^k}}_{\\text{gradients via backprop}}\\] Denote the ReLU function as: \\[\\textbf{ReLU}\\] The final heatmap, also called the localization map of Grad-CAM is denoted as: \\[\\L^{c}_{Grad-CAM} \\in \\R^{w \\times h}\\] where \\(w\\) and \\(h\\) is the width and height of the final output localization map. and is equals to \\[\\L^{c}_{Grad-CAM} \\in \\R^{w \\times h} = \\underbrace{ReLU}_{\\text{Pick positive values}}\\left(\\sum_{k} \\alpha_k^c A^k\\right)\\] The Algorithm We will only assume our input is 1 single image. Let us look at the recipe for Grad-CAM. Our goal is to find the localization map, which is defined as: \\[L^c_{Grad-CAM} \\in \\mathbb{R}^{u\\times v} = \\underbrace{ReLU}_{\\text{Pick positive values}}\\left(\\sum_{k} \\alpha_k^c A^k\\right)\\] Here, u is the width, v the height of the explanation and c the class of interest. Forward-propagate the input image through the convolutional neural network. Obtain the raw score for the class of interest, meaning the activation of the neuron before the softmax layer. Set all other class activations to zero. Back-propagate the gradient of the class of interest to the last convolutional layer before the fully connected layers: \\(\\frac{\\delta{}y^c}{\\delta{}A^k}\\) . Weight each feature map \"pixel\" by the gradient for the class. Indices i and j refer to the width and height dimensions: \\( \\(\\alpha_k^c = \\overbrace{\\frac{1}{Z}\\sum_{i}\\sum_{j}}^{\\text{global average pooling}} \\underbrace{\\frac{\\delta y^c}{\\delta A_{ij}^k}}_{\\text{gradients via backprop}}\\) \\) This means that the gradients are globally pooled. Calculate an average of the feature maps, weighted per pixel by the gradient. Apply ReLU to the averaged feature map. For visualization: Scale values to the interval between 0 and 1. Upscale the image and overlay it over the original image. Additional step for Guided Grad-CAM: Multiply heatmap with guided backpropagation. Step 1: Forward-propagate This step just takes in one single image \\(\\mathcal{X}\\) and perform a forward pass the input image through the convolutional neural network and save all the forward pass feature map activations . # the y_logits before softmax; forward pass to populate the forward_activations y_logits = model ( image ) # this dict will be populated with the feature map outputs forward_activations = { \"features.11_ReLU(inplace=True)\" : feature_map_logits } We now have the feature maps stored and we can get our target conv layer. \\[ \\begin{bmatrix} \\A^1 & \\A^2 & \\A^3 & \\cdots & \\A^k \\end{bmatrix} \\] where each \\(\\A^i \\in \\R^{f \\times f}\\) . Step 2: Backward-propagate In Grad-CAM, we need the gradients of the target category with respect to the target convolutional layer . That is to say, when we call backwards, we are only interested in the particular class's gradients wrt feature maps. if target_category is None : # get the most likely prediction of the model target_category = model ( image ) . argmax ( dim = 1 ) # convert to scalar target_category = target_category . item () # call backward on the model to populate backward_gradients y_logits [:, target_category ] . backward () # this dict will be populated with the gradients backward_gradients = { \"features.11_ReLU(inplace=True)\" : gradients_yc_wrt_features .11 } The above code just says, if we did not choose a target category , the model will choose the one with the highest logit activation. We then call y_logits[: target_category] backwards to store the gradients in backward_gradients . We now have the gradients of the class of interest with respect to the target conv layer (usually last conv layer). \\[ \\begin{bmatrix} \\dfrac{d\\y^{c}}{d\\A^{1}} & \\dfrac{d\\y^{c}}{d\\A^{2}} & \\dfrac{d\\y^{c}}{d\\A^{3}} & \\cdots & \\dfrac{d\\y^{c}}{d\\A^{k}} \\end{bmatrix} \\] where each \\(\\dfrac{d\\y^{c}}{d\\A^{i}} \\in \\R^{f \\times f}\\) . Step 3: Global Average Pool Gradients The intuition we explained earlier tells us that we need to assign an importance score to each feature map \\(\\A^i\\) . How to do that? In my naive thought, since \\(\\dfrac{d\\y^{c}}{d\\A^{i}}\\) is the same shape as the feature map \\(f \\times f\\) , I would have thought we can just multiply them elementwise. More concretely, If \\[ \\A^1 = \\begin{bmatrix} 2 & 3 \\\\ 1 & 5 \\end{bmatrix} \\quad \\A^2 = \\begin{bmatrix} 3 & 1 \\\\ 2 & 8 \\end{bmatrix} \\] and \\[ \\dfrac{d\\y^{c}}{d\\A^{1}} = \\begin{bmatrix} 0.1 & 0.2 \\\\ 0.3 & 0.5 \\end{bmatrix} \\quad \\dfrac{d\\y^{c}}{d\\A^{2}} = \\begin{bmatrix} 0.3 & 0.1 \\\\ 0.2 & 0.3 \\end{bmatrix} \\] Then we can weigh each feature map on a pixel level: \\[ \\A^1 * \\dfrac{d\\y^{c}}{d\\A^{1}} = \\begin{bmatrix} 0.2 & 0.6 \\\\ 0.3 & 0.25 \\end{bmatrix} \\quad \\A^2 * \\dfrac{d\\y^{c}}{d\\A^{2}} = \\begin{bmatrix} 0.9 & 0.1 \\\\ 0.4 & 0.24 \\end{bmatrix} \\] However, the paper suggested that we can perform a Global Average Pooling on the gradient maps first, the idea is the same, we assume that the average of each gradient map should be representative of the rate of change of \\(y^c\\) with respect to each feature map. \\[\\alpha_k^c = \\overbrace{\\frac{1}{Z}\\sum_{i}\\sum_{j}}^{\\text{global average pooling}} \\underbrace{\\frac{\\delta y^c}{\\delta A_{ij}^k}}_{\\text{gradients via backprop}}\\] where \\(Z\\) is the total number of pixels in this gradient map. Applying this to our example: \\[ \\alpha_1^c = \\overbrace{\\frac{1}{Z}\\sum_{i}\\sum_{j}}^{\\text{global average pooling}} \\underbrace{\\frac{\\delta y^c}{\\delta A_{ij}^1}}_{\\text{gradients via backprop}} = \\frac{1}{4}\\left(0.2+0.6+0.3+0.25\\right) = 0.3375 \\] \\[ \\alpha_2^c = \\overbrace{\\frac{1}{Z}\\sum_{i}\\sum_{j}}^{\\text{global average pooling}} \\underbrace{\\frac{\\delta y^c}{\\delta A_{ij}^2}}_{\\text{gradients via backprop}} = \\frac{1}{4}\\left(0.9+0.1+0.4+0.24\\right) = 0.41 \\] These \\(\\alpha_k^c\\) will be the importance score of each feature map \\(\\A^k\\) . We will stack them into vectors: \\[ \\begin{bmatrix} \\alpha_1^c & \\alpha_2^c & \\alpha_3^c & \\cdots & \\alpha_k^c \\end{bmatrix} \\] and \\(\\alpha_k^c \\in \\R\\) is a scalar. In code it is of the form: dyc_dA = backward_gradients [ target_layer_name ] # average pool the gradients across the channels global_average_pooled_gradients = torch . mean ( dyc_dA , dim = [ 0 , 2 , 3 ]) Step 4: Weighted Global Sum: Localized Feature Maps Recall that good old linear regression has the form \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k\\) . Now we do the same here and treat \\(\\alpha_k^c\\) as the beta weight coefficients, and feature maps \\(\\A^k\\) as our features to get: \\[\\textbf{Localized_Feature_Map} = \\left(\\sum_{k} \\alpha_k^c A^k\\right)\\] which has a shape of \\(\\R^{f \\times f}\\) . The intuition is that some unimportant feature maps will go down to \\(0\\) or near \\(0\\) , and the important feature maps are magnified! The code has this structure: weighted_localization_map = torch . clone ( forward_conv_activations ) # weight the channels by corresponding gradients for i in range ( num_feature_maps ): weighted_localization_map [ :, i , :, : ] *= global_average_pooled_gradients [ i ] # sum the channels of the activations weighted_localization_map = torch . sum ( weighted_localization_map , dim = 1 ) . squeeze () Step 5: Grad-CAM Localization Map with ReLU The last step is to apply ReLU to the \\(\\textbf{Localized_Feature_Map}\\) above. This is really to zero out all negative entries. Why? Well, for one, in typical logistic regression, negative weights are associated with the model placing importance in the \"other class of interest\", and we do not want that. The same analogy can be applied here though I believe mine is hand-wavy at best. \\[L^c_{Grad-CAM} \\in \\mathbb{R}^{f \\times f} = \\underbrace{ReLU}_{\\text{Pick positive values}}\\left(\\sum_{k} \\alpha_k^c A^k\\right)\\] The code has structure like: # relu on top of the heatmap expression (2) in https://arxiv.org/pdf/1610.02391.pdf relu_weighted_localization_map = torch . nn . ReLU ( inplace = False )( weighted_localization_map ) # normalize the heatmap, scale features to between 0 and 1 to plot gradcam_heatmap = relu_weighted_localization_map / torch . max ( relu_weighted_localization_map ) Step 6: Superimpose the Grad-CAM Heatmap to Original Image This step is just doing some resizing from the \\(f \\times f\\) to say \\(224 \\times 224\\) to overlay on the original image. Dissecting KERAS's code https://keras.io/examples/vision/grad_cam/ has sample code but I converted to PyTorch, before converting, I annotated each of the variable inside. (See my KERAS notebook as I changed some variables name). last_conv_layer_output Shape: \\(2 \\times 2 \\times 3\\) with 3 filters of 2 by 2 shape. Note this is just \\(\\A^1, \\A^2, \\A^3\\) where each \\(\\A^k \\in \\R^{2 \\times 2}\\) . y_logits Shape: \\(1 \\times 1000\\) since there are 1000 classes in ImageNet. Note that this is our \\(\\y\\) . If we are interested in the elephant class at index 386, then we denote it as \\(\\y^{c} = \\y^{386}\\) . Thus, \\(\\y \\in \\R^{1 \\times 1000}\\) but \\(\\y^{c} \\in \\R^{1 \\times 1}\\) . Note carefully this is the logits output of all the layers of the CNN, and is just right before the softmax where we transform the logits to probabilities. We can imagine that among these 1000 logits, the highest number means the model thinks that this index is the most probable class. Theoretically speaking, there is no difference in differentiation of the logits wrt to the feature maps versus the softmax probs wrt to the feature maps. This is because softmax is monotonic , and the pre-softmax logits output will tell us already which class is most probable, and so will the softmax. So there should not be any confusion here on why we did not differentiate softmax probs wrt to the feature maps instead since the ranking is preserved in the sense that values in logits the highest is the most probable when transformed by softmax. target_category This is an optional argument in the function. If None , we will automatically assign it to the highest logit's index. In this example, the highest logit in \\(\\y\\) is at index 386 with a value of \\(23.632\\) , corresponding to the target class of elephant. If specified, then the \\(\\y^{c}\\) will change. target_category_logits Once our target_category is defined, we will just slice y_logits[:, target_category] to get the logit value of that particular class. This variable is just \\(\\y^{c}\\) if you look carefully. grads = tape.gradient(target_category_logits, last_conv_layer_output) This is the gradient of the output neuron (top predicted or chosen) with regard to the output feature map of the last conv layer. This has shape \\(2 \\times 2 \\times 3\\) in our simple example. In the python example, the shape at this moment has an additional axis like \\(1 \\times 2 \\times 2 \\times 3\\) which we will eventually squeeze it anyways. Notice it has the same shape as last_conv_layer_output aka the feature maps. In other words, this variable has 3 feature maps stacked together and can be visualized as: \\[\\begin{bmatrix} \\dfrac{d\\y^{c}}{d\\A^{1}} & \\dfrac{d\\y^{c}}{d\\A^{2}} & \\dfrac{d\\y^{c}}{d\\A^{3}} \\end{bmatrix}\\] pooled_grads Shape: \\(3 \\times 1\\) This is a vector where each entry is the mean intensity of the gradient over a specific feature map channel. In other words, in grads[0] we have \\(\\frac{d\\y^c}{d\\A^1}\\) which is of shape \\(2 \\times 2\\) . What we do not is Global Average Pooling on these gradients where we just take the average of all available pixels in this \\(2 \\times 2\\) gradient map for feature map 1. You can find more intuition of GAP above, but in general this is similar logic to how you perform GAP on the output of the feature logits from the last conv layer. \\[\\alpha_k^c = \\overbrace{\\frac{1}{Z}\\sum_{i}\\sum_{j}}^{\\text{global average pooling}} \\underbrace{\\frac{\\delta y^c}{\\delta A_{ij}^k}}_{\\text{gradients via backprop}}\\] where \\(Z\\) is the total number of pixels in this gradient map. heatmap Shape: \\(2 \\times 2\\) which is also the filter size and also the feature map size. This is the weighted sum of all feature maps \\(\\A^1, \\A^2, \\A^3\\) and is \\[\\textbf{Localized_Map} = \\left(\\sum_{k} \\alpha_k^c A^k\\right)\\]","title":"Grad-CAM Explained"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_explained/#grad-cam-an-introduction","text":"We will be discussing the paper written by R.R Selvaraju et al on Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization on how we can interpret large scale Deep Neural Networks (CNN).","title":"Grad-CAM, an Introduction"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_explained/#terminologies","text":"","title":"Terminologies"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_explained/#feature-maps","text":"Feature Maps has an intuitive interpretation , understanding it intuitively should be prioritized than knowing the underlying mathematical structure. Imagine a 32 by 32 image, consider that a convolutional layer applied on the input and we assume only 2 kernels are applied, then only 2 feature maps are obtained: The first feature map is a color filter that is responsible to detect color contrast of an image; The second feature map is a sobel's filter where it detects edges. Then assume kernel has size 5 by 5, when convolved on the 32 by 32 image, yields a feature map output of size 28 by 28, although \"downsampled\", this feature map should still capture valueble spatial information of the 32 by 32 image, and in this case, all edges of the input image (say a cat) will be shown in the feature map 1, we call it \\(\\A^1\\) , and the color contrast is displayed on feature map 2, \\(\\A^2\\) . So the full settings is: An input image \\(\\mathcal{X}\\) of size \\((3, 32, 32)\\) . A CNN but we will only look at its very first layer, a conv layer that has: 2 kernels of size 5 by 5 and default paddings and stride such that the output feature map is 28 by 28. Original Cat Image Feature Map A1 and A2; By Hongnan G. Notice in the 2 feature maps above, after we pass in the RGB cat image of 3 channels to the first conv layer, we will receive two outputs, called feature maps , each of them will describe the image in one way or another. One thing to note is that earlier conv layers will retain a lot detailed information of the image such as the edges, color contrast, but as we go on later, the feature maps become more abstract as they capture high level and abstract details that we human cannot comprehend easily.","title":"Feature Maps"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_explained/#global-average-pooling-gap","text":"The GAP is usually applied after the last conv layer. But for the sake of explanation, we will assume that our \"last conv layer\" is our first conv layer. Instead of down sampling patches of the input feature map, global pooling down samples the entire feature map to a single value. In the last few years, experts have turned to global average pooling (GAP) layers to minimize overfitting by reducing the total number of parameters in the model. Similar to max pooling layers, GAP layers are used to reduce the spatial dimensions of a three-dimensional tensor. However, GAP layers perform a more extreme type of dimensionality reduction, where a tensor with dimensions h\u00d7w\u00d7d is reduced in size to have dimensions 1\u00d71\u00d7d. GAP layers reduce each h\u00d7w feature map to a single number by simply taking the average of all hw values. Visualize GAP; Courtesy of Alexis Cook Eventually our task is to classify whether the image is a cat or something else. We are unable to succinctly pass these 2 feature maps directly to a linear classifier to classify the cats (i.e. passing in two 28 by 28 kernels to a linear layer does not work but we need linear layer to give us an output). Therefore, we need to flatten these feature maps generated by the conv layers. One way to do this is Global Average Pooling . A wrong example First, I show you a source of confusion on what some perceive GAP as: For example: \\[ \\A^1 = \\begin{bmatrix} 2 & 3 \\\\ 1 & 5 \\end{bmatrix} \\quad \\A^2 = \\begin{bmatrix} 3 & 1 \\\\ 2 & 8 \\end{bmatrix} \\] Assume for a moment \\(\\A^1\\) and \\(\\A^2\\) are our feature maps (although I mentioned it was 28 by 28 but this illustrates the idea). Then to retain the spatial information, we can perform an averaging across depths/channels of each kernel as such: \\[ \\A_{\\text{average feature maps across depth}} = \\dfrac{\\A^1 + \\A^2}{\\textbf{num_feature_maps}} = \\dfrac{\\A^1 + \\A^2}{2} = \\begin{bmatrix} 2.5 & 2 \\\\ 1.5 & 6.5 \\end{bmatrix} \\] Then we flatten the \\(\\A_{gap}\\) to a single vector and pass it to linear layer. Feature Map A1 and A2 but Averaged over Channels; By Hongnan G. A correct example \\[ \\A^1 = \\begin{bmatrix} 2 & 3 \\\\ 1 & 5 \\end{bmatrix} \\quad \\A^2 = \\begin{bmatrix} 3 & 1 \\\\ 2 & 8 \\end{bmatrix} \\] Assume for a moment \\(\\A^1\\) and \\(\\A^2\\) are our feature maps (although I mentioned it was 28 by 28 but this illustrates the idea). Then to retain the spatial information, we can perform an averaging across each kernel as such: \\[ \\text{mean}\\left(\\A^1\\right) = \\dfrac{2+3+1+5}{2 \\times 2} = 2.75 \\quad \\text{mean}\\left(\\A^2\\right) = \\dfrac{3+1+2+8}{2 \\times 2} = 3.5 \\] where by \\(2 \\times 2\\) means the number of pixels in the feature map; and thus the \\[ \\A_{\\text{gap}} = \\begin{bmatrix}2.75 \\\\ 3.5 \\end{bmatrix} \\] The Grad-CAM paper also applied the GAP idea over each feature map \\(\\A_k\\) . One can refer to here to understand the intuition here. Basically each unique feature map will be reduced to a single value, for eg, \\(2.75\\) , which summarizes the entire feature map.","title":"Global Average Pooling (GAP)"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_explained/#the-intuition","text":"This part is referenced from Interpretable Machine Learning by Christoph Molnar . He started off with an intuitive description of Grad-CAM. High level idea: Grad-CAM is to understand at which parts of an image a convolutional layer \"looks\" for a certain classification. As a reminder, the first convolutional layer of a CNN takes as input the images and outputs feature maps that encode learned features (see the chapter on Learned Features ). The higher-level convolutional layers do the same, but take as input the feature maps of the previous convolutional layers. To understand how the CNN makes decisions, Grad-CAM analyzes which regions are activated in the feature maps of the last convolutional layers. There are \\(k\\) feature maps in the last convolutional layer, and I will call them \\(\\A^1, \\A^2, \\ldots, \\A^k\\) . How can we \"see\" from the feature maps how the convolutional neural network has made a certain classification? In the first approach, we could simply visualize the raw values of each feature map, average over the feature maps and overlay this over our image. This is the method of visualization feature map activations. This would not be helpful since the feature maps encode information for all classes , but we are interested in a particular class. Grad-CAM has to decide how important each of the k feature map was to our class c that we are interested in. We have to weight each pixel of each feature map with the gradient before we average over the feature maps. This gives us a heatmap which highlights regions that positively or negatively affect the class of interest. This heatmap is send through the ReLU function, which is a fancy way of saying that we set all negative values to zero. Grad-CAM removes all negative values by using a ReLU function, with the argument that we are only interested in the parts that contribute to the selected class c and not to other classes. The word pixel might be misleading here as the feature map is smaller than the image (because of the pooling units) but is mapped back to the original image. We then scale the Grad-CAM map to the interval [0,1] for visualization purposes and overlay it over the original image.","title":"The Intuition"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_explained/#the-big-picture","text":"This part can be best understood alongside my drawings. The big picture is when you \"reduce\" the 3 feature maps into 1 single new \"feature map\" (localization map) by way of a linear sum where each feature map \\(\\A^k\\) is multiplied by the global average pooled gradients \\(\\alpha_k^c\\) . In the example on my drawings, it can be seen that feature maps \\(\\A^1\\) and \\(\\A^2\\) are more important for the model to distinguish the elephant class than that of \\(\\A^3\\) . By design, I made \\(\\A^3\\) in itself already look different in terms of values from \\(\\A^1\\) and \\(\\A^2\\) on the get go. But at this point in time, even though we know the values in feature map \\(\\A^3\\) are vastly different, we cannot say for sure whether \\(\\A^3\\) in itself contributed to the prediction of our class elephant or not. It could also be the case that \\(\\A^1\\) and \\(\\A^2\\) are bonkers. Thus, the concept of taking the gradient of the class \\(\\y^c\\) with respect to each of these feature maps \\(\\A^k\\) becomes increasingly important as now we have a mathematical way to assign some importance to each of these feature maps. Now to even further bring out the intuition, imagine the 3 feature maps for the input image elephant as follows: Feature map \\(\\A^1\\) corresponds to Sobel Filter where it detect edges in particular their trunk of the elephants . Feature map \\(\\A^2\\) corresponds to Another Filter where it detects the ear of the elephants . Feature map \\(\\A^3\\) corresponds to a Background Filter where it detect the background and in particular the grass patch under the elephants . Now this is apparent why \\(\\A^3\\) is vastly different in terms of values since it is actually representing the background and grass. We know that grass is a non-factor in determining an elephant since the african grass field can hold many other animals as well. We really do not want the model to assign important to the grass, if any at all. We can then compute the gradient of \\(\\y^c\\) wrt each feature map and we can intuitively understand it as (see my diagrams): Gradient map \\(\\frac{d\\y^{386}}{d\\A^1}\\) has values \\(\\begin{bmatrix} 2 & 3 \\\\ 4 & 2 \\end{bmatrix}\\) These values are the gradients of feature map \\(\\A^1\\) at a pixel level. We may find it difficult to comprehend these gradients and we want a quick summary of how feature map \\(\\A^1\\) changes our model's decision for \\(\\y^{386}\\) . Thus we use GAP. and when we perform GAP on them, we have \\(\\alpha_{1}^{386} = 2.75\\) . This value roughly translates to a pertubation of \\(2.75\\) in feature map \\(\\A^1\\) will result in a unit change in our class \\(\\y^{386}\\) . Gradient map \\(\\frac{d\\y^{386}}{d\\A^2}\\) has values \\(\\begin{bmatrix} 3 & 5 \\\\ 6 & 3 \\end{bmatrix}\\) and when we perform GAP on them, we have \\(4.25\\) . The same logic applies. Gradient map \\(\\frac{d\\y^{386}}{d\\A^3}\\) has values \\(\\begin{bmatrix} 0.1 & -0.1 \\\\ 0.2 & 0.2 \\end{bmatrix}\\) and when we perform GAP on them, we have \\(0.1\\) . The same logic applies and we see that the gradient is small here and hence we can deduce that this feature map is not very important. Ultimately however, we want to know how the change in all feature maps \\(\\A^k\\) affects our final decision. This is where we perform a linear sum of \\(\\left(\\sum_{k} \\alpha_k^c A^k\\right)\\) of all feature maps with the weight coefficient being their global average pooled gradient map. Just like our good old linear regression model, the intuition is the same as in \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...\\) . The coefficients (the gradient values of \\(2.75, 4.25, 0.1\\) decides which feature map are important and gets \"activated and highlighted\"). We get the weighted sum heatmap to be the shape \\((2, 2)\\) of values \\([8.925, -2.025, 16.125, 2.6]\\) . Now this is a very small map and is likely not going to happen in real life. More often the map at this stage is of size \\((10, 10)\\) and beyond. Imagine once more how \\(\\A^3\\) played almost no role. Lastly, an elementwise ReLU operation is applied to the heatmap to get \\([8,925, 0, 16.125, 2.6]\\) and from the paper: We apply a ReLU to the linear combination of maps because we are only interested in the features that have a positive influence on the class of interest, i.e. pixels whose intensity should be increased in order to increase \\(y^{c}\\) . Intuitively, the idea is that negative values contribute to other classes and we should not really care about it. We can scale back the heatmap to overlay on the original image. From here , author said: Another potential question that can arise is why wouldn\u2019t we just compute the gradient of the class logit with respect to the input image. Remember that a convolutional neural network works as a feature extractor and deeper layers of the network operate in increasingly abstract spaces. We want to see which of the features actually influenced the model\u2019s choice of the class rather than just individual image pixels. That is why it is crucial to take the activation maps of deeper convolutional layers.","title":"The Big Picture"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_explained/#the-algorithm","text":"With reference to Interpretable Machine Learning by Christoph Molnar .","title":"The Algorithm"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_explained/#notation","text":"Input image: \\(\\X\\) Feature Maps of the last convolutional layer in a CNN is denoted as: \\[\\A_1, \\A_2, \\ldots, \\A_k\\] The \\(k\\) is an arbitrary number for the number of feature maps. The Feature Logits of a particular class \\(c\\) is denoted as: \\[\\y^{c}\\] In other words, in ImageNet , the elephant class is indexed \\(386\\) , and is denoted \\(\\y^{386}\\) . This is also the raw feature logits output before the softmax layer. The gradient of the fully-connected logits for class \\(c\\) , \\(\\y^{c}\\) (before the softmax), with respect to feature map activations \\(\\A_k\\) of a convolutional layer : \\[\\dfrac{d\\y^{c}}{d\\A^{k}}\\] It follows that the gradient of the fully-connected for each class \\(c\\) , \\(\\y^{c}\\) , with respect to each pixel on the feature map activations \\(\\A_k\\) is denoted as : \\[\\dfrac{d\\y^{c}}{d\\A^{k}_{ij}}\\] Let us define the gradient of \\(\\y^c\\) with respect to the GAP of each feature map \\(\\A^k\\) to be: \\[\\alpha_k^c = \\overbrace{\\frac{1}{Z}\\sum_{i}\\sum_{j}}^{\\text{global average pooling}} \\underbrace{\\frac{\\delta y^c}{\\delta A_{ij}^k}}_{\\text{gradients via backprop}}\\] Denote the ReLU function as: \\[\\textbf{ReLU}\\] The final heatmap, also called the localization map of Grad-CAM is denoted as: \\[\\L^{c}_{Grad-CAM} \\in \\R^{w \\times h}\\] where \\(w\\) and \\(h\\) is the width and height of the final output localization map. and is equals to \\[\\L^{c}_{Grad-CAM} \\in \\R^{w \\times h} = \\underbrace{ReLU}_{\\text{Pick positive values}}\\left(\\sum_{k} \\alpha_k^c A^k\\right)\\]","title":"Notation"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_explained/#the-algorithm_1","text":"We will only assume our input is 1 single image. Let us look at the recipe for Grad-CAM. Our goal is to find the localization map, which is defined as: \\[L^c_{Grad-CAM} \\in \\mathbb{R}^{u\\times v} = \\underbrace{ReLU}_{\\text{Pick positive values}}\\left(\\sum_{k} \\alpha_k^c A^k\\right)\\] Here, u is the width, v the height of the explanation and c the class of interest. Forward-propagate the input image through the convolutional neural network. Obtain the raw score for the class of interest, meaning the activation of the neuron before the softmax layer. Set all other class activations to zero. Back-propagate the gradient of the class of interest to the last convolutional layer before the fully connected layers: \\(\\frac{\\delta{}y^c}{\\delta{}A^k}\\) . Weight each feature map \"pixel\" by the gradient for the class. Indices i and j refer to the width and height dimensions: \\( \\(\\alpha_k^c = \\overbrace{\\frac{1}{Z}\\sum_{i}\\sum_{j}}^{\\text{global average pooling}} \\underbrace{\\frac{\\delta y^c}{\\delta A_{ij}^k}}_{\\text{gradients via backprop}}\\) \\) This means that the gradients are globally pooled. Calculate an average of the feature maps, weighted per pixel by the gradient. Apply ReLU to the averaged feature map. For visualization: Scale values to the interval between 0 and 1. Upscale the image and overlay it over the original image. Additional step for Guided Grad-CAM: Multiply heatmap with guided backpropagation.","title":"The Algorithm"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_explained/#step-1-forward-propagate","text":"This step just takes in one single image \\(\\mathcal{X}\\) and perform a forward pass the input image through the convolutional neural network and save all the forward pass feature map activations . # the y_logits before softmax; forward pass to populate the forward_activations y_logits = model ( image ) # this dict will be populated with the feature map outputs forward_activations = { \"features.11_ReLU(inplace=True)\" : feature_map_logits } We now have the feature maps stored and we can get our target conv layer. \\[ \\begin{bmatrix} \\A^1 & \\A^2 & \\A^3 & \\cdots & \\A^k \\end{bmatrix} \\] where each \\(\\A^i \\in \\R^{f \\times f}\\) .","title":"Step 1: Forward-propagate"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_explained/#step-2-backward-propagate","text":"In Grad-CAM, we need the gradients of the target category with respect to the target convolutional layer . That is to say, when we call backwards, we are only interested in the particular class's gradients wrt feature maps. if target_category is None : # get the most likely prediction of the model target_category = model ( image ) . argmax ( dim = 1 ) # convert to scalar target_category = target_category . item () # call backward on the model to populate backward_gradients y_logits [:, target_category ] . backward () # this dict will be populated with the gradients backward_gradients = { \"features.11_ReLU(inplace=True)\" : gradients_yc_wrt_features .11 } The above code just says, if we did not choose a target category , the model will choose the one with the highest logit activation. We then call y_logits[: target_category] backwards to store the gradients in backward_gradients . We now have the gradients of the class of interest with respect to the target conv layer (usually last conv layer). \\[ \\begin{bmatrix} \\dfrac{d\\y^{c}}{d\\A^{1}} & \\dfrac{d\\y^{c}}{d\\A^{2}} & \\dfrac{d\\y^{c}}{d\\A^{3}} & \\cdots & \\dfrac{d\\y^{c}}{d\\A^{k}} \\end{bmatrix} \\] where each \\(\\dfrac{d\\y^{c}}{d\\A^{i}} \\in \\R^{f \\times f}\\) .","title":"Step 2: Backward-propagate"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_explained/#step-3-global-average-pool-gradients","text":"The intuition we explained earlier tells us that we need to assign an importance score to each feature map \\(\\A^i\\) . How to do that? In my naive thought, since \\(\\dfrac{d\\y^{c}}{d\\A^{i}}\\) is the same shape as the feature map \\(f \\times f\\) , I would have thought we can just multiply them elementwise. More concretely, If \\[ \\A^1 = \\begin{bmatrix} 2 & 3 \\\\ 1 & 5 \\end{bmatrix} \\quad \\A^2 = \\begin{bmatrix} 3 & 1 \\\\ 2 & 8 \\end{bmatrix} \\] and \\[ \\dfrac{d\\y^{c}}{d\\A^{1}} = \\begin{bmatrix} 0.1 & 0.2 \\\\ 0.3 & 0.5 \\end{bmatrix} \\quad \\dfrac{d\\y^{c}}{d\\A^{2}} = \\begin{bmatrix} 0.3 & 0.1 \\\\ 0.2 & 0.3 \\end{bmatrix} \\] Then we can weigh each feature map on a pixel level: \\[ \\A^1 * \\dfrac{d\\y^{c}}{d\\A^{1}} = \\begin{bmatrix} 0.2 & 0.6 \\\\ 0.3 & 0.25 \\end{bmatrix} \\quad \\A^2 * \\dfrac{d\\y^{c}}{d\\A^{2}} = \\begin{bmatrix} 0.9 & 0.1 \\\\ 0.4 & 0.24 \\end{bmatrix} \\] However, the paper suggested that we can perform a Global Average Pooling on the gradient maps first, the idea is the same, we assume that the average of each gradient map should be representative of the rate of change of \\(y^c\\) with respect to each feature map. \\[\\alpha_k^c = \\overbrace{\\frac{1}{Z}\\sum_{i}\\sum_{j}}^{\\text{global average pooling}} \\underbrace{\\frac{\\delta y^c}{\\delta A_{ij}^k}}_{\\text{gradients via backprop}}\\] where \\(Z\\) is the total number of pixels in this gradient map. Applying this to our example: \\[ \\alpha_1^c = \\overbrace{\\frac{1}{Z}\\sum_{i}\\sum_{j}}^{\\text{global average pooling}} \\underbrace{\\frac{\\delta y^c}{\\delta A_{ij}^1}}_{\\text{gradients via backprop}} = \\frac{1}{4}\\left(0.2+0.6+0.3+0.25\\right) = 0.3375 \\] \\[ \\alpha_2^c = \\overbrace{\\frac{1}{Z}\\sum_{i}\\sum_{j}}^{\\text{global average pooling}} \\underbrace{\\frac{\\delta y^c}{\\delta A_{ij}^2}}_{\\text{gradients via backprop}} = \\frac{1}{4}\\left(0.9+0.1+0.4+0.24\\right) = 0.41 \\] These \\(\\alpha_k^c\\) will be the importance score of each feature map \\(\\A^k\\) . We will stack them into vectors: \\[ \\begin{bmatrix} \\alpha_1^c & \\alpha_2^c & \\alpha_3^c & \\cdots & \\alpha_k^c \\end{bmatrix} \\] and \\(\\alpha_k^c \\in \\R\\) is a scalar. In code it is of the form: dyc_dA = backward_gradients [ target_layer_name ] # average pool the gradients across the channels global_average_pooled_gradients = torch . mean ( dyc_dA , dim = [ 0 , 2 , 3 ])","title":"Step 3: Global Average Pool Gradients"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_explained/#step-4-weighted-global-sum-localized-feature-maps","text":"Recall that good old linear regression has the form \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k\\) . Now we do the same here and treat \\(\\alpha_k^c\\) as the beta weight coefficients, and feature maps \\(\\A^k\\) as our features to get: \\[\\textbf{Localized_Feature_Map} = \\left(\\sum_{k} \\alpha_k^c A^k\\right)\\] which has a shape of \\(\\R^{f \\times f}\\) . The intuition is that some unimportant feature maps will go down to \\(0\\) or near \\(0\\) , and the important feature maps are magnified! The code has this structure: weighted_localization_map = torch . clone ( forward_conv_activations ) # weight the channels by corresponding gradients for i in range ( num_feature_maps ): weighted_localization_map [ :, i , :, : ] *= global_average_pooled_gradients [ i ] # sum the channels of the activations weighted_localization_map = torch . sum ( weighted_localization_map , dim = 1 ) . squeeze ()","title":"Step 4: Weighted Global Sum: Localized Feature Maps"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_explained/#step-5-grad-cam-localization-map-with-relu","text":"The last step is to apply ReLU to the \\(\\textbf{Localized_Feature_Map}\\) above. This is really to zero out all negative entries. Why? Well, for one, in typical logistic regression, negative weights are associated with the model placing importance in the \"other class of interest\", and we do not want that. The same analogy can be applied here though I believe mine is hand-wavy at best. \\[L^c_{Grad-CAM} \\in \\mathbb{R}^{f \\times f} = \\underbrace{ReLU}_{\\text{Pick positive values}}\\left(\\sum_{k} \\alpha_k^c A^k\\right)\\] The code has structure like: # relu on top of the heatmap expression (2) in https://arxiv.org/pdf/1610.02391.pdf relu_weighted_localization_map = torch . nn . ReLU ( inplace = False )( weighted_localization_map ) # normalize the heatmap, scale features to between 0 and 1 to plot gradcam_heatmap = relu_weighted_localization_map / torch . max ( relu_weighted_localization_map )","title":"Step 5: Grad-CAM Localization Map with ReLU"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_explained/#step-6-superimpose-the-grad-cam-heatmap-to-original-image","text":"This step is just doing some resizing from the \\(f \\times f\\) to say \\(224 \\times 224\\) to overlay on the original image.","title":"Step 6: Superimpose the Grad-CAM Heatmap to Original Image"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_explained/#dissecting-kerass-code","text":"https://keras.io/examples/vision/grad_cam/ has sample code but I converted to PyTorch, before converting, I annotated each of the variable inside. (See my KERAS notebook as I changed some variables name). last_conv_layer_output Shape: \\(2 \\times 2 \\times 3\\) with 3 filters of 2 by 2 shape. Note this is just \\(\\A^1, \\A^2, \\A^3\\) where each \\(\\A^k \\in \\R^{2 \\times 2}\\) . y_logits Shape: \\(1 \\times 1000\\) since there are 1000 classes in ImageNet. Note that this is our \\(\\y\\) . If we are interested in the elephant class at index 386, then we denote it as \\(\\y^{c} = \\y^{386}\\) . Thus, \\(\\y \\in \\R^{1 \\times 1000}\\) but \\(\\y^{c} \\in \\R^{1 \\times 1}\\) . Note carefully this is the logits output of all the layers of the CNN, and is just right before the softmax where we transform the logits to probabilities. We can imagine that among these 1000 logits, the highest number means the model thinks that this index is the most probable class. Theoretically speaking, there is no difference in differentiation of the logits wrt to the feature maps versus the softmax probs wrt to the feature maps. This is because softmax is monotonic , and the pre-softmax logits output will tell us already which class is most probable, and so will the softmax. So there should not be any confusion here on why we did not differentiate softmax probs wrt to the feature maps instead since the ranking is preserved in the sense that values in logits the highest is the most probable when transformed by softmax. target_category This is an optional argument in the function. If None , we will automatically assign it to the highest logit's index. In this example, the highest logit in \\(\\y\\) is at index 386 with a value of \\(23.632\\) , corresponding to the target class of elephant. If specified, then the \\(\\y^{c}\\) will change. target_category_logits Once our target_category is defined, we will just slice y_logits[:, target_category] to get the logit value of that particular class. This variable is just \\(\\y^{c}\\) if you look carefully. grads = tape.gradient(target_category_logits, last_conv_layer_output) This is the gradient of the output neuron (top predicted or chosen) with regard to the output feature map of the last conv layer. This has shape \\(2 \\times 2 \\times 3\\) in our simple example. In the python example, the shape at this moment has an additional axis like \\(1 \\times 2 \\times 2 \\times 3\\) which we will eventually squeeze it anyways. Notice it has the same shape as last_conv_layer_output aka the feature maps. In other words, this variable has 3 feature maps stacked together and can be visualized as: \\[\\begin{bmatrix} \\dfrac{d\\y^{c}}{d\\A^{1}} & \\dfrac{d\\y^{c}}{d\\A^{2}} & \\dfrac{d\\y^{c}}{d\\A^{3}} \\end{bmatrix}\\] pooled_grads Shape: \\(3 \\times 1\\) This is a vector where each entry is the mean intensity of the gradient over a specific feature map channel. In other words, in grads[0] we have \\(\\frac{d\\y^c}{d\\A^1}\\) which is of shape \\(2 \\times 2\\) . What we do not is Global Average Pooling on these gradients where we just take the average of all available pixels in this \\(2 \\times 2\\) gradient map for feature map 1. You can find more intuition of GAP above, but in general this is similar logic to how you perform GAP on the output of the feature logits from the last conv layer. \\[\\alpha_k^c = \\overbrace{\\frac{1}{Z}\\sum_{i}\\sum_{j}}^{\\text{global average pooling}} \\underbrace{\\frac{\\delta y^c}{\\delta A_{ij}^k}}_{\\text{gradients via backprop}}\\] where \\(Z\\) is the total number of pixels in this gradient map. heatmap Shape: \\(2 \\times 2\\) which is also the filter size and also the feature map size. This is the weighted sum of all feature maps \\(\\A^1, \\A^2, \\A^3\\) and is \\[\\textbf{Localized_Map} = \\left(\\sum_{k} \\alpha_k^c A^k\\right)\\]","title":"Dissecting KERAS's code"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_from_scratch/","text":"\\[ \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\L}{\\mathbf{L}} \\newcommand{\\X}{\\mathbf{X}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\] Dependencies and Config from typing import * import cv2 import matplotlib.pyplot as plt import numpy as np import torch import torch.nn as nn import torchsummary import torchvision from torch.utils import data from torchvision import datasets , transforms from torchvision.models import * import glob import os import random # Display from IPython.display import Image , display from pytorch_grad_cam import GradCAM from pytorch_grad_cam.utils.image import show_cam_on_image import PIL % cd .. import utils C:\\Users\\reighns\\reighns_ml\\reighns_ml_blog\\docs\\reighns_ml_journey\\deep_learning\\computer_vision\\general\\neural_network_interpretation device = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) utils . seed_all () Using Seed Number 1992 Load Images We load the images and plot the original image. We use the subplot function from the repo here. image_paths = glob . glob ( \"./images/animals/*.*\" ) # elephant has RGBA idk why so need convert images = list ( map ( lambda x : PIL . Image . open ( x ) . convert ( \"RGB\" ), image_paths )) plt . rcParams [ \"figure.figsize\" ] = 16 , 8 utils . subplot ( images , title = \"inputs\" , rows_titles = [ \"cat\" , \"dog_and_cat\" , \"african_elephant\" ], nrows = 1 , ncols = 3 , ) Transforms Params (ImageNet) imagenet_mean : List [ float ] = [ 0.485 , 0.456 , 0.406 ] imagenet_std : List [ float ] = [ 0.229 , 0.224 , 0.225 ] image_size : int = 224 normalized_transform = torchvision . transforms . Compose ( [ torchvision . transforms . Resize (( image_size , image_size )), torchvision . transforms . ToTensor (), torchvision . transforms . Normalize ( mean = imagenet_mean , std = imagenet_std ), ] ) inverse_normalized_transform = torchvision . transforms . Compose ( [ utils . NormalizeInverse ( mean = imagenet_mean , std = imagenet_std )] ) # We use torchvision's transform to transform the cat image with resize and normalization. # Note the tensors below should all be channels first! cat_tensor = normalized_transform ( images [ 0 ]) cat_and_dog_tensor = normalized_transform ( images [ 1 ]) elephant_tensor = normalized_transform ( images [ 2 ]) # Now feature_extractor expects batch_size x C x H x W, so we expand one dimension in the 0th dim and put them on device cat_tensor = cat_tensor . unsqueeze ( dim = 0 ) . to ( device ) cat_and_dog_tensor = cat_and_dog_tensor . unsqueeze ( dim = 0 ) . to ( device ) elephant_tensor = elephant_tensor . unsqueeze ( dim = 0 ) . to ( device ) # Construct an images_dict to store these tensors images_dict : Dict [ str , torch . Tensor ] = { \"cat\" : cat_tensor , \"cat_and_dog\" : cat_and_dog_tensor , \"elephant\" : elephant_tensor } Working with Torch Models Load the Models alexnet_ = alexnet ( pretrained = True ) . to ( device ) vgg16_ = vgg16 ( pretrained = True ) . to ( device ) resnet34_ = resnet34 ( pretrained = True ) . to ( device ) Torch Summary We use a library to output the layer information like Keras's model.summary() . We need to identify the last convolutional layer. import torchsummary def torchsummary_wrapper ( model , image_size : Tuple [ int , int , int ] ) -> torchsummary . model_statistics . ModelStatistics : \"\"\"A torch wrapper to print out layers of a Model. Args: model (CustomNeuralNet): Model. image_size (Tuple[int, int, int]): Image size as a tuple of (channels, height, width). Returns: model_summary (torchsummary.model_statistics.ModelStatistics): Model summary. \"\"\" model_summary = torchsummary . summary ( model , image_size ) return model_summary alexnet_model_summary = torchsummary_wrapper ( alexnet_ , image_size = ( 3 , 224 , 224 )) ========================================================================================== Layer (type:depth-idx) Output Shape Param # ========================================================================================== \u251c\u2500Sequential: 1-1 [-1, 256, 6, 6] -- | \u2514\u2500Conv2d: 2-1 [-1, 64, 55, 55] 23,296 | \u2514\u2500ReLU: 2-2 [-1, 64, 55, 55] -- | \u2514\u2500MaxPool2d: 2-3 [-1, 64, 27, 27] -- | \u2514\u2500Conv2d: 2-4 [-1, 192, 27, 27] 307,392 | \u2514\u2500ReLU: 2-5 [-1, 192, 27, 27] -- | \u2514\u2500MaxPool2d: 2-6 [-1, 192, 13, 13] -- | \u2514\u2500Conv2d: 2-7 [-1, 384, 13, 13] 663,936 | \u2514\u2500ReLU: 2-8 [-1, 384, 13, 13] -- | \u2514\u2500Conv2d: 2-9 [-1, 256, 13, 13] 884,992 | \u2514\u2500ReLU: 2-10 [-1, 256, 13, 13] -- | \u2514\u2500Conv2d: 2-11 [-1, 256, 13, 13] 590,080 | \u2514\u2500ReLU: 2-12 [-1, 256, 13, 13] -- | \u2514\u2500MaxPool2d: 2-13 [-1, 256, 6, 6] -- \u251c\u2500AdaptiveAvgPool2d: 1-2 [-1, 256, 6, 6] -- \u251c\u2500Sequential: 1-3 [-1, 1000] -- | \u2514\u2500Dropout: 2-14 [-1, 9216] -- | \u2514\u2500Linear: 2-15 [-1, 4096] 37,752,832 | \u2514\u2500ReLU: 2-16 [-1, 4096] -- | \u2514\u2500Dropout: 2-17 [-1, 4096] -- | \u2514\u2500Linear: 2-18 [-1, 4096] 16,781,312 | \u2514\u2500ReLU: 2-19 [-1, 4096] -- | \u2514\u2500Linear: 2-20 [-1, 1000] 4,097,000 ========================================================================================== Total params: 61,100,840 Trainable params: 61,100,840 Non-trainable params: 0 Total mult-adds (M): 775.28 ========================================================================================== Input size (MB): 0.57 Forward/backward pass size (MB): 3.77 Params size (MB): 233.08 Estimated Total Size (MB): 237.43 ========================================================================================== vgg16_model_summary = torchsummary_wrapper ( vgg16_ , image_size = ( 3 , 224 , 224 )) ========================================================================================== Layer (type:depth-idx) Output Shape Param # ========================================================================================== \u251c\u2500Sequential: 1-1 [-1, 512, 7, 7] -- | \u2514\u2500Conv2d: 2-1 [-1, 64, 224, 224] 1,792 | \u2514\u2500ReLU: 2-2 [-1, 64, 224, 224] -- | \u2514\u2500Conv2d: 2-3 [-1, 64, 224, 224] 36,928 | \u2514\u2500ReLU: 2-4 [-1, 64, 224, 224] -- | \u2514\u2500MaxPool2d: 2-5 [-1, 64, 112, 112] -- | \u2514\u2500Conv2d: 2-6 [-1, 128, 112, 112] 73,856 | \u2514\u2500ReLU: 2-7 [-1, 128, 112, 112] -- | \u2514\u2500Conv2d: 2-8 [-1, 128, 112, 112] 147,584 | \u2514\u2500ReLU: 2-9 [-1, 128, 112, 112] -- | \u2514\u2500MaxPool2d: 2-10 [-1, 128, 56, 56] -- | \u2514\u2500Conv2d: 2-11 [-1, 256, 56, 56] 295,168 | \u2514\u2500ReLU: 2-12 [-1, 256, 56, 56] -- | \u2514\u2500Conv2d: 2-13 [-1, 256, 56, 56] 590,080 | \u2514\u2500ReLU: 2-14 [-1, 256, 56, 56] -- | \u2514\u2500Conv2d: 2-15 [-1, 256, 56, 56] 590,080 | \u2514\u2500ReLU: 2-16 [-1, 256, 56, 56] -- | \u2514\u2500MaxPool2d: 2-17 [-1, 256, 28, 28] -- | \u2514\u2500Conv2d: 2-18 [-1, 512, 28, 28] 1,180,160 | \u2514\u2500ReLU: 2-19 [-1, 512, 28, 28] -- | \u2514\u2500Conv2d: 2-20 [-1, 512, 28, 28] 2,359,808 | \u2514\u2500ReLU: 2-21 [-1, 512, 28, 28] -- | \u2514\u2500Conv2d: 2-22 [-1, 512, 28, 28] 2,359,808 | \u2514\u2500ReLU: 2-23 [-1, 512, 28, 28] -- | \u2514\u2500MaxPool2d: 2-24 [-1, 512, 14, 14] -- | \u2514\u2500Conv2d: 2-25 [-1, 512, 14, 14] 2,359,808 | \u2514\u2500ReLU: 2-26 [-1, 512, 14, 14] -- | \u2514\u2500Conv2d: 2-27 [-1, 512, 14, 14] 2,359,808 | \u2514\u2500ReLU: 2-28 [-1, 512, 14, 14] -- | \u2514\u2500Conv2d: 2-29 [-1, 512, 14, 14] 2,359,808 | \u2514\u2500ReLU: 2-30 [-1, 512, 14, 14] -- | \u2514\u2500MaxPool2d: 2-31 [-1, 512, 7, 7] -- \u251c\u2500AdaptiveAvgPool2d: 1-2 [-1, 512, 7, 7] -- \u251c\u2500Sequential: 1-3 [-1, 1000] -- | \u2514\u2500Linear: 2-32 [-1, 4096] 102,764,544 | \u2514\u2500ReLU: 2-33 [-1, 4096] -- | \u2514\u2500Dropout: 2-34 [-1, 4096] -- | \u2514\u2500Linear: 2-35 [-1, 4096] 16,781,312 | \u2514\u2500ReLU: 2-36 [-1, 4096] -- | \u2514\u2500Dropout: 2-37 [-1, 4096] -- | \u2514\u2500Linear: 2-38 [-1, 1000] 4,097,000 ========================================================================================== Total params: 138,357,544 Trainable params: 138,357,544 Non-trainable params: 0 Total mult-adds (G): 15.61 ========================================================================================== Input size (MB): 0.57 Forward/backward pass size (MB): 103.43 Params size (MB): 527.79 Estimated Total Size (MB): 631.80 ========================================================================================== resnet34_model_summary = torchsummary_wrapper ( resnet34_ , image_size = ( 3 , 224 , 224 )) ========================================================================================== Layer (type:depth-idx) Output Shape Param # ========================================================================================== \u251c\u2500Conv2d: 1-1 [-1, 64, 112, 112] 9,408 \u251c\u2500BatchNorm2d: 1-2 [-1, 64, 112, 112] 128 \u251c\u2500ReLU: 1-3 [-1, 64, 112, 112] -- \u251c\u2500MaxPool2d: 1-4 [-1, 64, 56, 56] -- \u251c\u2500Sequential: 1-5 [-1, 64, 56, 56] -- | \u2514\u2500BasicBlock: 2-1 [-1, 64, 56, 56] -- | | \u2514\u2500Conv2d: 3-1 [-1, 64, 56, 56] 36,864 | | \u2514\u2500BatchNorm2d: 3-2 [-1, 64, 56, 56] 128 | | \u2514\u2500ReLU: 3-3 [-1, 64, 56, 56] -- | | \u2514\u2500Conv2d: 3-4 [-1, 64, 56, 56] 36,864 | | \u2514\u2500BatchNorm2d: 3-5 [-1, 64, 56, 56] 128 | | \u2514\u2500ReLU: 3-6 [-1, 64, 56, 56] -- | \u2514\u2500BasicBlock: 2-2 [-1, 64, 56, 56] -- | | \u2514\u2500Conv2d: 3-7 [-1, 64, 56, 56] 36,864 | | \u2514\u2500BatchNorm2d: 3-8 [-1, 64, 56, 56] 128 | | \u2514\u2500ReLU: 3-9 [-1, 64, 56, 56] -- | | \u2514\u2500Conv2d: 3-10 [-1, 64, 56, 56] 36,864 | | \u2514\u2500BatchNorm2d: 3-11 [-1, 64, 56, 56] 128 | | \u2514\u2500ReLU: 3-12 [-1, 64, 56, 56] -- | \u2514\u2500BasicBlock: 2-3 [-1, 64, 56, 56] -- | | \u2514\u2500Conv2d: 3-13 [-1, 64, 56, 56] 36,864 | | \u2514\u2500BatchNorm2d: 3-14 [-1, 64, 56, 56] 128 | | \u2514\u2500ReLU: 3-15 [-1, 64, 56, 56] -- | | \u2514\u2500Conv2d: 3-16 [-1, 64, 56, 56] 36,864 | | \u2514\u2500BatchNorm2d: 3-17 [-1, 64, 56, 56] 128 | | \u2514\u2500ReLU: 3-18 [-1, 64, 56, 56] -- \u251c\u2500Sequential: 1-6 [-1, 128, 28, 28] -- | \u2514\u2500BasicBlock: 2-4 [-1, 128, 28, 28] -- | | \u2514\u2500Conv2d: 3-19 [-1, 128, 28, 28] 73,728 | | \u2514\u2500BatchNorm2d: 3-20 [-1, 128, 28, 28] 256 | | \u2514\u2500ReLU: 3-21 [-1, 128, 28, 28] -- | | \u2514\u2500Conv2d: 3-22 [-1, 128, 28, 28] 147,456 | | \u2514\u2500BatchNorm2d: 3-23 [-1, 128, 28, 28] 256 | | \u2514\u2500Sequential: 3-24 [-1, 128, 28, 28] 8,448 | | \u2514\u2500ReLU: 3-25 [-1, 128, 28, 28] -- | \u2514\u2500BasicBlock: 2-5 [-1, 128, 28, 28] -- | | \u2514\u2500Conv2d: 3-26 [-1, 128, 28, 28] 147,456 | | \u2514\u2500BatchNorm2d: 3-27 [-1, 128, 28, 28] 256 | | \u2514\u2500ReLU: 3-28 [-1, 128, 28, 28] -- | | \u2514\u2500Conv2d: 3-29 [-1, 128, 28, 28] 147,456 | | \u2514\u2500BatchNorm2d: 3-30 [-1, 128, 28, 28] 256 | | \u2514\u2500ReLU: 3-31 [-1, 128, 28, 28] -- | \u2514\u2500BasicBlock: 2-6 [-1, 128, 28, 28] -- | | \u2514\u2500Conv2d: 3-32 [-1, 128, 28, 28] 147,456 | | \u2514\u2500BatchNorm2d: 3-33 [-1, 128, 28, 28] 256 | | \u2514\u2500ReLU: 3-34 [-1, 128, 28, 28] -- | | \u2514\u2500Conv2d: 3-35 [-1, 128, 28, 28] 147,456 | | \u2514\u2500BatchNorm2d: 3-36 [-1, 128, 28, 28] 256 | | \u2514\u2500ReLU: 3-37 [-1, 128, 28, 28] -- | \u2514\u2500BasicBlock: 2-7 [-1, 128, 28, 28] -- | | \u2514\u2500Conv2d: 3-38 [-1, 128, 28, 28] 147,456 | | \u2514\u2500BatchNorm2d: 3-39 [-1, 128, 28, 28] 256 | | \u2514\u2500ReLU: 3-40 [-1, 128, 28, 28] -- | | \u2514\u2500Conv2d: 3-41 [-1, 128, 28, 28] 147,456 | | \u2514\u2500BatchNorm2d: 3-42 [-1, 128, 28, 28] 256 | | \u2514\u2500ReLU: 3-43 [-1, 128, 28, 28] -- \u251c\u2500Sequential: 1-7 [-1, 256, 14, 14] -- | \u2514\u2500BasicBlock: 2-8 [-1, 256, 14, 14] -- | | \u2514\u2500Conv2d: 3-44 [-1, 256, 14, 14] 294,912 | | \u2514\u2500BatchNorm2d: 3-45 [-1, 256, 14, 14] 512 | | \u2514\u2500ReLU: 3-46 [-1, 256, 14, 14] -- | | \u2514\u2500Conv2d: 3-47 [-1, 256, 14, 14] 589,824 | | \u2514\u2500BatchNorm2d: 3-48 [-1, 256, 14, 14] 512 | | \u2514\u2500Sequential: 3-49 [-1, 256, 14, 14] 33,280 | | \u2514\u2500ReLU: 3-50 [-1, 256, 14, 14] -- | \u2514\u2500BasicBlock: 2-9 [-1, 256, 14, 14] -- | | \u2514\u2500Conv2d: 3-51 [-1, 256, 14, 14] 589,824 | | \u2514\u2500BatchNorm2d: 3-52 [-1, 256, 14, 14] 512 | | \u2514\u2500ReLU: 3-53 [-1, 256, 14, 14] -- | | \u2514\u2500Conv2d: 3-54 [-1, 256, 14, 14] 589,824 | | \u2514\u2500BatchNorm2d: 3-55 [-1, 256, 14, 14] 512 | | \u2514\u2500ReLU: 3-56 [-1, 256, 14, 14] -- | \u2514\u2500BasicBlock: 2-10 [-1, 256, 14, 14] -- | | \u2514\u2500Conv2d: 3-57 [-1, 256, 14, 14] 589,824 | | \u2514\u2500BatchNorm2d: 3-58 [-1, 256, 14, 14] 512 | | \u2514\u2500ReLU: 3-59 [-1, 256, 14, 14] -- | | \u2514\u2500Conv2d: 3-60 [-1, 256, 14, 14] 589,824 | | \u2514\u2500BatchNorm2d: 3-61 [-1, 256, 14, 14] 512 | | \u2514\u2500ReLU: 3-62 [-1, 256, 14, 14] -- | \u2514\u2500BasicBlock: 2-11 [-1, 256, 14, 14] -- | | \u2514\u2500Conv2d: 3-63 [-1, 256, 14, 14] 589,824 | | \u2514\u2500BatchNorm2d: 3-64 [-1, 256, 14, 14] 512 | | \u2514\u2500ReLU: 3-65 [-1, 256, 14, 14] -- | | \u2514\u2500Conv2d: 3-66 [-1, 256, 14, 14] 589,824 | | \u2514\u2500BatchNorm2d: 3-67 [-1, 256, 14, 14] 512 | | \u2514\u2500ReLU: 3-68 [-1, 256, 14, 14] -- | \u2514\u2500BasicBlock: 2-12 [-1, 256, 14, 14] -- | | \u2514\u2500Conv2d: 3-69 [-1, 256, 14, 14] 589,824 | | \u2514\u2500BatchNorm2d: 3-70 [-1, 256, 14, 14] 512 | | \u2514\u2500ReLU: 3-71 [-1, 256, 14, 14] -- | | \u2514\u2500Conv2d: 3-72 [-1, 256, 14, 14] 589,824 | | \u2514\u2500BatchNorm2d: 3-73 [-1, 256, 14, 14] 512 | | \u2514\u2500ReLU: 3-74 [-1, 256, 14, 14] -- | \u2514\u2500BasicBlock: 2-13 [-1, 256, 14, 14] -- | | \u2514\u2500Conv2d: 3-75 [-1, 256, 14, 14] 589,824 | | \u2514\u2500BatchNorm2d: 3-76 [-1, 256, 14, 14] 512 | | \u2514\u2500ReLU: 3-77 [-1, 256, 14, 14] -- | | \u2514\u2500Conv2d: 3-78 [-1, 256, 14, 14] 589,824 | | \u2514\u2500BatchNorm2d: 3-79 [-1, 256, 14, 14] 512 | | \u2514\u2500ReLU: 3-80 [-1, 256, 14, 14] -- \u251c\u2500Sequential: 1-8 [-1, 512, 7, 7] -- | \u2514\u2500BasicBlock: 2-14 [-1, 512, 7, 7] -- | | \u2514\u2500Conv2d: 3-81 [-1, 512, 7, 7] 1,179,648 | | \u2514\u2500BatchNorm2d: 3-82 [-1, 512, 7, 7] 1,024 | | \u2514\u2500ReLU: 3-83 [-1, 512, 7, 7] -- | | \u2514\u2500Conv2d: 3-84 [-1, 512, 7, 7] 2,359,296 | | \u2514\u2500BatchNorm2d: 3-85 [-1, 512, 7, 7] 1,024 | | \u2514\u2500Sequential: 3-86 [-1, 512, 7, 7] 132,096 | | \u2514\u2500ReLU: 3-87 [-1, 512, 7, 7] -- | \u2514\u2500BasicBlock: 2-15 [-1, 512, 7, 7] -- | | \u2514\u2500Conv2d: 3-88 [-1, 512, 7, 7] 2,359,296 | | \u2514\u2500BatchNorm2d: 3-89 [-1, 512, 7, 7] 1,024 | | \u2514\u2500ReLU: 3-90 [-1, 512, 7, 7] -- | | \u2514\u2500Conv2d: 3-91 [-1, 512, 7, 7] 2,359,296 | | \u2514\u2500BatchNorm2d: 3-92 [-1, 512, 7, 7] 1,024 | | \u2514\u2500ReLU: 3-93 [-1, 512, 7, 7] -- | \u2514\u2500BasicBlock: 2-16 [-1, 512, 7, 7] -- | | \u2514\u2500Conv2d: 3-94 [-1, 512, 7, 7] 2,359,296 | | \u2514\u2500BatchNorm2d: 3-95 [-1, 512, 7, 7] 1,024 | | \u2514\u2500ReLU: 3-96 [-1, 512, 7, 7] -- | | \u2514\u2500Conv2d: 3-97 [-1, 512, 7, 7] 2,359,296 | | \u2514\u2500BatchNorm2d: 3-98 [-1, 512, 7, 7] 1,024 | | \u2514\u2500ReLU: 3-99 [-1, 512, 7, 7] -- \u251c\u2500AdaptiveAvgPool2d: 1-9 [-1, 512, 1, 1] -- \u251c\u2500Linear: 1-10 [-1, 1000] 513,000 ========================================================================================== Total params: 21,797,672 Trainable params: 21,797,672 Non-trainable params: 0 Total mult-adds (G): 3.71 ========================================================================================== Input size (MB): 0.57 Forward/backward pass size (MB): 57.05 Params size (MB): 83.15 Estimated Total Size (MB): 140.77 ========================================================================================== Forward Backward Hooks def forward_hook ( module_name : str , forward_activations : Dict [ str , torch . Tensor ] ) -> Callable : \"\"\"In-place forward hook to save activations of a layer. Args: module_name (str): The name of the layer to hook. forward_activations (Dict[str, torch.Tensor]): The dictionary to save the activations. Returns: forward_hook_: The forward hook function. \"\"\" def forward_hook_ ( module , input , output ): # Save forward feature map activations forward_activations [ module_name ] = output . detach () return forward_hook_ def backward_hook ( module_name : str , backward_gradients : Dict [ str , torch . Tensor ] ) -> Callable : \"\"\"In-place backward hook to save gradients of a layer. Args: module_name (str): The name of the layer to hook. backward_gradients (Dict[str, torch.Tensor]): The dictionary to save the gradients. Returns: Callable: The backward hook function. \"\"\" def backward_hook_ ( module , grad_input , grad_output ): # Save the gradients correspond to the feature maps # This will only be saved when backwards is called. backward_gradients [ module_name ] = grad_output [ 0 ] . detach () return backward_hook_ Saving Forward Activations and Backward Gradients We define a function to get feature map activations and the model's backwards gradients. def get_forward_activations_and_backward_gradients ( model : Callable , image : torch . Tensor , target_category : Optional [ int ] = None , ) -> Union [ List [ Callable ], Dict [ str , torch . Tensor ]]: \"\"\"Get feature maps and gradients from a model. Args: model (Callable): A model. image (torch.Tensor): The input image. target_category (Optional[int], optional): The target category for the model to focus on. Defaults to None. Returns: handlers List[Callable]: A list of handlers. forward_activations Dict[str, torch.Tensor]: A dictionary of forward activations. \"\"\" forward_activations = OrderedDict () backward_gradients = OrderedDict () handlers = [] for name , module in model . named_modules (): module_name = name + \"_\" + str ( module ) handlers . append ( module . register_forward_hook ( forward_hook ( module_name , forward_activations ) ) ) handlers . append ( module . register_backward_hook ( backward_hook ( module_name , backward_gradients ) ) ) model = model . eval () # the y_logits before softmax # forward pass to populate the forward_activations y_logits = model ( image ) if target_category is None : # get the most likely prediction of the model target_category = model ( image ) . argmax ( dim = 1 ) # convert to scalar target_category = target_category . item () # call backward on the model to populate backward_gradients y_logits [:, target_category ] . backward () return handlers , forward_activations , backward_gradients Grad-CAM Storing Forward Activations and Backward Gradients This part is the same as Saving Forward Activations and Backward Gradients. def get_forward_activations_and_backward_gradients ( model : Callable , image : torch . Tensor , target_category : Optional [ int ] = None , ) -> Union [ List [ Callable ], Dict [ str , torch . Tensor ]]: \"\"\"Get feature maps and gradients from a model. Args: model (Callable): A model. image (torch.Tensor): The input image. target_category (Optional[int], optional): The target category for the model to focus on. Defaults to None. Returns: handlers List[Callable]: A list of handlers. forward_activations Dict[str, torch.Tensor]: A dictionary of forward activations. \"\"\" forward_activations = OrderedDict () backward_gradients = OrderedDict () handlers = [] for name , module in model . named_modules (): module_name = name + \"_\" + str ( module ) handlers . append ( module . register_forward_hook ( forward_hook ( module_name , forward_activations ) ) ) handlers . append ( module . register_backward_hook ( backward_hook ( module_name , backward_gradients ) ) ) model = model . eval () # the y_logits before softmax # forward pass to populate the forward_activations y_logits = model ( image ) if target_category is None : # get the most likely prediction of the model target_category = model ( image ) . argmax ( dim = 1 ) # convert to scalar target_category = target_category . item () # call backward on the model to populate backward_gradients y_logits [:, target_category ] . backward () return handlers , forward_activations , backward_gradients Grad-CAM Heatmap Function def compute_gradcam_localization_heatmap ( target_layer_name : str , forward_activations : Dict [ str , torch . Tensor ], backward_gradients : Dict [ str , torch . Tensor ], ) -> torch . Tensor : \"\"\"Compute GradCAM for a specific target layer. Args: target_layer_name (str): The name of the target layer. Usually the last convolutional layer. forward_activations (Dict[str, torch.Tensor]): The forward activations of the model. A dictionary of activations. backward_gradients (Dict[str, torch.Tensor]): The backward gradients of the model. A dictionary of gradients. Returns: gradcam_heatmap (torch.Tensor): The heatmap of the GradCAM. \"\"\" # get the activations of the last convolutional layer forward_conv_activations = forward_activations [ target_layer_name ] dyc_dA = backward_gradients [ target_layer_name ] # average pool the gradients across the channels global_average_pooled_gradients = torch . mean ( dyc_dA , dim = [ 0 , 2 , 3 ]) num_feature_maps = forward_conv_activations . squeeze () . shape [ 0 ] weighted_localization_map = torch . clone ( forward_conv_activations ) # weight the channels by corresponding gradients for i in range ( num_feature_maps ): weighted_localization_map [ :, i , :, : ] *= global_average_pooled_gradients [ i ] # sum the channels of the activations weighted_localization_map = torch . sum ( weighted_localization_map , dim = 1 ) . squeeze () # relu on top of the heatmap expression (2) in https://arxiv.org/pdf/1610.02391.pdf relu_weighted_localization_map = torch . nn . ReLU ( inplace = False )( weighted_localization_map ) # normalize the heatmap, scale features to between 0 and 1 to plot gradcam_heatmap = relu_weighted_localization_map / torch . max ( relu_weighted_localization_map ) # draw the heatmap plt . rcParams [ \"figure.figsize\" ] = 16 , 8 plt . matshow ( gradcam_heatmap . squeeze ()) return gradcam_heatmap SuperImpose Image Superimpose heatmap to original image. def display_gradcam ( image : torch . Tensor , gradcam_heatmap : torch . Tensor , alpha = 1.0 , ): \"\"\"Display the GradCAM heatmap. Args: image (torch.Tensor): _description_ gradcam_heatmap (torch.Tensor): _description_ alpha (float, optional): _description_. Defaults to 0.8. \"\"\" # step 1: Unnormalize the input image to be between 0 and 1, and multiply to become within 255 # squeeze dimensions when there is 1. because [1, 3, 224, 224] is [3, 224, 224] # shape = (3, 224, 224) original_image_unnormalized = inverse_normalized_transform ( image . squeeze () . cpu () ) original_image_unnormalized = original_image_unnormalized * 255 # step 2: make CxHxW to HxWxC and convert to numpy original_image_unnormalized = original_image_unnormalized . permute ( 1 , 2 , 0 ) . numpy () # step 3: Normalize the heatmap to between 0 and 1 and resize it to the image size we want, then multiply to within 255 # first make it into numpy gradcam_heatmap = gradcam_heatmap . detach () . cpu () . numpy () h , w , c = original_image_unnormalized . shape # normalize and resize gradcam_heatmap -= np . min ( gradcam_heatmap ) gradcam_heatmap /= np . max ( gradcam_heatmap ) # Normalize between 0-1 gradcam_heatmap = cv2 . resize ( gradcam_heatmap , ( w , h )) gradcam_heatmap = np . uint8 ( gradcam_heatmap * 255.0 ) # step 4: Apply color maps and overlay the heatmap on the original image to get superimposed image gradcam_heatmap = cv2 . applyColorMap ( gradcam_heatmap , cv2 . COLORMAP_JET ) gradcam_heatmap = cv2 . cvtColor ( gradcam_heatmap , cv2 . COLOR_BGR2RGB ) superimposed_image = gradcam_heatmap * alpha + original_image_unnormalized superimposed_image /= np . max ( superimposed_image ) # step 5: Display the image plt . rcParams [ \"figure.figsize\" ] = 16 , 8 plt . imshow ( superimposed_image ) return superimposed_image Alexnet Gradcam _ , f_alexnet , b_alexnet = get_forward_activations_and_backward_gradients ( alexnet_ , image = images_dict [ \"cat\" ], target_category = None ) C:\\Users\\reighns\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1025: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior. warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \" For this model, the last layer can be alexnet_.features.11 . In our case it is features.11_ReLU(inplace=True) . target_layer_name = \"features.11_ReLU(inplace=True)\" alexnet_gradcam_heatmap = compute_gradcam_localization_heatmap ( target_layer_name , f_alexnet , b_alexnet ) _ = display_gradcam ( image = images_dict [ \"cat\" ], gradcam_heatmap = alexnet_gradcam_heatmap , alpha = 1.0 ) VGG16 Gradcam _ , f_vgg16 , b_vgg16 = get_forward_activations_and_backward_gradients ( vgg16_ , image = images_dict [ \"cat\" ], target_category = None ) For this model, the last layer can be alexnet_.features.12 . In our case it is features.12_MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) . target_layer_name = \"features.29_ReLU(inplace=True)\" vgg16_gradcam_heatmap = compute_gradcam_localization_heatmap ( target_layer_name , f_vgg16 , b_vgg16 ) _ = display_gradcam ( image = images_dict [ \"cat\" ], gradcam_heatmap = vgg16_gradcam_heatmap , alpha = 1.0 ) VGG16 Grad-CAM with Target Category If we specifically pass into a target category, where 285 is cat and 260 is dog, we see the model sees different things! _ , f_vgg16 , b_vgg16 = get_forward_activations_and_backward_gradients ( vgg16_ , image = images_dict [ \"cat_and_dog\" ], target_category = 285 ) target_layer_name = \"features.29_ReLU(inplace=True)\" vgg16_gradcam_heatmap = compute_gradcam_localization_heatmap ( target_layer_name , f_vgg16 , b_vgg16 ) _ = display_gradcam ( image = images_dict [ \"cat_and_dog\" ], gradcam_heatmap = vgg16_gradcam_heatmap , alpha = 1.0 ) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). _ , f_vgg16 , b_vgg16 = get_forward_activations_and_backward_gradients ( vgg16_ , image = images_dict [ \"cat_and_dog\" ], target_category = 260 ) target_layer_name = \"features.29_ReLU(inplace=True)\" vgg16_gradcam_heatmap = compute_gradcam_localization_heatmap ( target_layer_name , f_vgg16 , b_vgg16 ) _ = display_gradcam ( image = images_dict [ \"cat_and_dog\" ], gradcam_heatmap = vgg16_gradcam_heatmap , alpha = 1.0 ) Out of the Box Library from pytorch_grad_cam import GradCAM , ScoreCAM , GradCAMPlusPlus , AblationCAM , XGradCAM , EigenCAM , FullGrad # from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget from pytorch_grad_cam.utils.image import show_cam_on_image from torchvision.models import resnet50 , vgg19 , resnet34 dog_and_cat_path = \"./images/animals/dog_and_cat.jpg\" image = cv2 . imread ( dog_and_cat_path ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) # needed for gradcam. original_image = cv2 . resize ( image , ( 224 , 224 )) model = vgg16 ( pretrained = True ) target_layers = [ model . features [ - 1 ]] input_tensor = images_dict [ \"cat_and_dog\" ] # Create an input tensor image for your model.. # Note: input_tensor can be a batch tensor with several images! # Construct the CAM object once, and then re-use it on many images: cam = GradCAM ( model = model , target_layers = target_layers , use_cuda = False ) # You can also use it within a with statement, to make sure it is freed, # In case you need to re-create it inside an outer loop: # with GradCAM(model=model, target_layers=target_layers, use_cuda=args.use_cuda) as cam: # ... # We have to specify the target we want to generate # the Class Activation Maps for. # If targets is None, the highest scoring category # will be used for every image in the batch. # Here we use ClassifierOutputTarget, but you can define your own custom targets # That are, for example, combinations of categories, or specific outputs in a non standard model. target_category = 285 # You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing. grayscale_cam = cam ( input_tensor = input_tensor , target_category = target_category ) # In this example grayscale_cam has only one image in the batch: grayscale_cam = grayscale_cam [ 0 , :] image_normalized = original_image / 255. visualization = show_cam_on_image ( image_normalized , grayscale_cam , use_rgb = True ) _fig , axes = plt . subplots ( figsize = ( 8 , 8 ), ncols = 2 ) axes [ 0 ] . imshow ( image_normalized ) axes [ 1 ] . imshow ( visualization ) plt . show () torch . cuda . empty_cache () dog_and_cat_path = \"./images/animals/dog_and_cat.jpg\" image = cv2 . imread ( dog_and_cat_path ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) # needed for gradcam. original_image = cv2 . resize ( image , ( 224 , 224 )) model = vgg16 ( pretrained = True ) target_layers = [ model . features [ - 1 ]] input_tensor = images_dict [ \"cat_and_dog\" ] # Create an input tensor image for your model.. # Note: input_tensor can be a batch tensor with several images! # Construct the CAM object once, and then re-use it on many images: cam = GradCAM ( model = model , target_layers = target_layers , use_cuda = False ) # You can also use it within a with statement, to make sure it is freed, # In case you need to re-create it inside an outer loop: # with GradCAM(model=model, target_layers=target_layers, use_cuda=args.use_cuda) as cam: # ... # We have to specify the target we want to generate # the Class Activation Maps for. # If targets is None, the highest scoring category # will be used for every image in the batch. # Here we use ClassifierOutputTarget, but you can define your own custom targets # That are, for example, combinations of categories, or specific outputs in a non standard model. target_category = 260 # You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing. grayscale_cam = cam ( input_tensor = input_tensor , target_category = target_category ) # In this example grayscale_cam has only one image in the batch: grayscale_cam = grayscale_cam [ 0 , :] image_normalized = original_image / 255. visualization = show_cam_on_image ( image_normalized , grayscale_cam , use_rgb = True ) _fig , axes = plt . subplots ( figsize = ( 8 , 8 ), ncols = 2 ) axes [ 0 ] . imshow ( image_normalized ) axes [ 1 ] . imshow ( visualization ) plt . show () torch . cuda . empty_cache ()","title":"Grad-CAM from Scratch"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_from_scratch/#dependencies-and-config","text":"from typing import * import cv2 import matplotlib.pyplot as plt import numpy as np import torch import torch.nn as nn import torchsummary import torchvision from torch.utils import data from torchvision import datasets , transforms from torchvision.models import * import glob import os import random # Display from IPython.display import Image , display from pytorch_grad_cam import GradCAM from pytorch_grad_cam.utils.image import show_cam_on_image import PIL % cd .. import utils C:\\Users\\reighns\\reighns_ml\\reighns_ml_blog\\docs\\reighns_ml_journey\\deep_learning\\computer_vision\\general\\neural_network_interpretation device = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) utils . seed_all () Using Seed Number 1992","title":"Dependencies and Config"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_from_scratch/#load-images","text":"We load the images and plot the original image. We use the subplot function from the repo here. image_paths = glob . glob ( \"./images/animals/*.*\" ) # elephant has RGBA idk why so need convert images = list ( map ( lambda x : PIL . Image . open ( x ) . convert ( \"RGB\" ), image_paths )) plt . rcParams [ \"figure.figsize\" ] = 16 , 8 utils . subplot ( images , title = \"inputs\" , rows_titles = [ \"cat\" , \"dog_and_cat\" , \"african_elephant\" ], nrows = 1 , ncols = 3 , )","title":"Load Images"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_from_scratch/#transforms-params-imagenet","text":"imagenet_mean : List [ float ] = [ 0.485 , 0.456 , 0.406 ] imagenet_std : List [ float ] = [ 0.229 , 0.224 , 0.225 ] image_size : int = 224 normalized_transform = torchvision . transforms . Compose ( [ torchvision . transforms . Resize (( image_size , image_size )), torchvision . transforms . ToTensor (), torchvision . transforms . Normalize ( mean = imagenet_mean , std = imagenet_std ), ] ) inverse_normalized_transform = torchvision . transforms . Compose ( [ utils . NormalizeInverse ( mean = imagenet_mean , std = imagenet_std )] ) # We use torchvision's transform to transform the cat image with resize and normalization. # Note the tensors below should all be channels first! cat_tensor = normalized_transform ( images [ 0 ]) cat_and_dog_tensor = normalized_transform ( images [ 1 ]) elephant_tensor = normalized_transform ( images [ 2 ]) # Now feature_extractor expects batch_size x C x H x W, so we expand one dimension in the 0th dim and put them on device cat_tensor = cat_tensor . unsqueeze ( dim = 0 ) . to ( device ) cat_and_dog_tensor = cat_and_dog_tensor . unsqueeze ( dim = 0 ) . to ( device ) elephant_tensor = elephant_tensor . unsqueeze ( dim = 0 ) . to ( device ) # Construct an images_dict to store these tensors images_dict : Dict [ str , torch . Tensor ] = { \"cat\" : cat_tensor , \"cat_and_dog\" : cat_and_dog_tensor , \"elephant\" : elephant_tensor }","title":"Transforms Params (ImageNet)"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_from_scratch/#working-with-torch-models","text":"","title":"Working with Torch Models"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_from_scratch/#load-the-models","text":"alexnet_ = alexnet ( pretrained = True ) . to ( device ) vgg16_ = vgg16 ( pretrained = True ) . to ( device ) resnet34_ = resnet34 ( pretrained = True ) . to ( device )","title":"Load the Models"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_from_scratch/#torch-summary","text":"We use a library to output the layer information like Keras's model.summary() . We need to identify the last convolutional layer. import torchsummary def torchsummary_wrapper ( model , image_size : Tuple [ int , int , int ] ) -> torchsummary . model_statistics . ModelStatistics : \"\"\"A torch wrapper to print out layers of a Model. Args: model (CustomNeuralNet): Model. image_size (Tuple[int, int, int]): Image size as a tuple of (channels, height, width). Returns: model_summary (torchsummary.model_statistics.ModelStatistics): Model summary. \"\"\" model_summary = torchsummary . summary ( model , image_size ) return model_summary alexnet_model_summary = torchsummary_wrapper ( alexnet_ , image_size = ( 3 , 224 , 224 )) ========================================================================================== Layer (type:depth-idx) Output Shape Param # ========================================================================================== \u251c\u2500Sequential: 1-1 [-1, 256, 6, 6] -- | \u2514\u2500Conv2d: 2-1 [-1, 64, 55, 55] 23,296 | \u2514\u2500ReLU: 2-2 [-1, 64, 55, 55] -- | \u2514\u2500MaxPool2d: 2-3 [-1, 64, 27, 27] -- | \u2514\u2500Conv2d: 2-4 [-1, 192, 27, 27] 307,392 | \u2514\u2500ReLU: 2-5 [-1, 192, 27, 27] -- | \u2514\u2500MaxPool2d: 2-6 [-1, 192, 13, 13] -- | \u2514\u2500Conv2d: 2-7 [-1, 384, 13, 13] 663,936 | \u2514\u2500ReLU: 2-8 [-1, 384, 13, 13] -- | \u2514\u2500Conv2d: 2-9 [-1, 256, 13, 13] 884,992 | \u2514\u2500ReLU: 2-10 [-1, 256, 13, 13] -- | \u2514\u2500Conv2d: 2-11 [-1, 256, 13, 13] 590,080 | \u2514\u2500ReLU: 2-12 [-1, 256, 13, 13] -- | \u2514\u2500MaxPool2d: 2-13 [-1, 256, 6, 6] -- \u251c\u2500AdaptiveAvgPool2d: 1-2 [-1, 256, 6, 6] -- \u251c\u2500Sequential: 1-3 [-1, 1000] -- | \u2514\u2500Dropout: 2-14 [-1, 9216] -- | \u2514\u2500Linear: 2-15 [-1, 4096] 37,752,832 | \u2514\u2500ReLU: 2-16 [-1, 4096] -- | \u2514\u2500Dropout: 2-17 [-1, 4096] -- | \u2514\u2500Linear: 2-18 [-1, 4096] 16,781,312 | \u2514\u2500ReLU: 2-19 [-1, 4096] -- | \u2514\u2500Linear: 2-20 [-1, 1000] 4,097,000 ========================================================================================== Total params: 61,100,840 Trainable params: 61,100,840 Non-trainable params: 0 Total mult-adds (M): 775.28 ========================================================================================== Input size (MB): 0.57 Forward/backward pass size (MB): 3.77 Params size (MB): 233.08 Estimated Total Size (MB): 237.43 ========================================================================================== vgg16_model_summary = torchsummary_wrapper ( vgg16_ , image_size = ( 3 , 224 , 224 )) ========================================================================================== Layer (type:depth-idx) Output Shape Param # ========================================================================================== \u251c\u2500Sequential: 1-1 [-1, 512, 7, 7] -- | \u2514\u2500Conv2d: 2-1 [-1, 64, 224, 224] 1,792 | \u2514\u2500ReLU: 2-2 [-1, 64, 224, 224] -- | \u2514\u2500Conv2d: 2-3 [-1, 64, 224, 224] 36,928 | \u2514\u2500ReLU: 2-4 [-1, 64, 224, 224] -- | \u2514\u2500MaxPool2d: 2-5 [-1, 64, 112, 112] -- | \u2514\u2500Conv2d: 2-6 [-1, 128, 112, 112] 73,856 | \u2514\u2500ReLU: 2-7 [-1, 128, 112, 112] -- | \u2514\u2500Conv2d: 2-8 [-1, 128, 112, 112] 147,584 | \u2514\u2500ReLU: 2-9 [-1, 128, 112, 112] -- | \u2514\u2500MaxPool2d: 2-10 [-1, 128, 56, 56] -- | \u2514\u2500Conv2d: 2-11 [-1, 256, 56, 56] 295,168 | \u2514\u2500ReLU: 2-12 [-1, 256, 56, 56] -- | \u2514\u2500Conv2d: 2-13 [-1, 256, 56, 56] 590,080 | \u2514\u2500ReLU: 2-14 [-1, 256, 56, 56] -- | \u2514\u2500Conv2d: 2-15 [-1, 256, 56, 56] 590,080 | \u2514\u2500ReLU: 2-16 [-1, 256, 56, 56] -- | \u2514\u2500MaxPool2d: 2-17 [-1, 256, 28, 28] -- | \u2514\u2500Conv2d: 2-18 [-1, 512, 28, 28] 1,180,160 | \u2514\u2500ReLU: 2-19 [-1, 512, 28, 28] -- | \u2514\u2500Conv2d: 2-20 [-1, 512, 28, 28] 2,359,808 | \u2514\u2500ReLU: 2-21 [-1, 512, 28, 28] -- | \u2514\u2500Conv2d: 2-22 [-1, 512, 28, 28] 2,359,808 | \u2514\u2500ReLU: 2-23 [-1, 512, 28, 28] -- | \u2514\u2500MaxPool2d: 2-24 [-1, 512, 14, 14] -- | \u2514\u2500Conv2d: 2-25 [-1, 512, 14, 14] 2,359,808 | \u2514\u2500ReLU: 2-26 [-1, 512, 14, 14] -- | \u2514\u2500Conv2d: 2-27 [-1, 512, 14, 14] 2,359,808 | \u2514\u2500ReLU: 2-28 [-1, 512, 14, 14] -- | \u2514\u2500Conv2d: 2-29 [-1, 512, 14, 14] 2,359,808 | \u2514\u2500ReLU: 2-30 [-1, 512, 14, 14] -- | \u2514\u2500MaxPool2d: 2-31 [-1, 512, 7, 7] -- \u251c\u2500AdaptiveAvgPool2d: 1-2 [-1, 512, 7, 7] -- \u251c\u2500Sequential: 1-3 [-1, 1000] -- | \u2514\u2500Linear: 2-32 [-1, 4096] 102,764,544 | \u2514\u2500ReLU: 2-33 [-1, 4096] -- | \u2514\u2500Dropout: 2-34 [-1, 4096] -- | \u2514\u2500Linear: 2-35 [-1, 4096] 16,781,312 | \u2514\u2500ReLU: 2-36 [-1, 4096] -- | \u2514\u2500Dropout: 2-37 [-1, 4096] -- | \u2514\u2500Linear: 2-38 [-1, 1000] 4,097,000 ========================================================================================== Total params: 138,357,544 Trainable params: 138,357,544 Non-trainable params: 0 Total mult-adds (G): 15.61 ========================================================================================== Input size (MB): 0.57 Forward/backward pass size (MB): 103.43 Params size (MB): 527.79 Estimated Total Size (MB): 631.80 ========================================================================================== resnet34_model_summary = torchsummary_wrapper ( resnet34_ , image_size = ( 3 , 224 , 224 )) ========================================================================================== Layer (type:depth-idx) Output Shape Param # ========================================================================================== \u251c\u2500Conv2d: 1-1 [-1, 64, 112, 112] 9,408 \u251c\u2500BatchNorm2d: 1-2 [-1, 64, 112, 112] 128 \u251c\u2500ReLU: 1-3 [-1, 64, 112, 112] -- \u251c\u2500MaxPool2d: 1-4 [-1, 64, 56, 56] -- \u251c\u2500Sequential: 1-5 [-1, 64, 56, 56] -- | \u2514\u2500BasicBlock: 2-1 [-1, 64, 56, 56] -- | | \u2514\u2500Conv2d: 3-1 [-1, 64, 56, 56] 36,864 | | \u2514\u2500BatchNorm2d: 3-2 [-1, 64, 56, 56] 128 | | \u2514\u2500ReLU: 3-3 [-1, 64, 56, 56] -- | | \u2514\u2500Conv2d: 3-4 [-1, 64, 56, 56] 36,864 | | \u2514\u2500BatchNorm2d: 3-5 [-1, 64, 56, 56] 128 | | \u2514\u2500ReLU: 3-6 [-1, 64, 56, 56] -- | \u2514\u2500BasicBlock: 2-2 [-1, 64, 56, 56] -- | | \u2514\u2500Conv2d: 3-7 [-1, 64, 56, 56] 36,864 | | \u2514\u2500BatchNorm2d: 3-8 [-1, 64, 56, 56] 128 | | \u2514\u2500ReLU: 3-9 [-1, 64, 56, 56] -- | | \u2514\u2500Conv2d: 3-10 [-1, 64, 56, 56] 36,864 | | \u2514\u2500BatchNorm2d: 3-11 [-1, 64, 56, 56] 128 | | \u2514\u2500ReLU: 3-12 [-1, 64, 56, 56] -- | \u2514\u2500BasicBlock: 2-3 [-1, 64, 56, 56] -- | | \u2514\u2500Conv2d: 3-13 [-1, 64, 56, 56] 36,864 | | \u2514\u2500BatchNorm2d: 3-14 [-1, 64, 56, 56] 128 | | \u2514\u2500ReLU: 3-15 [-1, 64, 56, 56] -- | | \u2514\u2500Conv2d: 3-16 [-1, 64, 56, 56] 36,864 | | \u2514\u2500BatchNorm2d: 3-17 [-1, 64, 56, 56] 128 | | \u2514\u2500ReLU: 3-18 [-1, 64, 56, 56] -- \u251c\u2500Sequential: 1-6 [-1, 128, 28, 28] -- | \u2514\u2500BasicBlock: 2-4 [-1, 128, 28, 28] -- | | \u2514\u2500Conv2d: 3-19 [-1, 128, 28, 28] 73,728 | | \u2514\u2500BatchNorm2d: 3-20 [-1, 128, 28, 28] 256 | | \u2514\u2500ReLU: 3-21 [-1, 128, 28, 28] -- | | \u2514\u2500Conv2d: 3-22 [-1, 128, 28, 28] 147,456 | | \u2514\u2500BatchNorm2d: 3-23 [-1, 128, 28, 28] 256 | | \u2514\u2500Sequential: 3-24 [-1, 128, 28, 28] 8,448 | | \u2514\u2500ReLU: 3-25 [-1, 128, 28, 28] -- | \u2514\u2500BasicBlock: 2-5 [-1, 128, 28, 28] -- | | \u2514\u2500Conv2d: 3-26 [-1, 128, 28, 28] 147,456 | | \u2514\u2500BatchNorm2d: 3-27 [-1, 128, 28, 28] 256 | | \u2514\u2500ReLU: 3-28 [-1, 128, 28, 28] -- | | \u2514\u2500Conv2d: 3-29 [-1, 128, 28, 28] 147,456 | | \u2514\u2500BatchNorm2d: 3-30 [-1, 128, 28, 28] 256 | | \u2514\u2500ReLU: 3-31 [-1, 128, 28, 28] -- | \u2514\u2500BasicBlock: 2-6 [-1, 128, 28, 28] -- | | \u2514\u2500Conv2d: 3-32 [-1, 128, 28, 28] 147,456 | | \u2514\u2500BatchNorm2d: 3-33 [-1, 128, 28, 28] 256 | | \u2514\u2500ReLU: 3-34 [-1, 128, 28, 28] -- | | \u2514\u2500Conv2d: 3-35 [-1, 128, 28, 28] 147,456 | | \u2514\u2500BatchNorm2d: 3-36 [-1, 128, 28, 28] 256 | | \u2514\u2500ReLU: 3-37 [-1, 128, 28, 28] -- | \u2514\u2500BasicBlock: 2-7 [-1, 128, 28, 28] -- | | \u2514\u2500Conv2d: 3-38 [-1, 128, 28, 28] 147,456 | | \u2514\u2500BatchNorm2d: 3-39 [-1, 128, 28, 28] 256 | | \u2514\u2500ReLU: 3-40 [-1, 128, 28, 28] -- | | \u2514\u2500Conv2d: 3-41 [-1, 128, 28, 28] 147,456 | | \u2514\u2500BatchNorm2d: 3-42 [-1, 128, 28, 28] 256 | | \u2514\u2500ReLU: 3-43 [-1, 128, 28, 28] -- \u251c\u2500Sequential: 1-7 [-1, 256, 14, 14] -- | \u2514\u2500BasicBlock: 2-8 [-1, 256, 14, 14] -- | | \u2514\u2500Conv2d: 3-44 [-1, 256, 14, 14] 294,912 | | \u2514\u2500BatchNorm2d: 3-45 [-1, 256, 14, 14] 512 | | \u2514\u2500ReLU: 3-46 [-1, 256, 14, 14] -- | | \u2514\u2500Conv2d: 3-47 [-1, 256, 14, 14] 589,824 | | \u2514\u2500BatchNorm2d: 3-48 [-1, 256, 14, 14] 512 | | \u2514\u2500Sequential: 3-49 [-1, 256, 14, 14] 33,280 | | \u2514\u2500ReLU: 3-50 [-1, 256, 14, 14] -- | \u2514\u2500BasicBlock: 2-9 [-1, 256, 14, 14] -- | | \u2514\u2500Conv2d: 3-51 [-1, 256, 14, 14] 589,824 | | \u2514\u2500BatchNorm2d: 3-52 [-1, 256, 14, 14] 512 | | \u2514\u2500ReLU: 3-53 [-1, 256, 14, 14] -- | | \u2514\u2500Conv2d: 3-54 [-1, 256, 14, 14] 589,824 | | \u2514\u2500BatchNorm2d: 3-55 [-1, 256, 14, 14] 512 | | \u2514\u2500ReLU: 3-56 [-1, 256, 14, 14] -- | \u2514\u2500BasicBlock: 2-10 [-1, 256, 14, 14] -- | | \u2514\u2500Conv2d: 3-57 [-1, 256, 14, 14] 589,824 | | \u2514\u2500BatchNorm2d: 3-58 [-1, 256, 14, 14] 512 | | \u2514\u2500ReLU: 3-59 [-1, 256, 14, 14] -- | | \u2514\u2500Conv2d: 3-60 [-1, 256, 14, 14] 589,824 | | \u2514\u2500BatchNorm2d: 3-61 [-1, 256, 14, 14] 512 | | \u2514\u2500ReLU: 3-62 [-1, 256, 14, 14] -- | \u2514\u2500BasicBlock: 2-11 [-1, 256, 14, 14] -- | | \u2514\u2500Conv2d: 3-63 [-1, 256, 14, 14] 589,824 | | \u2514\u2500BatchNorm2d: 3-64 [-1, 256, 14, 14] 512 | | \u2514\u2500ReLU: 3-65 [-1, 256, 14, 14] -- | | \u2514\u2500Conv2d: 3-66 [-1, 256, 14, 14] 589,824 | | \u2514\u2500BatchNorm2d: 3-67 [-1, 256, 14, 14] 512 | | \u2514\u2500ReLU: 3-68 [-1, 256, 14, 14] -- | \u2514\u2500BasicBlock: 2-12 [-1, 256, 14, 14] -- | | \u2514\u2500Conv2d: 3-69 [-1, 256, 14, 14] 589,824 | | \u2514\u2500BatchNorm2d: 3-70 [-1, 256, 14, 14] 512 | | \u2514\u2500ReLU: 3-71 [-1, 256, 14, 14] -- | | \u2514\u2500Conv2d: 3-72 [-1, 256, 14, 14] 589,824 | | \u2514\u2500BatchNorm2d: 3-73 [-1, 256, 14, 14] 512 | | \u2514\u2500ReLU: 3-74 [-1, 256, 14, 14] -- | \u2514\u2500BasicBlock: 2-13 [-1, 256, 14, 14] -- | | \u2514\u2500Conv2d: 3-75 [-1, 256, 14, 14] 589,824 | | \u2514\u2500BatchNorm2d: 3-76 [-1, 256, 14, 14] 512 | | \u2514\u2500ReLU: 3-77 [-1, 256, 14, 14] -- | | \u2514\u2500Conv2d: 3-78 [-1, 256, 14, 14] 589,824 | | \u2514\u2500BatchNorm2d: 3-79 [-1, 256, 14, 14] 512 | | \u2514\u2500ReLU: 3-80 [-1, 256, 14, 14] -- \u251c\u2500Sequential: 1-8 [-1, 512, 7, 7] -- | \u2514\u2500BasicBlock: 2-14 [-1, 512, 7, 7] -- | | \u2514\u2500Conv2d: 3-81 [-1, 512, 7, 7] 1,179,648 | | \u2514\u2500BatchNorm2d: 3-82 [-1, 512, 7, 7] 1,024 | | \u2514\u2500ReLU: 3-83 [-1, 512, 7, 7] -- | | \u2514\u2500Conv2d: 3-84 [-1, 512, 7, 7] 2,359,296 | | \u2514\u2500BatchNorm2d: 3-85 [-1, 512, 7, 7] 1,024 | | \u2514\u2500Sequential: 3-86 [-1, 512, 7, 7] 132,096 | | \u2514\u2500ReLU: 3-87 [-1, 512, 7, 7] -- | \u2514\u2500BasicBlock: 2-15 [-1, 512, 7, 7] -- | | \u2514\u2500Conv2d: 3-88 [-1, 512, 7, 7] 2,359,296 | | \u2514\u2500BatchNorm2d: 3-89 [-1, 512, 7, 7] 1,024 | | \u2514\u2500ReLU: 3-90 [-1, 512, 7, 7] -- | | \u2514\u2500Conv2d: 3-91 [-1, 512, 7, 7] 2,359,296 | | \u2514\u2500BatchNorm2d: 3-92 [-1, 512, 7, 7] 1,024 | | \u2514\u2500ReLU: 3-93 [-1, 512, 7, 7] -- | \u2514\u2500BasicBlock: 2-16 [-1, 512, 7, 7] -- | | \u2514\u2500Conv2d: 3-94 [-1, 512, 7, 7] 2,359,296 | | \u2514\u2500BatchNorm2d: 3-95 [-1, 512, 7, 7] 1,024 | | \u2514\u2500ReLU: 3-96 [-1, 512, 7, 7] -- | | \u2514\u2500Conv2d: 3-97 [-1, 512, 7, 7] 2,359,296 | | \u2514\u2500BatchNorm2d: 3-98 [-1, 512, 7, 7] 1,024 | | \u2514\u2500ReLU: 3-99 [-1, 512, 7, 7] -- \u251c\u2500AdaptiveAvgPool2d: 1-9 [-1, 512, 1, 1] -- \u251c\u2500Linear: 1-10 [-1, 1000] 513,000 ========================================================================================== Total params: 21,797,672 Trainable params: 21,797,672 Non-trainable params: 0 Total mult-adds (G): 3.71 ========================================================================================== Input size (MB): 0.57 Forward/backward pass size (MB): 57.05 Params size (MB): 83.15 Estimated Total Size (MB): 140.77 ==========================================================================================","title":"Torch Summary"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_from_scratch/#forward-backward-hooks","text":"def forward_hook ( module_name : str , forward_activations : Dict [ str , torch . Tensor ] ) -> Callable : \"\"\"In-place forward hook to save activations of a layer. Args: module_name (str): The name of the layer to hook. forward_activations (Dict[str, torch.Tensor]): The dictionary to save the activations. Returns: forward_hook_: The forward hook function. \"\"\" def forward_hook_ ( module , input , output ): # Save forward feature map activations forward_activations [ module_name ] = output . detach () return forward_hook_ def backward_hook ( module_name : str , backward_gradients : Dict [ str , torch . Tensor ] ) -> Callable : \"\"\"In-place backward hook to save gradients of a layer. Args: module_name (str): The name of the layer to hook. backward_gradients (Dict[str, torch.Tensor]): The dictionary to save the gradients. Returns: Callable: The backward hook function. \"\"\" def backward_hook_ ( module , grad_input , grad_output ): # Save the gradients correspond to the feature maps # This will only be saved when backwards is called. backward_gradients [ module_name ] = grad_output [ 0 ] . detach () return backward_hook_","title":"Forward Backward Hooks"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_from_scratch/#saving-forward-activations-and-backward-gradients","text":"We define a function to get feature map activations and the model's backwards gradients. def get_forward_activations_and_backward_gradients ( model : Callable , image : torch . Tensor , target_category : Optional [ int ] = None , ) -> Union [ List [ Callable ], Dict [ str , torch . Tensor ]]: \"\"\"Get feature maps and gradients from a model. Args: model (Callable): A model. image (torch.Tensor): The input image. target_category (Optional[int], optional): The target category for the model to focus on. Defaults to None. Returns: handlers List[Callable]: A list of handlers. forward_activations Dict[str, torch.Tensor]: A dictionary of forward activations. \"\"\" forward_activations = OrderedDict () backward_gradients = OrderedDict () handlers = [] for name , module in model . named_modules (): module_name = name + \"_\" + str ( module ) handlers . append ( module . register_forward_hook ( forward_hook ( module_name , forward_activations ) ) ) handlers . append ( module . register_backward_hook ( backward_hook ( module_name , backward_gradients ) ) ) model = model . eval () # the y_logits before softmax # forward pass to populate the forward_activations y_logits = model ( image ) if target_category is None : # get the most likely prediction of the model target_category = model ( image ) . argmax ( dim = 1 ) # convert to scalar target_category = target_category . item () # call backward on the model to populate backward_gradients y_logits [:, target_category ] . backward () return handlers , forward_activations , backward_gradients","title":"Saving Forward Activations and Backward Gradients"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_from_scratch/#grad-cam","text":"","title":"Grad-CAM"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_from_scratch/#storing-forward-activations-and-backward-gradients","text":"This part is the same as Saving Forward Activations and Backward Gradients. def get_forward_activations_and_backward_gradients ( model : Callable , image : torch . Tensor , target_category : Optional [ int ] = None , ) -> Union [ List [ Callable ], Dict [ str , torch . Tensor ]]: \"\"\"Get feature maps and gradients from a model. Args: model (Callable): A model. image (torch.Tensor): The input image. target_category (Optional[int], optional): The target category for the model to focus on. Defaults to None. Returns: handlers List[Callable]: A list of handlers. forward_activations Dict[str, torch.Tensor]: A dictionary of forward activations. \"\"\" forward_activations = OrderedDict () backward_gradients = OrderedDict () handlers = [] for name , module in model . named_modules (): module_name = name + \"_\" + str ( module ) handlers . append ( module . register_forward_hook ( forward_hook ( module_name , forward_activations ) ) ) handlers . append ( module . register_backward_hook ( backward_hook ( module_name , backward_gradients ) ) ) model = model . eval () # the y_logits before softmax # forward pass to populate the forward_activations y_logits = model ( image ) if target_category is None : # get the most likely prediction of the model target_category = model ( image ) . argmax ( dim = 1 ) # convert to scalar target_category = target_category . item () # call backward on the model to populate backward_gradients y_logits [:, target_category ] . backward () return handlers , forward_activations , backward_gradients","title":"Storing Forward Activations and Backward Gradients"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_from_scratch/#grad-cam-heatmap-function","text":"def compute_gradcam_localization_heatmap ( target_layer_name : str , forward_activations : Dict [ str , torch . Tensor ], backward_gradients : Dict [ str , torch . Tensor ], ) -> torch . Tensor : \"\"\"Compute GradCAM for a specific target layer. Args: target_layer_name (str): The name of the target layer. Usually the last convolutional layer. forward_activations (Dict[str, torch.Tensor]): The forward activations of the model. A dictionary of activations. backward_gradients (Dict[str, torch.Tensor]): The backward gradients of the model. A dictionary of gradients. Returns: gradcam_heatmap (torch.Tensor): The heatmap of the GradCAM. \"\"\" # get the activations of the last convolutional layer forward_conv_activations = forward_activations [ target_layer_name ] dyc_dA = backward_gradients [ target_layer_name ] # average pool the gradients across the channels global_average_pooled_gradients = torch . mean ( dyc_dA , dim = [ 0 , 2 , 3 ]) num_feature_maps = forward_conv_activations . squeeze () . shape [ 0 ] weighted_localization_map = torch . clone ( forward_conv_activations ) # weight the channels by corresponding gradients for i in range ( num_feature_maps ): weighted_localization_map [ :, i , :, : ] *= global_average_pooled_gradients [ i ] # sum the channels of the activations weighted_localization_map = torch . sum ( weighted_localization_map , dim = 1 ) . squeeze () # relu on top of the heatmap expression (2) in https://arxiv.org/pdf/1610.02391.pdf relu_weighted_localization_map = torch . nn . ReLU ( inplace = False )( weighted_localization_map ) # normalize the heatmap, scale features to between 0 and 1 to plot gradcam_heatmap = relu_weighted_localization_map / torch . max ( relu_weighted_localization_map ) # draw the heatmap plt . rcParams [ \"figure.figsize\" ] = 16 , 8 plt . matshow ( gradcam_heatmap . squeeze ()) return gradcam_heatmap","title":"Grad-CAM Heatmap Function"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_from_scratch/#superimpose-image","text":"Superimpose heatmap to original image. def display_gradcam ( image : torch . Tensor , gradcam_heatmap : torch . Tensor , alpha = 1.0 , ): \"\"\"Display the GradCAM heatmap. Args: image (torch.Tensor): _description_ gradcam_heatmap (torch.Tensor): _description_ alpha (float, optional): _description_. Defaults to 0.8. \"\"\" # step 1: Unnormalize the input image to be between 0 and 1, and multiply to become within 255 # squeeze dimensions when there is 1. because [1, 3, 224, 224] is [3, 224, 224] # shape = (3, 224, 224) original_image_unnormalized = inverse_normalized_transform ( image . squeeze () . cpu () ) original_image_unnormalized = original_image_unnormalized * 255 # step 2: make CxHxW to HxWxC and convert to numpy original_image_unnormalized = original_image_unnormalized . permute ( 1 , 2 , 0 ) . numpy () # step 3: Normalize the heatmap to between 0 and 1 and resize it to the image size we want, then multiply to within 255 # first make it into numpy gradcam_heatmap = gradcam_heatmap . detach () . cpu () . numpy () h , w , c = original_image_unnormalized . shape # normalize and resize gradcam_heatmap -= np . min ( gradcam_heatmap ) gradcam_heatmap /= np . max ( gradcam_heatmap ) # Normalize between 0-1 gradcam_heatmap = cv2 . resize ( gradcam_heatmap , ( w , h )) gradcam_heatmap = np . uint8 ( gradcam_heatmap * 255.0 ) # step 4: Apply color maps and overlay the heatmap on the original image to get superimposed image gradcam_heatmap = cv2 . applyColorMap ( gradcam_heatmap , cv2 . COLORMAP_JET ) gradcam_heatmap = cv2 . cvtColor ( gradcam_heatmap , cv2 . COLOR_BGR2RGB ) superimposed_image = gradcam_heatmap * alpha + original_image_unnormalized superimposed_image /= np . max ( superimposed_image ) # step 5: Display the image plt . rcParams [ \"figure.figsize\" ] = 16 , 8 plt . imshow ( superimposed_image ) return superimposed_image","title":"SuperImpose Image"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_from_scratch/#alexnet-gradcam","text":"_ , f_alexnet , b_alexnet = get_forward_activations_and_backward_gradients ( alexnet_ , image = images_dict [ \"cat\" ], target_category = None ) C:\\Users\\reighns\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1025: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior. warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \" For this model, the last layer can be alexnet_.features.11 . In our case it is features.11_ReLU(inplace=True) . target_layer_name = \"features.11_ReLU(inplace=True)\" alexnet_gradcam_heatmap = compute_gradcam_localization_heatmap ( target_layer_name , f_alexnet , b_alexnet ) _ = display_gradcam ( image = images_dict [ \"cat\" ], gradcam_heatmap = alexnet_gradcam_heatmap , alpha = 1.0 )","title":"Alexnet Gradcam"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_from_scratch/#vgg16-gradcam","text":"_ , f_vgg16 , b_vgg16 = get_forward_activations_and_backward_gradients ( vgg16_ , image = images_dict [ \"cat\" ], target_category = None ) For this model, the last layer can be alexnet_.features.12 . In our case it is features.12_MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) . target_layer_name = \"features.29_ReLU(inplace=True)\" vgg16_gradcam_heatmap = compute_gradcam_localization_heatmap ( target_layer_name , f_vgg16 , b_vgg16 ) _ = display_gradcam ( image = images_dict [ \"cat\" ], gradcam_heatmap = vgg16_gradcam_heatmap , alpha = 1.0 )","title":"VGG16 Gradcam"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_from_scratch/#vgg16-grad-cam-with-target-category","text":"If we specifically pass into a target category, where 285 is cat and 260 is dog, we see the model sees different things! _ , f_vgg16 , b_vgg16 = get_forward_activations_and_backward_gradients ( vgg16_ , image = images_dict [ \"cat_and_dog\" ], target_category = 285 ) target_layer_name = \"features.29_ReLU(inplace=True)\" vgg16_gradcam_heatmap = compute_gradcam_localization_heatmap ( target_layer_name , f_vgg16 , b_vgg16 ) _ = display_gradcam ( image = images_dict [ \"cat_and_dog\" ], gradcam_heatmap = vgg16_gradcam_heatmap , alpha = 1.0 ) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). _ , f_vgg16 , b_vgg16 = get_forward_activations_and_backward_gradients ( vgg16_ , image = images_dict [ \"cat_and_dog\" ], target_category = 260 ) target_layer_name = \"features.29_ReLU(inplace=True)\" vgg16_gradcam_heatmap = compute_gradcam_localization_heatmap ( target_layer_name , f_vgg16 , b_vgg16 ) _ = display_gradcam ( image = images_dict [ \"cat_and_dog\" ], gradcam_heatmap = vgg16_gradcam_heatmap , alpha = 1.0 )","title":"VGG16 Grad-CAM with Target Category"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/05_gradcam_and_variants/gradcam_from_scratch/#out-of-the-box-library","text":"from pytorch_grad_cam import GradCAM , ScoreCAM , GradCAMPlusPlus , AblationCAM , XGradCAM , EigenCAM , FullGrad # from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget from pytorch_grad_cam.utils.image import show_cam_on_image from torchvision.models import resnet50 , vgg19 , resnet34 dog_and_cat_path = \"./images/animals/dog_and_cat.jpg\" image = cv2 . imread ( dog_and_cat_path ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) # needed for gradcam. original_image = cv2 . resize ( image , ( 224 , 224 )) model = vgg16 ( pretrained = True ) target_layers = [ model . features [ - 1 ]] input_tensor = images_dict [ \"cat_and_dog\" ] # Create an input tensor image for your model.. # Note: input_tensor can be a batch tensor with several images! # Construct the CAM object once, and then re-use it on many images: cam = GradCAM ( model = model , target_layers = target_layers , use_cuda = False ) # You can also use it within a with statement, to make sure it is freed, # In case you need to re-create it inside an outer loop: # with GradCAM(model=model, target_layers=target_layers, use_cuda=args.use_cuda) as cam: # ... # We have to specify the target we want to generate # the Class Activation Maps for. # If targets is None, the highest scoring category # will be used for every image in the batch. # Here we use ClassifierOutputTarget, but you can define your own custom targets # That are, for example, combinations of categories, or specific outputs in a non standard model. target_category = 285 # You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing. grayscale_cam = cam ( input_tensor = input_tensor , target_category = target_category ) # In this example grayscale_cam has only one image in the batch: grayscale_cam = grayscale_cam [ 0 , :] image_normalized = original_image / 255. visualization = show_cam_on_image ( image_normalized , grayscale_cam , use_rgb = True ) _fig , axes = plt . subplots ( figsize = ( 8 , 8 ), ncols = 2 ) axes [ 0 ] . imshow ( image_normalized ) axes [ 1 ] . imshow ( visualization ) plt . show () torch . cuda . empty_cache () dog_and_cat_path = \"./images/animals/dog_and_cat.jpg\" image = cv2 . imread ( dog_and_cat_path ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) # needed for gradcam. original_image = cv2 . resize ( image , ( 224 , 224 )) model = vgg16 ( pretrained = True ) target_layers = [ model . features [ - 1 ]] input_tensor = images_dict [ \"cat_and_dog\" ] # Create an input tensor image for your model.. # Note: input_tensor can be a batch tensor with several images! # Construct the CAM object once, and then re-use it on many images: cam = GradCAM ( model = model , target_layers = target_layers , use_cuda = False ) # You can also use it within a with statement, to make sure it is freed, # In case you need to re-create it inside an outer loop: # with GradCAM(model=model, target_layers=target_layers, use_cuda=args.use_cuda) as cam: # ... # We have to specify the target we want to generate # the Class Activation Maps for. # If targets is None, the highest scoring category # will be used for every image in the batch. # Here we use ClassifierOutputTarget, but you can define your own custom targets # That are, for example, combinations of categories, or specific outputs in a non standard model. target_category = 260 # You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing. grayscale_cam = cam ( input_tensor = input_tensor , target_category = target_category ) # In this example grayscale_cam has only one image in the batch: grayscale_cam = grayscale_cam [ 0 , :] image_normalized = original_image / 255. visualization = show_cam_on_image ( image_normalized , grayscale_cam , use_rgb = True ) _fig , axes = plt . subplots ( figsize = ( 8 , 8 ), ncols = 2 ) axes [ 0 ] . imshow ( image_normalized ) axes [ 1 ] . imshow ( visualization ) plt . show () torch . cuda . empty_cache ()","title":"Out of the Box Library"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/FrancescoSaverioZuppichin_A-journey-into-Convolutional-Neural-Network-visualization/","text":"A journey into Convolutional Neural Network visualization Francesco Saverio Zuppichini There is one famous urban legend about computer vision. Around the 80s, the US military wanted to use neural networks to automatically detect camouflaged enemy tanks. They took a number of pictures of trees without tanks and then pictures with the same trees with tanks behind them. The results were impressive. So impressive that the army wanted to be sure the net had correctly generalized. They took new pictures of woods with and without tanks and they showed them again to the network. This time, the model performed terribly, it was not able to discriminate between pictures with tanks behind woods and just trees.It turned out that all the pictures without tanks were taken on a cloudy day while the ones with tanks on a sunny day! In reality, the network learn to recognize the weather, not the enemy tanks. Nosce te ipsum With this article, we are going to see different techniques to understand what it is going on inside a Convolutional Neural Network to avoid making the same US' army mistake. We are going to use Pytorch . All the code can be found here . Most of the visualizations were developed from scratch, however, some inspiration and parts were taken from here . We will first introduce each technique by briefly explain it and making some example and comparison between different classic computer vision models, alexnet , vgg16 and resnet . Then we will try to better understand a model used in robotics to predict the local distance sensor using only the frontal camera's images. Our goal is not to explain in detail how each technique works since this is already done extremely well by each paper, but to use them to help the reader visualize different model with different inputs to better understand and highlight what and how different models react to a given input. Later on, we show a workflow in which we utilize some of the techniques you will learn in this journey to test the robustness of a model, this is extremely useful to understand and fix its limitations. The curios reader could further improve is understand by looking and the source code for each visulisation and by reading the references. Preambula Disclaimer I am not a fan of jupyter. So apologize in advance if there are some warnings in the outputs and some figures are not well made Let's start our journey by selecting a network. Our first model will be the old school alexnet . It is already available in the torchvision.models package from Pytorch % load_ext autoreload % autoreload 2 from torchvision.models import * from visualisation.core.utils import device model = alexnet ( pretrained = True ) . to ( device ) print ( model ) AlexNet( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)) (1): ReLU(inplace) (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (4): ReLU(inplace) (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): ReLU(inplace) (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (9): ReLU(inplace) (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace) (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) ) (classifier): Sequential( (0): Dropout(p=0.5) (1): Linear(in_features=9216, out_features=4096, bias=True) (2): ReLU(inplace) (3): Dropout(p=0.5) (4): Linear(in_features=4096, out_features=4096, bias=True) (5): ReLU(inplace) (6): Linear(in_features=4096, out_features=1000, bias=True) ) ) Now we need some inputs # %matplotlib notebook Now we need some inputs images. We are going to use three pictures, a cat, the beautiful Basilica di San Pietro and an image with a dog and a cat. import glob import matplotlib.pyplot as plt import numpy as np import torch from utils import * from PIL import Image plt . rcParams [ \"figure.figsize\" ] = 16 , 8 We loaded a few packages. In utils there are several utility function to creates the plots. import glob import matplotlib.pyplot as plt import numpy as np from visualisation.core.utils import device from PIL import Image image_paths = glob . glob ( './images/*.*' ) images = list ( map ( lambda x : Image . open ( x ), image_paths )) subplot ( images , title = 'inputs' , rows_titles = [ 'cat' , 'san pietro' , 'dog_cat' ], nrows = 1 , ncols = 3 ) Since all of our models were trained on imagenet , a huge dataset with 1000 different classes, we need to parse and normalize them. In Pytorch, we have to manually send the data to a device. In this case the device if the fist gpu if you have one, otherwise cpu is selected. Be aware that jupyter have not a garbage collected so we will need to manually free the gpu memory. from torchvision.transforms import ToTensor , Resize , Compose , ToPILImage from visualisation.core import * from visualisation.core.utils import image_net_preprocessing inputs = [ Compose ([ Resize (( 224 , 224 )), ToTensor (), image_net_preprocessing ])( x ) . unsqueeze ( 0 ) for x in images ] # add 1 dim for batch inputs = [ i . to ( device ) for i in inputs ] We also define an utility function to clean the gpu cache def free ( modules ): for m in modules : del m torch . cuda . empty_cache () As we said, imagenet is a huge dataset with 1000 classes, represented by an integer not very human interpetable. We can associate each class id to its label by loading the imaganet2human.txt and create a python dictionary. imagenet2human = {} with open ( 'imaganet2human.txt' ) as f : for line in f . readlines (): key , value = line . split ( ':' ) key = key . replace ( '{' , '' ) . replace ( '}' , '' ) # I forget how regex works :) value = value . replace ( \"'\" , '' ) . replace ( \",\" , '' ) imagenet2human [ int ( key . strip ())] = str ( value . strip ()) list ( imagenet2human . items ())[: 2 ] [(0, 'tench Tinca tinca'), (1, 'goldfish Carassius auratus')] Weights Visualization The first straightforward visualization is to just plot the weights of a target Layer. Obviously, the deeper we go the smaller each image becomes while the channels number increases. We are going to show each channel as a grey array image. Unfortunately, each Pytorch module can be nested and nested, so to make our code as general as possible we first need to trace each sub-module that the input traverse and then store each layer in order. We first need to trace our model to get a list of all the layers so we can select a target layer without following the nested structure of a model. In PyTorch models can be infinitely nested. In other words, we are flattering the model's layers, this is implemented in the module2traced function. model_traced = module2traced ( model , inputs [ 0 ]) model_traced [Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)), ReLU(inplace), MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)), ReLU(inplace), MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace), Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace), MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False), Dropout(p=0.5), Linear(in_features=9216, out_features=4096, bias=True), ReLU(inplace), Dropout(p=0.5), Linear(in_features=4096, out_features=4096, bias=True), ReLU(inplace), Linear(in_features=4096, out_features=1000, bias=True)] Let's plot the first layer's weight. We also print the shape of the weight to give a correct idea to the reader of the dimensional reduction. vis = Weights ( model , device ) first_layer = model_traced [ 0 ] plt . rcParams [ \"figure.figsize\" ] = 16 , 16 run_vis_plot ( vis , inputs [ 0 ], first_layer , ncols = 4 , nrows = 4 ) torch.Size([1, 55, 55]) Let's stop for a minute to explain what those images represent. We traced the input through the computational graph in order to find out all the layers of our models, in this case, alexnet . Then we instantiate the Weights class implemented in visualisation.core and we call it by passing the current input, the cat image and a target layer . As outputs, we get all the current layer's weights as grey images. Then, we plot 16 of them. We can notice that they, in some way, makes sense; for example, some pixels are brighter in the edges of the images. Let's plot the first MaxPool layer to better see this effect, dimensional reduction and higher brightness pixels in some interesting areas. If you are wondering what the maxpolling operations is doing, check this awesome repo first_maxpool_layer = model_traced [ 2 ] run_vis_plot ( vis , inputs [ 0 ], first_maxpool_layer , ncols = 4 , nrows = 4 ) torch.Size([1, 27, 27]) Let's try with an other input, the San Pietro Basilica run_vis_plot ( vis , inputs [ 1 ], first_maxpool_layer , ncols = 4 , nrows = 4 ) torch.Size([1, 27, 27]) By looking at them, these images make somehow sense; they highlight the basilica layout but it is hard to understand what the model is actually doing. We got the idea that is computing something correctly but we could ask some questions, for example: is it looking at the cupola? Which are the most important features of the Basilica? Moreover, the deeper we go the harder it becomes to even recognize the input. deeper_layer = model_traced [ 6 ] run_vis_plot ( vis , inputs [ 1 ], deeper_layer , ncols = 4 , nrows = 4 ) torch.Size([1, 13, 13]) In this case, we have no idea of what is going on. It can be argued that weights visualization does not carry any useful information about the model, even if this is almost true, there is one nice reason of plotting the weights especially at the first layer. When a model is poorly trained or not trained at all, the first weights have lots of noise, since they are just randomly initialized, and they are a lot more similar to the inputs images than the trained ones. This feature can be useful to understand on the fly is a model is trained or not. However, except for this, weights visualization is not the way to go to understand what your black box is thinking. Below we plot the first layer's weight first for the untraind version of alexnet and the for the trained one. alexnet_not_pretrained = alexnet ( pretrained = False ) . to ( device ) run_vis_plot ( Weights ( alexnet_not_pretrained , device ), inputs [ 0 ], module2traced ( alexnet_not_pretrained , inputs [ 0 ])[ 0 ], ncols = 4 , nrows = 4 ) alexnet_pretrained = alexnet ( pretrained = True ) . to ( device ) run_vis_plot ( Weights ( alexnet_pretrained , device ), inputs [ 0 ], module2traced ( alexnet_pretrained , inputs [ 0 ])[ 0 ], ncols = 4 , nrows = 4 ) del alexnet_not_pretrained torch.Size([1, 55, 55]) torch.Size([1, 55, 55]) You can notice that in the first image is simpler to see the input image. Hoewer, this is not a general rule, but in some cases it can help. Similarities with other models We have seen alexnet 's weights, but are they similar across models? Below we plot the first 4 channel of each first layer's weight for alexnet , vgg and resnet modules_instances = [ alexnet , vgg16 , resnet34 ] modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], 0 , Weights , 'weights' , device , ncols = 4 ) free ( modules ) The resnet and vgg weights looks more similar to the input images than alexnet . But, again, what does it mean? Remember that at least resnet is initialized in a different way than the other two models. Saliency Visualisation One idea proposed by Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps is to back-prop the output of the network with respect to a target class until the input and plot the computed gradient. This will highligh the part of the image responsible for that class. Let's start with alexnet. Let's first print the prediction of the network (this could change if you re-run the cell) model . eval () pred = model ( inputs [ 0 ]) _ , id = torch . max ( pred , 1 ) print ( 'predicted class {} ' . format ( imagenet2human [ id . item ()])) predicted class tiger cat Each visualisation is implemented in its own class. You can find the code here . It will backproprop the output with respect to the one hot encoding representation of the number corresponding to class tiger cat from visualisation.core.utils import image_net_postprocessing model . eval () model = model . to ( device ) vis = SaliencyMap ( model , device ) out , info = vis ( inputs [ 0 ], first_layer ) subplot ([ image_net_postprocessing ( inputs [ 0 ] . squeeze () . cpu ()), out ], rows_titles = [ 'original' , 'saliency map' ], parse = tensor2img , nrows = 1 , ncols = 2 ) We can see that alexnet gets exited on the cat. We can even do better! We can set to 0 each negative relu gradient when backprop. This is techinique is called guided . out , info = vis ( inputs [ 0 ], first_layer , guide = True ) subplot ([ image_net_postprocessing ( inputs [ 0 ] . squeeze () . cpu ()), out ], rows_titles = [ 'original' , 'guided saliency map' ], parse = tensor2img , nrows = 1 , ncols = 2 ) Now we can clearly see that the network is looking at the eyes and the nose of the cat. We can try to compare different models modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], 0 , SaliencyMap , 'Saliency' , device , nrows = 1 , ncols = 3 , target_class = 231 , guide = True ) free ( modules ) Alextnet seems more interested to the eyes, while VGG looks at the ears and resnet is similar to alexnet . Now we can clearly understand which part of the inputs help the network gives that prediction. While guiding yields a better human interpretable image, the vanilla implementation can be used for localizing an object of interest. In other words, we can find object of interest for free by cropping out of the input image the region corresponding to the gradient. Let's plot each input image for each model. modules_instances = [ alexnet , vgg16 , resnet34 , resnet152 ] modules = ( m ( pretrained = True ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], None , SaliencyMap , 'SaliencyMap' , device , nrows = 4 , ncols = 3 , inputs = inputs , idx2label = imagenet2human , annotations = [ 'alexnet' , 'vgg16' , 'resnet34' , 'resnet152' ], guide = True ) free ( modules ) The Basilica is very interesting, all four networks correctly classify it as a dome but only resnet152 is more interested in the sky than on the cupola. In the last column, we have an image with two classes, dog and cat . All the networks highlighted booths, like the eyes of the dog and the ears of the cat in vgg16 . What if we would like to discover only the region of the inputs that are related to a specific class? With this technique is impossible. Class Activation Mapping Class Activation Mapping is a techniques presented in Learning Deep Features for Discriminative Localization . The idea is to use the last convolutional layer output and the neurons in the linear layer of the model responsable for a target class, the map is generated by taking the dot product of those. However, to make this work the model has to have some constrains. First of all, the output from the convolution must first go trought an global average polling and it requires feature maps to directly precede softmax layers. To make it works with other architecture, such as alexnet and vgg we have to change some layers in the model and retrain it. This is a major drawback that will be solved with the next section. For now, we can use it for free with resnet! Since its architecture is perfect. The implementation can be found here . We can pass to the visualisation a target_class parameter to get the relative weights from the fc layer. Notice that by changing the target class, we can see different part of the image highlighted. The first image uses the prediction class, while the second an other type of cat and the last one bookcase , just to see what the model will do with a wrong class. from visualisation.core.utils import imshow # we are using resnet 34 since the model has only one fc layer before the softmax and it is preceded by av avg pool # as required from the paper module = resnet34 ( True ) . to ( device ) module . eval () vis = ClassActivationMapping ( module , device ) classes = [ None , 285 , 453 ] def vis_outs2images_classes ( outs ): images = [ x [ 0 ] for x in outs ] classes = [ imagenet2human [ int ( x [ 1 ][ 'prediction' ])] for x in outs ] return images , classes outs = [ vis ( inputs [ 0 ], None , postprocessing = image_net_postprocessing , target_class = c , guide = True ) for c in classes ] images , classes = vis_outs2images_classes ( outs ) subplot ( images , rows_titles = classes , nrows = 1 , ncols = 3 , parse = tensor2img ) It makes sense, the only thing is that in the last row we still have some part of the cat highlighted for bookcase Let's plot the CAM on the cat images for different resnet architecture. For resnet > 34 the Bottleneck module is used modules_instances = [ resnet18 , resnet34 , resnet101 , resnet152 ] cat = inputs [ 2 ] modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , cat , None , ClassActivationMapping , 'ClassActivationMapping' , device , nrows = len ( modules_instances ), ncols = 1 , postprocessing = image_net_postprocessing , rows_name = [ 'resnet18' , 'resnet34' , 'resnet101' , 'resnet152' ], target_class = None ) free ( modules ) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). They are all very similar as expected. One big drawback of this technique is that force you to use a network with a specific architecture, global polling before the decoder part. The next technique generalize this approach by taking advantage of the gradient at one specific layer. Remember that with the class activation we are using the weights of the feature map as a scaling factor for the channels of the last layer. The features map must be before a softmax layer and right after the average pooling. The next technique propose a more general approach. Grad Cam Grad Cam was introduced by Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization . The idea is actually simple, we backprop the output with respect to a target class while storing the gradient and the output at a given layer, in our case the last convolution. Then we perform a global average of the saved gradient keeping the channel dimension in order to get a 1-d tensor, this will represent the importance of each channel in the target convolutional layer. We then multiply each element of the convolutional layer outputs by the averaged gradients to create the grad cam. This whole procedure is fast and it is architecture independent. Interesting, the authors show that is a generalization of the previous technique. The code is here We can use it to higlight what different models are looking at. modules_instances = [ alexnet , vgg16 , resnet34 , resnet152 ] modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], None , GradCam , 'Gradcam' , device , nrows = 1 , ncols = 4 , target_class = None , postprocessing = image_net_postprocessing ) free ( modules ) It is really interesting to see how alexnet looks at the nose, while vgg at the ears and resnet at the whole cat. It is interesting to see that the two resnet version looks at different part of the cat. Below we plot the same input for resnet34 but we change the target class in each column to show the reader how the grad cam change accordingly. from visualisation.core.utils import imshow module = module . to ( device ) vis = GradCam ( module , device ) classes = [ None , 285 , 453 ] outs = [ vis ( inputs [ 0 ], None , postprocessing = image_net_postprocessing , target_class = c ) for c in classes ] images , classes = vis_outs2images_classes ( outs ) subplot ( images , title = 'resnet34' , rows_titles = classes , nrows = 1 , ncols = len ( outs ), parse = tensor2img ) Notice how similar to the CAM output they are. To better compore our three models, below we plot the grad cam for each input with respect to each model modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], None , GradCam , 'Gradcam' , device , nrows = 4 , ncols = 3 , target_class = None , inputs = inputs , idx2label = imagenet2human , annotations = [ 'alexnet' , 'vgg16' , 'resnet34' , 'resnet152' ], postprocessing = image_net_postprocessing ) free ( modules ) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). The reader can immediately notice the difference across the models. Interesting region We talk before about interesting region localizations. Grad-cam can be also used to extract the class object out of the image. Easily, once the have the grad-cam image we can used it as mask to crop out form the input image what we want. The reader can play with the TR parameter to see different effects. TR = 0.3 alexnet_pretrained . eval () vis = GradCam ( alexnet_pretrained , device ) _ = vis ( inputs [ 0 ], None , postprocessing = image_net_postprocessing ) import cv2 def gradcam2crop ( cam , original_img ): b , c , w , h = inputs [ 0 ] . shape cam = cam . numpy () cam -= np . min ( cam ) cam /= np . max ( cam ) cam = cv2 . resize ( cam , ( w , h )) mask = cam > TR original_img = tensor2img ( image_net_postprocessing ( original_img [ 0 ] . squeeze ())) crop = original_img . copy () crop [ mask == 0 ] = 0 return crop crop = gradcam2crop ( vis . cam . cpu (), inputs [ 0 ] . cpu ()) fig = plt . figure () plt . imshow ( crop ) <matplotlib.image.AxesImage at 0x7f79debee048> et voil\u00e0 ! We can also change again class, and crop the interest region for that class. _ = vis ( inputs [ 0 ], None , target_class = 231 , postprocessing = image_net_postprocessing ) crop = gradcam2crop ( vis . cam . cpu (), inputs [ 0 ] . cpu ()) fig = plt . figure () plt . imshow ( crop ) <matplotlib.image.AxesImage at 0x7f79f40c4c18> Different models We have seen all these techniques used with classic classicification models trained on imagenet . What about use them on a different domain? I have ported this paper to Pytorch and retrain it. The model learn from the frontal camera's image of a robot to predict the local distance sensors in order to avoid obstacles. Let's see what if, by using those techniques, we can understand better what is going on inside the model. Learning Long-range Perception using Self-Supervision from Short-Range Sensors and Odometry The idea is to predict the future outputs of a short-range sensor (such as a proximity sensor) given the current outputs of a long-range sensor (such as a camera). They trained a very simple CNN from the robot's camera images to predict the proximity sensor values. If you are interested in their work, you can read the full paper here I have made a PyTorch implementation and retrain the model from scratch. Be awere that I did not fine-tune or try different sets of hyper-parameters so probably my model is not performing as well as the author's one. Let's import it from os import path LONG_RANGE_PERCEPTION_PATH = path . abspath ( './models/long_range_perception/model.pt' ) from models.long_range_perception.model import SimpleCNN from models.long_range_perception.utils import get_dl , H5_PATH , imshow , post_processing , pre_processing , MODEL_PATH free ([ module ]) module = torch . load ( LONG_RANGE_PERCEPTION_PATH , map_location = lambda storage , loc : storage ) module = module . to ( device ) module /home/francesco/Documents/A-journey-into-Convolutional-Neural-Network-visualization-/model.pt SimpleCNN( (encoder): Sequential( (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): ReLU() (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (6): Conv2d(10, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): ReLU() (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (decoder): Sequential( (0): Dropout(p=0.2) (1): Linear(in_features=640, out_features=256, bias=True) (2): ReLU() (3): Linear(in_features=256, out_features=325, bias=True) (4): Sigmoid() ) ) We know need some inputs to test the model, they are taken directly from the test set import os def make_and_show_inputs ( path , transform ): image_paths = glob . glob ( path ) image_paths = filter ( lambda x : os . path . isfile ( x ), image_paths ) images = list ( map ( lambda x : Image . open ( x ) . convert ( 'RGB' ), image_paths )) subplot ( images , title = 'inputs' , rows_titles = [ '1' , '2' , '3' , '4' ], nrows = 1 , ncols = 4 ) plt . show () inputs = [ pre_processing ( x ) . unsqueeze ( 0 ) . to ( device ) for x in images ] # add 1 dim for batch subplot ( inputs , parse = tensor2img , title = 'inputs' , rows_titles = [ '1' , '2' , '3' , '4' ], nrows = 1 , ncols = 4 ) return images , inputs images , inputs = make_and_show_inputs ( 'images/long_range_perception/*' , pre_processing ) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Then author normalize each image, this is done by callind pre_processing . For some reason the inpupts images are different on mac and ubuntu, they should not be like these if you run the notebook on mac the result is different. This is probably due to the warning message. We are going to use the SaliencyMap and the GradCam since those are the best from torch.autograd import Variable module . eval () def run_long_range_vis (): grad = GradCam ( module , device ) all_true = torch . ones ( 1 , 65 * 5 ) . float () . to ( device ) outs_grad = [ grad ( input , None , target_class = all_true , postprocessing = post_processing , regression = True )[ 0 ] for input in inputs ] sal = SaliencyMap ( module , device ) outs_saliency = [ sal ( input , None , guide = True , target_class = all_true , regression = True )[ 0 ] for input in inputs ] subplot ([ * outs_grad , * outs_saliency ], title = 'long_range' , cols_titles = [ '1' , '2' , '3' , '4' ], nrows = 2 , ncols = 4 , parse = tensor2img ) run_long_range_vis () We can clearly see that the model looks at the objects. In the GradCam row, on the second picture, the plan is basically segmented by the heatmap. There is one problem, if you look at the third picture, the white box in front of the camera is not clearly highlighted. This is probably due to the white color of the floor that is very similar to the box's color. Let's investigate this problem. In the second row, the SaliencyMaps highlights all the objects, including the white box. The reader can notice that the reflection in the first picture on the left seems to excite the network in that region. We should also investigate this case but due to time limitations, we will leave it as an exercise for the curious reader. For completeness, let's also print the predicted sensor output. The model tries to predict five frontal distance sensors give the image camera. import seaborn as sns module . eval () preds = module ( torch . stack ( inputs ) . squeeze ( 1 )) fig = plt . figure () sns . heatmap ( preds [ 2 ] . view ( - 1 , 5 ) . detach () . cpu () . numpy ()) <matplotlib.axes._subplots.AxesSubplot at 0x7f79d26dc240> If you compare with the authors pictures, my prediction are worse. This is due to the fact that to speed up everything I did not used all the training set and I did not perform any hyper paramater optimisation. All the code con be found here . Let's now investigate the first problem, object with a similar color to the ground. Similar colors To test if the model has a problem with obstacles with a the same color of the ground, we created in blender four different scenarios with an obstacle. They are showed in the picture below. image_paths = [ * sorted ( glob . glob ( 'images/long_range_perception/equal_color/*' )), * sorted ( glob . glob ( 'images/long_range_perception/different_color/*' ))] image_paths = filter ( lambda x : os . path . isfile ( x ), image_paths ) images = list ( map ( lambda x : Image . open ( x ) . convert ( 'RGB' ), image_paths )) subplot ( images , title = 'inputs' , nrows = 2 , ncols = 4 ) plt . show () There are four different lights configuration and two differents cube colors, one equal to the ground and the second different. The first column represents a realistic situation, while the second has a really strong light from behind that generates a shadow in front of the camera. The third column has a shadow on the left and the last one has a little shadow on the left. This is a perfect scenario to use gradcam to see what the model is looking in each image. In the picture below we plotted the gradcam results. inputs = [ pre_processing ( x ) . unsqueeze ( 0 ) . to ( device ) for x in images ] # add 1 dim for batch run_long_range_vis () The big black shadow in the second column definitly confuses the model. In the first and last column, the grad cam highlights better the corners of the red cube, especially in the first picture. We can definitely say that this model has some hard time with the object of the same colour as the ground. Thanks to this consideration, we could improve the number equal object/ground in the dataset, perform a better preprocessing, change the model structure etc and hopefully increase the robustness of the network. Conclusion In this article, we present different convolutional neural network visualization techniques. In the first section, we introduced each one by applying to a set of famous classification networks. We compared different networks on different inputs and highlight the similarities and difference between them. Then we apply them to a model adopted in robotics to test its robustness and we were able to successfully reveal a problem in the network. Moreover, as a side project, I developed an interactive convolutional neural network visualization application called mirro that receives in just a few days more than a hundred stars on GitHub reflecting the interest of the deep learning community on this topic. All these visualizations are implemented using a common interface and there are available as python module so they can be used in any other module. Thank for reading Francesco Saverio Zuppichini","title":"A journey into Convolutional Neural Network visualization"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/FrancescoSaverioZuppichin_A-journey-into-Convolutional-Neural-Network-visualization/#a-journey-into-convolutional-neural-network-visualization","text":"Francesco Saverio Zuppichini There is one famous urban legend about computer vision. Around the 80s, the US military wanted to use neural networks to automatically detect camouflaged enemy tanks. They took a number of pictures of trees without tanks and then pictures with the same trees with tanks behind them. The results were impressive. So impressive that the army wanted to be sure the net had correctly generalized. They took new pictures of woods with and without tanks and they showed them again to the network. This time, the model performed terribly, it was not able to discriminate between pictures with tanks behind woods and just trees.It turned out that all the pictures without tanks were taken on a cloudy day while the ones with tanks on a sunny day! In reality, the network learn to recognize the weather, not the enemy tanks.","title":"A journey into Convolutional Neural Network visualization"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/FrancescoSaverioZuppichin_A-journey-into-Convolutional-Neural-Network-visualization/#nosce-te-ipsum","text":"With this article, we are going to see different techniques to understand what it is going on inside a Convolutional Neural Network to avoid making the same US' army mistake. We are going to use Pytorch . All the code can be found here . Most of the visualizations were developed from scratch, however, some inspiration and parts were taken from here . We will first introduce each technique by briefly explain it and making some example and comparison between different classic computer vision models, alexnet , vgg16 and resnet . Then we will try to better understand a model used in robotics to predict the local distance sensor using only the frontal camera's images. Our goal is not to explain in detail how each technique works since this is already done extremely well by each paper, but to use them to help the reader visualize different model with different inputs to better understand and highlight what and how different models react to a given input. Later on, we show a workflow in which we utilize some of the techniques you will learn in this journey to test the robustness of a model, this is extremely useful to understand and fix its limitations. The curios reader could further improve is understand by looking and the source code for each visulisation and by reading the references.","title":"Nosce te ipsum"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/FrancescoSaverioZuppichin_A-journey-into-Convolutional-Neural-Network-visualization/#preambula","text":"Disclaimer I am not a fan of jupyter. So apologize in advance if there are some warnings in the outputs and some figures are not well made Let's start our journey by selecting a network. Our first model will be the old school alexnet . It is already available in the torchvision.models package from Pytorch % load_ext autoreload % autoreload 2 from torchvision.models import * from visualisation.core.utils import device model = alexnet ( pretrained = True ) . to ( device ) print ( model ) AlexNet( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)) (1): ReLU(inplace) (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (4): ReLU(inplace) (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): ReLU(inplace) (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (9): ReLU(inplace) (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace) (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) ) (classifier): Sequential( (0): Dropout(p=0.5) (1): Linear(in_features=9216, out_features=4096, bias=True) (2): ReLU(inplace) (3): Dropout(p=0.5) (4): Linear(in_features=4096, out_features=4096, bias=True) (5): ReLU(inplace) (6): Linear(in_features=4096, out_features=1000, bias=True) ) ) Now we need some inputs # %matplotlib notebook Now we need some inputs images. We are going to use three pictures, a cat, the beautiful Basilica di San Pietro and an image with a dog and a cat. import glob import matplotlib.pyplot as plt import numpy as np import torch from utils import * from PIL import Image plt . rcParams [ \"figure.figsize\" ] = 16 , 8 We loaded a few packages. In utils there are several utility function to creates the plots. import glob import matplotlib.pyplot as plt import numpy as np from visualisation.core.utils import device from PIL import Image image_paths = glob . glob ( './images/*.*' ) images = list ( map ( lambda x : Image . open ( x ), image_paths )) subplot ( images , title = 'inputs' , rows_titles = [ 'cat' , 'san pietro' , 'dog_cat' ], nrows = 1 , ncols = 3 ) Since all of our models were trained on imagenet , a huge dataset with 1000 different classes, we need to parse and normalize them. In Pytorch, we have to manually send the data to a device. In this case the device if the fist gpu if you have one, otherwise cpu is selected. Be aware that jupyter have not a garbage collected so we will need to manually free the gpu memory. from torchvision.transforms import ToTensor , Resize , Compose , ToPILImage from visualisation.core import * from visualisation.core.utils import image_net_preprocessing inputs = [ Compose ([ Resize (( 224 , 224 )), ToTensor (), image_net_preprocessing ])( x ) . unsqueeze ( 0 ) for x in images ] # add 1 dim for batch inputs = [ i . to ( device ) for i in inputs ] We also define an utility function to clean the gpu cache def free ( modules ): for m in modules : del m torch . cuda . empty_cache () As we said, imagenet is a huge dataset with 1000 classes, represented by an integer not very human interpetable. We can associate each class id to its label by loading the imaganet2human.txt and create a python dictionary. imagenet2human = {} with open ( 'imaganet2human.txt' ) as f : for line in f . readlines (): key , value = line . split ( ':' ) key = key . replace ( '{' , '' ) . replace ( '}' , '' ) # I forget how regex works :) value = value . replace ( \"'\" , '' ) . replace ( \",\" , '' ) imagenet2human [ int ( key . strip ())] = str ( value . strip ()) list ( imagenet2human . items ())[: 2 ] [(0, 'tench Tinca tinca'), (1, 'goldfish Carassius auratus')]","title":"Preambula"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/FrancescoSaverioZuppichin_A-journey-into-Convolutional-Neural-Network-visualization/#weights-visualization","text":"The first straightforward visualization is to just plot the weights of a target Layer. Obviously, the deeper we go the smaller each image becomes while the channels number increases. We are going to show each channel as a grey array image. Unfortunately, each Pytorch module can be nested and nested, so to make our code as general as possible we first need to trace each sub-module that the input traverse and then store each layer in order. We first need to trace our model to get a list of all the layers so we can select a target layer without following the nested structure of a model. In PyTorch models can be infinitely nested. In other words, we are flattering the model's layers, this is implemented in the module2traced function. model_traced = module2traced ( model , inputs [ 0 ]) model_traced [Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)), ReLU(inplace), MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)), ReLU(inplace), MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace), Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace), MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False), Dropout(p=0.5), Linear(in_features=9216, out_features=4096, bias=True), ReLU(inplace), Dropout(p=0.5), Linear(in_features=4096, out_features=4096, bias=True), ReLU(inplace), Linear(in_features=4096, out_features=1000, bias=True)] Let's plot the first layer's weight. We also print the shape of the weight to give a correct idea to the reader of the dimensional reduction. vis = Weights ( model , device ) first_layer = model_traced [ 0 ] plt . rcParams [ \"figure.figsize\" ] = 16 , 16 run_vis_plot ( vis , inputs [ 0 ], first_layer , ncols = 4 , nrows = 4 ) torch.Size([1, 55, 55]) Let's stop for a minute to explain what those images represent. We traced the input through the computational graph in order to find out all the layers of our models, in this case, alexnet . Then we instantiate the Weights class implemented in visualisation.core and we call it by passing the current input, the cat image and a target layer . As outputs, we get all the current layer's weights as grey images. Then, we plot 16 of them. We can notice that they, in some way, makes sense; for example, some pixels are brighter in the edges of the images. Let's plot the first MaxPool layer to better see this effect, dimensional reduction and higher brightness pixels in some interesting areas. If you are wondering what the maxpolling operations is doing, check this awesome repo first_maxpool_layer = model_traced [ 2 ] run_vis_plot ( vis , inputs [ 0 ], first_maxpool_layer , ncols = 4 , nrows = 4 ) torch.Size([1, 27, 27]) Let's try with an other input, the San Pietro Basilica run_vis_plot ( vis , inputs [ 1 ], first_maxpool_layer , ncols = 4 , nrows = 4 ) torch.Size([1, 27, 27]) By looking at them, these images make somehow sense; they highlight the basilica layout but it is hard to understand what the model is actually doing. We got the idea that is computing something correctly but we could ask some questions, for example: is it looking at the cupola? Which are the most important features of the Basilica? Moreover, the deeper we go the harder it becomes to even recognize the input. deeper_layer = model_traced [ 6 ] run_vis_plot ( vis , inputs [ 1 ], deeper_layer , ncols = 4 , nrows = 4 ) torch.Size([1, 13, 13]) In this case, we have no idea of what is going on. It can be argued that weights visualization does not carry any useful information about the model, even if this is almost true, there is one nice reason of plotting the weights especially at the first layer. When a model is poorly trained or not trained at all, the first weights have lots of noise, since they are just randomly initialized, and they are a lot more similar to the inputs images than the trained ones. This feature can be useful to understand on the fly is a model is trained or not. However, except for this, weights visualization is not the way to go to understand what your black box is thinking. Below we plot the first layer's weight first for the untraind version of alexnet and the for the trained one. alexnet_not_pretrained = alexnet ( pretrained = False ) . to ( device ) run_vis_plot ( Weights ( alexnet_not_pretrained , device ), inputs [ 0 ], module2traced ( alexnet_not_pretrained , inputs [ 0 ])[ 0 ], ncols = 4 , nrows = 4 ) alexnet_pretrained = alexnet ( pretrained = True ) . to ( device ) run_vis_plot ( Weights ( alexnet_pretrained , device ), inputs [ 0 ], module2traced ( alexnet_pretrained , inputs [ 0 ])[ 0 ], ncols = 4 , nrows = 4 ) del alexnet_not_pretrained torch.Size([1, 55, 55]) torch.Size([1, 55, 55]) You can notice that in the first image is simpler to see the input image. Hoewer, this is not a general rule, but in some cases it can help.","title":"Weights Visualization"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/FrancescoSaverioZuppichin_A-journey-into-Convolutional-Neural-Network-visualization/#similarities-with-other-models","text":"We have seen alexnet 's weights, but are they similar across models? Below we plot the first 4 channel of each first layer's weight for alexnet , vgg and resnet modules_instances = [ alexnet , vgg16 , resnet34 ] modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], 0 , Weights , 'weights' , device , ncols = 4 ) free ( modules ) The resnet and vgg weights looks more similar to the input images than alexnet . But, again, what does it mean? Remember that at least resnet is initialized in a different way than the other two models.","title":"Similarities with other models"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/FrancescoSaverioZuppichin_A-journey-into-Convolutional-Neural-Network-visualization/#saliency-visualisation","text":"One idea proposed by Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps is to back-prop the output of the network with respect to a target class until the input and plot the computed gradient. This will highligh the part of the image responsible for that class. Let's start with alexnet. Let's first print the prediction of the network (this could change if you re-run the cell) model . eval () pred = model ( inputs [ 0 ]) _ , id = torch . max ( pred , 1 ) print ( 'predicted class {} ' . format ( imagenet2human [ id . item ()])) predicted class tiger cat Each visualisation is implemented in its own class. You can find the code here . It will backproprop the output with respect to the one hot encoding representation of the number corresponding to class tiger cat from visualisation.core.utils import image_net_postprocessing model . eval () model = model . to ( device ) vis = SaliencyMap ( model , device ) out , info = vis ( inputs [ 0 ], first_layer ) subplot ([ image_net_postprocessing ( inputs [ 0 ] . squeeze () . cpu ()), out ], rows_titles = [ 'original' , 'saliency map' ], parse = tensor2img , nrows = 1 , ncols = 2 ) We can see that alexnet gets exited on the cat. We can even do better! We can set to 0 each negative relu gradient when backprop. This is techinique is called guided . out , info = vis ( inputs [ 0 ], first_layer , guide = True ) subplot ([ image_net_postprocessing ( inputs [ 0 ] . squeeze () . cpu ()), out ], rows_titles = [ 'original' , 'guided saliency map' ], parse = tensor2img , nrows = 1 , ncols = 2 ) Now we can clearly see that the network is looking at the eyes and the nose of the cat. We can try to compare different models modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], 0 , SaliencyMap , 'Saliency' , device , nrows = 1 , ncols = 3 , target_class = 231 , guide = True ) free ( modules ) Alextnet seems more interested to the eyes, while VGG looks at the ears and resnet is similar to alexnet . Now we can clearly understand which part of the inputs help the network gives that prediction. While guiding yields a better human interpretable image, the vanilla implementation can be used for localizing an object of interest. In other words, we can find object of interest for free by cropping out of the input image the region corresponding to the gradient. Let's plot each input image for each model. modules_instances = [ alexnet , vgg16 , resnet34 , resnet152 ] modules = ( m ( pretrained = True ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], None , SaliencyMap , 'SaliencyMap' , device , nrows = 4 , ncols = 3 , inputs = inputs , idx2label = imagenet2human , annotations = [ 'alexnet' , 'vgg16' , 'resnet34' , 'resnet152' ], guide = True ) free ( modules ) The Basilica is very interesting, all four networks correctly classify it as a dome but only resnet152 is more interested in the sky than on the cupola. In the last column, we have an image with two classes, dog and cat . All the networks highlighted booths, like the eyes of the dog and the ears of the cat in vgg16 . What if we would like to discover only the region of the inputs that are related to a specific class? With this technique is impossible.","title":"Saliency Visualisation"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/FrancescoSaverioZuppichin_A-journey-into-Convolutional-Neural-Network-visualization/#class-activation-mapping","text":"Class Activation Mapping is a techniques presented in Learning Deep Features for Discriminative Localization . The idea is to use the last convolutional layer output and the neurons in the linear layer of the model responsable for a target class, the map is generated by taking the dot product of those. However, to make this work the model has to have some constrains. First of all, the output from the convolution must first go trought an global average polling and it requires feature maps to directly precede softmax layers. To make it works with other architecture, such as alexnet and vgg we have to change some layers in the model and retrain it. This is a major drawback that will be solved with the next section. For now, we can use it for free with resnet! Since its architecture is perfect. The implementation can be found here . We can pass to the visualisation a target_class parameter to get the relative weights from the fc layer. Notice that by changing the target class, we can see different part of the image highlighted. The first image uses the prediction class, while the second an other type of cat and the last one bookcase , just to see what the model will do with a wrong class. from visualisation.core.utils import imshow # we are using resnet 34 since the model has only one fc layer before the softmax and it is preceded by av avg pool # as required from the paper module = resnet34 ( True ) . to ( device ) module . eval () vis = ClassActivationMapping ( module , device ) classes = [ None , 285 , 453 ] def vis_outs2images_classes ( outs ): images = [ x [ 0 ] for x in outs ] classes = [ imagenet2human [ int ( x [ 1 ][ 'prediction' ])] for x in outs ] return images , classes outs = [ vis ( inputs [ 0 ], None , postprocessing = image_net_postprocessing , target_class = c , guide = True ) for c in classes ] images , classes = vis_outs2images_classes ( outs ) subplot ( images , rows_titles = classes , nrows = 1 , ncols = 3 , parse = tensor2img ) It makes sense, the only thing is that in the last row we still have some part of the cat highlighted for bookcase Let's plot the CAM on the cat images for different resnet architecture. For resnet > 34 the Bottleneck module is used modules_instances = [ resnet18 , resnet34 , resnet101 , resnet152 ] cat = inputs [ 2 ] modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , cat , None , ClassActivationMapping , 'ClassActivationMapping' , device , nrows = len ( modules_instances ), ncols = 1 , postprocessing = image_net_postprocessing , rows_name = [ 'resnet18' , 'resnet34' , 'resnet101' , 'resnet152' ], target_class = None ) free ( modules ) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). They are all very similar as expected. One big drawback of this technique is that force you to use a network with a specific architecture, global polling before the decoder part. The next technique generalize this approach by taking advantage of the gradient at one specific layer. Remember that with the class activation we are using the weights of the feature map as a scaling factor for the channels of the last layer. The features map must be before a softmax layer and right after the average pooling. The next technique propose a more general approach.","title":"Class Activation Mapping"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/FrancescoSaverioZuppichin_A-journey-into-Convolutional-Neural-Network-visualization/#grad-cam","text":"Grad Cam was introduced by Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization . The idea is actually simple, we backprop the output with respect to a target class while storing the gradient and the output at a given layer, in our case the last convolution. Then we perform a global average of the saved gradient keeping the channel dimension in order to get a 1-d tensor, this will represent the importance of each channel in the target convolutional layer. We then multiply each element of the convolutional layer outputs by the averaged gradients to create the grad cam. This whole procedure is fast and it is architecture independent. Interesting, the authors show that is a generalization of the previous technique. The code is here We can use it to higlight what different models are looking at. modules_instances = [ alexnet , vgg16 , resnet34 , resnet152 ] modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], None , GradCam , 'Gradcam' , device , nrows = 1 , ncols = 4 , target_class = None , postprocessing = image_net_postprocessing ) free ( modules ) It is really interesting to see how alexnet looks at the nose, while vgg at the ears and resnet at the whole cat. It is interesting to see that the two resnet version looks at different part of the cat. Below we plot the same input for resnet34 but we change the target class in each column to show the reader how the grad cam change accordingly. from visualisation.core.utils import imshow module = module . to ( device ) vis = GradCam ( module , device ) classes = [ None , 285 , 453 ] outs = [ vis ( inputs [ 0 ], None , postprocessing = image_net_postprocessing , target_class = c ) for c in classes ] images , classes = vis_outs2images_classes ( outs ) subplot ( images , title = 'resnet34' , rows_titles = classes , nrows = 1 , ncols = len ( outs ), parse = tensor2img ) Notice how similar to the CAM output they are. To better compore our three models, below we plot the grad cam for each input with respect to each model modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], None , GradCam , 'Gradcam' , device , nrows = 4 , ncols = 3 , target_class = None , inputs = inputs , idx2label = imagenet2human , annotations = [ 'alexnet' , 'vgg16' , 'resnet34' , 'resnet152' ], postprocessing = image_net_postprocessing ) free ( modules ) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). The reader can immediately notice the difference across the models.","title":"Grad Cam"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/FrancescoSaverioZuppichin_A-journey-into-Convolutional-Neural-Network-visualization/#interesting-region","text":"We talk before about interesting region localizations. Grad-cam can be also used to extract the class object out of the image. Easily, once the have the grad-cam image we can used it as mask to crop out form the input image what we want. The reader can play with the TR parameter to see different effects. TR = 0.3 alexnet_pretrained . eval () vis = GradCam ( alexnet_pretrained , device ) _ = vis ( inputs [ 0 ], None , postprocessing = image_net_postprocessing ) import cv2 def gradcam2crop ( cam , original_img ): b , c , w , h = inputs [ 0 ] . shape cam = cam . numpy () cam -= np . min ( cam ) cam /= np . max ( cam ) cam = cv2 . resize ( cam , ( w , h )) mask = cam > TR original_img = tensor2img ( image_net_postprocessing ( original_img [ 0 ] . squeeze ())) crop = original_img . copy () crop [ mask == 0 ] = 0 return crop crop = gradcam2crop ( vis . cam . cpu (), inputs [ 0 ] . cpu ()) fig = plt . figure () plt . imshow ( crop ) <matplotlib.image.AxesImage at 0x7f79debee048> et voil\u00e0 ! We can also change again class, and crop the interest region for that class. _ = vis ( inputs [ 0 ], None , target_class = 231 , postprocessing = image_net_postprocessing ) crop = gradcam2crop ( vis . cam . cpu (), inputs [ 0 ] . cpu ()) fig = plt . figure () plt . imshow ( crop ) <matplotlib.image.AxesImage at 0x7f79f40c4c18>","title":"Interesting region"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/FrancescoSaverioZuppichin_A-journey-into-Convolutional-Neural-Network-visualization/#different-models","text":"We have seen all these techniques used with classic classicification models trained on imagenet . What about use them on a different domain? I have ported this paper to Pytorch and retrain it. The model learn from the frontal camera's image of a robot to predict the local distance sensors in order to avoid obstacles. Let's see what if, by using those techniques, we can understand better what is going on inside the model.","title":"Different models"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/FrancescoSaverioZuppichin_A-journey-into-Convolutional-Neural-Network-visualization/#learning-long-range-perception-using-self-supervision-from-short-range-sensors-and-odometry","text":"The idea is to predict the future outputs of a short-range sensor (such as a proximity sensor) given the current outputs of a long-range sensor (such as a camera). They trained a very simple CNN from the robot's camera images to predict the proximity sensor values. If you are interested in their work, you can read the full paper here I have made a PyTorch implementation and retrain the model from scratch. Be awere that I did not fine-tune or try different sets of hyper-parameters so probably my model is not performing as well as the author's one. Let's import it from os import path LONG_RANGE_PERCEPTION_PATH = path . abspath ( './models/long_range_perception/model.pt' ) from models.long_range_perception.model import SimpleCNN from models.long_range_perception.utils import get_dl , H5_PATH , imshow , post_processing , pre_processing , MODEL_PATH free ([ module ]) module = torch . load ( LONG_RANGE_PERCEPTION_PATH , map_location = lambda storage , loc : storage ) module = module . to ( device ) module /home/francesco/Documents/A-journey-into-Convolutional-Neural-Network-visualization-/model.pt SimpleCNN( (encoder): Sequential( (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): ReLU() (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (6): Conv2d(10, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): ReLU() (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (decoder): Sequential( (0): Dropout(p=0.2) (1): Linear(in_features=640, out_features=256, bias=True) (2): ReLU() (3): Linear(in_features=256, out_features=325, bias=True) (4): Sigmoid() ) ) We know need some inputs to test the model, they are taken directly from the test set import os def make_and_show_inputs ( path , transform ): image_paths = glob . glob ( path ) image_paths = filter ( lambda x : os . path . isfile ( x ), image_paths ) images = list ( map ( lambda x : Image . open ( x ) . convert ( 'RGB' ), image_paths )) subplot ( images , title = 'inputs' , rows_titles = [ '1' , '2' , '3' , '4' ], nrows = 1 , ncols = 4 ) plt . show () inputs = [ pre_processing ( x ) . unsqueeze ( 0 ) . to ( device ) for x in images ] # add 1 dim for batch subplot ( inputs , parse = tensor2img , title = 'inputs' , rows_titles = [ '1' , '2' , '3' , '4' ], nrows = 1 , ncols = 4 ) return images , inputs images , inputs = make_and_show_inputs ( 'images/long_range_perception/*' , pre_processing ) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Then author normalize each image, this is done by callind pre_processing . For some reason the inpupts images are different on mac and ubuntu, they should not be like these if you run the notebook on mac the result is different. This is probably due to the warning message. We are going to use the SaliencyMap and the GradCam since those are the best from torch.autograd import Variable module . eval () def run_long_range_vis (): grad = GradCam ( module , device ) all_true = torch . ones ( 1 , 65 * 5 ) . float () . to ( device ) outs_grad = [ grad ( input , None , target_class = all_true , postprocessing = post_processing , regression = True )[ 0 ] for input in inputs ] sal = SaliencyMap ( module , device ) outs_saliency = [ sal ( input , None , guide = True , target_class = all_true , regression = True )[ 0 ] for input in inputs ] subplot ([ * outs_grad , * outs_saliency ], title = 'long_range' , cols_titles = [ '1' , '2' , '3' , '4' ], nrows = 2 , ncols = 4 , parse = tensor2img ) run_long_range_vis () We can clearly see that the model looks at the objects. In the GradCam row, on the second picture, the plan is basically segmented by the heatmap. There is one problem, if you look at the third picture, the white box in front of the camera is not clearly highlighted. This is probably due to the white color of the floor that is very similar to the box's color. Let's investigate this problem. In the second row, the SaliencyMaps highlights all the objects, including the white box. The reader can notice that the reflection in the first picture on the left seems to excite the network in that region. We should also investigate this case but due to time limitations, we will leave it as an exercise for the curious reader. For completeness, let's also print the predicted sensor output. The model tries to predict five frontal distance sensors give the image camera. import seaborn as sns module . eval () preds = module ( torch . stack ( inputs ) . squeeze ( 1 )) fig = plt . figure () sns . heatmap ( preds [ 2 ] . view ( - 1 , 5 ) . detach () . cpu () . numpy ()) <matplotlib.axes._subplots.AxesSubplot at 0x7f79d26dc240> If you compare with the authors pictures, my prediction are worse. This is due to the fact that to speed up everything I did not used all the training set and I did not perform any hyper paramater optimisation. All the code con be found here . Let's now investigate the first problem, object with a similar color to the ground.","title":"Learning Long-range Perception using Self-Supervision from Short-Range Sensors and Odometry"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/FrancescoSaverioZuppichin_A-journey-into-Convolutional-Neural-Network-visualization/#similar-colors","text":"To test if the model has a problem with obstacles with a the same color of the ground, we created in blender four different scenarios with an obstacle. They are showed in the picture below. image_paths = [ * sorted ( glob . glob ( 'images/long_range_perception/equal_color/*' )), * sorted ( glob . glob ( 'images/long_range_perception/different_color/*' ))] image_paths = filter ( lambda x : os . path . isfile ( x ), image_paths ) images = list ( map ( lambda x : Image . open ( x ) . convert ( 'RGB' ), image_paths )) subplot ( images , title = 'inputs' , nrows = 2 , ncols = 4 ) plt . show () There are four different lights configuration and two differents cube colors, one equal to the ground and the second different. The first column represents a realistic situation, while the second has a really strong light from behind that generates a shadow in front of the camera. The third column has a shadow on the left and the last one has a little shadow on the left. This is a perfect scenario to use gradcam to see what the model is looking in each image. In the picture below we plotted the gradcam results. inputs = [ pre_processing ( x ) . unsqueeze ( 0 ) . to ( device ) for x in images ] # add 1 dim for batch run_long_range_vis () The big black shadow in the second column definitly confuses the model. In the first and last column, the grad cam highlights better the corners of the red cube, especially in the first picture. We can definitely say that this model has some hard time with the object of the same colour as the ground. Thanks to this consideration, we could improve the number equal object/ground in the dataset, perform a better preprocessing, change the model structure etc and hopefully increase the robustness of the network.","title":"Similar colors"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/FrancescoSaverioZuppichin_A-journey-into-Convolutional-Neural-Network-visualization/#conclusion","text":"In this article, we present different convolutional neural network visualization techniques. In the first section, we introduced each one by applying to a set of famous classification networks. We compared different networks on different inputs and highlight the similarities and difference between them. Then we apply them to a model adopted in robotics to test its robustness and we were able to successfully reveal a problem in the network. Moreover, as a side project, I developed an interactive convolutional neural network visualization application called mirro that receives in just a few days more than a hundred stars on GitHub reflecting the interest of the deep learning community on this topic. All these visualizations are implemented using a common interface and there are available as python module so they can be used in any other module. Thank for reading Francesco Saverio Zuppichini","title":"Conclusion"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/SkinLesionClassification/","text":"Skin Cancer Classification - An Educational Guide In this tutorial we aim to provide a simple step-by-step guide to anyone who wants to work on the problem of skin lesion classification regardless of their level or expertise; from medical doctors, to master students and more experienced researchers. Using this guide you will learn: How to load the data, visualise it and uncover more about the class distribution and meta-data. How to utilise architectures with varying complexity from a few convolutional layers to hundreds of them. How to train a model with appropriate optimisers and loss functions. How to rigorously test your trained model, providing not only metrics such as accuracy but also visualisations like confusion matrix and Grad \u2014 Cam. How to analyse and understand your results. To conclude with, we will provide a few more tips that are usually utilised by the participants of the ISIC Challenges, that will help you increase your model\u2019s performance even more so that you can beat our performance and explore more advanced training schemes. https://github.com/IFL-CAMP/SLClassificationAnEducationalCode-MEC2019 # !pip install imageio # !pip install scikit-image import torch from torch import nn import torch.nn.functional as F import torchvision import torchvision.transforms as transforms import numpy as np import os import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix , precision_recall_fscore_support import scipy.ndimage from scipy import misc from glob import glob from scipy import stats from sklearn.preprocessing import LabelEncoder , StandardScaler import skimage import imageio import seaborn as sns from PIL import Image import glob import matplotlib.pyplot as plt import matplotlib % matplotlib inline device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) device --------------------------------------------------------------------------- ModuleNotFoundError Traceback (most recent call last) <ipython-input-3-5b38befe3f1d> in <module> 18 import skimage 19 import imageio ---> 20 import seaborn as sns 21 from PIL import Image 22 import glob ModuleNotFoundError: No module named 'seaborn' What about data? The HAM10000 (\"Human Against Machine with 10000 training images\") dataset which contains 10,015 dermatoscopic images was made publically available by the Harvard database on June 2018 in the hopes to provide training data for automating the process of skin cancer lesion classifications. The motivation behind this act was to provide the public with an abundance and variability of data source for machine learning training purposes such that the results may be compared with that of human experts. If successful, the appplications would bring cost and time saving regimes to hospitals and medical professions alike. Apart from the 10,015 images, a metadata file with demographic information of each lesion is provided as well. More than 50% of lesions are confirmed through histopathology (histo), the ground truth for the rest of the cases is either follow-up examination (follow_up), expert consensus (consensus), or confirmation by in-vivo confocal microscopy (confocal) You can download the dataset here: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T The 7 classes of skin cancer lesions included in this dataset are: Melanocytic nevi Melanoma Benign keratosis-like lesions Basal cell carcinoma Actinic keratoses Vascular lesions Dermatofibroma Let's analyze the metadata of the dataset # importing metadata and checking for its shape data_dir = \"/data/HAM10000\" metadata = pd . read_csv ( data_dir + '/HAM10000_metadata.csv' ) print ( metadata . shape ) # label encoding the seven classes for skin cancers le = LabelEncoder () le . fit ( metadata [ 'dx' ]) LabelEncoder () print ( \"Classes:\" , list ( le . classes_ )) metadata [ 'label' ] = le . transform ( metadata [ \"dx\" ]) metadata . sample ( 10 ) --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-2-3d00b7e51bd3> in <module> 2 data_dir = \"/data/HAM10000\" 3 ----> 4 metadata = pd.read_csv(data_dir + '/HAM10000_metadata.csv') 5 print(metadata.shape) 6 NameError: name 'pd' is not defined # Getting a sense of what the distribution of each column looks like fig = plt . figure ( figsize = ( 40 , 25 )) ax1 = fig . add_subplot ( 221 ) metadata [ 'dx' ] . value_counts () . plot ( kind = 'bar' , ax = ax1 ) ax1 . set_ylabel ( 'Count' , size = 50 ) ax1 . set_title ( 'Cell Type' , size = 50 ) ax2 = fig . add_subplot ( 222 ) metadata [ 'sex' ] . value_counts () . plot ( kind = 'bar' , ax = ax2 ) ax2 . set_ylabel ( 'Count' , size = 50 ) ax2 . set_title ( 'Sex' , size = 50 ); ax3 = fig . add_subplot ( 223 ) metadata [ 'localization' ] . value_counts () . plot ( kind = 'bar' ) ax3 . set_ylabel ( 'Count' , size = 50 ) ax3 . set_title ( 'Localization' , size = 50 ) ax4 = fig . add_subplot ( 224 ) sample_age = metadata [ pd . notnull ( metadata [ 'age' ])] sns . distplot ( sample_age [ 'age' ], fit = stats . norm , color = 'red' ); ax4 . set_title ( 'Age' , size = 50 ) ax4 . set_xlabel ( 'Year' , size = 50 ) plt . tight_layout () plt . show () As you can see there is imbalance in the number of images per class. There are much more images for the lesion type \"Melanocytic Nevi\" compared to other types. This is an usual occurence for medical datasets and so it is very important to analyze the data from beforehand. Let's visualize some examples #Visualizing the images label = [ 'akiec' , 'bcc' , 'bkl' , 'df' , 'mel' , 'nv' , 'vasc' ] label_images = [] classes = [ 'actinic keratoses' , 'basal cell carcinoma' , 'benign keratosis-like lesions' , 'dermatofibroma' , 'melanoma' , 'melanocytic nevi' , 'vascular lesions' ] fig = plt . figure ( figsize = ( 55 , 55 )) k = range ( 7 ) for i in label : sample = metadata [ metadata [ 'dx' ] == i ][ 'image_id' ][: 5 ] label_images . extend ( sample ) for position , ID in enumerate ( label_images ): labl = metadata [ metadata [ 'image_id' ] == ID ][ 'dx' ] im_sample = data_dir + \"/\" + labl . values [ 0 ] + f '/ { ID } .jpg' im_sample = imageio . imread ( im_sample ) plt . subplot ( 7 , 5 , position + 1 ) plt . imshow ( im_sample ) plt . axis ( 'off' ) if position % 5 == 0 : title = int ( position / 5 ) plt . title ( classes [ title ], loc = 'left' , size = 50 , weight = \"bold\" ) plt . tight_layout () plt . show () Median Frequency Balancing As we saw above that there is class imbalance in our dataset. To solve that we use this method. #print(metadata['dx'].value_counts()) #print(metadata[metadata['dx']=='nv']['dx'].value_counts()) label = [ 'akiec' , 'bcc' , 'bkl' , 'df' , 'mel' , 'nv' , 'vasc' ] def estimate_weights_mfb ( label ): class_weights = np . zeros_like ( label , dtype = np . float ) counts = np . zeros_like ( label ) for i , l in enumerate ( label ): counts [ i ] = metadata [ metadata [ 'dx' ] == str ( l )][ 'dx' ] . value_counts ()[ 0 ] counts = counts . astype ( np . float ) #print(counts) median_freq = np . median ( counts ) #print(median_freq) #print(weights.shape) for i , label in enumerate ( label ): #print(label) class_weights [ i ] = median_freq / counts [ i ] return class_weights classweight = estimate_weights_mfb ( label ) for i in range ( len ( label )): print ( label [ i ], \":\" , classweight [ i ]) akiec : 1.5718654434250765 bcc : 1.0 bkl : 0.467697907188353 df : 4.469565217391304 mel : 0.4618149146451033 nv : 0.07665920954511558 vasc : 3.619718309859155 Pre-processing the dataset Before we load the data we need to alter the dataset structure. When you download the dataset, all the images are together in a folder. To use Pytorch dataloader we need to seggregrate the images into folders of their respetive labels. You can use the following script to automate the process. # import os # import shutil # data_dir = os.getcwd() + \"/HAM10000/\" # dest_dir = data_dir + \"test/\" # metadata = pd.read_csv(data_dir + '/HAM10000_metadata.csv') # label = ['bkl', 'nv', 'df', 'mel', 'vasc', 'bcc', 'akiec'] # label_images = [] # for i in label: # os.mkdir(dest_dir + str(i) + \"/\") # sample = metadata[metadata['dx'] == i]['image_id'][:5] # label_images.extend(sample) # for id in label_images: # shutil.copyfile((data_dir + i + \"/\"+ id +\".jpg\"), (dest_dir + i + \"/\"+id+\".jpg\")) # label_images=[] Data Augmentation It is a common fact that medical data is scarce. But to learn a very good model, the network needs a lot of data. So to tackle the problem we perform data augmentation. First we normalize the images. Data normalization is an important step which ensures that each input parameter (pixel, in this case) has a similar data distribution. This makes convergence faster while training the network. Data normalization is done by subtracting the mean from each pixel and then dividing the result by the standard deviation. The distribution of such data would resemble a Gaussian curve centered at zero. Since, skin lesion images are natural images, we use the normalization values (mean and standard deviation) of Imagenet dataset. We also perform data augmentation: - Flipping the image horizontally: RandomHorizontalFlip() - Rotating image 60 degrees: RandomRotation() . 60 degrees is chosen as best practice. You can experiment with other angles. The augmentation is applied using the transform.Compose() function of Pytorch. Take note, we only augment the training set. This is because, augmentation is done to aid the training process. So there is no point in augmenting the test set. data_dir = \"/data/HAM10000\" # normalization values for pretrained resnet on Imagenet norm_mean = ( 0.4914 , 0.4822 , 0.4465 ) norm_std = ( 0.2023 , 0.1994 , 0.2010 ) batch_size = 50 validation_batch_size = 10 test_batch_size = 10 # We compute the weights of individual classes and convert them to tensors class_weights = estimate_weights_mfb ( label ) class_weights = torch . FloatTensor ( class_weights ) transform_train = transforms . Compose ([ transforms . Resize (( 224 , 224 )), transforms . RandomHorizontalFlip (), transforms . RandomRotation ( degrees = 60 ), transforms . ToTensor (), transforms . Normalize ( norm_mean , norm_std ), ]) transform_test = transforms . Compose ([ transforms . Resize (( 224 , 224 )), transforms . ToTensor (), transforms . Normalize (( 0.4914 , 0.4822 , 0.4465 ), ( 0.2023 , 0.1994 , 0.2010 )), ]) Train, Test and Validation Split We split the entire dataset into 3 parts: - Train: 80% - Test: 20% - Validation: 16% The splitting is done class wise so that we have equal representation of all classes in each subset of the data. import torch as th import math test_size = 0.2 val_size = 0.2 class Sampler ( object ): \"\"\"Base class for all Samplers. \"\"\" def __init__ ( self , data_source ): pass def __iter__ ( self ): raise NotImplementedError def __len__ ( self ): raise NotImplementedError class StratifiedSampler ( Sampler ): \"\"\"Stratified Sampling Provides equal representation of target classes \"\"\" def __init__ ( self , class_vector ): \"\"\" Arguments --------- class_vector : torch tensor a vector of class labels batch_size : integer batch_size \"\"\" self . n_splits = 1 self . class_vector = class_vector self . test_size = test_size def gen_sample_array ( self ): try : from sklearn.model_selection import StratifiedShuffleSplit except : print ( 'Need scikit-learn for this functionality' ) import numpy as np s = StratifiedShuffleSplit ( n_splits = self . n_splits , test_size = self . test_size ) X = th . randn ( self . class_vector . size ( 0 ), 2 ) . numpy () y = self . class_vector . numpy () s . get_n_splits ( X , y ) train_index , test_index = next ( s . split ( X , y )) return train_index , test_index def __iter__ ( self ): return iter ( self . gen_sample_array ()) def __len__ ( self ): return len ( self . class_vector ) dataset = torchvision . datasets . ImageFolder ( root = data_dir ) data_label = [ s [ 1 ] for s in dataset . samples ] ss = StratifiedSampler ( torch . FloatTensor ( data_label ), test_size ) pre_train_indices , test_indices = ss . gen_sample_array () # The \"pre\" is necessary to use array to identify train/ val indices with indices generated by second sampler train_label = np . delete ( data_label , test_indices , None ) ss = StratifiedSampler ( torch . FloatTensor ( train_label ), test_size ) train_indices , val_indices = ss . gen_sample_array () indices = { 'train' : pre_train_indices [ train_indices ], # Indices of second sampler are used on pre_train_indices 'val' : pre_train_indices [ val_indices ], # Indices of second sampler are used on pre_train_indices 'test' : test_indices } train_indices = indices [ 'train' ] val_indices = indices [ 'val' ] test_indices = indices [ 'test' ] print ( \"Train Data Size:\" , len ( train_indices )) print ( \"Test Data Size:\" , len ( test_indices )) print ( \"Validation Data Size:\" , len ( val_indices )) Train Data Size: 6409 Test Data Size: 2003 Validation Data Size: 1603 Now we use Pytorch data loader to load the dataset into the memory. SubsetRandomSampler = torch . utils . data . sampler . SubsetRandomSampler dataset = torchvision . datasets . ImageFolder ( root = data_dir , transform = transform_train ) train_samples = SubsetRandomSampler ( train_indices ) val_samples = SubsetRandomSampler ( val_indices ) test_samples = SubsetRandomSampler ( test_indices ) train_data_loader = torch . utils . data . DataLoader ( dataset , batch_size = batch_size , shuffle = False , num_workers = 1 , sampler = train_samples ) validation_data_loader = torch . utils . data . DataLoader ( dataset , batch_size = validation_batch_size , shuffle = False , sampler = val_samples ) dataset = torchvision . datasets . ImageFolder ( root = data_dir , transform = transform_test ) test_data_loader = torch . utils . data . DataLoader ( dataset , batch_size = test_batch_size , shuffle = False , sampler = test_samples ) Let us see some of the training images. # functions to show an image fig = plt . figure ( figsize = ( 10 , 15 )) def imshow ( img ): img = img / 2 + 0.5 # denormalize change this npimg = img . numpy () plt . imshow ( np . transpose ( npimg , ( 1 , 2 , 0 ))) # get some random training images dataiter = iter ( train_data_loader ) images , labels = dataiter . next () # show images imshow ( torchvision . utils . make_grid ( images )) # print labels print ( ' ' . join ( ' %5s , ' % classes [ labels [ j ]] for j in range ( len ( labels )))) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). melanocytic nevi, actinic keratoses, benign keratosis-like lesions, benign keratosis-like lesions, melanocytic nevi, melanocytic nevi, basal cell carcinoma, melanocytic nevi, melanocytic nevi, benign keratosis-like lesions, melanocytic nevi, melanocytic nevi, melanocytic nevi, melanocytic nevi, melanocytic nevi, melanocytic nevi, melanocytic nevi, melanocytic nevi, melanocytic nevi, melanocytic nevi, benign keratosis-like lesions, melanocytic nevi, melanoma, melanocytic nevi, melanocytic nevi, melanoma, melanoma, melanocytic nevi, actinic keratoses, melanocytic nevi, actinic keratoses, melanocytic nevi, melanocytic nevi, melanoma, melanocytic nevi, melanoma, melanoma, melanocytic nevi, melanoma, melanocytic nevi, melanocytic nevi, melanocytic nevi, melanocytic nevi, melanoma, melanocytic nevi, melanocytic nevi, melanocytic nevi, dermatofibroma, basal cell carcinoma, basal cell carcinoma, Define a Convolutional Neural Network Pytorch makes it very easy to define a neural network. We have layers like Convolutions, ReLU non-linearity, Maxpooling etc. directly from torch library. In this tutorial, we use The LeNet architecture introduced by LeCun et al. in their 1998 paper, Gradient-Based Learning Applied to Document Recognition . As the name of the paper suggests, the authors\u2019 implementation of LeNet was used primarily for OCR and character recognition in documents. The LeNet architecture is straightforward and small, (in terms of memory footprint), making it perfect for teaching the basics of CNNs. num_classes = len ( classes ) class LeNet ( nn . Module ): def __init__ ( self ): super ( LeNet , self ) . __init__ () self . conv1 = nn . Conv2d ( 3 , 6 , ( 5 , 5 ), padding = 2 ) self . conv2 = nn . Conv2d ( 6 , 16 , ( 5 , 5 )) self . fc1 = nn . Linear ( 16 * 54 * 54 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , num_classes ) def forward ( self , x ): x = F . max_pool2d ( F . relu ( self . conv1 ( x )), ( 2 , 2 )) x = F . max_pool2d ( F . relu ( self . conv2 ( x )), ( 2 , 2 )) x = x . view ( - 1 , self . num_flat_features ( x )) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x def num_flat_features ( self , x ): size = x . size ()[ 1 :] num_features = 1 for s in size : num_features *= s return num_features net = LeNet () net = net . to ( device ) Define a Loss function and Optimizer Let's use a Classification Cross-Entropy loss. \\(H_{y'} (y) := - \\sum_{i} y_{i}' \\log (y_i)\\) The most common and effective Optimizer currently used is Adam: Adaptive Moments . You can look here for more information. import torch.optim as optim class_weights = class_weights . to ( device ) criterion = nn . CrossEntropyLoss ( weight = class_weights ) optimizer = optim . Adam ( net . parameters (), lr = 1e-5 ) print ( net ) LeNet( (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear(in_features=46656, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=7, bias=True) ) These are some helper functions to evaluate the training process. from sklearn.metrics import accuracy_score def get_accuracy ( predicted , labels ): batch_len , correct = 0 , 0 batch_len = labels . size ( 0 ) correct = ( predicted == labels ) . sum () . item () return batch_len , correct def evaluate ( model , val_loader ): losses = 0 num_samples_total = 0 correct_total = 0 model . eval () for inputs , labels in val_loader : inputs , labels = inputs . to ( device ), labels . to ( device ) out = model ( inputs ) _ , predicted = torch . max ( out , 1 ) loss = criterion ( out , labels ) losses += loss . item () b_len , corr = get_accuracy ( predicted , labels ) num_samples_total += b_len correct_total += corr accuracy = correct_total / num_samples_total losses = losses / len ( val_loader ) return losses , accuracy Train the network This is when things start to get interesting. We simply loop over the training data iterator, and feed the inputs to the network and optimize. # number of loops over the dataset num_epochs = 50 accuracy = [] val_accuracy = [] losses = [] val_losses = [] for epoch in range ( num_epochs ): running_loss = 0.0 correct_total = 0.0 num_samples_total = 0.0 for i , data in enumerate ( train_data_loader ): # get the inputs inputs , labels = data inputs , labels = inputs . to ( device ), labels . to ( device ) # set the parameter gradients to zero optimizer . zero_grad () # forward + backward + optimize outputs = net ( inputs ) loss = criterion ( outputs , labels ) loss . backward () optimizer . step () #compute accuracy _ , predicted = torch . max ( outputs , 1 ) b_len , corr = get_accuracy ( predicted , labels ) num_samples_total += b_len correct_total += corr running_loss += loss . item () running_loss /= len ( train_data_loader ) train_accuracy = correct_total / num_samples_total val_loss , val_acc = evaluate ( net , validation_data_loader ) print ( 'Epoch: %d ' % ( epoch + 1 )) print ( 'Loss: %.3f Accuracy: %.3f ' % ( running_loss , train_accuracy )) print ( 'Validation Loss: %.3f Val Accuracy: %.3f ' % ( val_loss , val_acc )) losses . append ( running_loss ) val_losses . append ( val_loss ) accuracy . append ( train_accuracy ) val_accuracy . append ( val_acc ) print ( 'Finished Training' ) Epoch: 1 Loss: 1.806 Accuracy:0.314 Validation Loss: 1.727 Val Accuracy: 0.403 Epoch: 2 Loss: 1.663 Accuracy:0.371 Validation Loss: 1.631 Val Accuracy: 0.419 Epoch: 3 Loss: 1.589 Accuracy:0.459 Validation Loss: 1.565 Val Accuracy: 0.451 Epoch: 4 Loss: 1.542 Accuracy:0.486 Validation Loss: 1.558 Val Accuracy: 0.431 Epoch: 5 Loss: 1.499 Accuracy:0.494 Validation Loss: 1.483 Val Accuracy: 0.500 Epoch: 6 Loss: 1.472 Accuracy:0.500 Validation Loss: 1.457 Val Accuracy: 0.525 Epoch: 7 Loss: 1.447 Accuracy:0.509 Validation Loss: 1.454 Val Accuracy: 0.498 Epoch: 9 Loss: 1.421 Accuracy:0.503 Validation Loss: 1.406 Val Accuracy: 0.527 Epoch: 10 Loss: 1.399 Accuracy:0.516 Validation Loss: 1.413 Val Accuracy: 0.530 Epoch: 11 Loss: 1.390 Accuracy:0.519 Validation Loss: 1.436 Val Accuracy: 0.496 Epoch: 12 Loss: 1.384 Accuracy:0.519 Validation Loss: 1.396 Val Accuracy: 0.505 Epoch: 13 Loss: 1.373 Accuracy:0.529 Validation Loss: 1.391 Val Accuracy: 0.525 Epoch: 14 Loss: 1.363 Accuracy:0.534 Validation Loss: 1.376 Val Accuracy: 0.518 Epoch: 15 Loss: 1.364 Accuracy:0.531 Validation Loss: 1.365 Val Accuracy: 0.536 Epoch: 16 Loss: 1.352 Accuracy:0.534 Validation Loss: 1.363 Val Accuracy: 0.548 Epoch: 17 Loss: 1.337 Accuracy:0.536 Validation Loss: 1.365 Val Accuracy: 0.532 Epoch: 18 Loss: 1.333 Accuracy:0.540 Validation Loss: 1.340 Val Accuracy: 0.527 Epoch: 19 Loss: 1.327 Accuracy:0.542 Validation Loss: 1.333 Val Accuracy: 0.530 Epoch: 20 Loss: 1.320 Accuracy:0.537 Validation Loss: 1.304 Val Accuracy: 0.558 Epoch: 21 Loss: 1.303 Accuracy:0.555 Validation Loss: 1.373 Val Accuracy: 0.490 Epoch: 23 Loss: 1.293 Accuracy:0.550 Validation Loss: 1.348 Val Accuracy: 0.530 Epoch: 24 Loss: 1.308 Accuracy:0.554 Validation Loss: 1.344 Val Accuracy: 0.558 Epoch: 25 Loss: 1.288 Accuracy:0.559 Validation Loss: 1.329 Val Accuracy: 0.552 Epoch: 26 Loss: 1.288 Accuracy:0.558 Validation Loss: 1.321 Val Accuracy: 0.533 Epoch: 27 Loss: 1.295 Accuracy:0.560 Validation Loss: 1.310 Val Accuracy: 0.558 Epoch: 28 Loss: 1.290 Accuracy:0.562 Validation Loss: 1.303 Val Accuracy: 0.534 Epoch: 29 Loss: 1.278 Accuracy:0.560 Validation Loss: 1.315 Val Accuracy: 0.522 Epoch: 30 Loss: 1.280 Accuracy:0.559 Validation Loss: 1.318 Val Accuracy: 0.540 Epoch: 31 Loss: 1.266 Accuracy:0.561 Validation Loss: 1.284 Val Accuracy: 0.540 Epoch: 32 Loss: 1.275 Accuracy:0.565 Validation Loss: 1.289 Val Accuracy: 0.562 Epoch: 33 Loss: 1.259 Accuracy:0.575 Validation Loss: 1.284 Val Accuracy: 0.585 Epoch: 34 Loss: 1.268 Accuracy:0.561 Validation Loss: 1.313 Val Accuracy: 0.530 Epoch: 35 Loss: 1.257 Accuracy:0.574 Validation Loss: 1.328 Val Accuracy: 0.520 Epoch: 36 Loss: 1.254 Accuracy:0.569 Validation Loss: 1.288 Val Accuracy: 0.555 Epoch: 37 Loss: 1.259 Accuracy:0.574 Validation Loss: 1.289 Val Accuracy: 0.566 Epoch: 38 Loss: 1.251 Accuracy:0.576 Validation Loss: 1.319 Val Accuracy: 0.513 Epoch: 39 Loss: 1.248 Accuracy:0.572 Validation Loss: 1.276 Val Accuracy: 0.571 Epoch: 40 Loss: 1.253 Accuracy:0.582 Validation Loss: 1.300 Val Accuracy: 0.533 Epoch: 41 Loss: 1.236 Accuracy:0.570 Validation Loss: 1.288 Val Accuracy: 0.563 Epoch: 42 Loss: 1.246 Accuracy:0.572 Validation Loss: 1.263 Val Accuracy: 0.564 Epoch: 43 Loss: 1.240 Accuracy:0.581 Validation Loss: 1.332 Val Accuracy: 0.520 Epoch: 44 Loss: 1.253 Accuracy:0.573 Validation Loss: 1.311 Val Accuracy: 0.528 Epoch: 45 Loss: 1.229 Accuracy:0.579 Validation Loss: 1.266 Val Accuracy: 0.558 Epoch: 46 Loss: 1.221 Accuracy:0.585 Validation Loss: 1.264 Val Accuracy: 0.553 Epoch: 47 Loss: 1.232 Accuracy:0.586 Validation Loss: 1.258 Val Accuracy: 0.561 Epoch: 48 Loss: 1.236 Accuracy:0.582 Validation Loss: 1.255 Val Accuracy: 0.588 Epoch: 49 Loss: 1.224 Accuracy:0.583 Validation Loss: 1.258 Val Accuracy: 0.581 Epoch: 50 Loss: 1.234 Accuracy:0.582 Validation Loss: 1.261 Val Accuracy: 0.560 Finished Training Plot the training and validation loss curves. # plt.plot(losses) # plt.show() epoch = range ( 1 , num_epochs + 1 ) fig = plt . figure ( figsize = ( 10 , 15 )) plt . subplot ( 2 , 1 , 2 ) plt . plot ( epoch , losses , label = 'Training loss' ) plt . plot ( epoch , val_losses , label = 'Validation loss' ) plt . title ( 'Training and Validation Loss' ) plt . xlabel ( 'Epochs' ) plt . legend () plt . figure () plt . show () fig = plt . figure ( figsize = ( 10 , 15 )) plt . subplot ( 2 , 1 , 2 ) plt . plot ( epoch , accuracy , label = 'Training accuracy' ) plt . plot ( epoch , val_accuracy , label = 'Validation accuracy' ) plt . title ( 'Training and Validation Accuracy' ) plt . xlabel ( 'Epochs' ) plt . legend () plt . figure () plt . show () <Figure size 432x288 with 0 Axes> <Figure size 432x288 with 0 Axes> Test the network on the test data We have trained the network over the training dataset. But we need to check if the network has learnt anything at all. We will check this by predicting the class label that the neural network outputs, and checking it against the ground-truth. If the prediction is correct, we add the sample to the list of correct predictions. Okay, first step. Let us display an image from the test set to get familiar. fig = plt . figure ( figsize = ( 10 , 15 )) dataiter = iter ( test_data_loader ) images , labels = dataiter . next () # print images imshow ( torchvision . utils . make_grid ( images )) print ( 'GroundTruth: ' , ' ' . join ( ' %5s , ' % classes [ labels [ j ]] for j in range ( len ( labels )))) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). GroundTruth: melanocytic nevi, benign keratosis-like lesions, melanocytic nevi, melanoma, melanocytic nevi, benign keratosis-like lesions, melanocytic nevi, melanoma, benign keratosis-like lesions, melanocytic nevi, Okay, now let us check the performance on the test network: correct = 0 total = 0 net . eval () with torch . no_grad (): for data in test_data_loader : images , labels = data images , labels = images . to ( device ), labels . to ( device ) outputs = net ( images ) _ , predicted = torch . max ( outputs . data , 1 ) total += labels . size ( 0 ) correct += ( predicted == labels ) . sum () . item () print ( 'Accuracy of the network on the test images: %d %% ' % ( 100 * correct / total )) Accuracy of the network on the test images: 52 % That looks better than chance, which is about 14% accuracy (randomly picking a class out of 7 classes). Seems like the network learnt something. But maybe it doesn't learn all the classes equally. Let's check which classes that performed well, and which did not. class_correct = list ( 0. for i in range ( len ( classes ))) class_total = list ( 1e-7 for i in range ( len ( classes ))) with torch . no_grad (): for data in test_data_loader : images , labels = data images , labels = images . to ( device ), labels . to ( device ) outputs = net ( images ) _ , predicted = torch . max ( outputs , 1 ) c = ( predicted == labels ) . squeeze () for i in range ( 3 ): label = labels [ i ] class_correct [ label ] += c [ i ] . item () class_total [ label ] += 1 for i in range ( len ( classes )): print ( 'Accuracy of %5s : %2d %% ' % ( classes [ i ], 100 * class_correct [ i ] / class_total [ i ])) Accuracy of actinic keratoses : 27 % Accuracy of basal cell carcinoma : 76 % Accuracy of benign keratosis-like lesions : 4 % Accuracy of dermatofibroma : 39 % Accuracy of melanoma : 73 % Accuracy of melanocytic nevi : 58 % Accuracy of vascular lesions : 0 % Confusion Matrix confusion_matrix = torch . zeros ( len ( classes ), len ( classes )) with torch . no_grad (): for data in test_data_loader : images , labels = data images , labels = images . to ( device ), labels . to ( device ) outputs = net ( images ) _ , predicted = torch . max ( outputs , 1 ) for t , p in zip ( labels . view ( - 1 ), predicted . view ( - 1 )): confusion_matrix [ t . long (), p . long ()] += 1 cm = confusion_matrix . numpy () fig , ax = plt . subplots ( figsize = ( 7 , 7 )) sns . heatmap ( cm / ( cm . astype ( np . float ) . sum ( axis = 1 ) + 1e-9 ), annot = False , ax = ax ) # labels, title and ticks ax . set_xlabel ( 'Predicted' , size = 25 ); ax . set_ylabel ( 'True' , size = 25 ); ax . set_title ( 'Confusion Matrix' , size = 25 ); ax . xaxis . set_ticklabels ([ 'akiec' , 'bcc' , 'bkl' , 'df' , 'mel' , 'nv' , 'vasc' ], size = 15 ); \\ ax . yaxis . set_ticklabels ([ 'akiec' , 'bcc' , 'bkl' , 'df' , 'mel' , 'nv' , 'vasc' ], size = 15 ); Grad cam from collections import OrderedDict , Sequence class _BaseWrapper ( object ): \"\"\" Please modify forward() and backward() according to your task. \"\"\" def __init__ ( self , model ): super ( _BaseWrapper , self ) . __init__ () self . device = next ( model . parameters ()) . device self . model = model self . handlers = [] # a set of hook function handlers def _encode_one_hot ( self , ids ): one_hot = torch . zeros_like ( self . logits ) . to ( self . device ) one_hot . scatter_ ( 1 , ids , 1.0 ) return one_hot def forward ( self , image ): \"\"\" Simple classification \"\"\" self . model . zero_grad () self . logits = self . model ( image ) self . probs = F . softmax ( self . logits , dim = 1 ) return self . probs . sort ( dim = 1 , descending = True ) def backward ( self , ids ): \"\"\" Class-specific backpropagation Either way works: 1. self.logits.backward(gradient=one_hot, retain_graph=True) 2. (self.logits * one_hot).sum().backward(retain_graph=True) \"\"\" one_hot = self . _encode_one_hot ( ids ) self . logits . backward ( gradient = one_hot , retain_graph = True ) def generate ( self ): raise NotImplementedError def remove_hook ( self ): \"\"\" Remove all the forward/backward hook functions \"\"\" for handle in self . handlers : handle . remove () class GradCAM ( _BaseWrapper ): \"\"\" \"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\" https://arxiv.org/pdf/1610.02391.pdf Look at Figure 2 on page 4 \"\"\" def __init__ ( self , model , candidate_layers = None ): super ( GradCAM , self ) . __init__ ( model ) self . fmap_pool = OrderedDict () self . grad_pool = OrderedDict () self . candidate_layers = candidate_layers # list def forward_hook ( key ): def forward_hook_ ( module , input , output ): # Save featuremaps self . fmap_pool [ key ] = output . detach () return forward_hook_ def backward_hook ( key ): def backward_hook_ ( module , grad_in , grad_out ): # Save the gradients correspond to the featuremaps self . grad_pool [ key ] = grad_out [ 0 ] . detach () return backward_hook_ # If any candidates are not specified, the hook is registered to all the layers. for name , module in self . model . named_modules (): if self . candidate_layers is None or name in self . candidate_layers : self . handlers . append ( module . register_forward_hook ( forward_hook ( name ))) self . handlers . append ( module . register_backward_hook ( backward_hook ( name ))) def _find ( self , pool , target_layer ): if target_layer in pool . keys (): return pool [ target_layer ] else : raise ValueError ( \"Invalid layer name: {} \" . format ( target_layer )) def _compute_grad_weights ( self , grads ): return F . adaptive_avg_pool2d ( grads , 1 ) def forward ( self , image ): self . image_shape = image . shape [ 2 :] return super ( GradCAM , self ) . forward ( image ) def generate ( self , target_layer ): fmaps = self . _find ( self . fmap_pool , target_layer ) grads = self . _find ( self . grad_pool , target_layer ) weights = self . _compute_grad_weights ( grads ) gcam = torch . mul ( fmaps , weights ) . sum ( dim = 1 , keepdim = True ) gcam = F . relu ( gcam ) gcam = F . interpolate ( gcam , self . image_shape , mode = \"bilinear\" , align_corners = False ) B , C , H , W = gcam . shape gcam = gcam . view ( B , - 1 ) gcam -= gcam . min ( dim = 1 , keepdim = True )[ 0 ] gcam /= gcam . max ( dim = 1 , keepdim = True )[ 0 ] gcam = gcam . view ( B , C , H , W ) return gcam def demo2 ( image , label , model ): \"\"\" Generate Grad-CAM \"\"\" # Model model = model model . to ( device ) model . eval () # The layers target_layers = [ \"conv2\" ] target_class = label # Images images = image . unsqueeze ( 0 ) gcam = GradCAM ( model = model ) probs , ids = gcam . forward ( images ) ids_ = torch . LongTensor ([[ target_class ]] * len ( images )) . to ( device ) gcam . backward ( ids = ids_ ) for target_layer in target_layers : print ( \"Generating Grad-CAM @ {} \" . format ( target_layer )) # Grad-CAM regions = gcam . generate ( target_layer = target_layer ) for j in range ( len ( images )): print ( \" \\t # {} : {} ( {:.5f} )\" . format ( j , classes [ target_class ], float ( probs [ ids == target_class ]) ) ) gcam = regions [ j , 0 ] plt . imshow ( gcam . cpu ()) plt . show () image , label = next ( iter ( test_data_loader )) # Load the model model = net # Grad cam demo2 ( image [ 0 ] . to ( device ), label [ 0 ] . to ( device ), model ) image = np . transpose ( image [ 0 ], ( 1 , 2 , 0 )) image2 = np . add ( np . multiply ( image . numpy (), np . array ( norm_std )) , np . array ( norm_mean )) print ( \"True Class: \" , classes [ label [ 0 ] . cpu ()]) plt . imshow ( image ) plt . show () plt . imshow ( image2 ) plt . show () Generating Grad-CAM @conv2 #0: melanocytic nevi (0.12183) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). True Class: melanocytic nevi Analysis of the results As we can see from the results of the LeNet model, our system is not capable of processing the complexity of the given input images. Our final accuracy on the test data was 61%. About 39% of the images are missclassified, which is a terrible performance for any clinical use case. These results could be substantially improved if we opt for a deeper, more complex network architecture than LeNet, which will allow for a richer learning of the corresponding image features. Switching to superior network architecture:Resnet18 from torch import nn num_classes = len ( classes ) net = torchvision . models . resnet18 ( pretrained = True ) # We replace last layer of resnet to match our number of classes which is 7 net . fc = nn . Linear ( 512 , num_classes ) net = net . to ( device ) Define a Loss function and Optimizer Let's use a Classification Cross-Entropy loss. \\(H_{y'} (y) := - \\sum_{i} y_{i}' \\log (y_i)\\) The most common and effective Optimizer currently used is Adam: Adaptive Moments . You can look here for more information. import torch.optim as optim class_weights = class_weights . to ( device ) criterion = nn . CrossEntropyLoss ( weight = class_weights ) optimizer = optim . Adam ( net . parameters (), lr = 1e-5 ) print ( net ) ResNet( (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (layer1): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer2): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer3): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer4): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (avgpool): AdaptiveAvgPool2d(output_size=(1, 1)) (fc): Linear(in_features=512, out_features=7, bias=True) ) These are some helper functions to evaluate the training process. from sklearn.metrics import accuracy_score def get_accuracy ( predicted , labels ): batch_len , correct = 0 , 0 batch_len = labels . size ( 0 ) correct = ( predicted == labels ) . sum () . item () return batch_len , correct def evaluate ( model , val_loader ): losses = 0 num_samples_total = 0 correct_total = 0 model . eval () for inputs , labels in val_loader : inputs , labels = inputs . to ( device ), labels . to ( device ) out = model ( inputs ) _ , predicted = torch . max ( out , 1 ) loss = criterion ( out , labels ) losses += loss . item () b_len , corr = get_accuracy ( predicted , labels ) num_samples_total += b_len correct_total += corr accuracy = correct_total / num_samples_total losses = losses / len ( val_loader ) return losses , accuracy Train the network This is when things start to get interesting. We simply loop over the training data iterator, and feed the inputs to the network and optimize. # number of loops over the dataset num_epochs = 70 accuracy = [] val_accuracy = [] losses = [] val_losses = [] for epoch in range ( num_epochs ): running_loss = 0.0 correct_total = 0.0 num_samples_total = 0.0 for i , data in enumerate ( train_data_loader ): # get the inputs inputs , labels = data inputs , labels = inputs . to ( device ), labels . to ( device ) # set the parameter gradients to zero optimizer . zero_grad () # forward + backward + optimize outputs = net ( inputs ) loss = criterion ( outputs , labels ) loss . backward () optimizer . step () #compute accuracy _ , predicted = torch . max ( outputs , 1 ) b_len , corr = get_accuracy ( predicted , labels ) num_samples_total += b_len correct_total += corr running_loss += loss . item () running_loss /= len ( train_data_loader ) train_accuracy = correct_total / num_samples_total val_loss , val_acc = evaluate ( net , validation_data_loader ) print ( 'Epoch: %d ' % ( epoch + 1 )) print ( 'Loss: %.3f Accuracy: %.3f ' % ( running_loss , train_accuracy )) print ( 'Validation Loss: %.3f Val Accuracy: %.3f ' % ( val_loss , val_acc )) losses . append ( running_loss ) val_losses . append ( val_loss ) accuracy . append ( train_accuracy ) val_accuracy . append ( val_acc ) print ( 'Finished Training' ) Epoch: 1 Loss: 1.918 Accuracy:0.098 Validation Loss: 1.588 Val Accuracy: 0.243 Epoch: 2 Loss: 1.125 Accuracy:0.621 Validation Loss: 1.040 Val Accuracy: 0.648 Epoch: 3 Loss: 0.937 Accuracy:0.678 Validation Loss: 0.892 Val Accuracy: 0.674 Epoch: 5 Loss: 0.759 Accuracy:0.724 Validation Loss: 0.929 Val Accuracy: 0.651 Epoch: 6 Loss: 0.690 Accuracy:0.730 Validation Loss: 0.838 Val Accuracy: 0.718 Epoch: 7 Loss: 0.662 Accuracy:0.737 Validation Loss: 0.816 Val Accuracy: 0.709 Epoch: 8 Loss: 0.590 Accuracy:0.763 Validation Loss: 0.819 Val Accuracy: 0.705 Epoch: 9 Loss: 0.564 Accuracy:0.767 Validation Loss: 0.848 Val Accuracy: 0.726 Epoch: 10 Loss: 0.546 Accuracy:0.770 Validation Loss: 0.851 Val Accuracy: 0.719 Epoch: 11 Loss: 0.499 Accuracy:0.778 Validation Loss: 0.766 Val Accuracy: 0.749 Epoch: 12 Loss: 0.499 Accuracy:0.784 Validation Loss: 0.734 Val Accuracy: 0.751 Epoch: 13 Loss: 0.462 Accuracy:0.801 Validation Loss: 0.802 Val Accuracy: 0.757 Epoch: 14 Loss: 0.454 Accuracy:0.805 Validation Loss: 0.757 Val Accuracy: 0.744 Epoch: 15 Loss: 0.414 Accuracy:0.808 Validation Loss: 0.722 Val Accuracy: 0.769 Epoch: 16 Loss: 0.433 Accuracy:0.802 Validation Loss: 0.829 Val Accuracy: 0.787 Epoch: 17 Loss: 0.399 Accuracy:0.813 Validation Loss: 0.791 Val Accuracy: 0.768 Epoch: 18 Loss: 0.372 Accuracy:0.825 Validation Loss: 0.660 Val Accuracy: 0.791 Epoch: 19 Loss: 0.354 Accuracy:0.822 Validation Loss: 0.794 Val Accuracy: 0.784 Epoch: 20 Loss: 0.355 Accuracy:0.829 Validation Loss: 0.789 Val Accuracy: 0.750 Epoch: 21 Loss: 0.318 Accuracy:0.841 Validation Loss: 0.765 Val Accuracy: 0.800 Epoch: 22 Loss: 0.284 Accuracy:0.855 Validation Loss: 0.801 Val Accuracy: 0.777 Epoch: 23 Loss: 0.311 Accuracy:0.839 Validation Loss: 0.849 Val Accuracy: 0.815 Epoch: 24 Loss: 0.277 Accuracy:0.851 Validation Loss: 0.839 Val Accuracy: 0.792 Epoch: 25 Loss: 0.284 Accuracy:0.852 Validation Loss: 0.772 Val Accuracy: 0.770 Epoch: 26 Loss: 0.261 Accuracy:0.863 Validation Loss: 0.770 Val Accuracy: 0.802 Epoch: 27 Loss: 0.244 Accuracy:0.863 Validation Loss: 0.782 Val Accuracy: 0.782 Epoch: 28 Loss: 0.239 Accuracy:0.862 Validation Loss: 0.731 Val Accuracy: 0.805 Epoch: 29 Loss: 0.234 Accuracy:0.870 Validation Loss: 0.786 Val Accuracy: 0.816 Epoch: 30 Loss: 0.248 Accuracy:0.860 Validation Loss: 0.739 Val Accuracy: 0.803 Epoch: 31 Loss: 0.239 Accuracy:0.868 Validation Loss: 0.908 Val Accuracy: 0.779 Epoch: 33 Loss: 0.188 Accuracy:0.884 Validation Loss: 0.771 Val Accuracy: 0.780 Epoch: 34 Loss: 0.180 Accuracy:0.893 Validation Loss: 0.868 Val Accuracy: 0.803 Epoch: 35 Loss: 0.174 Accuracy:0.895 Validation Loss: 0.825 Val Accuracy: 0.800 Epoch: 36 Loss: 0.172 Accuracy:0.893 Validation Loss: 0.893 Val Accuracy: 0.823 Epoch: 37 Loss: 0.180 Accuracy:0.895 Validation Loss: 0.774 Val Accuracy: 0.778 Epoch: 38 Loss: 0.164 Accuracy:0.903 Validation Loss: 0.768 Val Accuracy: 0.776 Epoch: 39 Loss: 0.175 Accuracy:0.897 Validation Loss: 1.080 Val Accuracy: 0.784 Epoch: 40 Loss: 0.150 Accuracy:0.906 Validation Loss: 0.905 Val Accuracy: 0.826 Epoch: 41 Loss: 0.128 Accuracy:0.917 Validation Loss: 0.973 Val Accuracy: 0.809 Epoch: 42 Loss: 0.142 Accuracy:0.912 Validation Loss: 0.944 Val Accuracy: 0.834 Epoch: 43 Loss: 0.127 Accuracy:0.912 Validation Loss: 0.825 Val Accuracy: 0.819 Epoch: 44 Loss: 0.138 Accuracy:0.911 Validation Loss: 0.752 Val Accuracy: 0.810 Plot the training and validation loss curves. # plt.plot(losses) # plt.show() epoch = range ( 1 , num_epochs + 1 ) fig = plt . figure ( figsize = ( 10 , 15 )) plt . subplot ( 2 , 1 , 2 ) plt . plot ( epoch , losses , label = 'Training loss' ) plt . plot ( epoch , val_losses , label = 'Validation loss' ) plt . title ( 'Training and Validation Loss' ) plt . xlabel ( 'Epochs' ) plt . legend () plt . figure () plt . show () fig = plt . figure ( figsize = ( 10 , 15 )) plt . subplot ( 2 , 1 , 2 ) plt . plot ( epoch , accuracy , label = 'Training accuracy' ) plt . plot ( epoch , val_accuracy , label = 'Validation accuracy' ) plt . title ( 'Training and Validation Accuracy' ) plt . xlabel ( 'Epochs' ) plt . legend () plt . figure () plt . show () <Figure size 432x288 with 0 Axes> <Figure size 432x288 with 0 Axes> Test the network on the test data We have trained the network over the training dataset. But we need to check if the network has learnt anything at all. We will check this by predicting the class label that the neural network outputs, and checking it against the ground-truth. If the prediction is correct, we add the sample to the list of correct predictions. Okay, first step. Let us display an image from the test set to get familiar. fig = plt . figure ( figsize = ( 10 , 15 )) dataiter = iter ( test_data_loader ) images , labels = dataiter . next () # print images imshow ( torchvision . utils . make_grid ( images )) print ( 'GroundTruth: ' , ' ' . join ( ' %5s , ' % classes [ labels [ j ]] for j in range ( len ( labels )))) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). GroundTruth: melanocytic nevi, melanoma, melanocytic nevi, melanoma, melanoma, melanocytic nevi, melanocytic nevi, benign keratosis-like lesions, basal cell carcinoma, melanocytic nevi, Okay, now let us check the performance on the test network: correct = 0 total = 0 net . eval () with torch . no_grad (): for data in test_data_loader : images , labels = data images , labels = images . to ( device ), labels . to ( device ) outputs = net ( images ) _ , predicted = torch . max ( outputs . data , 1 ) total += labels . size ( 0 ) correct += ( predicted == labels ) . sum () . item () print ( 'Accuracy of the network on the test images: %d %% ' % ( 100 * correct / total )) Accuracy of the network on the test images: 85 % class_correct = list ( 0. for i in range ( len ( classes ))) class_total = list ( 1e-7 for i in range ( len ( classes ))) with torch . no_grad (): for data in test_data_loader : images , labels = data images , labels = images . to ( device ), labels . to ( device ) outputs = net ( images ) _ , predicted = torch . max ( outputs , 1 ) c = ( predicted == labels ) . squeeze () for i in range ( 3 ): label = labels [ i ] class_correct [ label ] += c [ i ] . item () class_total [ label ] += 1 for i in range ( len ( classes )): print ( 'Accuracy of %5s : %2d %% ' % ( classes [ i ], 100 * class_correct [ i ] / class_total [ i ])) Accuracy of actinic keratoses : 92 % Accuracy of basal cell carcinoma : 96 % Accuracy of benign keratosis-like lesions : 92 % Accuracy of dermatofibroma : 79 % Accuracy of melanoma : 97 % Accuracy of melanocytic nevi : 85 % Accuracy of vascular lesions : 0 % class_correct = list ( 0. for i in range ( len ( classes ))) class_total = list ( 1e-7 for i in range ( len ( classes ))) net . eval () with torch . no_grad (): for data in validation_data_loader : images , labels = data images , labels = images . to ( device ), labels . to ( device ) outputs = net ( images ) _ , predicted = torch . max ( outputs , 1 ) c = ( predicted == labels ) . squeeze () for i in range ( 3 ): label = labels [ i ] class_correct [ label ] += c [ i ] . item () class_total [ label ] += 1 for i in range ( len ( classes )): print ( 'Accuracy of %5s : %2d %% ' % ( classes [ i ], 100 * class_correct [ i ] / class_total [ i ])) Accuracy of actinic keratoses : 66 % Accuracy of basal cell carcinoma : 77 % Accuracy of benign keratosis-like lesions : 77 % Accuracy of dermatofibroma : 69 % Accuracy of melanoma : 79 % Accuracy of melanocytic nevi : 88 % Accuracy of vascular lesions : 0 % Confusion Matrix confusion_matrix = torch . zeros ( len ( classes ), len ( classes )) with torch . no_grad (): for data in test_data_loader : images , labels = data images , labels = images . to ( device ), labels . to ( device ) outputs = net ( images ) _ , predicted = torch . max ( outputs , 1 ) for t , p in zip ( labels . view ( - 1 ), predicted . view ( - 1 )): confusion_matrix [ t . long (), p . long ()] += 1 print ( confusion_matrix ) cm = confusion_matrix . numpy () tensor([[6.0000e+01, 2.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00], [0.0000e+00, 1.0200e+02, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00], [2.0000e+00, 0.0000e+00, 2.0700e+02, 0.0000e+00, 5.0000e+00, 6.0000e+00, 0.0000e+00], [1.0000e+00, 0.0000e+00, 0.0000e+00, 2.0000e+01, 1.0000e+00, 1.0000e+00, 0.0000e+00], [0.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00, 2.1400e+02, 7.0000e+00, 0.0000e+00], [3.0000e+00, 5.0000e+00, 3.8000e+01, 1.0000e+00, 1.7500e+02, 1.1190e+03, 0.0000e+00], [0.0000e+00, 9.0000e+00, 1.0000e+00, 0.0000e+00, 8.0000e+00, 1.0000e+01, 0.0000e+00]]) fig , ax = plt . subplots ( figsize = ( 7 , 7 )) sns . heatmap ( cm / ( cm . astype ( np . float ) . sum ( axis = 1 ) + 1e-9 ), annot = False , ax = ax ) # labels, title and ticks ax . set_xlabel ( 'Predicted' , size = 25 ); ax . set_ylabel ( 'True' , size = 25 ); ax . set_title ( 'Confusion Matrix' , size = 25 ); ax . xaxis . set_ticklabels ([ 'akiec' , 'bcc' , 'bkl' , 'df' , 'mel' , 'nv' , 'vasc' ], size = 15 ); \\ ax . yaxis . set_ticklabels ([ 'akiec' , 'bcc' , 'bkl' , 'df' , 'mel' , 'nv' , 'vasc' ], size = 15 ); Grad cam from collections import OrderedDict , Sequence class _BaseWrapper ( object ): \"\"\" Please modify forward() and backward() according to your task. \"\"\" def __init__ ( self , model ): super ( _BaseWrapper , self ) . __init__ () self . device = next ( model . parameters ()) . device self . model = model self . handlers = [] # a set of hook function handlers def _encode_one_hot ( self , ids ): one_hot = torch . zeros_like ( self . logits ) . to ( self . device ) one_hot . scatter_ ( 1 , ids , 1.0 ) return one_hot def forward ( self , image ): \"\"\" Simple classification \"\"\" self . model . zero_grad () self . logits = self . model ( image ) self . probs = F . softmax ( self . logits , dim = 1 ) return self . probs . sort ( dim = 1 , descending = True ) def backward ( self , ids ): \"\"\" Class-specific backpropagation Either way works: 1. self.logits.backward(gradient=one_hot, retain_graph=True) 2. (self.logits * one_hot).sum().backward(retain_graph=True) \"\"\" one_hot = self . _encode_one_hot ( ids ) self . logits . backward ( gradient = one_hot , retain_graph = True ) def generate ( self ): raise NotImplementedError def remove_hook ( self ): \"\"\" Remove all the forward/backward hook functions \"\"\" for handle in self . handlers : handle . remove () class GradCAM ( _BaseWrapper ): \"\"\" \"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\" https://arxiv.org/pdf/1610.02391.pdf Look at Figure 2 on page 4 \"\"\" def __init__ ( self , model , candidate_layers = None ): super ( GradCAM , self ) . __init__ ( model ) self . fmap_pool = OrderedDict () self . grad_pool = OrderedDict () self . candidate_layers = candidate_layers # list def forward_hook ( key ): def forward_hook_ ( module , input , output ): # Save featuremaps self . fmap_pool [ key ] = output . detach () return forward_hook_ def backward_hook ( key ): def backward_hook_ ( module , grad_in , grad_out ): # Save the gradients correspond to the featuremaps self . grad_pool [ key ] = grad_out [ 0 ] . detach () return backward_hook_ # If any candidates are not specified, the hook is registered to all the layers. for name , module in self . model . named_modules (): if self . candidate_layers is None or name in self . candidate_layers : self . handlers . append ( module . register_forward_hook ( forward_hook ( name ))) self . handlers . append ( module . register_backward_hook ( backward_hook ( name ))) def _find ( self , pool , target_layer ): if target_layer in pool . keys (): return pool [ target_layer ] else : raise ValueError ( \"Invalid layer name: {} \" . format ( target_layer )) def _compute_grad_weights ( self , grads ): return F . adaptive_avg_pool2d ( grads , 1 ) def forward ( self , image ): self . image_shape = image . shape [ 2 :] return super ( GradCAM , self ) . forward ( image ) def generate ( self , target_layer ): fmaps = self . _find ( self . fmap_pool , target_layer ) grads = self . _find ( self . grad_pool , target_layer ) weights = self . _compute_grad_weights ( grads ) gcam = torch . mul ( fmaps , weights ) . sum ( dim = 1 , keepdim = True ) gcam = F . relu ( gcam ) gcam = F . interpolate ( gcam , self . image_shape , mode = \"bilinear\" , align_corners = False ) B , C , H , W = gcam . shape gcam = gcam . view ( B , - 1 ) gcam -= gcam . min ( dim = 1 , keepdim = True )[ 0 ] gcam /= gcam . max ( dim = 1 , keepdim = True )[ 0 ] gcam = gcam . view ( B , C , H , W ) return gcam def demo2 ( image , label , model ): \"\"\" Generate Grad-CAM \"\"\" # Model model = model model . to ( device ) model . eval () # The layers target_layers = [ \"layer4\" ] target_class = label # Images images = image . unsqueeze ( 0 ) gcam = GradCAM ( model = model ) probs , ids = gcam . forward ( images ) ids_ = torch . LongTensor ([[ target_class ]] * len ( images )) . to ( device ) gcam . backward ( ids = ids_ ) for target_layer in target_layers : print ( \"Generating Grad-CAM @ {} \" . format ( target_layer )) # Grad-CAM regions = gcam . generate ( target_layer = target_layer ) for j in range ( len ( images )): print ( \" \\t # {} : {} ( {:.5f} )\" . format ( j , classes [ target_class ], float ( probs [ ids == target_class ]) ) ) gcam = regions [ j , 0 ] plt . imshow ( gcam . cpu ()) plt . show () image , label = next ( iter ( test_data_loader )) # Load the model model = net # Grad cam demo2 ( image [ 0 ] . to ( device ), label [ 0 ] . to ( device ), model ) image = np . transpose ( image [ 0 ], ( 1 , 2 , 0 )) image2 = np . add ( np . multiply ( image . numpy (), np . array ( norm_std )) , np . array ( norm_mean )) print ( \"True Class: \" , classes [ label [ 0 ] . cpu ()]) plt . imshow ( image ) plt . show () plt . imshow ( image2 ) plt . show () Generating Grad-CAM @layer4 #0: melanocytic nevi (0.76489) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). True Class: melanocytic nevi Conclusion Training a neural network can be a daunting task, especially for a beginner. Here, are some useful practices to get the best out of your network. Training Ensembles \u2014 Combine learning from multiple networks. Always go for a lower learning rate. In cases of limited data try better augmentation techniques[20]. Network architectures that have the appropriate depth for our problem \u2014 too many hyperparameters could lead to suboptimal results if we don\u2019t have enough images. Improving loss function and class balancing. In this tutorial we learned how to train a deep neural network for the challenging task of skin-lesion classification. We experimented with two network architectures and provided insights in the attention of the models. Additionally, we achieved 83% overall accuracy on HAM10000 and provided you with more tips and tricks to tackle overfitting and class imbalance. Now you have all the tools to not only beat our performance and participate in the exciting MICCAI Challenges, but to also solve many more medical imaging problems. Happy training!","title":"SkinLesionClassification"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/SkinLesionClassification/#skin-cancer-classification-an-educational-guide","text":"In this tutorial we aim to provide a simple step-by-step guide to anyone who wants to work on the problem of skin lesion classification regardless of their level or expertise; from medical doctors, to master students and more experienced researchers.","title":"Skin Cancer Classification - An Educational Guide"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/SkinLesionClassification/#using-this-guide-you-will-learn","text":"How to load the data, visualise it and uncover more about the class distribution and meta-data. How to utilise architectures with varying complexity from a few convolutional layers to hundreds of them. How to train a model with appropriate optimisers and loss functions. How to rigorously test your trained model, providing not only metrics such as accuracy but also visualisations like confusion matrix and Grad \u2014 Cam. How to analyse and understand your results. To conclude with, we will provide a few more tips that are usually utilised by the participants of the ISIC Challenges, that will help you increase your model\u2019s performance even more so that you can beat our performance and explore more advanced training schemes.","title":"Using this guide you will learn:"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/SkinLesionClassification/#httpsgithubcomifl-campslclassificationaneducationalcode-mec2019","text":"# !pip install imageio # !pip install scikit-image import torch from torch import nn import torch.nn.functional as F import torchvision import torchvision.transforms as transforms import numpy as np import os import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix , precision_recall_fscore_support import scipy.ndimage from scipy import misc from glob import glob from scipy import stats from sklearn.preprocessing import LabelEncoder , StandardScaler import skimage import imageio import seaborn as sns from PIL import Image import glob import matplotlib.pyplot as plt import matplotlib % matplotlib inline device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) device --------------------------------------------------------------------------- ModuleNotFoundError Traceback (most recent call last) <ipython-input-3-5b38befe3f1d> in <module> 18 import skimage 19 import imageio ---> 20 import seaborn as sns 21 from PIL import Image 22 import glob ModuleNotFoundError: No module named 'seaborn'","title":"https://github.com/IFL-CAMP/SLClassificationAnEducationalCode-MEC2019"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/SkinLesionClassification/#what-about-data","text":"The HAM10000 (\"Human Against Machine with 10000 training images\") dataset which contains 10,015 dermatoscopic images was made publically available by the Harvard database on June 2018 in the hopes to provide training data for automating the process of skin cancer lesion classifications. The motivation behind this act was to provide the public with an abundance and variability of data source for machine learning training purposes such that the results may be compared with that of human experts. If successful, the appplications would bring cost and time saving regimes to hospitals and medical professions alike. Apart from the 10,015 images, a metadata file with demographic information of each lesion is provided as well. More than 50% of lesions are confirmed through histopathology (histo), the ground truth for the rest of the cases is either follow-up examination (follow_up), expert consensus (consensus), or confirmation by in-vivo confocal microscopy (confocal) You can download the dataset here: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T The 7 classes of skin cancer lesions included in this dataset are: Melanocytic nevi Melanoma Benign keratosis-like lesions Basal cell carcinoma Actinic keratoses Vascular lesions Dermatofibroma","title":"What about data?"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/SkinLesionClassification/#lets-analyze-the-metadata-of-the-dataset","text":"# importing metadata and checking for its shape data_dir = \"/data/HAM10000\" metadata = pd . read_csv ( data_dir + '/HAM10000_metadata.csv' ) print ( metadata . shape ) # label encoding the seven classes for skin cancers le = LabelEncoder () le . fit ( metadata [ 'dx' ]) LabelEncoder () print ( \"Classes:\" , list ( le . classes_ )) metadata [ 'label' ] = le . transform ( metadata [ \"dx\" ]) metadata . sample ( 10 ) --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-2-3d00b7e51bd3> in <module> 2 data_dir = \"/data/HAM10000\" 3 ----> 4 metadata = pd.read_csv(data_dir + '/HAM10000_metadata.csv') 5 print(metadata.shape) 6 NameError: name 'pd' is not defined # Getting a sense of what the distribution of each column looks like fig = plt . figure ( figsize = ( 40 , 25 )) ax1 = fig . add_subplot ( 221 ) metadata [ 'dx' ] . value_counts () . plot ( kind = 'bar' , ax = ax1 ) ax1 . set_ylabel ( 'Count' , size = 50 ) ax1 . set_title ( 'Cell Type' , size = 50 ) ax2 = fig . add_subplot ( 222 ) metadata [ 'sex' ] . value_counts () . plot ( kind = 'bar' , ax = ax2 ) ax2 . set_ylabel ( 'Count' , size = 50 ) ax2 . set_title ( 'Sex' , size = 50 ); ax3 = fig . add_subplot ( 223 ) metadata [ 'localization' ] . value_counts () . plot ( kind = 'bar' ) ax3 . set_ylabel ( 'Count' , size = 50 ) ax3 . set_title ( 'Localization' , size = 50 ) ax4 = fig . add_subplot ( 224 ) sample_age = metadata [ pd . notnull ( metadata [ 'age' ])] sns . distplot ( sample_age [ 'age' ], fit = stats . norm , color = 'red' ); ax4 . set_title ( 'Age' , size = 50 ) ax4 . set_xlabel ( 'Year' , size = 50 ) plt . tight_layout () plt . show () As you can see there is imbalance in the number of images per class. There are much more images for the lesion type \"Melanocytic Nevi\" compared to other types. This is an usual occurence for medical datasets and so it is very important to analyze the data from beforehand.","title":"Let's analyze the metadata of the dataset"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/SkinLesionClassification/#lets-visualize-some-examples","text":"#Visualizing the images label = [ 'akiec' , 'bcc' , 'bkl' , 'df' , 'mel' , 'nv' , 'vasc' ] label_images = [] classes = [ 'actinic keratoses' , 'basal cell carcinoma' , 'benign keratosis-like lesions' , 'dermatofibroma' , 'melanoma' , 'melanocytic nevi' , 'vascular lesions' ] fig = plt . figure ( figsize = ( 55 , 55 )) k = range ( 7 ) for i in label : sample = metadata [ metadata [ 'dx' ] == i ][ 'image_id' ][: 5 ] label_images . extend ( sample ) for position , ID in enumerate ( label_images ): labl = metadata [ metadata [ 'image_id' ] == ID ][ 'dx' ] im_sample = data_dir + \"/\" + labl . values [ 0 ] + f '/ { ID } .jpg' im_sample = imageio . imread ( im_sample ) plt . subplot ( 7 , 5 , position + 1 ) plt . imshow ( im_sample ) plt . axis ( 'off' ) if position % 5 == 0 : title = int ( position / 5 ) plt . title ( classes [ title ], loc = 'left' , size = 50 , weight = \"bold\" ) plt . tight_layout () plt . show ()","title":"Let's visualize some examples"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/SkinLesionClassification/#median-frequency-balancing","text":"As we saw above that there is class imbalance in our dataset. To solve that we use this method. #print(metadata['dx'].value_counts()) #print(metadata[metadata['dx']=='nv']['dx'].value_counts()) label = [ 'akiec' , 'bcc' , 'bkl' , 'df' , 'mel' , 'nv' , 'vasc' ] def estimate_weights_mfb ( label ): class_weights = np . zeros_like ( label , dtype = np . float ) counts = np . zeros_like ( label ) for i , l in enumerate ( label ): counts [ i ] = metadata [ metadata [ 'dx' ] == str ( l )][ 'dx' ] . value_counts ()[ 0 ] counts = counts . astype ( np . float ) #print(counts) median_freq = np . median ( counts ) #print(median_freq) #print(weights.shape) for i , label in enumerate ( label ): #print(label) class_weights [ i ] = median_freq / counts [ i ] return class_weights classweight = estimate_weights_mfb ( label ) for i in range ( len ( label )): print ( label [ i ], \":\" , classweight [ i ]) akiec : 1.5718654434250765 bcc : 1.0 bkl : 0.467697907188353 df : 4.469565217391304 mel : 0.4618149146451033 nv : 0.07665920954511558 vasc : 3.619718309859155","title":"Median Frequency Balancing"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/SkinLesionClassification/#pre-processing-the-dataset","text":"Before we load the data we need to alter the dataset structure. When you download the dataset, all the images are together in a folder. To use Pytorch dataloader we need to seggregrate the images into folders of their respetive labels. You can use the following script to automate the process. # import os # import shutil # data_dir = os.getcwd() + \"/HAM10000/\" # dest_dir = data_dir + \"test/\" # metadata = pd.read_csv(data_dir + '/HAM10000_metadata.csv') # label = ['bkl', 'nv', 'df', 'mel', 'vasc', 'bcc', 'akiec'] # label_images = [] # for i in label: # os.mkdir(dest_dir + str(i) + \"/\") # sample = metadata[metadata['dx'] == i]['image_id'][:5] # label_images.extend(sample) # for id in label_images: # shutil.copyfile((data_dir + i + \"/\"+ id +\".jpg\"), (dest_dir + i + \"/\"+id+\".jpg\")) # label_images=[]","title":"Pre-processing the dataset"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/SkinLesionClassification/#data-augmentation","text":"It is a common fact that medical data is scarce. But to learn a very good model, the network needs a lot of data. So to tackle the problem we perform data augmentation. First we normalize the images. Data normalization is an important step which ensures that each input parameter (pixel, in this case) has a similar data distribution. This makes convergence faster while training the network. Data normalization is done by subtracting the mean from each pixel and then dividing the result by the standard deviation. The distribution of such data would resemble a Gaussian curve centered at zero. Since, skin lesion images are natural images, we use the normalization values (mean and standard deviation) of Imagenet dataset. We also perform data augmentation: - Flipping the image horizontally: RandomHorizontalFlip() - Rotating image 60 degrees: RandomRotation() . 60 degrees is chosen as best practice. You can experiment with other angles. The augmentation is applied using the transform.Compose() function of Pytorch. Take note, we only augment the training set. This is because, augmentation is done to aid the training process. So there is no point in augmenting the test set. data_dir = \"/data/HAM10000\" # normalization values for pretrained resnet on Imagenet norm_mean = ( 0.4914 , 0.4822 , 0.4465 ) norm_std = ( 0.2023 , 0.1994 , 0.2010 ) batch_size = 50 validation_batch_size = 10 test_batch_size = 10 # We compute the weights of individual classes and convert them to tensors class_weights = estimate_weights_mfb ( label ) class_weights = torch . FloatTensor ( class_weights ) transform_train = transforms . Compose ([ transforms . Resize (( 224 , 224 )), transforms . RandomHorizontalFlip (), transforms . RandomRotation ( degrees = 60 ), transforms . ToTensor (), transforms . Normalize ( norm_mean , norm_std ), ]) transform_test = transforms . Compose ([ transforms . Resize (( 224 , 224 )), transforms . ToTensor (), transforms . Normalize (( 0.4914 , 0.4822 , 0.4465 ), ( 0.2023 , 0.1994 , 0.2010 )), ])","title":"Data Augmentation"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/SkinLesionClassification/#train-test-and-validation-split","text":"We split the entire dataset into 3 parts: - Train: 80% - Test: 20% - Validation: 16% The splitting is done class wise so that we have equal representation of all classes in each subset of the data. import torch as th import math test_size = 0.2 val_size = 0.2 class Sampler ( object ): \"\"\"Base class for all Samplers. \"\"\" def __init__ ( self , data_source ): pass def __iter__ ( self ): raise NotImplementedError def __len__ ( self ): raise NotImplementedError class StratifiedSampler ( Sampler ): \"\"\"Stratified Sampling Provides equal representation of target classes \"\"\" def __init__ ( self , class_vector ): \"\"\" Arguments --------- class_vector : torch tensor a vector of class labels batch_size : integer batch_size \"\"\" self . n_splits = 1 self . class_vector = class_vector self . test_size = test_size def gen_sample_array ( self ): try : from sklearn.model_selection import StratifiedShuffleSplit except : print ( 'Need scikit-learn for this functionality' ) import numpy as np s = StratifiedShuffleSplit ( n_splits = self . n_splits , test_size = self . test_size ) X = th . randn ( self . class_vector . size ( 0 ), 2 ) . numpy () y = self . class_vector . numpy () s . get_n_splits ( X , y ) train_index , test_index = next ( s . split ( X , y )) return train_index , test_index def __iter__ ( self ): return iter ( self . gen_sample_array ()) def __len__ ( self ): return len ( self . class_vector ) dataset = torchvision . datasets . ImageFolder ( root = data_dir ) data_label = [ s [ 1 ] for s in dataset . samples ] ss = StratifiedSampler ( torch . FloatTensor ( data_label ), test_size ) pre_train_indices , test_indices = ss . gen_sample_array () # The \"pre\" is necessary to use array to identify train/ val indices with indices generated by second sampler train_label = np . delete ( data_label , test_indices , None ) ss = StratifiedSampler ( torch . FloatTensor ( train_label ), test_size ) train_indices , val_indices = ss . gen_sample_array () indices = { 'train' : pre_train_indices [ train_indices ], # Indices of second sampler are used on pre_train_indices 'val' : pre_train_indices [ val_indices ], # Indices of second sampler are used on pre_train_indices 'test' : test_indices } train_indices = indices [ 'train' ] val_indices = indices [ 'val' ] test_indices = indices [ 'test' ] print ( \"Train Data Size:\" , len ( train_indices )) print ( \"Test Data Size:\" , len ( test_indices )) print ( \"Validation Data Size:\" , len ( val_indices )) Train Data Size: 6409 Test Data Size: 2003 Validation Data Size: 1603 Now we use Pytorch data loader to load the dataset into the memory. SubsetRandomSampler = torch . utils . data . sampler . SubsetRandomSampler dataset = torchvision . datasets . ImageFolder ( root = data_dir , transform = transform_train ) train_samples = SubsetRandomSampler ( train_indices ) val_samples = SubsetRandomSampler ( val_indices ) test_samples = SubsetRandomSampler ( test_indices ) train_data_loader = torch . utils . data . DataLoader ( dataset , batch_size = batch_size , shuffle = False , num_workers = 1 , sampler = train_samples ) validation_data_loader = torch . utils . data . DataLoader ( dataset , batch_size = validation_batch_size , shuffle = False , sampler = val_samples ) dataset = torchvision . datasets . ImageFolder ( root = data_dir , transform = transform_test ) test_data_loader = torch . utils . data . DataLoader ( dataset , batch_size = test_batch_size , shuffle = False , sampler = test_samples ) Let us see some of the training images. # functions to show an image fig = plt . figure ( figsize = ( 10 , 15 )) def imshow ( img ): img = img / 2 + 0.5 # denormalize change this npimg = img . numpy () plt . imshow ( np . transpose ( npimg , ( 1 , 2 , 0 ))) # get some random training images dataiter = iter ( train_data_loader ) images , labels = dataiter . next () # show images imshow ( torchvision . utils . make_grid ( images )) # print labels print ( ' ' . join ( ' %5s , ' % classes [ labels [ j ]] for j in range ( len ( labels )))) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). melanocytic nevi, actinic keratoses, benign keratosis-like lesions, benign keratosis-like lesions, melanocytic nevi, melanocytic nevi, basal cell carcinoma, melanocytic nevi, melanocytic nevi, benign keratosis-like lesions, melanocytic nevi, melanocytic nevi, melanocytic nevi, melanocytic nevi, melanocytic nevi, melanocytic nevi, melanocytic nevi, melanocytic nevi, melanocytic nevi, melanocytic nevi, benign keratosis-like lesions, melanocytic nevi, melanoma, melanocytic nevi, melanocytic nevi, melanoma, melanoma, melanocytic nevi, actinic keratoses, melanocytic nevi, actinic keratoses, melanocytic nevi, melanocytic nevi, melanoma, melanocytic nevi, melanoma, melanoma, melanocytic nevi, melanoma, melanocytic nevi, melanocytic nevi, melanocytic nevi, melanocytic nevi, melanoma, melanocytic nevi, melanocytic nevi, melanocytic nevi, dermatofibroma, basal cell carcinoma, basal cell carcinoma,","title":"Train, Test and Validation Split"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/SkinLesionClassification/#define-a-convolutional-neural-network","text":"Pytorch makes it very easy to define a neural network. We have layers like Convolutions, ReLU non-linearity, Maxpooling etc. directly from torch library. In this tutorial, we use The LeNet architecture introduced by LeCun et al. in their 1998 paper, Gradient-Based Learning Applied to Document Recognition . As the name of the paper suggests, the authors\u2019 implementation of LeNet was used primarily for OCR and character recognition in documents. The LeNet architecture is straightforward and small, (in terms of memory footprint), making it perfect for teaching the basics of CNNs. num_classes = len ( classes ) class LeNet ( nn . Module ): def __init__ ( self ): super ( LeNet , self ) . __init__ () self . conv1 = nn . Conv2d ( 3 , 6 , ( 5 , 5 ), padding = 2 ) self . conv2 = nn . Conv2d ( 6 , 16 , ( 5 , 5 )) self . fc1 = nn . Linear ( 16 * 54 * 54 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , num_classes ) def forward ( self , x ): x = F . max_pool2d ( F . relu ( self . conv1 ( x )), ( 2 , 2 )) x = F . max_pool2d ( F . relu ( self . conv2 ( x )), ( 2 , 2 )) x = x . view ( - 1 , self . num_flat_features ( x )) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x def num_flat_features ( self , x ): size = x . size ()[ 1 :] num_features = 1 for s in size : num_features *= s return num_features net = LeNet () net = net . to ( device )","title":"Define a Convolutional Neural Network"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/SkinLesionClassification/#define-a-loss-function-and-optimizer","text":"Let's use a Classification Cross-Entropy loss. \\(H_{y'} (y) := - \\sum_{i} y_{i}' \\log (y_i)\\) The most common and effective Optimizer currently used is Adam: Adaptive Moments . You can look here for more information. import torch.optim as optim class_weights = class_weights . to ( device ) criterion = nn . CrossEntropyLoss ( weight = class_weights ) optimizer = optim . Adam ( net . parameters (), lr = 1e-5 ) print ( net ) LeNet( (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear(in_features=46656, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=7, bias=True) ) These are some helper functions to evaluate the training process. from sklearn.metrics import accuracy_score def get_accuracy ( predicted , labels ): batch_len , correct = 0 , 0 batch_len = labels . size ( 0 ) correct = ( predicted == labels ) . sum () . item () return batch_len , correct def evaluate ( model , val_loader ): losses = 0 num_samples_total = 0 correct_total = 0 model . eval () for inputs , labels in val_loader : inputs , labels = inputs . to ( device ), labels . to ( device ) out = model ( inputs ) _ , predicted = torch . max ( out , 1 ) loss = criterion ( out , labels ) losses += loss . item () b_len , corr = get_accuracy ( predicted , labels ) num_samples_total += b_len correct_total += corr accuracy = correct_total / num_samples_total losses = losses / len ( val_loader ) return losses , accuracy","title":"Define a Loss function and Optimizer"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/SkinLesionClassification/#train-the-network","text":"This is when things start to get interesting. We simply loop over the training data iterator, and feed the inputs to the network and optimize. # number of loops over the dataset num_epochs = 50 accuracy = [] val_accuracy = [] losses = [] val_losses = [] for epoch in range ( num_epochs ): running_loss = 0.0 correct_total = 0.0 num_samples_total = 0.0 for i , data in enumerate ( train_data_loader ): # get the inputs inputs , labels = data inputs , labels = inputs . to ( device ), labels . to ( device ) # set the parameter gradients to zero optimizer . zero_grad () # forward + backward + optimize outputs = net ( inputs ) loss = criterion ( outputs , labels ) loss . backward () optimizer . step () #compute accuracy _ , predicted = torch . max ( outputs , 1 ) b_len , corr = get_accuracy ( predicted , labels ) num_samples_total += b_len correct_total += corr running_loss += loss . item () running_loss /= len ( train_data_loader ) train_accuracy = correct_total / num_samples_total val_loss , val_acc = evaluate ( net , validation_data_loader ) print ( 'Epoch: %d ' % ( epoch + 1 )) print ( 'Loss: %.3f Accuracy: %.3f ' % ( running_loss , train_accuracy )) print ( 'Validation Loss: %.3f Val Accuracy: %.3f ' % ( val_loss , val_acc )) losses . append ( running_loss ) val_losses . append ( val_loss ) accuracy . append ( train_accuracy ) val_accuracy . append ( val_acc ) print ( 'Finished Training' ) Epoch: 1 Loss: 1.806 Accuracy:0.314 Validation Loss: 1.727 Val Accuracy: 0.403 Epoch: 2 Loss: 1.663 Accuracy:0.371 Validation Loss: 1.631 Val Accuracy: 0.419 Epoch: 3 Loss: 1.589 Accuracy:0.459 Validation Loss: 1.565 Val Accuracy: 0.451 Epoch: 4 Loss: 1.542 Accuracy:0.486 Validation Loss: 1.558 Val Accuracy: 0.431 Epoch: 5 Loss: 1.499 Accuracy:0.494 Validation Loss: 1.483 Val Accuracy: 0.500 Epoch: 6 Loss: 1.472 Accuracy:0.500 Validation Loss: 1.457 Val Accuracy: 0.525 Epoch: 7 Loss: 1.447 Accuracy:0.509 Validation Loss: 1.454 Val Accuracy: 0.498 Epoch: 9 Loss: 1.421 Accuracy:0.503 Validation Loss: 1.406 Val Accuracy: 0.527 Epoch: 10 Loss: 1.399 Accuracy:0.516 Validation Loss: 1.413 Val Accuracy: 0.530 Epoch: 11 Loss: 1.390 Accuracy:0.519 Validation Loss: 1.436 Val Accuracy: 0.496 Epoch: 12 Loss: 1.384 Accuracy:0.519 Validation Loss: 1.396 Val Accuracy: 0.505 Epoch: 13 Loss: 1.373 Accuracy:0.529 Validation Loss: 1.391 Val Accuracy: 0.525 Epoch: 14 Loss: 1.363 Accuracy:0.534 Validation Loss: 1.376 Val Accuracy: 0.518 Epoch: 15 Loss: 1.364 Accuracy:0.531 Validation Loss: 1.365 Val Accuracy: 0.536 Epoch: 16 Loss: 1.352 Accuracy:0.534 Validation Loss: 1.363 Val Accuracy: 0.548 Epoch: 17 Loss: 1.337 Accuracy:0.536 Validation Loss: 1.365 Val Accuracy: 0.532 Epoch: 18 Loss: 1.333 Accuracy:0.540 Validation Loss: 1.340 Val Accuracy: 0.527 Epoch: 19 Loss: 1.327 Accuracy:0.542 Validation Loss: 1.333 Val Accuracy: 0.530 Epoch: 20 Loss: 1.320 Accuracy:0.537 Validation Loss: 1.304 Val Accuracy: 0.558 Epoch: 21 Loss: 1.303 Accuracy:0.555 Validation Loss: 1.373 Val Accuracy: 0.490 Epoch: 23 Loss: 1.293 Accuracy:0.550 Validation Loss: 1.348 Val Accuracy: 0.530 Epoch: 24 Loss: 1.308 Accuracy:0.554 Validation Loss: 1.344 Val Accuracy: 0.558 Epoch: 25 Loss: 1.288 Accuracy:0.559 Validation Loss: 1.329 Val Accuracy: 0.552 Epoch: 26 Loss: 1.288 Accuracy:0.558 Validation Loss: 1.321 Val Accuracy: 0.533 Epoch: 27 Loss: 1.295 Accuracy:0.560 Validation Loss: 1.310 Val Accuracy: 0.558 Epoch: 28 Loss: 1.290 Accuracy:0.562 Validation Loss: 1.303 Val Accuracy: 0.534 Epoch: 29 Loss: 1.278 Accuracy:0.560 Validation Loss: 1.315 Val Accuracy: 0.522 Epoch: 30 Loss: 1.280 Accuracy:0.559 Validation Loss: 1.318 Val Accuracy: 0.540 Epoch: 31 Loss: 1.266 Accuracy:0.561 Validation Loss: 1.284 Val Accuracy: 0.540 Epoch: 32 Loss: 1.275 Accuracy:0.565 Validation Loss: 1.289 Val Accuracy: 0.562 Epoch: 33 Loss: 1.259 Accuracy:0.575 Validation Loss: 1.284 Val Accuracy: 0.585 Epoch: 34 Loss: 1.268 Accuracy:0.561 Validation Loss: 1.313 Val Accuracy: 0.530 Epoch: 35 Loss: 1.257 Accuracy:0.574 Validation Loss: 1.328 Val Accuracy: 0.520 Epoch: 36 Loss: 1.254 Accuracy:0.569 Validation Loss: 1.288 Val Accuracy: 0.555 Epoch: 37 Loss: 1.259 Accuracy:0.574 Validation Loss: 1.289 Val Accuracy: 0.566 Epoch: 38 Loss: 1.251 Accuracy:0.576 Validation Loss: 1.319 Val Accuracy: 0.513 Epoch: 39 Loss: 1.248 Accuracy:0.572 Validation Loss: 1.276 Val Accuracy: 0.571 Epoch: 40 Loss: 1.253 Accuracy:0.582 Validation Loss: 1.300 Val Accuracy: 0.533 Epoch: 41 Loss: 1.236 Accuracy:0.570 Validation Loss: 1.288 Val Accuracy: 0.563 Epoch: 42 Loss: 1.246 Accuracy:0.572 Validation Loss: 1.263 Val Accuracy: 0.564 Epoch: 43 Loss: 1.240 Accuracy:0.581 Validation Loss: 1.332 Val Accuracy: 0.520 Epoch: 44 Loss: 1.253 Accuracy:0.573 Validation Loss: 1.311 Val Accuracy: 0.528 Epoch: 45 Loss: 1.229 Accuracy:0.579 Validation Loss: 1.266 Val Accuracy: 0.558 Epoch: 46 Loss: 1.221 Accuracy:0.585 Validation Loss: 1.264 Val Accuracy: 0.553 Epoch: 47 Loss: 1.232 Accuracy:0.586 Validation Loss: 1.258 Val Accuracy: 0.561 Epoch: 48 Loss: 1.236 Accuracy:0.582 Validation Loss: 1.255 Val Accuracy: 0.588 Epoch: 49 Loss: 1.224 Accuracy:0.583 Validation Loss: 1.258 Val Accuracy: 0.581 Epoch: 50 Loss: 1.234 Accuracy:0.582 Validation Loss: 1.261 Val Accuracy: 0.560 Finished Training Plot the training and validation loss curves. # plt.plot(losses) # plt.show() epoch = range ( 1 , num_epochs + 1 ) fig = plt . figure ( figsize = ( 10 , 15 )) plt . subplot ( 2 , 1 , 2 ) plt . plot ( epoch , losses , label = 'Training loss' ) plt . plot ( epoch , val_losses , label = 'Validation loss' ) plt . title ( 'Training and Validation Loss' ) plt . xlabel ( 'Epochs' ) plt . legend () plt . figure () plt . show () fig = plt . figure ( figsize = ( 10 , 15 )) plt . subplot ( 2 , 1 , 2 ) plt . plot ( epoch , accuracy , label = 'Training accuracy' ) plt . plot ( epoch , val_accuracy , label = 'Validation accuracy' ) plt . title ( 'Training and Validation Accuracy' ) plt . xlabel ( 'Epochs' ) plt . legend () plt . figure () plt . show () <Figure size 432x288 with 0 Axes> <Figure size 432x288 with 0 Axes>","title":"Train the network"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/SkinLesionClassification/#test-the-network-on-the-test-data","text":"We have trained the network over the training dataset. But we need to check if the network has learnt anything at all. We will check this by predicting the class label that the neural network outputs, and checking it against the ground-truth. If the prediction is correct, we add the sample to the list of correct predictions. Okay, first step. Let us display an image from the test set to get familiar. fig = plt . figure ( figsize = ( 10 , 15 )) dataiter = iter ( test_data_loader ) images , labels = dataiter . next () # print images imshow ( torchvision . utils . make_grid ( images )) print ( 'GroundTruth: ' , ' ' . join ( ' %5s , ' % classes [ labels [ j ]] for j in range ( len ( labels )))) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). GroundTruth: melanocytic nevi, benign keratosis-like lesions, melanocytic nevi, melanoma, melanocytic nevi, benign keratosis-like lesions, melanocytic nevi, melanoma, benign keratosis-like lesions, melanocytic nevi, Okay, now let us check the performance on the test network: correct = 0 total = 0 net . eval () with torch . no_grad (): for data in test_data_loader : images , labels = data images , labels = images . to ( device ), labels . to ( device ) outputs = net ( images ) _ , predicted = torch . max ( outputs . data , 1 ) total += labels . size ( 0 ) correct += ( predicted == labels ) . sum () . item () print ( 'Accuracy of the network on the test images: %d %% ' % ( 100 * correct / total )) Accuracy of the network on the test images: 52 % That looks better than chance, which is about 14% accuracy (randomly picking a class out of 7 classes). Seems like the network learnt something. But maybe it doesn't learn all the classes equally. Let's check which classes that performed well, and which did not. class_correct = list ( 0. for i in range ( len ( classes ))) class_total = list ( 1e-7 for i in range ( len ( classes ))) with torch . no_grad (): for data in test_data_loader : images , labels = data images , labels = images . to ( device ), labels . to ( device ) outputs = net ( images ) _ , predicted = torch . max ( outputs , 1 ) c = ( predicted == labels ) . squeeze () for i in range ( 3 ): label = labels [ i ] class_correct [ label ] += c [ i ] . item () class_total [ label ] += 1 for i in range ( len ( classes )): print ( 'Accuracy of %5s : %2d %% ' % ( classes [ i ], 100 * class_correct [ i ] / class_total [ i ])) Accuracy of actinic keratoses : 27 % Accuracy of basal cell carcinoma : 76 % Accuracy of benign keratosis-like lesions : 4 % Accuracy of dermatofibroma : 39 % Accuracy of melanoma : 73 % Accuracy of melanocytic nevi : 58 % Accuracy of vascular lesions : 0 %","title":"Test the network on the test data"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/SkinLesionClassification/#confusion-matrix","text":"confusion_matrix = torch . zeros ( len ( classes ), len ( classes )) with torch . no_grad (): for data in test_data_loader : images , labels = data images , labels = images . to ( device ), labels . to ( device ) outputs = net ( images ) _ , predicted = torch . max ( outputs , 1 ) for t , p in zip ( labels . view ( - 1 ), predicted . view ( - 1 )): confusion_matrix [ t . long (), p . long ()] += 1 cm = confusion_matrix . numpy () fig , ax = plt . subplots ( figsize = ( 7 , 7 )) sns . heatmap ( cm / ( cm . astype ( np . float ) . sum ( axis = 1 ) + 1e-9 ), annot = False , ax = ax ) # labels, title and ticks ax . set_xlabel ( 'Predicted' , size = 25 ); ax . set_ylabel ( 'True' , size = 25 ); ax . set_title ( 'Confusion Matrix' , size = 25 ); ax . xaxis . set_ticklabels ([ 'akiec' , 'bcc' , 'bkl' , 'df' , 'mel' , 'nv' , 'vasc' ], size = 15 ); \\ ax . yaxis . set_ticklabels ([ 'akiec' , 'bcc' , 'bkl' , 'df' , 'mel' , 'nv' , 'vasc' ], size = 15 );","title":"Confusion Matrix"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/SkinLesionClassification/#grad-cam","text":"from collections import OrderedDict , Sequence class _BaseWrapper ( object ): \"\"\" Please modify forward() and backward() according to your task. \"\"\" def __init__ ( self , model ): super ( _BaseWrapper , self ) . __init__ () self . device = next ( model . parameters ()) . device self . model = model self . handlers = [] # a set of hook function handlers def _encode_one_hot ( self , ids ): one_hot = torch . zeros_like ( self . logits ) . to ( self . device ) one_hot . scatter_ ( 1 , ids , 1.0 ) return one_hot def forward ( self , image ): \"\"\" Simple classification \"\"\" self . model . zero_grad () self . logits = self . model ( image ) self . probs = F . softmax ( self . logits , dim = 1 ) return self . probs . sort ( dim = 1 , descending = True ) def backward ( self , ids ): \"\"\" Class-specific backpropagation Either way works: 1. self.logits.backward(gradient=one_hot, retain_graph=True) 2. (self.logits * one_hot).sum().backward(retain_graph=True) \"\"\" one_hot = self . _encode_one_hot ( ids ) self . logits . backward ( gradient = one_hot , retain_graph = True ) def generate ( self ): raise NotImplementedError def remove_hook ( self ): \"\"\" Remove all the forward/backward hook functions \"\"\" for handle in self . handlers : handle . remove () class GradCAM ( _BaseWrapper ): \"\"\" \"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\" https://arxiv.org/pdf/1610.02391.pdf Look at Figure 2 on page 4 \"\"\" def __init__ ( self , model , candidate_layers = None ): super ( GradCAM , self ) . __init__ ( model ) self . fmap_pool = OrderedDict () self . grad_pool = OrderedDict () self . candidate_layers = candidate_layers # list def forward_hook ( key ): def forward_hook_ ( module , input , output ): # Save featuremaps self . fmap_pool [ key ] = output . detach () return forward_hook_ def backward_hook ( key ): def backward_hook_ ( module , grad_in , grad_out ): # Save the gradients correspond to the featuremaps self . grad_pool [ key ] = grad_out [ 0 ] . detach () return backward_hook_ # If any candidates are not specified, the hook is registered to all the layers. for name , module in self . model . named_modules (): if self . candidate_layers is None or name in self . candidate_layers : self . handlers . append ( module . register_forward_hook ( forward_hook ( name ))) self . handlers . append ( module . register_backward_hook ( backward_hook ( name ))) def _find ( self , pool , target_layer ): if target_layer in pool . keys (): return pool [ target_layer ] else : raise ValueError ( \"Invalid layer name: {} \" . format ( target_layer )) def _compute_grad_weights ( self , grads ): return F . adaptive_avg_pool2d ( grads , 1 ) def forward ( self , image ): self . image_shape = image . shape [ 2 :] return super ( GradCAM , self ) . forward ( image ) def generate ( self , target_layer ): fmaps = self . _find ( self . fmap_pool , target_layer ) grads = self . _find ( self . grad_pool , target_layer ) weights = self . _compute_grad_weights ( grads ) gcam = torch . mul ( fmaps , weights ) . sum ( dim = 1 , keepdim = True ) gcam = F . relu ( gcam ) gcam = F . interpolate ( gcam , self . image_shape , mode = \"bilinear\" , align_corners = False ) B , C , H , W = gcam . shape gcam = gcam . view ( B , - 1 ) gcam -= gcam . min ( dim = 1 , keepdim = True )[ 0 ] gcam /= gcam . max ( dim = 1 , keepdim = True )[ 0 ] gcam = gcam . view ( B , C , H , W ) return gcam def demo2 ( image , label , model ): \"\"\" Generate Grad-CAM \"\"\" # Model model = model model . to ( device ) model . eval () # The layers target_layers = [ \"conv2\" ] target_class = label # Images images = image . unsqueeze ( 0 ) gcam = GradCAM ( model = model ) probs , ids = gcam . forward ( images ) ids_ = torch . LongTensor ([[ target_class ]] * len ( images )) . to ( device ) gcam . backward ( ids = ids_ ) for target_layer in target_layers : print ( \"Generating Grad-CAM @ {} \" . format ( target_layer )) # Grad-CAM regions = gcam . generate ( target_layer = target_layer ) for j in range ( len ( images )): print ( \" \\t # {} : {} ( {:.5f} )\" . format ( j , classes [ target_class ], float ( probs [ ids == target_class ]) ) ) gcam = regions [ j , 0 ] plt . imshow ( gcam . cpu ()) plt . show () image , label = next ( iter ( test_data_loader )) # Load the model model = net # Grad cam demo2 ( image [ 0 ] . to ( device ), label [ 0 ] . to ( device ), model ) image = np . transpose ( image [ 0 ], ( 1 , 2 , 0 )) image2 = np . add ( np . multiply ( image . numpy (), np . array ( norm_std )) , np . array ( norm_mean )) print ( \"True Class: \" , classes [ label [ 0 ] . cpu ()]) plt . imshow ( image ) plt . show () plt . imshow ( image2 ) plt . show () Generating Grad-CAM @conv2 #0: melanocytic nevi (0.12183) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). True Class: melanocytic nevi","title":"Grad cam"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/SkinLesionClassification/#analysis-of-the-results","text":"As we can see from the results of the LeNet model, our system is not capable of processing the complexity of the given input images. Our final accuracy on the test data was 61%. About 39% of the images are missclassified, which is a terrible performance for any clinical use case. These results could be substantially improved if we opt for a deeper, more complex network architecture than LeNet, which will allow for a richer learning of the corresponding image features.","title":"Analysis of the results"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/SkinLesionClassification/#switching-to-superior-network-architectureresnet18","text":"from torch import nn num_classes = len ( classes ) net = torchvision . models . resnet18 ( pretrained = True ) # We replace last layer of resnet to match our number of classes which is 7 net . fc = nn . Linear ( 512 , num_classes ) net = net . to ( device ) Define a Loss function and Optimizer Let's use a Classification Cross-Entropy loss. \\(H_{y'} (y) := - \\sum_{i} y_{i}' \\log (y_i)\\) The most common and effective Optimizer currently used is Adam: Adaptive Moments . You can look here for more information. import torch.optim as optim class_weights = class_weights . to ( device ) criterion = nn . CrossEntropyLoss ( weight = class_weights ) optimizer = optim . Adam ( net . parameters (), lr = 1e-5 ) print ( net ) ResNet( (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (layer1): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer2): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer3): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer4): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (avgpool): AdaptiveAvgPool2d(output_size=(1, 1)) (fc): Linear(in_features=512, out_features=7, bias=True) ) These are some helper functions to evaluate the training process. from sklearn.metrics import accuracy_score def get_accuracy ( predicted , labels ): batch_len , correct = 0 , 0 batch_len = labels . size ( 0 ) correct = ( predicted == labels ) . sum () . item () return batch_len , correct def evaluate ( model , val_loader ): losses = 0 num_samples_total = 0 correct_total = 0 model . eval () for inputs , labels in val_loader : inputs , labels = inputs . to ( device ), labels . to ( device ) out = model ( inputs ) _ , predicted = torch . max ( out , 1 ) loss = criterion ( out , labels ) losses += loss . item () b_len , corr = get_accuracy ( predicted , labels ) num_samples_total += b_len correct_total += corr accuracy = correct_total / num_samples_total losses = losses / len ( val_loader ) return losses , accuracy","title":"Switching to superior network architecture:Resnet18"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/SkinLesionClassification/#train-the-network_1","text":"This is when things start to get interesting. We simply loop over the training data iterator, and feed the inputs to the network and optimize. # number of loops over the dataset num_epochs = 70 accuracy = [] val_accuracy = [] losses = [] val_losses = [] for epoch in range ( num_epochs ): running_loss = 0.0 correct_total = 0.0 num_samples_total = 0.0 for i , data in enumerate ( train_data_loader ): # get the inputs inputs , labels = data inputs , labels = inputs . to ( device ), labels . to ( device ) # set the parameter gradients to zero optimizer . zero_grad () # forward + backward + optimize outputs = net ( inputs ) loss = criterion ( outputs , labels ) loss . backward () optimizer . step () #compute accuracy _ , predicted = torch . max ( outputs , 1 ) b_len , corr = get_accuracy ( predicted , labels ) num_samples_total += b_len correct_total += corr running_loss += loss . item () running_loss /= len ( train_data_loader ) train_accuracy = correct_total / num_samples_total val_loss , val_acc = evaluate ( net , validation_data_loader ) print ( 'Epoch: %d ' % ( epoch + 1 )) print ( 'Loss: %.3f Accuracy: %.3f ' % ( running_loss , train_accuracy )) print ( 'Validation Loss: %.3f Val Accuracy: %.3f ' % ( val_loss , val_acc )) losses . append ( running_loss ) val_losses . append ( val_loss ) accuracy . append ( train_accuracy ) val_accuracy . append ( val_acc ) print ( 'Finished Training' ) Epoch: 1 Loss: 1.918 Accuracy:0.098 Validation Loss: 1.588 Val Accuracy: 0.243 Epoch: 2 Loss: 1.125 Accuracy:0.621 Validation Loss: 1.040 Val Accuracy: 0.648 Epoch: 3 Loss: 0.937 Accuracy:0.678 Validation Loss: 0.892 Val Accuracy: 0.674 Epoch: 5 Loss: 0.759 Accuracy:0.724 Validation Loss: 0.929 Val Accuracy: 0.651 Epoch: 6 Loss: 0.690 Accuracy:0.730 Validation Loss: 0.838 Val Accuracy: 0.718 Epoch: 7 Loss: 0.662 Accuracy:0.737 Validation Loss: 0.816 Val Accuracy: 0.709 Epoch: 8 Loss: 0.590 Accuracy:0.763 Validation Loss: 0.819 Val Accuracy: 0.705 Epoch: 9 Loss: 0.564 Accuracy:0.767 Validation Loss: 0.848 Val Accuracy: 0.726 Epoch: 10 Loss: 0.546 Accuracy:0.770 Validation Loss: 0.851 Val Accuracy: 0.719 Epoch: 11 Loss: 0.499 Accuracy:0.778 Validation Loss: 0.766 Val Accuracy: 0.749 Epoch: 12 Loss: 0.499 Accuracy:0.784 Validation Loss: 0.734 Val Accuracy: 0.751 Epoch: 13 Loss: 0.462 Accuracy:0.801 Validation Loss: 0.802 Val Accuracy: 0.757 Epoch: 14 Loss: 0.454 Accuracy:0.805 Validation Loss: 0.757 Val Accuracy: 0.744 Epoch: 15 Loss: 0.414 Accuracy:0.808 Validation Loss: 0.722 Val Accuracy: 0.769 Epoch: 16 Loss: 0.433 Accuracy:0.802 Validation Loss: 0.829 Val Accuracy: 0.787 Epoch: 17 Loss: 0.399 Accuracy:0.813 Validation Loss: 0.791 Val Accuracy: 0.768 Epoch: 18 Loss: 0.372 Accuracy:0.825 Validation Loss: 0.660 Val Accuracy: 0.791 Epoch: 19 Loss: 0.354 Accuracy:0.822 Validation Loss: 0.794 Val Accuracy: 0.784 Epoch: 20 Loss: 0.355 Accuracy:0.829 Validation Loss: 0.789 Val Accuracy: 0.750 Epoch: 21 Loss: 0.318 Accuracy:0.841 Validation Loss: 0.765 Val Accuracy: 0.800 Epoch: 22 Loss: 0.284 Accuracy:0.855 Validation Loss: 0.801 Val Accuracy: 0.777 Epoch: 23 Loss: 0.311 Accuracy:0.839 Validation Loss: 0.849 Val Accuracy: 0.815 Epoch: 24 Loss: 0.277 Accuracy:0.851 Validation Loss: 0.839 Val Accuracy: 0.792 Epoch: 25 Loss: 0.284 Accuracy:0.852 Validation Loss: 0.772 Val Accuracy: 0.770 Epoch: 26 Loss: 0.261 Accuracy:0.863 Validation Loss: 0.770 Val Accuracy: 0.802 Epoch: 27 Loss: 0.244 Accuracy:0.863 Validation Loss: 0.782 Val Accuracy: 0.782 Epoch: 28 Loss: 0.239 Accuracy:0.862 Validation Loss: 0.731 Val Accuracy: 0.805 Epoch: 29 Loss: 0.234 Accuracy:0.870 Validation Loss: 0.786 Val Accuracy: 0.816 Epoch: 30 Loss: 0.248 Accuracy:0.860 Validation Loss: 0.739 Val Accuracy: 0.803 Epoch: 31 Loss: 0.239 Accuracy:0.868 Validation Loss: 0.908 Val Accuracy: 0.779 Epoch: 33 Loss: 0.188 Accuracy:0.884 Validation Loss: 0.771 Val Accuracy: 0.780 Epoch: 34 Loss: 0.180 Accuracy:0.893 Validation Loss: 0.868 Val Accuracy: 0.803 Epoch: 35 Loss: 0.174 Accuracy:0.895 Validation Loss: 0.825 Val Accuracy: 0.800 Epoch: 36 Loss: 0.172 Accuracy:0.893 Validation Loss: 0.893 Val Accuracy: 0.823 Epoch: 37 Loss: 0.180 Accuracy:0.895 Validation Loss: 0.774 Val Accuracy: 0.778 Epoch: 38 Loss: 0.164 Accuracy:0.903 Validation Loss: 0.768 Val Accuracy: 0.776 Epoch: 39 Loss: 0.175 Accuracy:0.897 Validation Loss: 1.080 Val Accuracy: 0.784 Epoch: 40 Loss: 0.150 Accuracy:0.906 Validation Loss: 0.905 Val Accuracy: 0.826 Epoch: 41 Loss: 0.128 Accuracy:0.917 Validation Loss: 0.973 Val Accuracy: 0.809 Epoch: 42 Loss: 0.142 Accuracy:0.912 Validation Loss: 0.944 Val Accuracy: 0.834 Epoch: 43 Loss: 0.127 Accuracy:0.912 Validation Loss: 0.825 Val Accuracy: 0.819 Epoch: 44 Loss: 0.138 Accuracy:0.911 Validation Loss: 0.752 Val Accuracy: 0.810","title":"Train the network"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/SkinLesionClassification/#plot-the-training-and-validation-loss-curves","text":"# plt.plot(losses) # plt.show() epoch = range ( 1 , num_epochs + 1 ) fig = plt . figure ( figsize = ( 10 , 15 )) plt . subplot ( 2 , 1 , 2 ) plt . plot ( epoch , losses , label = 'Training loss' ) plt . plot ( epoch , val_losses , label = 'Validation loss' ) plt . title ( 'Training and Validation Loss' ) plt . xlabel ( 'Epochs' ) plt . legend () plt . figure () plt . show () fig = plt . figure ( figsize = ( 10 , 15 )) plt . subplot ( 2 , 1 , 2 ) plt . plot ( epoch , accuracy , label = 'Training accuracy' ) plt . plot ( epoch , val_accuracy , label = 'Validation accuracy' ) plt . title ( 'Training and Validation Accuracy' ) plt . xlabel ( 'Epochs' ) plt . legend () plt . figure () plt . show () <Figure size 432x288 with 0 Axes> <Figure size 432x288 with 0 Axes>","title":"Plot the training  and validation loss curves."},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/SkinLesionClassification/#test-the-network-on-the-test-data_1","text":"We have trained the network over the training dataset. But we need to check if the network has learnt anything at all. We will check this by predicting the class label that the neural network outputs, and checking it against the ground-truth. If the prediction is correct, we add the sample to the list of correct predictions. Okay, first step. Let us display an image from the test set to get familiar. fig = plt . figure ( figsize = ( 10 , 15 )) dataiter = iter ( test_data_loader ) images , labels = dataiter . next () # print images imshow ( torchvision . utils . make_grid ( images )) print ( 'GroundTruth: ' , ' ' . join ( ' %5s , ' % classes [ labels [ j ]] for j in range ( len ( labels )))) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). GroundTruth: melanocytic nevi, melanoma, melanocytic nevi, melanoma, melanoma, melanocytic nevi, melanocytic nevi, benign keratosis-like lesions, basal cell carcinoma, melanocytic nevi, Okay, now let us check the performance on the test network: correct = 0 total = 0 net . eval () with torch . no_grad (): for data in test_data_loader : images , labels = data images , labels = images . to ( device ), labels . to ( device ) outputs = net ( images ) _ , predicted = torch . max ( outputs . data , 1 ) total += labels . size ( 0 ) correct += ( predicted == labels ) . sum () . item () print ( 'Accuracy of the network on the test images: %d %% ' % ( 100 * correct / total )) Accuracy of the network on the test images: 85 % class_correct = list ( 0. for i in range ( len ( classes ))) class_total = list ( 1e-7 for i in range ( len ( classes ))) with torch . no_grad (): for data in test_data_loader : images , labels = data images , labels = images . to ( device ), labels . to ( device ) outputs = net ( images ) _ , predicted = torch . max ( outputs , 1 ) c = ( predicted == labels ) . squeeze () for i in range ( 3 ): label = labels [ i ] class_correct [ label ] += c [ i ] . item () class_total [ label ] += 1 for i in range ( len ( classes )): print ( 'Accuracy of %5s : %2d %% ' % ( classes [ i ], 100 * class_correct [ i ] / class_total [ i ])) Accuracy of actinic keratoses : 92 % Accuracy of basal cell carcinoma : 96 % Accuracy of benign keratosis-like lesions : 92 % Accuracy of dermatofibroma : 79 % Accuracy of melanoma : 97 % Accuracy of melanocytic nevi : 85 % Accuracy of vascular lesions : 0 % class_correct = list ( 0. for i in range ( len ( classes ))) class_total = list ( 1e-7 for i in range ( len ( classes ))) net . eval () with torch . no_grad (): for data in validation_data_loader : images , labels = data images , labels = images . to ( device ), labels . to ( device ) outputs = net ( images ) _ , predicted = torch . max ( outputs , 1 ) c = ( predicted == labels ) . squeeze () for i in range ( 3 ): label = labels [ i ] class_correct [ label ] += c [ i ] . item () class_total [ label ] += 1 for i in range ( len ( classes )): print ( 'Accuracy of %5s : %2d %% ' % ( classes [ i ], 100 * class_correct [ i ] / class_total [ i ])) Accuracy of actinic keratoses : 66 % Accuracy of basal cell carcinoma : 77 % Accuracy of benign keratosis-like lesions : 77 % Accuracy of dermatofibroma : 69 % Accuracy of melanoma : 79 % Accuracy of melanocytic nevi : 88 % Accuracy of vascular lesions : 0 %","title":"Test the network on the test data"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/SkinLesionClassification/#confusion-matrix_1","text":"confusion_matrix = torch . zeros ( len ( classes ), len ( classes )) with torch . no_grad (): for data in test_data_loader : images , labels = data images , labels = images . to ( device ), labels . to ( device ) outputs = net ( images ) _ , predicted = torch . max ( outputs , 1 ) for t , p in zip ( labels . view ( - 1 ), predicted . view ( - 1 )): confusion_matrix [ t . long (), p . long ()] += 1 print ( confusion_matrix ) cm = confusion_matrix . numpy () tensor([[6.0000e+01, 2.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00], [0.0000e+00, 1.0200e+02, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00], [2.0000e+00, 0.0000e+00, 2.0700e+02, 0.0000e+00, 5.0000e+00, 6.0000e+00, 0.0000e+00], [1.0000e+00, 0.0000e+00, 0.0000e+00, 2.0000e+01, 1.0000e+00, 1.0000e+00, 0.0000e+00], [0.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00, 2.1400e+02, 7.0000e+00, 0.0000e+00], [3.0000e+00, 5.0000e+00, 3.8000e+01, 1.0000e+00, 1.7500e+02, 1.1190e+03, 0.0000e+00], [0.0000e+00, 9.0000e+00, 1.0000e+00, 0.0000e+00, 8.0000e+00, 1.0000e+01, 0.0000e+00]]) fig , ax = plt . subplots ( figsize = ( 7 , 7 )) sns . heatmap ( cm / ( cm . astype ( np . float ) . sum ( axis = 1 ) + 1e-9 ), annot = False , ax = ax ) # labels, title and ticks ax . set_xlabel ( 'Predicted' , size = 25 ); ax . set_ylabel ( 'True' , size = 25 ); ax . set_title ( 'Confusion Matrix' , size = 25 ); ax . xaxis . set_ticklabels ([ 'akiec' , 'bcc' , 'bkl' , 'df' , 'mel' , 'nv' , 'vasc' ], size = 15 ); \\ ax . yaxis . set_ticklabels ([ 'akiec' , 'bcc' , 'bkl' , 'df' , 'mel' , 'nv' , 'vasc' ], size = 15 );","title":"Confusion Matrix"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/SkinLesionClassification/#grad-cam_1","text":"from collections import OrderedDict , Sequence class _BaseWrapper ( object ): \"\"\" Please modify forward() and backward() according to your task. \"\"\" def __init__ ( self , model ): super ( _BaseWrapper , self ) . __init__ () self . device = next ( model . parameters ()) . device self . model = model self . handlers = [] # a set of hook function handlers def _encode_one_hot ( self , ids ): one_hot = torch . zeros_like ( self . logits ) . to ( self . device ) one_hot . scatter_ ( 1 , ids , 1.0 ) return one_hot def forward ( self , image ): \"\"\" Simple classification \"\"\" self . model . zero_grad () self . logits = self . model ( image ) self . probs = F . softmax ( self . logits , dim = 1 ) return self . probs . sort ( dim = 1 , descending = True ) def backward ( self , ids ): \"\"\" Class-specific backpropagation Either way works: 1. self.logits.backward(gradient=one_hot, retain_graph=True) 2. (self.logits * one_hot).sum().backward(retain_graph=True) \"\"\" one_hot = self . _encode_one_hot ( ids ) self . logits . backward ( gradient = one_hot , retain_graph = True ) def generate ( self ): raise NotImplementedError def remove_hook ( self ): \"\"\" Remove all the forward/backward hook functions \"\"\" for handle in self . handlers : handle . remove () class GradCAM ( _BaseWrapper ): \"\"\" \"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\" https://arxiv.org/pdf/1610.02391.pdf Look at Figure 2 on page 4 \"\"\" def __init__ ( self , model , candidate_layers = None ): super ( GradCAM , self ) . __init__ ( model ) self . fmap_pool = OrderedDict () self . grad_pool = OrderedDict () self . candidate_layers = candidate_layers # list def forward_hook ( key ): def forward_hook_ ( module , input , output ): # Save featuremaps self . fmap_pool [ key ] = output . detach () return forward_hook_ def backward_hook ( key ): def backward_hook_ ( module , grad_in , grad_out ): # Save the gradients correspond to the featuremaps self . grad_pool [ key ] = grad_out [ 0 ] . detach () return backward_hook_ # If any candidates are not specified, the hook is registered to all the layers. for name , module in self . model . named_modules (): if self . candidate_layers is None or name in self . candidate_layers : self . handlers . append ( module . register_forward_hook ( forward_hook ( name ))) self . handlers . append ( module . register_backward_hook ( backward_hook ( name ))) def _find ( self , pool , target_layer ): if target_layer in pool . keys (): return pool [ target_layer ] else : raise ValueError ( \"Invalid layer name: {} \" . format ( target_layer )) def _compute_grad_weights ( self , grads ): return F . adaptive_avg_pool2d ( grads , 1 ) def forward ( self , image ): self . image_shape = image . shape [ 2 :] return super ( GradCAM , self ) . forward ( image ) def generate ( self , target_layer ): fmaps = self . _find ( self . fmap_pool , target_layer ) grads = self . _find ( self . grad_pool , target_layer ) weights = self . _compute_grad_weights ( grads ) gcam = torch . mul ( fmaps , weights ) . sum ( dim = 1 , keepdim = True ) gcam = F . relu ( gcam ) gcam = F . interpolate ( gcam , self . image_shape , mode = \"bilinear\" , align_corners = False ) B , C , H , W = gcam . shape gcam = gcam . view ( B , - 1 ) gcam -= gcam . min ( dim = 1 , keepdim = True )[ 0 ] gcam /= gcam . max ( dim = 1 , keepdim = True )[ 0 ] gcam = gcam . view ( B , C , H , W ) return gcam def demo2 ( image , label , model ): \"\"\" Generate Grad-CAM \"\"\" # Model model = model model . to ( device ) model . eval () # The layers target_layers = [ \"layer4\" ] target_class = label # Images images = image . unsqueeze ( 0 ) gcam = GradCAM ( model = model ) probs , ids = gcam . forward ( images ) ids_ = torch . LongTensor ([[ target_class ]] * len ( images )) . to ( device ) gcam . backward ( ids = ids_ ) for target_layer in target_layers : print ( \"Generating Grad-CAM @ {} \" . format ( target_layer )) # Grad-CAM regions = gcam . generate ( target_layer = target_layer ) for j in range ( len ( images )): print ( \" \\t # {} : {} ( {:.5f} )\" . format ( j , classes [ target_class ], float ( probs [ ids == target_class ]) ) ) gcam = regions [ j , 0 ] plt . imshow ( gcam . cpu ()) plt . show () image , label = next ( iter ( test_data_loader )) # Load the model model = net # Grad cam demo2 ( image [ 0 ] . to ( device ), label [ 0 ] . to ( device ), model ) image = np . transpose ( image [ 0 ], ( 1 , 2 , 0 )) image2 = np . add ( np . multiply ( image . numpy (), np . array ( norm_std )) , np . array ( norm_mean )) print ( \"True Class: \" , classes [ label [ 0 ] . cpu ()]) plt . imshow ( image ) plt . show () plt . imshow ( image2 ) plt . show () Generating Grad-CAM @layer4 #0: melanocytic nevi (0.76489) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). True Class: melanocytic nevi","title":"Grad cam"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/SkinLesionClassification/#conclusion","text":"Training a neural network can be a daunting task, especially for a beginner. Here, are some useful practices to get the best out of your network. Training Ensembles \u2014 Combine learning from multiple networks. Always go for a lower learning rate. In cases of limited data try better augmentation techniques[20]. Network architectures that have the appropriate depth for our problem \u2014 too many hyperparameters could lead to suboptimal results if we don\u2019t have enough images. Improving loss function and class balancing. In this tutorial we learned how to train a deep neural network for the challenging task of skin-lesion classification. We experimented with two network architectures and provided insights in the attention of the models. Additionally, we achieved 83% overall accuracy on HAM10000 and provided you with more tips and tricks to tackle overfitting and class imbalance. Now you have all the tools to not only beat our performance and participate in the exciting MICCAI Challenges, but to also solve many more medical imaging problems. Happy training!","title":"Conclusion"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/keras_google_colab/","text":"import numpy as np import tensorflow as tf from tensorflow import keras # Display from IPython.display import Image , display import matplotlib.pyplot as plt import matplotlib.cm as cm tf . keras . applications . vgg16 . VGG16 ( include_top = True , weights = 'imagenet' , input_tensor = None , input_shape = None , pooling = None , classes = 1000 , classifier_activation = 'softmax' ) . summary () Model: \"vgg16\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_3 (InputLayer) [(None, 224, 224, 3)] 0 block1_conv1 (Conv2D) (None, 224, 224, 64) 1792 block1_conv2 (Conv2D) (None, 224, 224, 64) 36928 block1_pool (MaxPooling2D) (None, 112, 112, 64) 0 block2_conv1 (Conv2D) (None, 112, 112, 128) 73856 block2_conv2 (Conv2D) (None, 112, 112, 128) 147584 block2_pool (MaxPooling2D) (None, 56, 56, 128) 0 block3_conv1 (Conv2D) (None, 56, 56, 256) 295168 block3_conv2 (Conv2D) (None, 56, 56, 256) 590080 block3_conv3 (Conv2D) (None, 56, 56, 256) 590080 block3_pool (MaxPooling2D) (None, 28, 28, 256) 0 block4_conv1 (Conv2D) (None, 28, 28, 512) 1180160 block4_conv2 (Conv2D) (None, 28, 28, 512) 2359808 block4_conv3 (Conv2D) (None, 28, 28, 512) 2359808 block4_pool (MaxPooling2D) (None, 14, 14, 512) 0 block5_conv1 (Conv2D) (None, 14, 14, 512) 2359808 block5_conv2 (Conv2D) (None, 14, 14, 512) 2359808 block5_conv3 (Conv2D) (None, 14, 14, 512) 2359808 block5_pool (MaxPooling2D) (None, 7, 7, 512) 0 flatten (Flatten) (None, 25088) 0 fc1 (Dense) (None, 4096) 102764544 fc2 (Dense) (None, 4096) 16781312 predictions (Dense) (None, 1000) 4097000 ================================================================= Total params: 138,357,544 Trainable params: 138,357,544 Non-trainable params: 0 _________________________________________________________________ def get_img_array ( img_path , size ): # `img` is a PIL image of size 224x224 img = keras . preprocessing . image . load_img ( img_path , target_size = size ) # `array` is a float32 Numpy array of shape (224, 224, 3) array = keras . preprocessing . image . img_to_array ( img ) # We add a dimension to transform our array into a \"batch\" # of size (1, 224, 224, 3) array = np . expand_dims ( array , axis = 0 ) return array def make_gradcam_heatmap ( img_array , model , last_conv_layer_name , target_category = None ): # First, we create a model that maps the input image to the activations # of the last conv layer as well as the output predictions. # In other words, this is like the forward hook of torch. grad_model = tf . keras . models . Model ( [ model . inputs ], [ model . get_layer ( last_conv_layer_name ) . output , model . output ], ) # Then, we compute the gradient of the top predicted class for our input image # with respect to the activations of the last conv layer with tf . GradientTape () as tape : # last_conv_layer_output: Note this is merely the output when the inputs propagate to the last conv layer. # i.e. this is the output of the last conv layer. and is not flattened yet last_conv_layer_output , y_logits = grad_model ( img_array ) print ( y_logits . shape ) # last_conv_layer_output: shape = (1, 14, 14, 512) # which is 512 number of filters of 14 x 14 filters. # print(last_conv_layer_output.shape) # print(y_logits[:, 386]) should be the same as what we had earlier! if target_category is None : target_category = tf . argmax ( y_logits [ 0 ]) target_category_logits = y_logits [:, target_category ] print ( target_category_logits ) # This is the gradient of the output neuron (top predicted or chosen) # with regard to the output feature map of the last conv layer grads = tape . gradient ( target_category_logits , last_conv_layer_output ) # print(grads.shape) same shape as last conv layer! (1, 14, 14, 512) # do not confuse gradient of loss fn with respect to inputs # we are talking about gradient of y_c with respect to feature maps (not the raw image input array), note the distinction. # so we r checking the rate of change of y_c wrt the feature maps # we are checking the rate of change of elephant wrt to elephants feature maps # This is a vector where each entry is the mean intensity of the gradient # over a specific feature map channel pooled_grads = tf . reduce_mean ( grads , axis = ( 0 , 1 , 2 )) # print(pooled_grads.shape) # (512,) # by hn grads_transposed = np . transpose ( np . squeeze ( grads , axis = 0 ), axes = None ) # print(grads_transposed.shape) for feature_map in grads_transposed : mean_feature_map = np . mean ( feature_map ) print ( mean_feature_map ) # same as print(pooled_grads[0]) print ( pooled_grads [ 0 ]) break # one should readily understand # We multiply each channel in the feature map array # by \"how important this channel is\" with regard to the top predicted class # then sum all the channels to obtain the heatmap class activation last_conv_layer_output = last_conv_layer_output [ 0 ] # this is just squeeze dimension from (1,14,14,512) to (14,14,512) print ( last_conv_layer_output . shape ) # in Python \"...\" means \"all dimensions prior to\" add new axis. # print(pooled_grads[..., tf.newaxis].shape) # (512, ) to (512, 1) print ( last_conv_layer_output ) heatmap = ( last_conv_layer_output @ pooled_grads [ ... , tf . newaxis ] ) # same as heatmap = last_conv_layer_output @ tf.expand_dims(pooled_grads, axis=-1) print ( heatmap . shape ) # (14, 14, 1) heatmap = tf . squeeze ( heatmap ) # (14, 14) # For visualization purpose, we will also normalize the heatmap between 0 & 1 RELU OPS heatmap = tf . maximum ( heatmap , 0 ) / tf . math . reduce_max ( heatmap ) return heatmap . numpy () model_builder = keras . applications . vgg16 . VGG16 img_size = ( 224 , 224 ) preprocess_input = keras . applications . vgg16 . preprocess_input decode_predictions = keras . applications . vgg16 . decode_predictions last_conv_layer_name = \"block5_conv3\" # The local path to our target image img_path = keras . utils . get_file ( \"african_elephant.jpg\" , \"https://i.imgur.com/Bvro0YD.png\" ) # Prepare image img_array = preprocess_input ( get_img_array ( img_path , size = img_size )) # Make model model = model_builder ( weights = \"imagenet\" ) # Remove last layer's softmax model . layers [ - 1 ] . activation = None # Print what the top predicted class is. # preds has a shape of (1, 1000) preds = model . predict ( img_array ) # if you print argmax along column, or simply put # argmax of all logits in the 1000 predictions, we have 386 the elephant as index print ( f \"preds argmax: { preds . argmax ( axis = 1 ) } \\n preds value: { preds [:, preds . argmax ( axis = 1 )] } \" ) # !!! # Important is we did not activate softmax here, in fact, since softmax is monotonic, when pre-softmax, the logits output will tell us already which class is most probable! # means to say even we did not make the logits in between 0 and 1, the ranking is preserved in the sense that values in logits the highest is the most probable when transformed by softmax # decode predictions - this is a convenient function from keras print ( \"Predicted:\" , decode_predictions ( preds , top = 1 )[ 0 ]) preds argmax: [386] preds value: [[23.632017]] Predicted: [('n02504458', 'African_elephant', 23.632017)] # Generate class activation heatmap heatmap = make_gradcam_heatmap ( img_array , model , last_conv_layer_name ) (1, 1000) tf.Tensor([23.632017], shape=(1,), dtype=float32) -0.00016475434 tf.Tensor(-0.00016475434, shape=(), dtype=float32) (14, 14, 512) tf.Tensor( [[[0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] ... [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.]] [[0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] ... [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.]] [[0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] ... [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.]] ... [[0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] ... [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.]] [[0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] ... [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.]] [[0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] ... [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.]]], shape=(14, 14, 512), dtype=float32) (14, 14, 1) # Display heatmap plt . matshow ( heatmap ) plt . show () def save_and_display_gradcam ( img_path , heatmap , cam_path = \"cam.jpg\" , alpha = 0.4 ): # Load the original image img = keras . preprocessing . image . load_img ( img_path ) img = keras . preprocessing . image . img_to_array ( img ) # Rescale heatmap to a range 0-255 heatmap = np . uint8 ( 255 * heatmap ) # Use jet colormap to colorize heatmap jet = cm . get_cmap ( \"jet\" ) # Use RGB values of the colormap jet_colors = jet ( np . arange ( 256 ))[:, : 3 ] jet_heatmap = jet_colors [ heatmap ] # Create an image with RGB colorized heatmap jet_heatmap = keras . preprocessing . image . array_to_img ( jet_heatmap ) jet_heatmap = jet_heatmap . resize (( img . shape [ 1 ], img . shape [ 0 ])) jet_heatmap = keras . preprocessing . image . img_to_array ( jet_heatmap ) # Superimpose the heatmap on original image superimposed_img = jet_heatmap * alpha + img superimposed_img = keras . preprocessing . image . array_to_img ( superimposed_img ) # Save the superimposed image superimposed_img . save ( cam_path ) # Display Grad CAM display ( Image ( cam_path )) save_and_display_gradcam ( img_path , heatmap ) img_path = keras . utils . get_file ( \"cat_and_dog.jpg\" , \"https://storage.googleapis.com/petbacker/images/blog/2017/dog-and-cat-cover.jpg\" , ) display ( Image ( img_path )) # Prepare image img_array = preprocess_input ( get_img_array ( img_path , size = img_size )) # Print what the two top predicted classes are preds = model . predict ( img_array ) print ( \"Predicted:\" , decode_predictions ( preds , top = 2 )[ 0 ]) Downloading data from https://storage.googleapis.com/petbacker/images/blog/2017/dog-and-cat-cover.jpg 73728/72452 [==============================] - 0s 0us/step 81920/72452 [=================================] - 0s 0us/step Predicted: [('n02106030', 'collie', 8.145561), ('n02110806', 'basenji', 7.812906)] heatmap = make_gradcam_heatmap ( img_array , model , last_conv_layer_name , pred_index = 260 ) save_and_display_gradcam ( img_path , heatmap ) heatmap = make_gradcam_heatmap ( img_array , model , last_conv_layer_name , pred_index = 285 ) save_and_display_gradcam ( img_path , heatmap )","title":"Keras google colab"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/","text":"A journey into Convolutional Neural Network visualization Francesco Saverio Zuppichini There is one famous urban legend about computer vision. Around the 80s, the US military wanted to use neural networks to automatically detect camouflaged enemy tanks. They took a number of pictures of trees without tanks and then pictures with the same trees with tanks behind them. The results were impressive. So impressive that the army wanted to be sure the net had correctly generalized. They took new pictures of woods with and without tanks and they showed them again to the network. This time, the model performed terribly, it was not able to discriminate between pictures with tanks behind woods and just trees.It turned out that all the pictures without tanks were taken on a cloudy day while the ones with tanks on a sunny day! In reality, the network learns to recognize the weather, not the enemy tanks. The source code can be found here . This article is also available as an interactive jupyter notebook Nosce te ipsum With this article, we are going to see different techniques to understand what it is going on inside a Convolutional Neural Network to avoid making the same US' army mistake. We are going to use Pytorch . All the code can be found here . Most of the visualizations were developed from scratch, however, some inspiration and parts were taken from here . We will first introduce each technique by briefly explain it and making some example and comparison between different classic computer vision models, alexnet , vgg16 and resnet . Then we will try to better understand a model used in robotics to predict the local distance sensor using only the frontal camera's images. Our goal is not to explain in detail how each technique works since this is already done extremely well by each paper, but to use them to help the reader visualize different model with different inputs to better understand and highlight what and how different models react to a given input. Later on, we show a workflow in which we utilize some of the techniques you will learn in this journey to test the robustness of a model, this is extremely useful to understand and fix its limitations. The curious reader could further improve is understand by looking and the source code for each visulization and by reading the references. Preambula Disclaimer I am not a fan of jupyter. So apologize in advance if there are some warnings in the outputs and some figures are not well made Let's start our journey by selecting a network. Our first model will be the old school alexnet . It is already available in the torchvision.models package from Pytorch % load_ext autoreload % autoreload 2 from torchvision.models import * from visualization.core.utils import device model = alexnet ( pretrained = True ) . to ( device ) print ( model ) AlexNet( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)) (1): ReLU(inplace) (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (4): ReLU(inplace) (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): ReLU(inplace) (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (9): ReLU(inplace) (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace) (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) ) (classifier): Sequential( (0): Dropout(p=0.5) (1): Linear(in_features=9216, out_features=4096, bias=True) (2): ReLU(inplace) (3): Dropout(p=0.5) (4): Linear(in_features=4096, out_features=4096, bias=True) (5): ReLU(inplace) (6): Linear(in_features=4096, out_features=1000, bias=True) ) ) Now we need some inputs # %matplotlib notebook Now we need some inputs images. We are going to use three pictures, a cat, the beautiful Basilica di San Pietro and an image with a dog and a cat. import glob import matplotlib.pyplot as plt import numpy as np import torch from utils import * from PIL import Image plt . rcParams [ \"figure.figsize\" ] = 16 , 8 We loaded a few packages. In utils there are several utility function to creates the plots. import glob import matplotlib.pyplot as plt import numpy as np from visualization.core.utils import device from PIL import Image image_paths = glob . glob ( './images/*.*' ) images = list ( map ( lambda x : Image . open ( x ), image_paths )) subplot ( images , title = 'inputs' , rows_titles = [ 'cat' , 'san pietro' , 'dog_cat' ], nrows = 1 , ncols = 3 ) Since all of our models were trained on imagenet , a huge dataset with 1000 different classes, we need to parse and normalize them. In Pytorch, we have to manually send the data to a device. In this case the device if the fist gpu if you have one, otherwise cpu is selected. Be aware that jupyter have not a garbage collected so we will need to manually free the gpu memory. from torchvision.transforms import ToTensor , Resize , Compose , ToPILImage from visualization.core import * from visualization.core.utils import image_net_preprocessing inputs = [ Compose ([ Resize (( 224 , 224 )), ToTensor (), image_net_preprocessing ])( x ) . unsqueeze ( 0 ) for x in images ] # add 1 dim for batch inputs = [ i . to ( device ) for i in inputs ] We also define an utility function to clean the gpu cache def free ( modules ): for m in modules : del m torch . cuda . empty_cache () As we said, imagenet is a huge dataset with 1000 classes, represented by an integer not very human interpretable. We can associate each class id to its label by loading the imaganet2human.txt and create a python dictionary. imagenet2human = {} with open ( 'imaganet2human.txt' ) as f : for line in f . readlines (): key , value = line . split ( ':' ) key = key . replace ( '{' , '' ) . replace ( '}' , '' ) # I forget how regex works :) value = value . replace ( \"'\" , '' ) . replace ( \",\" , '' ) imagenet2human [ int ( key . strip ())] = str ( value . strip ()) list ( imagenet2human . items ())[: 2 ] [(0, 'tench Tinca tinca'), (1, 'goldfish Carassius auratus')] Weights Visualization The first straightforward visualization is to just plot the weights of a target Layer. Obviously, the deeper we go the smaller each image becomes while the channels number increases. We are going to show each channel as a grey array image. Unfortunately, each Pytorch module can be nested and nested, so to make our code as general as possible we first need to trace each sub-module that the input traverse and then store each layer in order. We first need to trace our model to get a list of all the layers so we can select a target layer without following the nested structure of a model. In PyTorch models can be infinitely nested. In other words, we are flattering the model's layers, this is implemented in the module2traced function. model_traced = module2traced ( model , inputs [ 0 ]) model_traced [Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)), ReLU(inplace), MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)), ReLU(inplace), MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace), Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace), MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False), Dropout(p=0.5), Linear(in_features=9216, out_features=4096, bias=True), ReLU(inplace), Dropout(p=0.5), Linear(in_features=4096, out_features=4096, bias=True), ReLU(inplace), Linear(in_features=4096, out_features=1000, bias=True)] Let's plot the first layer's weight. We also print the shape of the weight to give a correct idea to the reader of the dimensional reduction. vis = Weights ( model , device ) first_layer = model_traced [ 0 ] plt . rcParams [ \"figure.figsize\" ] = 16 , 16 run_vis_plot ( vis , inputs [ 0 ], first_layer , ncols = 4 , nrows = 4 ) torch.Size([1, 55, 55]) Let's stop for a minute to explain what those images represent. We traced the input through the computational graph in order to find out all the layers of our models, in this case, alexnet . Then we instantiate the Weights class implemented in visualization.core and we call it by passing the current input, the cat image and a target layer . As outputs, we get all the current layer's weights as grey images. Then, we plot 16 of them. We can notice that they, in some way, makes sense; for example, some pixels are brighter in the edges of the images. Let's plot the first MaxPool layer to better see this effect, dimensional reduction and higher brightness pixels in some interesting areas. If you are wondering what the maxpolling operations is doing, check this awesome repo first_maxpool_layer = model_traced [ 2 ] run_vis_plot ( vis , inputs [ 0 ], first_maxpool_layer , ncols = 4 , nrows = 4 ) torch.Size([1, 27, 27]) Let's try with an other input, the San Pietro Basilica run_vis_plot ( vis , inputs [ 1 ], first_maxpool_layer , ncols = 4 , nrows = 4 ) torch.Size([1, 27, 27]) By looking at them, these images make somehow sense; they highlight the basilica layout but it is hard to understand what the model is actually doing. We got the idea that is computing something correctly but we could ask some questions, for example: is it looking at the cupola? Which are the most important features of the Basilica? Moreover, the deeper we go the harder it becomes to even recognize the input. deeper_layer = model_traced [ 6 ] run_vis_plot ( vis , inputs [ 1 ], deeper_layer , ncols = 4 , nrows = 4 ) torch.Size([1, 13, 13]) In this case, we have no idea of what is going on. It can be argued that weights visualization does not carry any useful information about the model, even if this is almost true, there is one nice reason of plotting the weights especially at the first layer. When a model is poorly trained or not trained at all, the first weights have lots of noise, since they are just randomly initialized, and they are a lot more similar to the inputs images than the trained ones. This feature can be useful to understand on the fly is a model is trained or not. However, except for this, weights visualization is not the way to go to understand what your black box is thinking. Below we plot the first layer's weight first for the untrainded version of alexnet and the for the trained one. alexnet_not_pretrained = alexnet ( pretrained = False ) . to ( device ) run_vis_plot ( Weights ( alexnet_not_pretrained , device ), inputs [ 0 ], module2traced ( alexnet_not_pretrained , inputs [ 0 ])[ 0 ], ncols = 4 , nrows = 4 ) alexnet_pretrained = alexnet ( pretrained = True ) . to ( device ) run_vis_plot ( Weights ( alexnet_pretrained , device ), inputs [ 0 ], module2traced ( alexnet_pretrained , inputs [ 0 ])[ 0 ], ncols = 4 , nrows = 4 ) del alexnet_not_pretrained torch.Size([1, 55, 55]) torch.Size([1, 55, 55]) You can notice that in the first image is simpler to see the input image. Hoewer, this is not a general rule, but in some cases it can help. Similarities with other models We have seen alexnet 's weights, but are they similar across models? Below we plot the first 4 channel of each first layer's weight for alexnet , vgg and resnet modules_instances = [ alexnet , vgg16 , resnet34 ] modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], 0 , Weights , 'weights' , device , ncols = 4 ) free ( modules ) The resnet and vgg weights looks more similar to the input images than alexnet . But, again, what does it mean? Remember that at least resnet is initialized in a different way than the other two models. Saliency visualization One idea proposed by Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps is to back-prop the output of the network with respect to a target class until the input and plot the computed gradient. This will highligh the part of the image responsible for that class. Let's start with alexnet. Let's first print the prediction of the network (this could change if you re-run the cell) model . eval () pred = model ( inputs [ 0 ]) _ , id = torch . max ( pred , 1 ) print ( 'predicted class {} ' . format ( imagenet2human [ id . item ()])) predicted class tiger cat Each visualization is implemented in its own class. You can find the code here . It will backproprop the output with respect to the one hot encoding representation of the number corresponding to class tiger cat from visualization.core.utils import image_net_postprocessing model . eval () model = model . to ( device ) vis = SaliencyMap ( model , device ) out , info = vis ( inputs [ 0 ], first_layer ) subplot ([ image_net_postprocessing ( inputs [ 0 ] . squeeze () . cpu ()), out ], rows_titles = [ 'original' , 'saliency map' ], parse = tensor2img , nrows = 1 , ncols = 2 ) We can see that alexnet gets exited on the cat. We can even do better! We can set to 0 each negative relu gradient when backprop. This is techinique is called guided . out , info = vis ( inputs [ 0 ], first_layer , guide = True ) subplot ([ image_net_postprocessing ( inputs [ 0 ] . squeeze () . cpu ()), out ], rows_titles = [ 'original' , 'guided saliency map' ], parse = tensor2img , nrows = 1 , ncols = 2 ) Now we can clearly see that the network is looking at the eyes and the nose of the cat. We can try to compare different models modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], 0 , SaliencyMap , 'Saliency' , device , nrows = 1 , ncols = 3 , target_class = 231 , guide = True ) free ( modules ) Alextnet seems more interested to the eyes, while VGG looks at the ears and resnet is similar to alexnet . Now we can clearly understand which part of the inputs help the network gives that prediction. While guiding yields a better human interpretable image, the vanilla implementation can be used for localizing an object of interest. In other words, we can find object of interest for free by cropping out of the input image the region corresponding to the gradient. Let's plot each input image for each model. modules_instances = [ alexnet , vgg16 , resnet34 , resnet152 ] modules = ( m ( pretrained = True ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], None , SaliencyMap , 'SaliencyMap' , device , nrows = 4 , ncols = 3 , inputs = inputs , idx2label = imagenet2human , annotations = [ 'alexnet' , 'vgg16' , 'resnet34' , 'resnet152' ], guide = True ) free ( modules ) The Basilica is very interesting, all four networks correctly classify it as a dome but only resnet152 is more interested in the sky than on the cupola. In the last column, we have an image with two classes, dog and cat . All the networks highlighted booths, like the eyes of the dog and the ears of the cat in vgg16 . What if we would like to discover only the region of the inputs that are related to a specific class? With this technique is impossible. Class Activation Mapping Class Activation Mapping is a techniques presented in Learning Deep Features for Discriminative Localization . The idea is to use the last convolutional layer output and the neurons in the linear layer of the model responsable for a target class, the map is generated by taking the dot product of those. However, to make this work the model has to have some constrains. First of all, the output from the convolution must first go trought an global average polling and it requires feature maps to directly precede softmax layers. To make it works with other architecture, such as alexnet and vgg we have to change some layers in the model and retrain it. This is a major drawback that will be solved with the next section. For now, we can use it for free with resnet! Since its architecture is perfect. The implementation can be found here . We can pass to the visualization a target_class parameter to get the relative weights from the fc layer. Notice that by changing the target class, we can see different part of the image highlighted. The first image uses the prediction class, while the second an other type of cat and the last one bookcase , just to see what the model will do with a wrong class. from visualization.core.utils import imshow # we are using resnet 34 since the model has only one fc layer before the softmax and it is preceded by av avg pool # as required from the paper module = resnet34 ( True ) . to ( device ) module . eval () vis = ClassActivationMapping ( module , device ) classes = [ None , 285 , 453 ] def vis_outs2images_classes ( outs ): images = [ x [ 0 ] for x in outs ] classes = [ imagenet2human [ int ( x [ 1 ][ 'prediction' ])] for x in outs ] return images , classes outs = [ vis ( inputs [ 0 ], None , postprocessing = image_net_postprocessing , target_class = c , guide = True ) for c in classes ] images , classes = vis_outs2images_classes ( outs ) subplot ( images , rows_titles = classes , nrows = 1 , ncols = 3 , parse = tensor2img ) It makes sense, the only thing is that in the last row we still have some part of the cat highlighted for bookcase Let's plot the CAM on the cat images for different resnet architecture. For resnet > 34 the Bottleneck module is used modules_instances = [ resnet18 , resnet34 , resnet101 , resnet152 ] cat = inputs [ 2 ] modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , cat , None , ClassActivationMapping , 'ClassActivationMapping' , device , nrows = len ( modules_instances ), ncols = 1 , postprocessing = image_net_postprocessing , rows_name = [ 'resnet18' , 'resnet34' , 'resnet101' , 'resnet152' ], target_class = None ) free ( modules ) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). They are all very similar as expected. One big drawback of this technique is that force you to use a network with a specific architecture, global polling before the decoder part. The next technique generalize this approach by taking advantage of the gradient at one specific layer. Remember that with the class activation we are using the weights of the feature map as a scaling factor for the channels of the last layer. The features map must be before a softmax layer and right after the average pooling. The next technique propose a more general approach. Grad Cam Grad Cam was introduced by Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization . The idea is actually simple, we backprop the output with respect to a target class while storing the gradient and the output at a given layer, in our case the last convolution. Then we perform a global average of the saved gradient keeping the channel dimension in order to get a 1-d tensor, this will represent the importance of each channel in the target convolutional layer. We then multiply each element of the convolutional layer outputs by the averaged gradients to create the grad cam. This whole procedure is fast and it is architecture independent. Interesting, the authors show that is a generalization of the previous technique. The code is here We can use it to higlight what different models are looking at. modules_instances = [ alexnet , vgg16 , resnet34 , resnet152 ] modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], None , GradCam , 'Gradcam' , device , nrows = 1 , ncols = 4 , target_class = None , postprocessing = image_net_postprocessing ) free ( modules ) It is really interesting to see how alexnet looks at the nose, while vgg at the ears and resnet at the whole cat. It is interesting to see that the two resnet version looks at different part of the cat. Below we plot the same input for resnet34 but we change the target class in each column to show the reader how the grad cam change accordingly. from visualization.core.utils import imshow module = module . to ( device ) vis = GradCam ( module , device ) classes = [ None , 285 , 453 ] outs = [ vis ( inputs [ 0 ], None , postprocessing = image_net_postprocessing , target_class = c ) for c in classes ] images , classes = vis_outs2images_classes ( outs ) subplot ( images , title = 'resnet34' , rows_titles = classes , nrows = 1 , ncols = len ( outs ), parse = tensor2img ) Notice how similar to the CAM output they are. To better compore our three models, below we plot the grad cam for each input with respect to each model modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], None , GradCam , 'Gradcam' , device , nrows = 4 , ncols = 3 , target_class = None , inputs = inputs , idx2label = imagenet2human , annotations = [ 'alexnet' , 'vgg16' , 'resnet34' , 'resnet152' ], postprocessing = image_net_postprocessing ) free ( modules ) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). The reader can immediately notice the difference across the models. Interesting region We talk before about interesting region localizations. Grad-cam can be also used to extract the class object out of the image. Easily, once the have the grad-cam image we can used it as mask to crop out form the input image what we want. The reader can play with the TR parameter to see different effects. TR = 0.3 alexnet_pretrained . eval () vis = GradCam ( alexnet_pretrained , device ) _ = vis ( inputs [ 0 ], None , postprocessing = image_net_postprocessing ) import cv2 def gradcam2crop ( cam , original_img ): b , c , w , h = inputs [ 0 ] . shape cam = cam . numpy () cam -= np . min ( cam ) cam /= np . max ( cam ) cam = cv2 . resize ( cam , ( w , h )) mask = cam > TR original_img = tensor2img ( image_net_postprocessing ( original_img [ 0 ] . squeeze ())) crop = original_img . copy () crop [ mask == 0 ] = 0 return crop crop = gradcam2crop ( vis . cam . cpu (), inputs [ 0 ] . cpu ()) fig = plt . figure () plt . imshow ( crop ) <matplotlib.image.AxesImage at 0x7f79debee048> et voil\u00e0 ! We can also change again class, and crop the interest region for that class. _ = vis ( inputs [ 0 ], None , target_class = 231 , postprocessing = image_net_postprocessing ) crop = gradcam2crop ( vis . cam . cpu (), inputs [ 0 ] . cpu ()) fig = plt . figure () plt . imshow ( crop ) <matplotlib.image.AxesImage at 0x7f79f40c4c18> Different models We have seen all these techniques used with classic classicification models trained on imagenet . What about use them on a different domain? I have ported this paper to Pytorch and retrain it. The model learn from the frontal camera's image of a robot to predict the local distance sensors in order to avoid obstacles. Let's see what if, by using those techniques, we can understand better what is going on inside the model. Learning Long-range Perception using Self-Supervision from Short-Range Sensors and Odometry The idea is to predict the future outputs of a short-range sensor (such as a proximity sensor) given the current outputs of a long-range sensor (such as a camera). They trained a very simple CNN from the robot's camera images to predict the proximity sensor values. If you are interested in their work, you can read the full paper here I have made a PyTorch implementation and retrain the model from scratch. Be awere that I did not fine-tune or try different sets of hyper-parameters so probably my model is not performing as well as the author's one. Let's import it from os import path LONG_RANGE_PERCEPTION_PATH = path . abspath ( './models/long_range_perception/model.pt' ) from models.long_range_perception.model import SimpleCNN from models.long_range_perception.utils import get_dl , H5_PATH , imshow , post_processing , pre_processing , MODEL_PATH free ([ module ]) module = torch . load ( LONG_RANGE_PERCEPTION_PATH , map_location = lambda storage , loc : storage ) module = module . to ( device ) module /home/francesco/Documents/A-journey-into-Convolutional-Neural-Network-visualization-/model.pt SimpleCNN( (encoder): Sequential( (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): ReLU() (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (6): Conv2d(10, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): ReLU() (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (decoder): Sequential( (0): Dropout(p=0.2) (1): Linear(in_features=640, out_features=256, bias=True) (2): ReLU() (3): Linear(in_features=256, out_features=325, bias=True) (4): Sigmoid() ) ) We know need some inputs to test the model, they are taken directly from the test set import os def make_and_show_inputs ( path , transform ): image_paths = glob . glob ( path ) image_paths = filter ( lambda x : os . path . isfile ( x ), image_paths ) images = list ( map ( lambda x : Image . open ( x ) . convert ( 'RGB' ), image_paths )) subplot ( images , title = 'inputs' , rows_titles = [ '1' , '2' , '3' , '4' ], nrows = 1 , ncols = 4 ) plt . show () inputs = [ pre_processing ( x ) . unsqueeze ( 0 ) . to ( device ) for x in images ] # add 1 dim for batch subplot ( inputs , parse = tensor2img , title = 'inputs' , rows_titles = [ '1' , '2' , '3' , '4' ], nrows = 1 , ncols = 4 ) return images , inputs images , inputs = make_and_show_inputs ( 'images/long_range_perception/*' , pre_processing ) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Then author normalize each image, this is done by callind pre_processing . For some reason the inpupts images are different on mac and ubuntu, they should not be like these if you run the notebook on mac the result is different. This is probably due to the warning message. We are going to use the SaliencyMap and the GradCam since those are the best from torch.autograd import Variable module . eval () def run_long_range_vis (): grad = GradCam ( module , device ) all_true = torch . ones ( 1 , 65 * 5 ) . float () . to ( device ) outs_grad = [ grad ( input , None , target_class = all_true , postprocessing = post_processing , regression = True )[ 0 ] for input in inputs ] sal = SaliencyMap ( module , device ) outs_saliency = [ sal ( input , None , guide = True , target_class = all_true , regression = True )[ 0 ] for input in inputs ] subplot ([ * outs_grad , * outs_saliency ], title = 'long_range' , cols_titles = [ '1' , '2' , '3' , '4' ], nrows = 2 , ncols = 4 , parse = tensor2img ) run_long_range_vis () We can clearly see that the model looks at the objects. In the GradCam row, on the second picture, the plan is basically segmented by the heatmap. There is one problem, if you look at the third picture, the white box in front of the camera is not clearly highlighted. This is probably due to the white color of the floor that is very similar to the box's color. Let's investigate this problem. In the second row, the SaliencyMaps highlights all the objects, including the white box. The reader can notice that the reflection in the first picture on the left seems to excite the network in that region. We should also investigate this case but due to time limitations, we will leave it as an exercise for the curious reader. For completeness, let's also print the predicted sensor output. The model tries to predict five frontal distance sensors give the image camera. import seaborn as sns module . eval () preds = module ( torch . stack ( inputs ) . squeeze ( 1 )) fig = plt . figure () sns . heatmap ( preds [ 2 ] . view ( - 1 , 5 ) . detach () . cpu () . numpy ()) <matplotlib.axes._subplots.AxesSubplot at 0x7f79d26dc240> If you compare with the authors pictures, my prediction are worse. This is due to the fact that to speed up everything I did not used all the training set and I did not perform any hyper paramater optimisation. All the code con be found here . Let's now investigate the first problem, object with a similar color to the ground. Similar colors To test if the model has a problem with obstacles with a the same color of the ground, we created in blender four different scenarios with an obstacle. They are showed in the picture below. image_paths = [ * sorted ( glob . glob ( 'images/long_range_perception/equal_color/*' )), * sorted ( glob . glob ( 'images/long_range_perception/different_color/*' ))] image_paths = filter ( lambda x : os . path . isfile ( x ), image_paths ) images = list ( map ( lambda x : Image . open ( x ) . convert ( 'RGB' ), image_paths )) subplot ( images , title = 'inputs' , nrows = 2 , ncols = 4 ) plt . show () There are four different lights configuration and two differents cube colors, one equal to the ground and the second different. The first column represents a realistic situation, while the second has a really strong light from behind that generates a shadow in front of the camera. The third column has a shadow on the left and the last one has a little shadow on the left. This is a perfect scenario to use gradcam to see what the model is looking in each image. In the picture below we plotted the gradcam results. inputs = [ pre_processing ( x ) . unsqueeze ( 0 ) . to ( device ) for x in images ] # add 1 dim for batch run_long_range_vis () The big black shadow in the second column definitly confuses the model. In the first and last column, the grad cam highlights better the corners of the red cube, especially in the first picture. We can definitely say that this model has some hard time with the object of the same colour as the ground. Thanks to this consideration, we could improve the number equal object/ground in the dataset, perform a better preprocessing, change the model structure etc and hopefully increase the robustness of the network. Conclusion In this article, we present different convolutional neural network visualization techniques. In the first section, we introduced each one by applying to a set of famous classification networks. We compared different networks on different inputs and highlight the similarities and difference between them. Then we apply them to a model adopted in robotics to test its robustness and we were able to successfully reveal a problem in the network. Moreover, as a side project, I developed an interactive convolutional neural network visualization application called mirro that receives in just a few days more than a hundred stars on GitHub reflecting the interest of the deep learning community on this topic. All these visualizations are implemented using a common interface and there are available as python module so they can be used in any other module. Thank you for reading Francesco Saverio Zuppichini","title":"A journey into Convolutional Neural Network visualization"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/#a-journey-into-convolutional-neural-network-visualization","text":"Francesco Saverio Zuppichini There is one famous urban legend about computer vision. Around the 80s, the US military wanted to use neural networks to automatically detect camouflaged enemy tanks. They took a number of pictures of trees without tanks and then pictures with the same trees with tanks behind them. The results were impressive. So impressive that the army wanted to be sure the net had correctly generalized. They took new pictures of woods with and without tanks and they showed them again to the network. This time, the model performed terribly, it was not able to discriminate between pictures with tanks behind woods and just trees.It turned out that all the pictures without tanks were taken on a cloudy day while the ones with tanks on a sunny day! In reality, the network learns to recognize the weather, not the enemy tanks. The source code can be found here . This article is also available as an interactive jupyter notebook","title":"A journey into Convolutional Neural Network visualization"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/#nosce-te-ipsum","text":"With this article, we are going to see different techniques to understand what it is going on inside a Convolutional Neural Network to avoid making the same US' army mistake. We are going to use Pytorch . All the code can be found here . Most of the visualizations were developed from scratch, however, some inspiration and parts were taken from here . We will first introduce each technique by briefly explain it and making some example and comparison between different classic computer vision models, alexnet , vgg16 and resnet . Then we will try to better understand a model used in robotics to predict the local distance sensor using only the frontal camera's images. Our goal is not to explain in detail how each technique works since this is already done extremely well by each paper, but to use them to help the reader visualize different model with different inputs to better understand and highlight what and how different models react to a given input. Later on, we show a workflow in which we utilize some of the techniques you will learn in this journey to test the robustness of a model, this is extremely useful to understand and fix its limitations. The curious reader could further improve is understand by looking and the source code for each visulization and by reading the references.","title":"Nosce te ipsum"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/#preambula","text":"Disclaimer I am not a fan of jupyter. So apologize in advance if there are some warnings in the outputs and some figures are not well made Let's start our journey by selecting a network. Our first model will be the old school alexnet . It is already available in the torchvision.models package from Pytorch % load_ext autoreload % autoreload 2 from torchvision.models import * from visualization.core.utils import device model = alexnet ( pretrained = True ) . to ( device ) print ( model ) AlexNet( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)) (1): ReLU(inplace) (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (4): ReLU(inplace) (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): ReLU(inplace) (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (9): ReLU(inplace) (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace) (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) ) (classifier): Sequential( (0): Dropout(p=0.5) (1): Linear(in_features=9216, out_features=4096, bias=True) (2): ReLU(inplace) (3): Dropout(p=0.5) (4): Linear(in_features=4096, out_features=4096, bias=True) (5): ReLU(inplace) (6): Linear(in_features=4096, out_features=1000, bias=True) ) ) Now we need some inputs # %matplotlib notebook Now we need some inputs images. We are going to use three pictures, a cat, the beautiful Basilica di San Pietro and an image with a dog and a cat. import glob import matplotlib.pyplot as plt import numpy as np import torch from utils import * from PIL import Image plt . rcParams [ \"figure.figsize\" ] = 16 , 8 We loaded a few packages. In utils there are several utility function to creates the plots. import glob import matplotlib.pyplot as plt import numpy as np from visualization.core.utils import device from PIL import Image image_paths = glob . glob ( './images/*.*' ) images = list ( map ( lambda x : Image . open ( x ), image_paths )) subplot ( images , title = 'inputs' , rows_titles = [ 'cat' , 'san pietro' , 'dog_cat' ], nrows = 1 , ncols = 3 ) Since all of our models were trained on imagenet , a huge dataset with 1000 different classes, we need to parse and normalize them. In Pytorch, we have to manually send the data to a device. In this case the device if the fist gpu if you have one, otherwise cpu is selected. Be aware that jupyter have not a garbage collected so we will need to manually free the gpu memory. from torchvision.transforms import ToTensor , Resize , Compose , ToPILImage from visualization.core import * from visualization.core.utils import image_net_preprocessing inputs = [ Compose ([ Resize (( 224 , 224 )), ToTensor (), image_net_preprocessing ])( x ) . unsqueeze ( 0 ) for x in images ] # add 1 dim for batch inputs = [ i . to ( device ) for i in inputs ] We also define an utility function to clean the gpu cache def free ( modules ): for m in modules : del m torch . cuda . empty_cache () As we said, imagenet is a huge dataset with 1000 classes, represented by an integer not very human interpretable. We can associate each class id to its label by loading the imaganet2human.txt and create a python dictionary. imagenet2human = {} with open ( 'imaganet2human.txt' ) as f : for line in f . readlines (): key , value = line . split ( ':' ) key = key . replace ( '{' , '' ) . replace ( '}' , '' ) # I forget how regex works :) value = value . replace ( \"'\" , '' ) . replace ( \",\" , '' ) imagenet2human [ int ( key . strip ())] = str ( value . strip ()) list ( imagenet2human . items ())[: 2 ] [(0, 'tench Tinca tinca'), (1, 'goldfish Carassius auratus')]","title":"Preambula"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/#weights-visualization","text":"The first straightforward visualization is to just plot the weights of a target Layer. Obviously, the deeper we go the smaller each image becomes while the channels number increases. We are going to show each channel as a grey array image. Unfortunately, each Pytorch module can be nested and nested, so to make our code as general as possible we first need to trace each sub-module that the input traverse and then store each layer in order. We first need to trace our model to get a list of all the layers so we can select a target layer without following the nested structure of a model. In PyTorch models can be infinitely nested. In other words, we are flattering the model's layers, this is implemented in the module2traced function. model_traced = module2traced ( model , inputs [ 0 ]) model_traced [Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)), ReLU(inplace), MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)), ReLU(inplace), MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace), Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace), MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False), Dropout(p=0.5), Linear(in_features=9216, out_features=4096, bias=True), ReLU(inplace), Dropout(p=0.5), Linear(in_features=4096, out_features=4096, bias=True), ReLU(inplace), Linear(in_features=4096, out_features=1000, bias=True)] Let's plot the first layer's weight. We also print the shape of the weight to give a correct idea to the reader of the dimensional reduction. vis = Weights ( model , device ) first_layer = model_traced [ 0 ] plt . rcParams [ \"figure.figsize\" ] = 16 , 16 run_vis_plot ( vis , inputs [ 0 ], first_layer , ncols = 4 , nrows = 4 ) torch.Size([1, 55, 55]) Let's stop for a minute to explain what those images represent. We traced the input through the computational graph in order to find out all the layers of our models, in this case, alexnet . Then we instantiate the Weights class implemented in visualization.core and we call it by passing the current input, the cat image and a target layer . As outputs, we get all the current layer's weights as grey images. Then, we plot 16 of them. We can notice that they, in some way, makes sense; for example, some pixels are brighter in the edges of the images. Let's plot the first MaxPool layer to better see this effect, dimensional reduction and higher brightness pixels in some interesting areas. If you are wondering what the maxpolling operations is doing, check this awesome repo first_maxpool_layer = model_traced [ 2 ] run_vis_plot ( vis , inputs [ 0 ], first_maxpool_layer , ncols = 4 , nrows = 4 ) torch.Size([1, 27, 27]) Let's try with an other input, the San Pietro Basilica run_vis_plot ( vis , inputs [ 1 ], first_maxpool_layer , ncols = 4 , nrows = 4 ) torch.Size([1, 27, 27]) By looking at them, these images make somehow sense; they highlight the basilica layout but it is hard to understand what the model is actually doing. We got the idea that is computing something correctly but we could ask some questions, for example: is it looking at the cupola? Which are the most important features of the Basilica? Moreover, the deeper we go the harder it becomes to even recognize the input. deeper_layer = model_traced [ 6 ] run_vis_plot ( vis , inputs [ 1 ], deeper_layer , ncols = 4 , nrows = 4 ) torch.Size([1, 13, 13]) In this case, we have no idea of what is going on. It can be argued that weights visualization does not carry any useful information about the model, even if this is almost true, there is one nice reason of plotting the weights especially at the first layer. When a model is poorly trained or not trained at all, the first weights have lots of noise, since they are just randomly initialized, and they are a lot more similar to the inputs images than the trained ones. This feature can be useful to understand on the fly is a model is trained or not. However, except for this, weights visualization is not the way to go to understand what your black box is thinking. Below we plot the first layer's weight first for the untrainded version of alexnet and the for the trained one. alexnet_not_pretrained = alexnet ( pretrained = False ) . to ( device ) run_vis_plot ( Weights ( alexnet_not_pretrained , device ), inputs [ 0 ], module2traced ( alexnet_not_pretrained , inputs [ 0 ])[ 0 ], ncols = 4 , nrows = 4 ) alexnet_pretrained = alexnet ( pretrained = True ) . to ( device ) run_vis_plot ( Weights ( alexnet_pretrained , device ), inputs [ 0 ], module2traced ( alexnet_pretrained , inputs [ 0 ])[ 0 ], ncols = 4 , nrows = 4 ) del alexnet_not_pretrained torch.Size([1, 55, 55]) torch.Size([1, 55, 55]) You can notice that in the first image is simpler to see the input image. Hoewer, this is not a general rule, but in some cases it can help.","title":"Weights Visualization"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/#similarities-with-other-models","text":"We have seen alexnet 's weights, but are they similar across models? Below we plot the first 4 channel of each first layer's weight for alexnet , vgg and resnet modules_instances = [ alexnet , vgg16 , resnet34 ] modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], 0 , Weights , 'weights' , device , ncols = 4 ) free ( modules ) The resnet and vgg weights looks more similar to the input images than alexnet . But, again, what does it mean? Remember that at least resnet is initialized in a different way than the other two models.","title":"Similarities with other models"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/#saliency-visualization","text":"One idea proposed by Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps is to back-prop the output of the network with respect to a target class until the input and plot the computed gradient. This will highligh the part of the image responsible for that class. Let's start with alexnet. Let's first print the prediction of the network (this could change if you re-run the cell) model . eval () pred = model ( inputs [ 0 ]) _ , id = torch . max ( pred , 1 ) print ( 'predicted class {} ' . format ( imagenet2human [ id . item ()])) predicted class tiger cat Each visualization is implemented in its own class. You can find the code here . It will backproprop the output with respect to the one hot encoding representation of the number corresponding to class tiger cat from visualization.core.utils import image_net_postprocessing model . eval () model = model . to ( device ) vis = SaliencyMap ( model , device ) out , info = vis ( inputs [ 0 ], first_layer ) subplot ([ image_net_postprocessing ( inputs [ 0 ] . squeeze () . cpu ()), out ], rows_titles = [ 'original' , 'saliency map' ], parse = tensor2img , nrows = 1 , ncols = 2 ) We can see that alexnet gets exited on the cat. We can even do better! We can set to 0 each negative relu gradient when backprop. This is techinique is called guided . out , info = vis ( inputs [ 0 ], first_layer , guide = True ) subplot ([ image_net_postprocessing ( inputs [ 0 ] . squeeze () . cpu ()), out ], rows_titles = [ 'original' , 'guided saliency map' ], parse = tensor2img , nrows = 1 , ncols = 2 ) Now we can clearly see that the network is looking at the eyes and the nose of the cat. We can try to compare different models modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], 0 , SaliencyMap , 'Saliency' , device , nrows = 1 , ncols = 3 , target_class = 231 , guide = True ) free ( modules ) Alextnet seems more interested to the eyes, while VGG looks at the ears and resnet is similar to alexnet . Now we can clearly understand which part of the inputs help the network gives that prediction. While guiding yields a better human interpretable image, the vanilla implementation can be used for localizing an object of interest. In other words, we can find object of interest for free by cropping out of the input image the region corresponding to the gradient. Let's plot each input image for each model. modules_instances = [ alexnet , vgg16 , resnet34 , resnet152 ] modules = ( m ( pretrained = True ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], None , SaliencyMap , 'SaliencyMap' , device , nrows = 4 , ncols = 3 , inputs = inputs , idx2label = imagenet2human , annotations = [ 'alexnet' , 'vgg16' , 'resnet34' , 'resnet152' ], guide = True ) free ( modules ) The Basilica is very interesting, all four networks correctly classify it as a dome but only resnet152 is more interested in the sky than on the cupola. In the last column, we have an image with two classes, dog and cat . All the networks highlighted booths, like the eyes of the dog and the ears of the cat in vgg16 . What if we would like to discover only the region of the inputs that are related to a specific class? With this technique is impossible.","title":"Saliency visualization"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/#class-activation-mapping","text":"Class Activation Mapping is a techniques presented in Learning Deep Features for Discriminative Localization . The idea is to use the last convolutional layer output and the neurons in the linear layer of the model responsable for a target class, the map is generated by taking the dot product of those. However, to make this work the model has to have some constrains. First of all, the output from the convolution must first go trought an global average polling and it requires feature maps to directly precede softmax layers. To make it works with other architecture, such as alexnet and vgg we have to change some layers in the model and retrain it. This is a major drawback that will be solved with the next section. For now, we can use it for free with resnet! Since its architecture is perfect. The implementation can be found here . We can pass to the visualization a target_class parameter to get the relative weights from the fc layer. Notice that by changing the target class, we can see different part of the image highlighted. The first image uses the prediction class, while the second an other type of cat and the last one bookcase , just to see what the model will do with a wrong class. from visualization.core.utils import imshow # we are using resnet 34 since the model has only one fc layer before the softmax and it is preceded by av avg pool # as required from the paper module = resnet34 ( True ) . to ( device ) module . eval () vis = ClassActivationMapping ( module , device ) classes = [ None , 285 , 453 ] def vis_outs2images_classes ( outs ): images = [ x [ 0 ] for x in outs ] classes = [ imagenet2human [ int ( x [ 1 ][ 'prediction' ])] for x in outs ] return images , classes outs = [ vis ( inputs [ 0 ], None , postprocessing = image_net_postprocessing , target_class = c , guide = True ) for c in classes ] images , classes = vis_outs2images_classes ( outs ) subplot ( images , rows_titles = classes , nrows = 1 , ncols = 3 , parse = tensor2img ) It makes sense, the only thing is that in the last row we still have some part of the cat highlighted for bookcase Let's plot the CAM on the cat images for different resnet architecture. For resnet > 34 the Bottleneck module is used modules_instances = [ resnet18 , resnet34 , resnet101 , resnet152 ] cat = inputs [ 2 ] modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , cat , None , ClassActivationMapping , 'ClassActivationMapping' , device , nrows = len ( modules_instances ), ncols = 1 , postprocessing = image_net_postprocessing , rows_name = [ 'resnet18' , 'resnet34' , 'resnet101' , 'resnet152' ], target_class = None ) free ( modules ) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). They are all very similar as expected. One big drawback of this technique is that force you to use a network with a specific architecture, global polling before the decoder part. The next technique generalize this approach by taking advantage of the gradient at one specific layer. Remember that with the class activation we are using the weights of the feature map as a scaling factor for the channels of the last layer. The features map must be before a softmax layer and right after the average pooling. The next technique propose a more general approach.","title":"Class Activation Mapping"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/#grad-cam","text":"Grad Cam was introduced by Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization . The idea is actually simple, we backprop the output with respect to a target class while storing the gradient and the output at a given layer, in our case the last convolution. Then we perform a global average of the saved gradient keeping the channel dimension in order to get a 1-d tensor, this will represent the importance of each channel in the target convolutional layer. We then multiply each element of the convolutional layer outputs by the averaged gradients to create the grad cam. This whole procedure is fast and it is architecture independent. Interesting, the authors show that is a generalization of the previous technique. The code is here We can use it to higlight what different models are looking at. modules_instances = [ alexnet , vgg16 , resnet34 , resnet152 ] modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], None , GradCam , 'Gradcam' , device , nrows = 1 , ncols = 4 , target_class = None , postprocessing = image_net_postprocessing ) free ( modules ) It is really interesting to see how alexnet looks at the nose, while vgg at the ears and resnet at the whole cat. It is interesting to see that the two resnet version looks at different part of the cat. Below we plot the same input for resnet34 but we change the target class in each column to show the reader how the grad cam change accordingly. from visualization.core.utils import imshow module = module . to ( device ) vis = GradCam ( module , device ) classes = [ None , 285 , 453 ] outs = [ vis ( inputs [ 0 ], None , postprocessing = image_net_postprocessing , target_class = c ) for c in classes ] images , classes = vis_outs2images_classes ( outs ) subplot ( images , title = 'resnet34' , rows_titles = classes , nrows = 1 , ncols = len ( outs ), parse = tensor2img ) Notice how similar to the CAM output they are. To better compore our three models, below we plot the grad cam for each input with respect to each model modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], None , GradCam , 'Gradcam' , device , nrows = 4 , ncols = 3 , target_class = None , inputs = inputs , idx2label = imagenet2human , annotations = [ 'alexnet' , 'vgg16' , 'resnet34' , 'resnet152' ], postprocessing = image_net_postprocessing ) free ( modules ) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). The reader can immediately notice the difference across the models.","title":"Grad Cam"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/#interesting-region","text":"We talk before about interesting region localizations. Grad-cam can be also used to extract the class object out of the image. Easily, once the have the grad-cam image we can used it as mask to crop out form the input image what we want. The reader can play with the TR parameter to see different effects. TR = 0.3 alexnet_pretrained . eval () vis = GradCam ( alexnet_pretrained , device ) _ = vis ( inputs [ 0 ], None , postprocessing = image_net_postprocessing ) import cv2 def gradcam2crop ( cam , original_img ): b , c , w , h = inputs [ 0 ] . shape cam = cam . numpy () cam -= np . min ( cam ) cam /= np . max ( cam ) cam = cv2 . resize ( cam , ( w , h )) mask = cam > TR original_img = tensor2img ( image_net_postprocessing ( original_img [ 0 ] . squeeze ())) crop = original_img . copy () crop [ mask == 0 ] = 0 return crop crop = gradcam2crop ( vis . cam . cpu (), inputs [ 0 ] . cpu ()) fig = plt . figure () plt . imshow ( crop ) <matplotlib.image.AxesImage at 0x7f79debee048> et voil\u00e0 ! We can also change again class, and crop the interest region for that class. _ = vis ( inputs [ 0 ], None , target_class = 231 , postprocessing = image_net_postprocessing ) crop = gradcam2crop ( vis . cam . cpu (), inputs [ 0 ] . cpu ()) fig = plt . figure () plt . imshow ( crop ) <matplotlib.image.AxesImage at 0x7f79f40c4c18>","title":"Interesting region"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/#different-models","text":"We have seen all these techniques used with classic classicification models trained on imagenet . What about use them on a different domain? I have ported this paper to Pytorch and retrain it. The model learn from the frontal camera's image of a robot to predict the local distance sensors in order to avoid obstacles. Let's see what if, by using those techniques, we can understand better what is going on inside the model.","title":"Different models"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/#learning-long-range-perception-using-self-supervision-from-short-range-sensors-and-odometry","text":"The idea is to predict the future outputs of a short-range sensor (such as a proximity sensor) given the current outputs of a long-range sensor (such as a camera). They trained a very simple CNN from the robot's camera images to predict the proximity sensor values. If you are interested in their work, you can read the full paper here I have made a PyTorch implementation and retrain the model from scratch. Be awere that I did not fine-tune or try different sets of hyper-parameters so probably my model is not performing as well as the author's one. Let's import it from os import path LONG_RANGE_PERCEPTION_PATH = path . abspath ( './models/long_range_perception/model.pt' ) from models.long_range_perception.model import SimpleCNN from models.long_range_perception.utils import get_dl , H5_PATH , imshow , post_processing , pre_processing , MODEL_PATH free ([ module ]) module = torch . load ( LONG_RANGE_PERCEPTION_PATH , map_location = lambda storage , loc : storage ) module = module . to ( device ) module /home/francesco/Documents/A-journey-into-Convolutional-Neural-Network-visualization-/model.pt SimpleCNN( (encoder): Sequential( (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): ReLU() (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (6): Conv2d(10, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): ReLU() (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (decoder): Sequential( (0): Dropout(p=0.2) (1): Linear(in_features=640, out_features=256, bias=True) (2): ReLU() (3): Linear(in_features=256, out_features=325, bias=True) (4): Sigmoid() ) ) We know need some inputs to test the model, they are taken directly from the test set import os def make_and_show_inputs ( path , transform ): image_paths = glob . glob ( path ) image_paths = filter ( lambda x : os . path . isfile ( x ), image_paths ) images = list ( map ( lambda x : Image . open ( x ) . convert ( 'RGB' ), image_paths )) subplot ( images , title = 'inputs' , rows_titles = [ '1' , '2' , '3' , '4' ], nrows = 1 , ncols = 4 ) plt . show () inputs = [ pre_processing ( x ) . unsqueeze ( 0 ) . to ( device ) for x in images ] # add 1 dim for batch subplot ( inputs , parse = tensor2img , title = 'inputs' , rows_titles = [ '1' , '2' , '3' , '4' ], nrows = 1 , ncols = 4 ) return images , inputs images , inputs = make_and_show_inputs ( 'images/long_range_perception/*' , pre_processing ) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Then author normalize each image, this is done by callind pre_processing . For some reason the inpupts images are different on mac and ubuntu, they should not be like these if you run the notebook on mac the result is different. This is probably due to the warning message. We are going to use the SaliencyMap and the GradCam since those are the best from torch.autograd import Variable module . eval () def run_long_range_vis (): grad = GradCam ( module , device ) all_true = torch . ones ( 1 , 65 * 5 ) . float () . to ( device ) outs_grad = [ grad ( input , None , target_class = all_true , postprocessing = post_processing , regression = True )[ 0 ] for input in inputs ] sal = SaliencyMap ( module , device ) outs_saliency = [ sal ( input , None , guide = True , target_class = all_true , regression = True )[ 0 ] for input in inputs ] subplot ([ * outs_grad , * outs_saliency ], title = 'long_range' , cols_titles = [ '1' , '2' , '3' , '4' ], nrows = 2 , ncols = 4 , parse = tensor2img ) run_long_range_vis () We can clearly see that the model looks at the objects. In the GradCam row, on the second picture, the plan is basically segmented by the heatmap. There is one problem, if you look at the third picture, the white box in front of the camera is not clearly highlighted. This is probably due to the white color of the floor that is very similar to the box's color. Let's investigate this problem. In the second row, the SaliencyMaps highlights all the objects, including the white box. The reader can notice that the reflection in the first picture on the left seems to excite the network in that region. We should also investigate this case but due to time limitations, we will leave it as an exercise for the curious reader. For completeness, let's also print the predicted sensor output. The model tries to predict five frontal distance sensors give the image camera. import seaborn as sns module . eval () preds = module ( torch . stack ( inputs ) . squeeze ( 1 )) fig = plt . figure () sns . heatmap ( preds [ 2 ] . view ( - 1 , 5 ) . detach () . cpu () . numpy ()) <matplotlib.axes._subplots.AxesSubplot at 0x7f79d26dc240> If you compare with the authors pictures, my prediction are worse. This is due to the fact that to speed up everything I did not used all the training set and I did not perform any hyper paramater optimisation. All the code con be found here . Let's now investigate the first problem, object with a similar color to the ground.","title":"Learning Long-range Perception using Self-Supervision from Short-Range Sensors and Odometry"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/#similar-colors","text":"To test if the model has a problem with obstacles with a the same color of the ground, we created in blender four different scenarios with an obstacle. They are showed in the picture below. image_paths = [ * sorted ( glob . glob ( 'images/long_range_perception/equal_color/*' )), * sorted ( glob . glob ( 'images/long_range_perception/different_color/*' ))] image_paths = filter ( lambda x : os . path . isfile ( x ), image_paths ) images = list ( map ( lambda x : Image . open ( x ) . convert ( 'RGB' ), image_paths )) subplot ( images , title = 'inputs' , nrows = 2 , ncols = 4 ) plt . show () There are four different lights configuration and two differents cube colors, one equal to the ground and the second different. The first column represents a realistic situation, while the second has a really strong light from behind that generates a shadow in front of the camera. The third column has a shadow on the left and the last one has a little shadow on the left. This is a perfect scenario to use gradcam to see what the model is looking in each image. In the picture below we plotted the gradcam results. inputs = [ pre_processing ( x ) . unsqueeze ( 0 ) . to ( device ) for x in images ] # add 1 dim for batch run_long_range_vis () The big black shadow in the second column definitly confuses the model. In the first and last column, the grad cam highlights better the corners of the red cube, especially in the first picture. We can definitely say that this model has some hard time with the object of the same colour as the ground. Thanks to this consideration, we could improve the number equal object/ground in the dataset, perform a better preprocessing, change the model structure etc and hopefully increase the robustness of the network.","title":"Similar colors"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/#conclusion","text":"In this article, we present different convolutional neural network visualization techniques. In the first section, we introduced each one by applying to a set of famous classification networks. We compared different networks on different inputs and highlight the similarities and difference between them. Then we apply them to a model adopted in robotics to test its robustness and we were able to successfully reveal a problem in the network. Moreover, as a side project, I developed an interactive convolutional neural network visualization application called mirro that receives in just a few days more than a hundred stars on GitHub reflecting the interest of the deep learning community on this topic. All these visualizations are implemented using a common interface and there are available as python module so they can be used in any other module. Thank you for reading Francesco Saverio Zuppichini","title":"Conclusion"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/notebook/","text":"A journey into Convolutional Neural Network visualization Francesco Saverio Zuppichini There is one famous urban legend about computer vision. Around the 80s, the US military wanted to use neural networks to automatically detect camouflaged enemy tanks. They took a number of pictures of trees without tanks and then pictures with the same trees with tanks behind them. The results were impressive. So impressive that the army wanted to be sure the net had correctly generalized. They took new pictures of woods with and without tanks and they showed them again to the network. This time, the model performed terribly, it was not able to discriminate between pictures with tanks behind woods and just trees.It turned out that all the pictures without tanks were taken on a cloudy day while the ones with tanks on a sunny day! In reality, the network learn to recognize the weather, not the enemy tanks. Nosce te ipsum With this article, we are going to see different techniques to understand what it is going on inside a Convolutional Neural Network to avoid making the same US' army mistake. We are going to use Pytorch . All the code can be found here . Most of the visualizations were developed from scratch, however, some inspiration and parts were taken from here . We will first introduce each technique by briefly explain it and making some example and comparison between different classic computer vision models, alexnet , vgg16 and resnet . Then we will try to better understand a model used in robotics to predict the local distance sensor using only the frontal camera's images. Our goal is not to explain in detail how each technique works since this is already done extremely well by each paper, but to use them to help the reader visualize different model with different inputs to better understand and highlight what and how different models react to a given input. Later on, we show a workflow in which we utilize some of the techniques you will learn in this journey to test the robustness of a model, this is extremely useful to understand and fix its limitations. The curios reader could further improve is understand by looking and the source code for each visulisation and by reading the references. Preambula Disclaimer I am not a fan of jupyter. So apologize in advance if there are some warnings in the outputs and some figures are not well made Let's start our journey by selecting a network. Our first model will be the old school alexnet . It is already available in the torchvision.models package from Pytorch % load_ext autoreload % autoreload 2 from torchvision.models import * from visualisation.core.utils import device model = alexnet ( pretrained = True ) . to ( device ) print ( model ) AlexNet( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)) (1): ReLU(inplace=True) (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (4): ReLU(inplace=True) (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): ReLU(inplace=True) (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (9): ReLU(inplace=True) (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace=True) (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) ) (avgpool): AdaptiveAvgPool2d(output_size=(6, 6)) (classifier): Sequential( (0): Dropout(p=0.5, inplace=False) (1): Linear(in_features=9216, out_features=4096, bias=True) (2): ReLU(inplace=True) (3): Dropout(p=0.5, inplace=False) (4): Linear(in_features=4096, out_features=4096, bias=True) (5): ReLU(inplace=True) (6): Linear(in_features=4096, out_features=1000, bias=True) ) ) Now we need some inputs # %matplotlib notebook Now we need some inputs images. We are going to use three pictures, a cat, the beautiful Basilica di San Pietro and an image with a dog and a cat. import glob import matplotlib.pyplot as plt import numpy as np import torch from utils import * from PIL import Image plt . rcParams [ \"figure.figsize\" ] = 16 , 8 We loaded a few packages. In utils there are several utility function to creates the plots. import glob import matplotlib.pyplot as plt import numpy as np from visualisation.core.utils import device from PIL import Image image_paths = glob . glob ( './images/*.*' ) images = list ( map ( lambda x : Image . open ( x ), image_paths )) subplot ( images , title = 'inputs' , rows_titles = [ 'cat' , 'san pietro' , 'dog_cat' ], nrows = 1 , ncols = 3 ) Since all of our models were trained on imagenet , a huge dataset with 1000 different classes, we need to parse and normalize them. In Pytorch, we have to manually send the data to a device. In this case the device if the fist gpu if you have one, otherwise cpu is selected. Be aware that jupyter have not a garbage collected so we will need to manually free the gpu memory. from torchvision.transforms import ToTensor , Resize , Compose , ToPILImage from visualisation.core import * from visualisation.core.utils import image_net_preprocessing inputs = [ Compose ([ Resize (( 224 , 224 )), ToTensor (), image_net_preprocessing ])( x ) . unsqueeze ( 0 ) for x in images ] # add 1 dim for batch inputs = [ i . to ( device ) for i in inputs ] We also define an utility function to clean the gpu cache def free ( modules ): for m in modules : del m torch . cuda . empty_cache () As we said, imagenet is a huge dataset with 1000 classes, represented by an integer not very human interpetable. We can associate each class id to its label by loading the imaganet2human.txt and create a python dictionary. imagenet2human = {} with open ( 'imaganet2human.txt' ) as f : for line in f . readlines (): key , value = line . split ( ':' ) key = key . replace ( '{' , '' ) . replace ( '}' , '' ) # I forget how regex works :) value = value . replace ( \"'\" , '' ) . replace ( \",\" , '' ) imagenet2human [ int ( key . strip ())] = str ( value . strip ()) list ( imagenet2human . items ())[: 2 ] [(0, 'tench Tinca tinca'), (1, 'goldfish Carassius auratus')] Weights Visualization The first straightforward visualization is to just plot the weights of a target Layer. Obviously, the deeper we go the smaller each image becomes while the channels number increases. We are going to show each channel as a grey array image. Unfortunately, each Pytorch module can be nested and nested, so to make our code as general as possible we first need to trace each sub-module that the input traverse and then store each layer in order. We first need to trace our model to get a list of all the layers so we can select a target layer without following the nested structure of a model. In PyTorch models can be infinitely nested. In other words, we are flattering the model's layers, this is implemented in the module2traced function. model_traced = module2traced ( model , inputs [ 0 ]) model_traced [Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)), ReLU(inplace=True), MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)), ReLU(inplace=True), MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False), AdaptiveAvgPool2d(output_size=(6, 6)), Dropout(p=0.5, inplace=False), Linear(in_features=9216, out_features=4096, bias=True), ReLU(inplace=True), Dropout(p=0.5, inplace=False), Linear(in_features=4096, out_features=4096, bias=True), ReLU(inplace=True), Linear(in_features=4096, out_features=1000, bias=True)] Let's plot the first layer's weight. We also print the shape of the weight to give a correct idea to the reader of the dimensional reduction. vis = Weights ( model , device ) first_layer = model_traced [ 0 ] plt . rcParams [ \"figure.figsize\" ] = 16 , 16 run_vis_plot ( vis , inputs [ 0 ], first_layer , ncols = 4 , nrows = 4 ) torch.Size([1, 55, 55]) Let's stop for a minute to explain what those images represent. We traced the input through the computational graph in order to find out all the layers of our models, in this case, alexnet . Then we instantiate the Weights class implemented in visualisation.core and we call it by passing the current input, the cat image and a target layer . As outputs, we get all the current layer's weights as grey images. Then, we plot 16 of them. We can notice that they, in some way, makes sense; for example, some pixels are brighter in the edges of the images. Let's plot the first MaxPool layer to better see this effect, dimensional reduction and higher brightness pixels in some interesting areas. If you are wondering what the maxpolling operations is doing, check this awesome repo first_maxpool_layer = model_traced [ 2 ] run_vis_plot ( vis , inputs [ 0 ], first_maxpool_layer , ncols = 4 , nrows = 4 ) torch.Size([1, 27, 27]) Let's try with an other input, the San Pietro Basilica run_vis_plot ( vis , inputs [ 1 ], first_maxpool_layer , ncols = 4 , nrows = 4 ) torch.Size([1, 27, 27]) By looking at them, these images make somehow sense; they highlight the basilica layout but it is hard to understand what the model is actually doing. We got the idea that is computing something correctly but we could ask some questions, for example: is it looking at the cupola? Which are the most important features of the Basilica? Moreover, the deeper we go the harder it becomes to even recognize the input. deeper_layer = model_traced [ 6 ] run_vis_plot ( vis , inputs [ 1 ], deeper_layer , ncols = 4 , nrows = 4 ) torch.Size([1, 13, 13]) In this case, we have no idea of what is going on. It can be argued that weights visualization does not carry any useful information about the model, even if this is almost true, there is one nice reason of plotting the weights especially at the first layer. When a model is poorly trained or not trained at all, the first weights have lots of noise, since they are just randomly initialized, and they are a lot more similar to the inputs images than the trained ones. This feature can be useful to understand on the fly is a model is trained or not. However, except for this, weights visualization is not the way to go to understand what your black box is thinking. Below we plot the first layer's weight first for the untraind version of alexnet and the for the trained one. alexnet_not_pretrained = alexnet ( pretrained = False ) . to ( device ) run_vis_plot ( Weights ( alexnet_not_pretrained , device ), inputs [ 0 ], module2traced ( alexnet_not_pretrained , inputs [ 0 ])[ 0 ], ncols = 4 , nrows = 4 ) alexnet_pretrained = alexnet ( pretrained = True ) . to ( device ) run_vis_plot ( Weights ( alexnet_pretrained , device ), inputs [ 0 ], module2traced ( alexnet_pretrained , inputs [ 0 ])[ 0 ], ncols = 4 , nrows = 4 ) del alexnet_not_pretrained torch.Size([1, 55, 55]) torch.Size([1, 55, 55]) You can notice that in the first image is simpler to see the input image. Hoewer, this is not a general rule, but in some cases it can help. Similarities with other models We have seen alexnet 's weights, but are they similar across models? Below we plot the first 4 channel of each first layer's weight for alexnet , vgg and resnet modules_instances = [ alexnet , vgg16 , resnet34 ] modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], 0 , Weights , 'weights' , device , ncols = 4 ) free ( modules ) The resnet and vgg weights looks more similar to the input images than alexnet . But, again, what does it mean? Remember that at least resnet is initialized in a different way than the other two models. Saliency Visualisation One idea proposed by Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps is to back-prop the output of the network with respect to a target class until the input and plot the computed gradient. This will highligh the part of the image responsible for that class. Let's start with alexnet. Let's first print the prediction of the network (this could change if you re-run the cell) model . eval () pred = model ( inputs [ 0 ]) _ , id = torch . max ( pred , 1 ) print ( 'predicted class {} ' . format ( imagenet2human [ id . item ()])) predicted class tiger cat from visualisation.core.utils import image_net_postprocessing Each visualisation is implemented in its own class. You can find the code here . It will backproprop the output with respect to the one hot encoding representation of the number corresponding to class tiger cat model . eval () model = model . to ( device ) vis = SaliencyMap ( model , device ) out , info = vis ( inputs [ 0 ], first_layer ) subplot ([ image_net_postprocessing ( inputs [ 0 ] . squeeze () . cpu ()), out ], rows_titles = [ 'original' , 'saliency map' ], parse = tensor2img , nrows = 1 , ncols = 2 ) --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) ~\\AppData\\Local\\Temp/ipykernel_26720/4067497372.py in <module> 4 vis = SaliencyMap(model, device) 5 ----> 6 out, info = vis(inputs[0], 7 first_layer) 8 ~\\reighns_ml\\reighns_ml_blog\\docs\\reighns_ml_journey\\deep_learning\\computer_vision\\general\\neural_network_interpretation\\references\\A-journey-into-Convolutional-Neural-Network-visualization--master\\visualisation\\core\\SaliencyMap.py in __call__(self, input_image, layer, guide, target_class, regression) 66 67 ---> 68 image = self.gradients.data.cpu().numpy()[0] 69 # 70 image = convert_to_grayscale(image) AttributeError: 'NoneType' object has no attribute 'data' We can see that alexnet gets exited on the cat. We can even do better! We can set to 0 each negative relu gradient when backprop. This is techinique is called guided . out , info = vis ( inputs [ 0 ], first_layer , guide = True ) subplot ([ image_net_postprocessing ( inputs [ 0 ] . squeeze () . cpu ()), out ], rows_titles = [ 'original' , 'guided saliency map' ], parse = tensor2img , nrows = 1 , ncols = 2 ) Now we can clearly see that the network is looking at the eyes and the nose of the cat. We can try to compare different models modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], 0 , SaliencyMap , 'Saliency' , device , nrows = 1 , ncols = 3 , target_class = 231 , guide = True ) free ( modules ) Alextnet seems more interested to the eyes, while VGG looks at the ears and resnet is similar to alexnet . Now we can clearly understand which part of the inputs help the network gives that prediction. While guiding yields a better human interpretable image, the vanilla implementation can be used for localizing an object of interest. In other words, we can find object of interest for free by cropping out of the input image the region corresponding to the gradient. Let's plot each input image for each model. modules_instances = [ alexnet , vgg16 , resnet34 , resnet152 ] modules = ( m ( pretrained = True ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], None , SaliencyMap , 'SaliencyMap' , device , nrows = 4 , ncols = 3 , inputs = inputs , idx2label = imagenet2human , annotations = [ 'alexnet' , 'vgg16' , 'resnet34' , 'resnet152' ], guide = True ) free ( modules ) The Basilica is very interesting, all four networks correctly classify it as a dome but only resnet152 is more interested in the sky than on the cupola. In the last column, we have an image with two classes, dog and cat . All the networks highlighted booths, like the eyes of the dog and the ears of the cat in vgg16 . What if we would like to discover only the region of the inputs that are related to a specific class? With this technique is impossible. Class Activation Mapping Class Activation Mapping is a techniques presented in Learning Deep Features for Discriminative Localization . The idea is to use the last convolutional layer output and the neurons in the linear layer of the model responsable for a target class, the map is generated by taking the dot product of those. However, to make this work the model has to have some constrains. First of all, the output from the convolution must first go trought an global average polling and it requires feature maps to directly precede softmax layers. To make it works with other architecture, such as alexnet and vgg we have to change some layers in the model and retrain it. This is a major drawback that will be solved with the next section. For now, we can use it for free with resnet! Since its architecture is perfect. The implementation can be found here . We can pass to the visualisation a target_class parameter to get the relative weights from the fc layer. Notice that by changing the target class, we can see different part of the image highlighted. The first image uses the prediction class, while the second an other type of cat and the last one bookcase , just to see what the model will do with a wrong class. from visualisation.core.utils import imshow # we are using resnet 34 since the model has only one fc layer before the softmax and it is preceded by av avg pool # as required from the paper module = resnet34 ( True ) . to ( device ) module . eval () vis = ClassActivationMapping ( module , device ) classes = [ None , 285 , 453 ] def vis_outs2images_classes ( outs ): images = [ x [ 0 ] for x in outs ] classes = [ imagenet2human [ int ( x [ 1 ][ 'prediction' ])] for x in outs ] return images , classes outs = [ vis ( inputs [ 0 ], None , postprocessing = image_net_postprocessing , target_class = c , guide = True ) for c in classes ] images , classes = vis_outs2images_classes ( outs ) subplot ( images , rows_titles = classes , nrows = 1 , ncols = 3 , parse = tensor2img ) It makes sense, the only thing is that in the last row we still have some part of the cat highlighted for bookcase Let's plot the CAM on the cat images for different resnet architecture. For resnet > 34 the Bottleneck module is used modules_instances = [ resnet18 , resnet34 , resnet101 , resnet152 ] cat = inputs [ 2 ] modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , cat , None , ClassActivationMapping , 'ClassActivationMapping' , device , nrows = len ( modules_instances ), ncols = 1 , postprocessing = image_net_postprocessing , rows_name = [ 'resnet18' , 'resnet34' , 'resnet101' , 'resnet152' ], target_class = None ) free ( modules ) They are all very similar as expected. One big drawback of this technique is that force you to use a network with a specific architecture, global polling before the decoder part. The next technique generalize this approach by taking advantage of the gradient at one specific layer. Remember that with the class activation we are using the weights of the feature map as a scaling factor for the channels of the last layer. The features map must be before a softmax layer and right after the average pooling. The next technique propose a more general approach. Grad Cam Grad Cam was introduced by Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization . The idea is actually simple, we backprop the output with respect to a target class while storing the gradient and the output at a given layer, in our case the last convolution. Then we perform a global average of the saved gradient keeping the channel dimension in order to get a 1-d tensor, this will represent the importance of each channel in the target convolutional layer. We then multiply each element of the convolutional layer outputs by the averaged gradients to create the grad cam. This whole procedure is fast and it is architecture independent. Interesting, the authors show that is a generalization of the previous technique. The code is here We can use it to higlight what different models are looking at. modules_instances = [ alexnet , vgg16 , resnet34 , resnet152 ] modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], None , GradCam , 'Gradcam' , device , nrows = 1 , ncols = 4 , target_class = None , postprocessing = image_net_postprocessing ) free ( modules ) C:\\Users\\reighns\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1025: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior. warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \" torch.Size([13, 13]) torch.Size([1, 3, 224, 224]) torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) torch.Size([14, 14]) torch.Size([1, 3, 224, 224]) torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) torch.Size([7, 7]) torch.Size([1, 3, 224, 224]) torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) torch.Size([7, 7]) torch.Size([1, 3, 224, 224]) torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) It is really interesting to see how alexnet looks at the nose, while vgg at the ears and resnet at the whole cat. It is interesting to see that the two resnet version looks at different part of the cat. Below we plot the same input for resnet34 but we change the target class in each column to show the reader how the grad cam change accordingly. from visualisation.core.utils import imshow module = module . to ( device ) vis = GradCam ( module , device ) classes = [ None , 285 , 453 ] outs = [ vis ( inputs [ 0 ], None , postprocessing = image_net_postprocessing , target_class = c ) for c in classes ] images , classes = vis_outs2images_classes ( outs ) subplot ( images , title = 'resnet34' , rows_titles = classes , nrows = 1 , ncols = len ( outs ), parse = tensor2img ) Notice how similar to the CAM output they are. To better compore our three models, below we plot the grad cam for each input with respect to each model modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], None , GradCam , 'Gradcam' , device , nrows = 4 , ncols = 3 , target_class = None , inputs = inputs , idx2label = imagenet2human , annotations = [ 'alexnet' , 'vgg16' , 'resnet34' , 'resnet152' ], postprocessing = image_net_postprocessing ) free ( modules ) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). The reader can immediately notice the difference across the models. Interesting region We talk before about interesting region localizations. Grad-cam can be also used to extract the class object out of the image. Easily, once the have the grad-cam image we can used it as mask to crop out form the input image what we want. The reader can play with the TR parameter to see different effects. TR = 0.3 alexnet_pretrained . eval () vis = GradCam ( alexnet_pretrained , device ) _ = vis ( inputs [ 0 ], None , postprocessing = image_net_postprocessing ) import cv2 def gradcam2crop ( cam , original_img ): b , c , w , h = inputs [ 0 ] . shape cam = cam . numpy () cam -= np . min ( cam ) cam /= np . max ( cam ) cam = cv2 . resize ( cam , ( w , h )) mask = cam > TR original_img = tensor2img ( image_net_postprocessing ( original_img [ 0 ] . squeeze ())) crop = original_img . copy () crop [ mask == 0 ] = 0 return crop crop = gradcam2crop ( vis . cam . cpu (), inputs [ 0 ] . cpu ()) fig = plt . figure () plt . imshow ( crop ) <matplotlib.image.AxesImage at 0x7f79debee048> et voil\u00e0 ! We can also change again class, and crop the interest region for that class. _ = vis ( inputs [ 0 ], None , target_class = 231 , postprocessing = image_net_postprocessing ) crop = gradcam2crop ( vis . cam . cpu (), inputs [ 0 ] . cpu ()) fig = plt . figure () plt . imshow ( crop ) <matplotlib.image.AxesImage at 0x7f79f40c4c18> Different models We have seen all these techniques used with classic classicification models trained on imagenet . What about use them on a different domain? I have ported this paper to Pytorch and retrain it. The model learn from the frontal camera's image of a robot to predict the local distance sensors in order to avoid obstacles. Let's see what if, by using those techniques, we can understand better what is going on inside the model. Learning Long-range Perception using Self-Supervision from Short-Range Sensors and Odometry The idea is to predict the future outputs of a short-range sensor (such as a proximity sensor) given the current outputs of a long-range sensor (such as a camera). They trained a very simple CNN from the robot's camera images to predict the proximity sensor values. If you are interested in their work, you can read the full paper here I have made a PyTorch implementation and retrain the model from scratch. Be awere that I did not fine-tune or try different sets of hyper-parameters so probably my model is not performing as well as the author's one. Let's import it from os import path LONG_RANGE_PERCEPTION_PATH = path . abspath ( './models/long_range_perception/model.pt' ) from models.long_range_perception.model import SimpleCNN from models.long_range_perception.utils import get_dl , H5_PATH , imshow , post_processing , pre_processing , MODEL_PATH free ([ module ]) module = torch . load ( LONG_RANGE_PERCEPTION_PATH , map_location = lambda storage , loc : storage ) module = module . to ( device ) module /home/francesco/Documents/A-journey-into-Convolutional-Neural-Network-visualization-/model.pt SimpleCNN( (encoder): Sequential( (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): ReLU() (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (6): Conv2d(10, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): ReLU() (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (decoder): Sequential( (0): Dropout(p=0.2) (1): Linear(in_features=640, out_features=256, bias=True) (2): ReLU() (3): Linear(in_features=256, out_features=325, bias=True) (4): Sigmoid() ) ) We know need some inputs to test the model, they are taken directly from the test set import os def make_and_show_inputs ( path , transform ): image_paths = glob . glob ( path ) image_paths = filter ( lambda x : os . path . isfile ( x ), image_paths ) images = list ( map ( lambda x : Image . open ( x ) . convert ( 'RGB' ), image_paths )) subplot ( images , title = 'inputs' , rows_titles = [ '1' , '2' , '3' , '4' ], nrows = 1 , ncols = 4 ) plt . show () inputs = [ pre_processing ( x ) . unsqueeze ( 0 ) . to ( device ) for x in images ] # add 1 dim for batch subplot ( inputs , parse = tensor2img , title = 'inputs' , rows_titles = [ '1' , '2' , '3' , '4' ], nrows = 1 , ncols = 4 ) return images , inputs images , inputs = make_and_show_inputs ( 'images/long_range_perception/*' , pre_processing ) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Then author normalize each image, this is done by callind pre_processing . For some reason the inpupts images are different on mac and ubuntu, they should not be like these if you run the notebook on mac the result is different. This is probably due to the warning message. We are going to use the SaliencyMap and the GradCam since those are the best from torch.autograd import Variable module . eval () def run_long_range_vis (): grad = GradCam ( module , device ) all_true = torch . ones ( 1 , 65 * 5 ) . float () . to ( device ) outs_grad = [ grad ( input , None , target_class = all_true , postprocessing = post_processing , regression = True )[ 0 ] for input in inputs ] sal = SaliencyMap ( module , device ) outs_saliency = [ sal ( input , None , guide = True , target_class = all_true , regression = True )[ 0 ] for input in inputs ] subplot ([ * outs_grad , * outs_saliency ], title = 'long_range' , cols_titles = [ '1' , '2' , '3' , '4' ], nrows = 2 , ncols = 4 , parse = tensor2img ) run_long_range_vis () We can clearly see that the model looks at the objects. In the GradCam row, on the second picture, the plan is basically segmented by the heatmap. There is one problem, if you look at the third picture, the white box in front of the camera is not clearly highlighted. This is probably due to the white color of the floor that is very similar to the box's color. Let's investigate this problem. In the second row, the SaliencyMaps highlights all the objects, including the white box. The reader can notice that the reflection in the first picture on the left seems to excite the network in that region. We should also investigate this case but due to time limitations, we will leave it as an exercise for the curious reader. For completeness, let's also print the predicted sensor output. The model tries to predict five frontal distance sensors give the image camera. import seaborn as sns module . eval () preds = module ( torch . stack ( inputs ) . squeeze ( 1 )) fig = plt . figure () sns . heatmap ( preds [ 2 ] . view ( - 1 , 5 ) . detach () . cpu () . numpy ()) <matplotlib.axes._subplots.AxesSubplot at 0x7f79d26dc240> If you compare with the authors pictures, my prediction are worse. This is due to the fact that to speed up everything I did not used all the training set and I did not perform any hyper paramater optimisation. All the code con be found here . Let's now investigate the first problem, object with a similar color to the ground. Similar colors To test if the model has a problem with obstacles with a the same color of the ground, we created in blender four different scenarios with an obstacle. They are showed in the picture below. image_paths = [ * sorted ( glob . glob ( 'images/long_range_perception/equal_color/*' )), * sorted ( glob . glob ( 'images/long_range_perception/different_color/*' ))] image_paths = filter ( lambda x : os . path . isfile ( x ), image_paths ) images = list ( map ( lambda x : Image . open ( x ) . convert ( 'RGB' ), image_paths )) subplot ( images , title = 'inputs' , nrows = 2 , ncols = 4 ) plt . show () There are four different lights configuration and two differents cube colors, one equal to the ground and the second different. The first column represents a realistic situation, while the second has a really strong light from behind that generates a shadow in front of the camera. The third column has a shadow on the left and the last one has a little shadow on the left. This is a perfect scenario to use gradcam to see what the model is looking in each image. In the picture below we plotted the gradcam results. inputs = [ pre_processing ( x ) . unsqueeze ( 0 ) . to ( device ) for x in images ] # add 1 dim for batch run_long_range_vis () The big black shadow in the second column definitly confuses the model. In the first and last column, the grad cam highlights better the corners of the red cube, especially in the first picture. We can definitely say that this model has some hard time with the object of the same colour as the ground. Thanks to this consideration, we could improve the number equal object/ground in the dataset, perform a better preprocessing, change the model structure etc and hopefully increase the robustness of the network. Conclusion In this article, we present different convolutional neural network visualization techniques. In the first section, we introduced each one by applying to a set of famous classification networks. We compared different networks on different inputs and highlight the similarities and difference between them. Then we apply them to a model adopted in robotics to test its robustness and we were able to successfully reveal a problem in the network. Moreover, as a side project, I developed an interactive convolutional neural network visualization application called mirro that receives in just a few days more than a hundred stars on GitHub reflecting the interest of the deep learning community on this topic. All these visualizations are implemented using a common interface and there are available as python module so they can be used in any other module. Thank for reading Francesco Saverio Zuppichini","title":"A journey into Convolutional Neural Network visualization"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/notebook/#a-journey-into-convolutional-neural-network-visualization","text":"Francesco Saverio Zuppichini There is one famous urban legend about computer vision. Around the 80s, the US military wanted to use neural networks to automatically detect camouflaged enemy tanks. They took a number of pictures of trees without tanks and then pictures with the same trees with tanks behind them. The results were impressive. So impressive that the army wanted to be sure the net had correctly generalized. They took new pictures of woods with and without tanks and they showed them again to the network. This time, the model performed terribly, it was not able to discriminate between pictures with tanks behind woods and just trees.It turned out that all the pictures without tanks were taken on a cloudy day while the ones with tanks on a sunny day! In reality, the network learn to recognize the weather, not the enemy tanks.","title":"A journey into Convolutional Neural Network visualization"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/notebook/#nosce-te-ipsum","text":"With this article, we are going to see different techniques to understand what it is going on inside a Convolutional Neural Network to avoid making the same US' army mistake. We are going to use Pytorch . All the code can be found here . Most of the visualizations were developed from scratch, however, some inspiration and parts were taken from here . We will first introduce each technique by briefly explain it and making some example and comparison between different classic computer vision models, alexnet , vgg16 and resnet . Then we will try to better understand a model used in robotics to predict the local distance sensor using only the frontal camera's images. Our goal is not to explain in detail how each technique works since this is already done extremely well by each paper, but to use them to help the reader visualize different model with different inputs to better understand and highlight what and how different models react to a given input. Later on, we show a workflow in which we utilize some of the techniques you will learn in this journey to test the robustness of a model, this is extremely useful to understand and fix its limitations. The curios reader could further improve is understand by looking and the source code for each visulisation and by reading the references.","title":"Nosce te ipsum"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/notebook/#preambula","text":"Disclaimer I am not a fan of jupyter. So apologize in advance if there are some warnings in the outputs and some figures are not well made Let's start our journey by selecting a network. Our first model will be the old school alexnet . It is already available in the torchvision.models package from Pytorch % load_ext autoreload % autoreload 2 from torchvision.models import * from visualisation.core.utils import device model = alexnet ( pretrained = True ) . to ( device ) print ( model ) AlexNet( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)) (1): ReLU(inplace=True) (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (4): ReLU(inplace=True) (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): ReLU(inplace=True) (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (9): ReLU(inplace=True) (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace=True) (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) ) (avgpool): AdaptiveAvgPool2d(output_size=(6, 6)) (classifier): Sequential( (0): Dropout(p=0.5, inplace=False) (1): Linear(in_features=9216, out_features=4096, bias=True) (2): ReLU(inplace=True) (3): Dropout(p=0.5, inplace=False) (4): Linear(in_features=4096, out_features=4096, bias=True) (5): ReLU(inplace=True) (6): Linear(in_features=4096, out_features=1000, bias=True) ) ) Now we need some inputs # %matplotlib notebook Now we need some inputs images. We are going to use three pictures, a cat, the beautiful Basilica di San Pietro and an image with a dog and a cat. import glob import matplotlib.pyplot as plt import numpy as np import torch from utils import * from PIL import Image plt . rcParams [ \"figure.figsize\" ] = 16 , 8 We loaded a few packages. In utils there are several utility function to creates the plots. import glob import matplotlib.pyplot as plt import numpy as np from visualisation.core.utils import device from PIL import Image image_paths = glob . glob ( './images/*.*' ) images = list ( map ( lambda x : Image . open ( x ), image_paths )) subplot ( images , title = 'inputs' , rows_titles = [ 'cat' , 'san pietro' , 'dog_cat' ], nrows = 1 , ncols = 3 ) Since all of our models were trained on imagenet , a huge dataset with 1000 different classes, we need to parse and normalize them. In Pytorch, we have to manually send the data to a device. In this case the device if the fist gpu if you have one, otherwise cpu is selected. Be aware that jupyter have not a garbage collected so we will need to manually free the gpu memory. from torchvision.transforms import ToTensor , Resize , Compose , ToPILImage from visualisation.core import * from visualisation.core.utils import image_net_preprocessing inputs = [ Compose ([ Resize (( 224 , 224 )), ToTensor (), image_net_preprocessing ])( x ) . unsqueeze ( 0 ) for x in images ] # add 1 dim for batch inputs = [ i . to ( device ) for i in inputs ] We also define an utility function to clean the gpu cache def free ( modules ): for m in modules : del m torch . cuda . empty_cache () As we said, imagenet is a huge dataset with 1000 classes, represented by an integer not very human interpetable. We can associate each class id to its label by loading the imaganet2human.txt and create a python dictionary. imagenet2human = {} with open ( 'imaganet2human.txt' ) as f : for line in f . readlines (): key , value = line . split ( ':' ) key = key . replace ( '{' , '' ) . replace ( '}' , '' ) # I forget how regex works :) value = value . replace ( \"'\" , '' ) . replace ( \",\" , '' ) imagenet2human [ int ( key . strip ())] = str ( value . strip ()) list ( imagenet2human . items ())[: 2 ] [(0, 'tench Tinca tinca'), (1, 'goldfish Carassius auratus')]","title":"Preambula"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/notebook/#weights-visualization","text":"The first straightforward visualization is to just plot the weights of a target Layer. Obviously, the deeper we go the smaller each image becomes while the channels number increases. We are going to show each channel as a grey array image. Unfortunately, each Pytorch module can be nested and nested, so to make our code as general as possible we first need to trace each sub-module that the input traverse and then store each layer in order. We first need to trace our model to get a list of all the layers so we can select a target layer without following the nested structure of a model. In PyTorch models can be infinitely nested. In other words, we are flattering the model's layers, this is implemented in the module2traced function. model_traced = module2traced ( model , inputs [ 0 ]) model_traced [Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)), ReLU(inplace=True), MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)), ReLU(inplace=True), MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False), AdaptiveAvgPool2d(output_size=(6, 6)), Dropout(p=0.5, inplace=False), Linear(in_features=9216, out_features=4096, bias=True), ReLU(inplace=True), Dropout(p=0.5, inplace=False), Linear(in_features=4096, out_features=4096, bias=True), ReLU(inplace=True), Linear(in_features=4096, out_features=1000, bias=True)] Let's plot the first layer's weight. We also print the shape of the weight to give a correct idea to the reader of the dimensional reduction. vis = Weights ( model , device ) first_layer = model_traced [ 0 ] plt . rcParams [ \"figure.figsize\" ] = 16 , 16 run_vis_plot ( vis , inputs [ 0 ], first_layer , ncols = 4 , nrows = 4 ) torch.Size([1, 55, 55]) Let's stop for a minute to explain what those images represent. We traced the input through the computational graph in order to find out all the layers of our models, in this case, alexnet . Then we instantiate the Weights class implemented in visualisation.core and we call it by passing the current input, the cat image and a target layer . As outputs, we get all the current layer's weights as grey images. Then, we plot 16 of them. We can notice that they, in some way, makes sense; for example, some pixels are brighter in the edges of the images. Let's plot the first MaxPool layer to better see this effect, dimensional reduction and higher brightness pixels in some interesting areas. If you are wondering what the maxpolling operations is doing, check this awesome repo first_maxpool_layer = model_traced [ 2 ] run_vis_plot ( vis , inputs [ 0 ], first_maxpool_layer , ncols = 4 , nrows = 4 ) torch.Size([1, 27, 27]) Let's try with an other input, the San Pietro Basilica run_vis_plot ( vis , inputs [ 1 ], first_maxpool_layer , ncols = 4 , nrows = 4 ) torch.Size([1, 27, 27]) By looking at them, these images make somehow sense; they highlight the basilica layout but it is hard to understand what the model is actually doing. We got the idea that is computing something correctly but we could ask some questions, for example: is it looking at the cupola? Which are the most important features of the Basilica? Moreover, the deeper we go the harder it becomes to even recognize the input. deeper_layer = model_traced [ 6 ] run_vis_plot ( vis , inputs [ 1 ], deeper_layer , ncols = 4 , nrows = 4 ) torch.Size([1, 13, 13]) In this case, we have no idea of what is going on. It can be argued that weights visualization does not carry any useful information about the model, even if this is almost true, there is one nice reason of plotting the weights especially at the first layer. When a model is poorly trained or not trained at all, the first weights have lots of noise, since they are just randomly initialized, and they are a lot more similar to the inputs images than the trained ones. This feature can be useful to understand on the fly is a model is trained or not. However, except for this, weights visualization is not the way to go to understand what your black box is thinking. Below we plot the first layer's weight first for the untraind version of alexnet and the for the trained one. alexnet_not_pretrained = alexnet ( pretrained = False ) . to ( device ) run_vis_plot ( Weights ( alexnet_not_pretrained , device ), inputs [ 0 ], module2traced ( alexnet_not_pretrained , inputs [ 0 ])[ 0 ], ncols = 4 , nrows = 4 ) alexnet_pretrained = alexnet ( pretrained = True ) . to ( device ) run_vis_plot ( Weights ( alexnet_pretrained , device ), inputs [ 0 ], module2traced ( alexnet_pretrained , inputs [ 0 ])[ 0 ], ncols = 4 , nrows = 4 ) del alexnet_not_pretrained torch.Size([1, 55, 55]) torch.Size([1, 55, 55]) You can notice that in the first image is simpler to see the input image. Hoewer, this is not a general rule, but in some cases it can help.","title":"Weights Visualization"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/notebook/#similarities-with-other-models","text":"We have seen alexnet 's weights, but are they similar across models? Below we plot the first 4 channel of each first layer's weight for alexnet , vgg and resnet modules_instances = [ alexnet , vgg16 , resnet34 ] modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], 0 , Weights , 'weights' , device , ncols = 4 ) free ( modules ) The resnet and vgg weights looks more similar to the input images than alexnet . But, again, what does it mean? Remember that at least resnet is initialized in a different way than the other two models.","title":"Similarities with other models"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/notebook/#saliency-visualisation","text":"One idea proposed by Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps is to back-prop the output of the network with respect to a target class until the input and plot the computed gradient. This will highligh the part of the image responsible for that class. Let's start with alexnet. Let's first print the prediction of the network (this could change if you re-run the cell) model . eval () pred = model ( inputs [ 0 ]) _ , id = torch . max ( pred , 1 ) print ( 'predicted class {} ' . format ( imagenet2human [ id . item ()])) predicted class tiger cat from visualisation.core.utils import image_net_postprocessing Each visualisation is implemented in its own class. You can find the code here . It will backproprop the output with respect to the one hot encoding representation of the number corresponding to class tiger cat model . eval () model = model . to ( device ) vis = SaliencyMap ( model , device ) out , info = vis ( inputs [ 0 ], first_layer ) subplot ([ image_net_postprocessing ( inputs [ 0 ] . squeeze () . cpu ()), out ], rows_titles = [ 'original' , 'saliency map' ], parse = tensor2img , nrows = 1 , ncols = 2 ) --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) ~\\AppData\\Local\\Temp/ipykernel_26720/4067497372.py in <module> 4 vis = SaliencyMap(model, device) 5 ----> 6 out, info = vis(inputs[0], 7 first_layer) 8 ~\\reighns_ml\\reighns_ml_blog\\docs\\reighns_ml_journey\\deep_learning\\computer_vision\\general\\neural_network_interpretation\\references\\A-journey-into-Convolutional-Neural-Network-visualization--master\\visualisation\\core\\SaliencyMap.py in __call__(self, input_image, layer, guide, target_class, regression) 66 67 ---> 68 image = self.gradients.data.cpu().numpy()[0] 69 # 70 image = convert_to_grayscale(image) AttributeError: 'NoneType' object has no attribute 'data' We can see that alexnet gets exited on the cat. We can even do better! We can set to 0 each negative relu gradient when backprop. This is techinique is called guided . out , info = vis ( inputs [ 0 ], first_layer , guide = True ) subplot ([ image_net_postprocessing ( inputs [ 0 ] . squeeze () . cpu ()), out ], rows_titles = [ 'original' , 'guided saliency map' ], parse = tensor2img , nrows = 1 , ncols = 2 ) Now we can clearly see that the network is looking at the eyes and the nose of the cat. We can try to compare different models modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], 0 , SaliencyMap , 'Saliency' , device , nrows = 1 , ncols = 3 , target_class = 231 , guide = True ) free ( modules ) Alextnet seems more interested to the eyes, while VGG looks at the ears and resnet is similar to alexnet . Now we can clearly understand which part of the inputs help the network gives that prediction. While guiding yields a better human interpretable image, the vanilla implementation can be used for localizing an object of interest. In other words, we can find object of interest for free by cropping out of the input image the region corresponding to the gradient. Let's plot each input image for each model. modules_instances = [ alexnet , vgg16 , resnet34 , resnet152 ] modules = ( m ( pretrained = True ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], None , SaliencyMap , 'SaliencyMap' , device , nrows = 4 , ncols = 3 , inputs = inputs , idx2label = imagenet2human , annotations = [ 'alexnet' , 'vgg16' , 'resnet34' , 'resnet152' ], guide = True ) free ( modules ) The Basilica is very interesting, all four networks correctly classify it as a dome but only resnet152 is more interested in the sky than on the cupola. In the last column, we have an image with two classes, dog and cat . All the networks highlighted booths, like the eyes of the dog and the ears of the cat in vgg16 . What if we would like to discover only the region of the inputs that are related to a specific class? With this technique is impossible.","title":"Saliency Visualisation"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/notebook/#class-activation-mapping","text":"Class Activation Mapping is a techniques presented in Learning Deep Features for Discriminative Localization . The idea is to use the last convolutional layer output and the neurons in the linear layer of the model responsable for a target class, the map is generated by taking the dot product of those. However, to make this work the model has to have some constrains. First of all, the output from the convolution must first go trought an global average polling and it requires feature maps to directly precede softmax layers. To make it works with other architecture, such as alexnet and vgg we have to change some layers in the model and retrain it. This is a major drawback that will be solved with the next section. For now, we can use it for free with resnet! Since its architecture is perfect. The implementation can be found here . We can pass to the visualisation a target_class parameter to get the relative weights from the fc layer. Notice that by changing the target class, we can see different part of the image highlighted. The first image uses the prediction class, while the second an other type of cat and the last one bookcase , just to see what the model will do with a wrong class. from visualisation.core.utils import imshow # we are using resnet 34 since the model has only one fc layer before the softmax and it is preceded by av avg pool # as required from the paper module = resnet34 ( True ) . to ( device ) module . eval () vis = ClassActivationMapping ( module , device ) classes = [ None , 285 , 453 ] def vis_outs2images_classes ( outs ): images = [ x [ 0 ] for x in outs ] classes = [ imagenet2human [ int ( x [ 1 ][ 'prediction' ])] for x in outs ] return images , classes outs = [ vis ( inputs [ 0 ], None , postprocessing = image_net_postprocessing , target_class = c , guide = True ) for c in classes ] images , classes = vis_outs2images_classes ( outs ) subplot ( images , rows_titles = classes , nrows = 1 , ncols = 3 , parse = tensor2img ) It makes sense, the only thing is that in the last row we still have some part of the cat highlighted for bookcase Let's plot the CAM on the cat images for different resnet architecture. For resnet > 34 the Bottleneck module is used modules_instances = [ resnet18 , resnet34 , resnet101 , resnet152 ] cat = inputs [ 2 ] modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , cat , None , ClassActivationMapping , 'ClassActivationMapping' , device , nrows = len ( modules_instances ), ncols = 1 , postprocessing = image_net_postprocessing , rows_name = [ 'resnet18' , 'resnet34' , 'resnet101' , 'resnet152' ], target_class = None ) free ( modules ) They are all very similar as expected. One big drawback of this technique is that force you to use a network with a specific architecture, global polling before the decoder part. The next technique generalize this approach by taking advantage of the gradient at one specific layer. Remember that with the class activation we are using the weights of the feature map as a scaling factor for the channels of the last layer. The features map must be before a softmax layer and right after the average pooling. The next technique propose a more general approach.","title":"Class Activation Mapping"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/notebook/#grad-cam","text":"Grad Cam was introduced by Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization . The idea is actually simple, we backprop the output with respect to a target class while storing the gradient and the output at a given layer, in our case the last convolution. Then we perform a global average of the saved gradient keeping the channel dimension in order to get a 1-d tensor, this will represent the importance of each channel in the target convolutional layer. We then multiply each element of the convolutional layer outputs by the averaged gradients to create the grad cam. This whole procedure is fast and it is architecture independent. Interesting, the authors show that is a generalization of the previous technique. The code is here We can use it to higlight what different models are looking at. modules_instances = [ alexnet , vgg16 , resnet34 , resnet152 ] modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], None , GradCam , 'Gradcam' , device , nrows = 1 , ncols = 4 , target_class = None , postprocessing = image_net_postprocessing ) free ( modules ) C:\\Users\\reighns\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1025: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior. warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \" torch.Size([13, 13]) torch.Size([1, 3, 224, 224]) torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) torch.Size([14, 14]) torch.Size([1, 3, 224, 224]) torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) torch.Size([7, 7]) torch.Size([1, 3, 224, 224]) torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) torch.Size([7, 7]) torch.Size([1, 3, 224, 224]) torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) It is really interesting to see how alexnet looks at the nose, while vgg at the ears and resnet at the whole cat. It is interesting to see that the two resnet version looks at different part of the cat. Below we plot the same input for resnet34 but we change the target class in each column to show the reader how the grad cam change accordingly. from visualisation.core.utils import imshow module = module . to ( device ) vis = GradCam ( module , device ) classes = [ None , 285 , 453 ] outs = [ vis ( inputs [ 0 ], None , postprocessing = image_net_postprocessing , target_class = c ) for c in classes ] images , classes = vis_outs2images_classes ( outs ) subplot ( images , title = 'resnet34' , rows_titles = classes , nrows = 1 , ncols = len ( outs ), parse = tensor2img ) Notice how similar to the CAM output they are. To better compore our three models, below we plot the grad cam for each input with respect to each model modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], None , GradCam , 'Gradcam' , device , nrows = 4 , ncols = 3 , target_class = None , inputs = inputs , idx2label = imagenet2human , annotations = [ 'alexnet' , 'vgg16' , 'resnet34' , 'resnet152' ], postprocessing = image_net_postprocessing ) free ( modules ) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). The reader can immediately notice the difference across the models.","title":"Grad Cam"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/notebook/#interesting-region","text":"We talk before about interesting region localizations. Grad-cam can be also used to extract the class object out of the image. Easily, once the have the grad-cam image we can used it as mask to crop out form the input image what we want. The reader can play with the TR parameter to see different effects. TR = 0.3 alexnet_pretrained . eval () vis = GradCam ( alexnet_pretrained , device ) _ = vis ( inputs [ 0 ], None , postprocessing = image_net_postprocessing ) import cv2 def gradcam2crop ( cam , original_img ): b , c , w , h = inputs [ 0 ] . shape cam = cam . numpy () cam -= np . min ( cam ) cam /= np . max ( cam ) cam = cv2 . resize ( cam , ( w , h )) mask = cam > TR original_img = tensor2img ( image_net_postprocessing ( original_img [ 0 ] . squeeze ())) crop = original_img . copy () crop [ mask == 0 ] = 0 return crop crop = gradcam2crop ( vis . cam . cpu (), inputs [ 0 ] . cpu ()) fig = plt . figure () plt . imshow ( crop ) <matplotlib.image.AxesImage at 0x7f79debee048> et voil\u00e0 ! We can also change again class, and crop the interest region for that class. _ = vis ( inputs [ 0 ], None , target_class = 231 , postprocessing = image_net_postprocessing ) crop = gradcam2crop ( vis . cam . cpu (), inputs [ 0 ] . cpu ()) fig = plt . figure () plt . imshow ( crop ) <matplotlib.image.AxesImage at 0x7f79f40c4c18>","title":"Interesting region"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/notebook/#different-models","text":"We have seen all these techniques used with classic classicification models trained on imagenet . What about use them on a different domain? I have ported this paper to Pytorch and retrain it. The model learn from the frontal camera's image of a robot to predict the local distance sensors in order to avoid obstacles. Let's see what if, by using those techniques, we can understand better what is going on inside the model.","title":"Different models"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/notebook/#learning-long-range-perception-using-self-supervision-from-short-range-sensors-and-odometry","text":"The idea is to predict the future outputs of a short-range sensor (such as a proximity sensor) given the current outputs of a long-range sensor (such as a camera). They trained a very simple CNN from the robot's camera images to predict the proximity sensor values. If you are interested in their work, you can read the full paper here I have made a PyTorch implementation and retrain the model from scratch. Be awere that I did not fine-tune or try different sets of hyper-parameters so probably my model is not performing as well as the author's one. Let's import it from os import path LONG_RANGE_PERCEPTION_PATH = path . abspath ( './models/long_range_perception/model.pt' ) from models.long_range_perception.model import SimpleCNN from models.long_range_perception.utils import get_dl , H5_PATH , imshow , post_processing , pre_processing , MODEL_PATH free ([ module ]) module = torch . load ( LONG_RANGE_PERCEPTION_PATH , map_location = lambda storage , loc : storage ) module = module . to ( device ) module /home/francesco/Documents/A-journey-into-Convolutional-Neural-Network-visualization-/model.pt SimpleCNN( (encoder): Sequential( (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): ReLU() (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (6): Conv2d(10, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): ReLU() (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (decoder): Sequential( (0): Dropout(p=0.2) (1): Linear(in_features=640, out_features=256, bias=True) (2): ReLU() (3): Linear(in_features=256, out_features=325, bias=True) (4): Sigmoid() ) ) We know need some inputs to test the model, they are taken directly from the test set import os def make_and_show_inputs ( path , transform ): image_paths = glob . glob ( path ) image_paths = filter ( lambda x : os . path . isfile ( x ), image_paths ) images = list ( map ( lambda x : Image . open ( x ) . convert ( 'RGB' ), image_paths )) subplot ( images , title = 'inputs' , rows_titles = [ '1' , '2' , '3' , '4' ], nrows = 1 , ncols = 4 ) plt . show () inputs = [ pre_processing ( x ) . unsqueeze ( 0 ) . to ( device ) for x in images ] # add 1 dim for batch subplot ( inputs , parse = tensor2img , title = 'inputs' , rows_titles = [ '1' , '2' , '3' , '4' ], nrows = 1 , ncols = 4 ) return images , inputs images , inputs = make_and_show_inputs ( 'images/long_range_perception/*' , pre_processing ) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Then author normalize each image, this is done by callind pre_processing . For some reason the inpupts images are different on mac and ubuntu, they should not be like these if you run the notebook on mac the result is different. This is probably due to the warning message. We are going to use the SaliencyMap and the GradCam since those are the best from torch.autograd import Variable module . eval () def run_long_range_vis (): grad = GradCam ( module , device ) all_true = torch . ones ( 1 , 65 * 5 ) . float () . to ( device ) outs_grad = [ grad ( input , None , target_class = all_true , postprocessing = post_processing , regression = True )[ 0 ] for input in inputs ] sal = SaliencyMap ( module , device ) outs_saliency = [ sal ( input , None , guide = True , target_class = all_true , regression = True )[ 0 ] for input in inputs ] subplot ([ * outs_grad , * outs_saliency ], title = 'long_range' , cols_titles = [ '1' , '2' , '3' , '4' ], nrows = 2 , ncols = 4 , parse = tensor2img ) run_long_range_vis () We can clearly see that the model looks at the objects. In the GradCam row, on the second picture, the plan is basically segmented by the heatmap. There is one problem, if you look at the third picture, the white box in front of the camera is not clearly highlighted. This is probably due to the white color of the floor that is very similar to the box's color. Let's investigate this problem. In the second row, the SaliencyMaps highlights all the objects, including the white box. The reader can notice that the reflection in the first picture on the left seems to excite the network in that region. We should also investigate this case but due to time limitations, we will leave it as an exercise for the curious reader. For completeness, let's also print the predicted sensor output. The model tries to predict five frontal distance sensors give the image camera. import seaborn as sns module . eval () preds = module ( torch . stack ( inputs ) . squeeze ( 1 )) fig = plt . figure () sns . heatmap ( preds [ 2 ] . view ( - 1 , 5 ) . detach () . cpu () . numpy ()) <matplotlib.axes._subplots.AxesSubplot at 0x7f79d26dc240> If you compare with the authors pictures, my prediction are worse. This is due to the fact that to speed up everything I did not used all the training set and I did not perform any hyper paramater optimisation. All the code con be found here . Let's now investigate the first problem, object with a similar color to the ground.","title":"Learning Long-range Perception using Self-Supervision from Short-Range Sensors and Odometry"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/notebook/#similar-colors","text":"To test if the model has a problem with obstacles with a the same color of the ground, we created in blender four different scenarios with an obstacle. They are showed in the picture below. image_paths = [ * sorted ( glob . glob ( 'images/long_range_perception/equal_color/*' )), * sorted ( glob . glob ( 'images/long_range_perception/different_color/*' ))] image_paths = filter ( lambda x : os . path . isfile ( x ), image_paths ) images = list ( map ( lambda x : Image . open ( x ) . convert ( 'RGB' ), image_paths )) subplot ( images , title = 'inputs' , nrows = 2 , ncols = 4 ) plt . show () There are four different lights configuration and two differents cube colors, one equal to the ground and the second different. The first column represents a realistic situation, while the second has a really strong light from behind that generates a shadow in front of the camera. The third column has a shadow on the left and the last one has a little shadow on the left. This is a perfect scenario to use gradcam to see what the model is looking in each image. In the picture below we plotted the gradcam results. inputs = [ pre_processing ( x ) . unsqueeze ( 0 ) . to ( device ) for x in images ] # add 1 dim for batch run_long_range_vis () The big black shadow in the second column definitly confuses the model. In the first and last column, the grad cam highlights better the corners of the red cube, especially in the first picture. We can definitely say that this model has some hard time with the object of the same colour as the ground. Thanks to this consideration, we could improve the number equal object/ground in the dataset, perform a better preprocessing, change the model structure etc and hopefully increase the robustness of the network.","title":"Similar colors"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/notebook/#conclusion","text":"In this article, we present different convolutional neural network visualization techniques. In the first section, we introduced each one by applying to a set of famous classification networks. We compared different networks on different inputs and highlight the similarities and difference between them. Then we apply them to a model adopted in robotics to test its robustness and we were able to successfully reveal a problem in the network. Moreover, as a side project, I developed an interactive convolutional neural network visualization application called mirro that receives in just a few days more than a hundred stars on GitHub reflecting the interest of the deep learning community on this topic. All these visualizations are implemented using a common interface and there are available as python module so they can be used in any other module. Thank for reading Francesco Saverio Zuppichini","title":"Conclusion"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/medium/notebook/","text":"A journey into Convolutional Neural Network visualization Francesco Saverio Zuppichini There is one famous urban legend about computer vision. Around the 80s, the US military wanted to use neural networks to automatically detect camouflaged enemy tanks. They took a number of pictures of trees without tanks and then pictures with the same trees with tanks behind them. The results were impressive. So impressive that the army wanted to be sure the net had correctly generalized. They took new pictures of woods with and without tanks and they showed them again to the network. This time, the model performed terribly, it was not able to discriminate between pictures with tanks behind woods and just trees.It turned out that all the pictures without tanks were taken on a cloudy day while the ones with tanks on a sunny day! In reality, the network learn to recognize the weather, not the enemy tanks. Nosce te ipsum With this article, we are going to see different techniques to understand what it is going on inside a Convolutional Neural Network to avoid making the same US' army mistake. We are going to use Pytorch . All the code can be found here . Most of the visualizations were developed from scratch, however, some inspiration and parts were taken from here . We will first introduce each technique by briefly explain it and making some example and comparison between different classic computer vision models, alexnet , vgg16 and resnet . Then we will try to better understand a model used in robotics to predict the local distance sensor using only the frontal camera's images. Our goal is not to explain in detail how each technique works since this is already done extremely well by each paper, but to use them to help the reader visualize different model with different inputs to better understand and highlight what and how different models react to a given input. Later on, we show a workflow in which we utilize some of the techniques you will learn in this journey to test the robustness of a model, this is extremely useful to understand and fix its limitations. The curios reader could further improve is understand by looking and the source code for each visulisation and by reading the references. Preambula Disclaimer I am not a fan of jupyter. So apologize in advance if there are some warnings in the outputs and some figures are not well made Let's start our journey by selecting a network. Our first model will be the old school alexnet . It is already available in the torchvision.models package from Pytorch % load_ext autoreload % autoreload 2 from torchvision.models import * from visualisation.core.utils import device model = alexnet ( pretrained = True ) . to ( device ) print ( model ) AlexNet( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)) (1): ReLU(inplace) (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (4): ReLU(inplace) (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): ReLU(inplace) (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (9): ReLU(inplace) (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace) (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) ) (classifier): Sequential( (0): Dropout(p=0.5) (1): Linear(in_features=9216, out_features=4096, bias=True) (2): ReLU(inplace) (3): Dropout(p=0.5) (4): Linear(in_features=4096, out_features=4096, bias=True) (5): ReLU(inplace) (6): Linear(in_features=4096, out_features=1000, bias=True) ) ) Now we need some inputs # %matplotlib notebook Now we need some inputs images. We are going to use three pictures, a cat, the beautiful Basilica di San Pietro and an image with a dog and a cat. import glob import matplotlib.pyplot as plt import numpy as np import torch from utils import * from PIL import Image plt . rcParams [ \"figure.figsize\" ] = 16 , 8 We loaded a few packages. In utils there are several utility function to creates the plots. import glob import matplotlib.pyplot as plt import numpy as np from visualisation.core.utils import device from PIL import Image image_paths = glob . glob ( './images/*.*' ) images = list ( map ( lambda x : Image . open ( x ), image_paths )) subplot ( images , title = 'inputs' , rows_titles = [ 'cat' , 'san pietro' , 'dog_cat' ], nrows = 1 , ncols = 3 ) Since all of our models were trained on imagenet , a huge dataset with 1000 different classes, we need to parse and normalize them. In Pytorch, we have to manually send the data to a device. In this case the device if the fist gpu if you have one, otherwise cpu is selected. Be aware that jupyter have not a garbage collected so we will need to manually free the gpu memory. from torchvision.transforms import ToTensor , Resize , Compose , ToPILImage from visualisation.core import * from visualisation.core.utils import image_net_preprocessing inputs = [ Compose ([ Resize (( 224 , 224 )), ToTensor (), image_net_preprocessing ])( x ) . unsqueeze ( 0 ) for x in images ] # add 1 dim for batch inputs = [ i . to ( device ) for i in inputs ] We also define an utility function to clean the gpu cache def free ( modules ): for m in modules : del m torch . cuda . empty_cache () As we said, imagenet is a huge dataset with 1000 classes, represented by an integer not very human interpetable. We can associate each class id to its label by loading the imaganet2human.txt and create a python dictionary. imagenet2human = {} with open ( 'imaganet2human.txt' ) as f : for line in f . readlines (): key , value = line . split ( ':' ) key = key . replace ( '{' , '' ) . replace ( '}' , '' ) # I forget how regex works :) value = value . replace ( \"'\" , '' ) . replace ( \",\" , '' ) imagenet2human [ int ( key . strip ())] = str ( value . strip ()) list ( imagenet2human . items ())[: 2 ] [(0, 'tench Tinca tinca'), (1, 'goldfish Carassius auratus')] Weights Visualization The first straightforward visualization is to just plot the weights of a target Layer. Obviously, the deeper we go the smaller each image becomes while the channels number increases. We are going to show each channel as a grey array image. Unfortunately, each Pytorch module can be nested and nested, so to make our code as general as possible we first need to trace each sub-module that the input traverse and then store each layer in order. We first need to trace our model to get a list of all the layers so we can select a target layer without following the nested structure of a model. In PyTorch models can be infinitely nested. In other words, we are flattering the model's layers, this is implemented in the module2traced function. model_traced = module2traced ( model , inputs [ 0 ]) model_traced [Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)), ReLU(inplace), MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)), ReLU(inplace), MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace), Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace), MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False), Dropout(p=0.5), Linear(in_features=9216, out_features=4096, bias=True), ReLU(inplace), Dropout(p=0.5), Linear(in_features=4096, out_features=4096, bias=True), ReLU(inplace), Linear(in_features=4096, out_features=1000, bias=True)] Let's plot the first layer's weight. We also print the shape of the weight to give a correct idea to the reader of the dimensional reduction. vis = Weights ( model , device ) first_layer = model_traced [ 0 ] plt . rcParams [ \"figure.figsize\" ] = 16 , 16 run_vis_plot ( vis , inputs [ 0 ], first_layer , ncols = 4 , nrows = 4 ) torch.Size([1, 55, 55]) Let's stop for a minute to explain what those images represent. We traced the input through the computational graph in order to find out all the layers of our models, in this case, alexnet . Then we instantiate the Weights class implemented in visualisation.core and we call it by passing the current input, the cat image and a target layer . As outputs, we get all the current layer's weights as grey images. Then, we plot 16 of them. We can notice that they, in some way, makes sense; for example, some pixels are brighter in the edges of the images. Let's plot the first MaxPool layer to better see this effect, dimensional reduction and higher brightness pixels in some interesting areas. If you are wondering what the maxpolling operations is doing, check this awesome repo first_maxpool_layer = model_traced [ 2 ] run_vis_plot ( vis , inputs [ 0 ], first_maxpool_layer , ncols = 4 , nrows = 4 ) torch.Size([1, 27, 27]) Let's try with an other input, the San Pietro Basilica run_vis_plot ( vis , inputs [ 1 ], first_maxpool_layer , ncols = 4 , nrows = 4 ) torch.Size([1, 27, 27]) By looking at them, these images make somehow sense; they highlight the basilica layout but it is hard to understand what the model is actually doing. We got the idea that is computing something correctly but we could ask some questions, for example: is it looking at the cupola? Which are the most important features of the Basilica? Moreover, the deeper we go the harder it becomes to even recognize the input. deeper_layer = model_traced [ 6 ] run_vis_plot ( vis , inputs [ 1 ], deeper_layer , ncols = 4 , nrows = 4 ) torch.Size([1, 13, 13]) In this case, we have no idea of what is going on. It can be argued that weights visualization does not carry any useful information about the model, even if this is almost true, there is one nice reason of plotting the weights especially at the first layer. When a model is poorly trained or not trained at all, the first weights have lots of noise, since they are just randomly initialized, and they are a lot more similar to the inputs images than the trained ones. This feature can be useful to understand on the fly is a model is trained or not. However, except for this, weights visualization is not the way to go to understand what your black box is thinking. Below we plot the first layer's weight first for the untraind version of alexnet and the for the trained one. alexnet_not_pretrained = alexnet ( pretrained = False ) . to ( device ) run_vis_plot ( Weights ( alexnet_not_pretrained , device ), inputs [ 0 ], module2traced ( alexnet_not_pretrained , inputs [ 0 ])[ 0 ], ncols = 4 , nrows = 4 ) alexnet_pretrained = alexnet ( pretrained = True ) . to ( device ) run_vis_plot ( Weights ( alexnet_pretrained , device ), inputs [ 0 ], module2traced ( alexnet_pretrained , inputs [ 0 ])[ 0 ], ncols = 4 , nrows = 4 ) del alexnet_not_pretrained torch.Size([1, 55, 55]) torch.Size([1, 55, 55]) You can notice that in the first image is simpler to see the input image. Hoewer, this is not a general rule, but in some cases it can help. Similarities with other models We have seen alexnet 's weights, but are they similar across models? Below we plot the first 4 channel of each first layer's weight for alexnet , vgg and resnet modules_instances = [ alexnet , vgg16 , resnet34 ] modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], 0 , Weights , 'weights' , device , ncols = 4 ) free ( modules ) The resnet and vgg weights looks more similar to the input images than alexnet . But, again, what does it mean? Remember that at least resnet is initialized in a different way than the other two models. Saliency Visualisation One idea proposed by Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps is to back-prop the output of the network with respect to a target class until the input and plot the computed gradient. This will highligh the part of the image responsible for that class. Let's start with alexnet. Let's first print the prediction of the network (this could change if you re-run the cell) model . eval () pred = model ( inputs [ 0 ]) _ , id = torch . max ( pred , 1 ) print ( 'predicted class {} ' . format ( imagenet2human [ id . item ()])) predicted class tiger cat Each visualisation is implemented in its own class. You can find the code here . It will backproprop the output with respect to the one hot encoding representation of the number corresponding to class tiger cat from visualisation.core.utils import image_net_postprocessing model . eval () model = model . to ( device ) vis = SaliencyMap ( model , device ) out , info = vis ( inputs [ 0 ], first_layer ) subplot ([ image_net_postprocessing ( inputs [ 0 ] . squeeze () . cpu ()), out ], rows_titles = [ 'original' , 'saliency map' ], parse = tensor2img , nrows = 1 , ncols = 2 ) We can see that alexnet gets exited on the cat. We can even do better! We can set to 0 each negative relu gradient when backprop. This is techinique is called guided . out , info = vis ( inputs [ 0 ], first_layer , guide = True ) subplot ([ image_net_postprocessing ( inputs [ 0 ] . squeeze () . cpu ()), out ], rows_titles = [ 'original' , 'guided saliency map' ], parse = tensor2img , nrows = 1 , ncols = 2 ) Now we can clearly see that the network is looking at the eyes and the nose of the cat. We can try to compare different models modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], 0 , SaliencyMap , 'Saliency' , device , nrows = 1 , ncols = 3 , target_class = 231 , guide = True ) free ( modules ) Alextnet seems more interested to the eyes, while VGG looks at the ears and resnet is similar to alexnet . Now we can clearly understand which part of the inputs help the network gives that prediction. While guiding yields a better human interpretable image, the vanilla implementation can be used for localizing an object of interest. In other words, we can find object of interest for free by cropping out of the input image the region corresponding to the gradient. Let's plot each input image for each model. modules_instances = [ alexnet , vgg16 , resnet34 , resnet152 ] modules = ( m ( pretrained = True ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], None , SaliencyMap , 'SaliencyMap' , device , nrows = 4 , ncols = 3 , inputs = inputs , idx2label = imagenet2human , annotations = [ 'alexnet' , 'vgg16' , 'resnet34' , 'resnet152' ], guide = True ) free ( modules ) The Basilica is very interesting, all four networks correctly classify it as a dome but only resnet152 is more interested in the sky than on the cupola. In the last column, we have an image with two classes, dog and cat . All the networks highlighted booths, like the eyes of the dog and the ears of the cat in vgg16 . What if we would like to discover only the region of the inputs that are related to a specific class? With this technique is impossible. Class Activation Mapping Class Activation Mapping is a techniques presented in Learning Deep Features for Discriminative Localization . The idea is to use the last convolutional layer output and the neurons in the linear layer of the model responsable for a target class, the map is generated by taking the dot product of those. However, to make this work the model has to have some constrains. First of all, the output from the convolution must first go trought an global average polling and it requires feature maps to directly precede softmax layers. To make it works with other architecture, such as alexnet and vgg we have to change some layers in the model and retrain it. This is a major drawback that will be solved with the next section. For now, we can use it for free with resnet! Since its architecture is perfect. The implementation can be found here . We can pass to the visualisation a target_class parameter to get the relative weights from the fc layer. Notice that by changing the target class, we can see different part of the image highlighted. The first image uses the prediction class, while the second an other type of cat and the last one bookcase , just to see what the model will do with a wrong class. from visualisation.core.utils import imshow # we are using resnet 34 since the model has only one fc layer before the softmax and it is preceded by av avg pool # as required from the paper module = resnet34 ( True ) . to ( device ) module . eval () vis = ClassActivationMapping ( module , device ) classes = [ None , 285 , 453 ] def vis_outs2images_classes ( outs ): images = [ x [ 0 ] for x in outs ] classes = [ imagenet2human [ int ( x [ 1 ][ 'prediction' ])] for x in outs ] return images , classes outs = [ vis ( inputs [ 0 ], None , postprocessing = image_net_postprocessing , target_class = c , guide = True ) for c in classes ] images , classes = vis_outs2images_classes ( outs ) subplot ( images , rows_titles = classes , nrows = 1 , ncols = 3 , parse = tensor2img ) It makes sense, the only thing is that in the last row we still have some part of the cat highlighted for bookcase Let's plot the CAM on the cat images for different resnet architecture. For resnet > 34 the Bottleneck module is used modules_instances = [ resnet18 , resnet34 , resnet101 , resnet152 ] cat = inputs [ 2 ] modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , cat , None , ClassActivationMapping , 'ClassActivationMapping' , device , nrows = len ( modules_instances ), ncols = 1 , postprocessing = image_net_postprocessing , rows_name = [ 'resnet18' , 'resnet34' , 'resnet101' , 'resnet152' ], target_class = None ) free ( modules ) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). They are all very similar as expected. One big drawback of this technique is that force you to use a network with a specific architecture, global polling before the decoder part. The next technique generalize this approach by taking advantage of the gradient at one specific layer. Remember that with the class activation we are using the weights of the feature map as a scaling factor for the channels of the last layer. The features map must be before a softmax layer and right after the average pooling. The next technique propose a more general approach. Grad Cam Grad Cam was introduced by Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization . The idea is actually simple, we backprop the output with respect to a target class while storing the gradient and the output at a given layer, in our case the last convolution. Then we perform a global average of the saved gradient keeping the channel dimension in order to get a 1-d tensor, this will represent the importance of each channel in the target convolutional layer. We then multiply each element of the convolutional layer outputs by the averaged gradients to create the grad cam. This whole procedure is fast and it is architecture independent. Interesting, the authors show that is a generalization of the previous technique. The code is here We can use it to higlight what different models are looking at. modules_instances = [ alexnet , vgg16 , resnet34 , resnet152 ] modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], None , GradCam , 'Gradcam' , device , nrows = 1 , ncols = 4 , target_class = None , postprocessing = image_net_postprocessing ) free ( modules ) It is really interesting to see how alexnet looks at the nose, while vgg at the ears and resnet at the whole cat. It is interesting to see that the two resnet version looks at different part of the cat. Below we plot the same input for resnet34 but we change the target class in each column to show the reader how the grad cam change accordingly. from visualisation.core.utils import imshow module = module . to ( device ) vis = GradCam ( module , device ) classes = [ None , 285 , 453 ] outs = [ vis ( inputs [ 0 ], None , postprocessing = image_net_postprocessing , target_class = c ) for c in classes ] images , classes = vis_outs2images_classes ( outs ) subplot ( images , title = 'resnet34' , rows_titles = classes , nrows = 1 , ncols = len ( outs ), parse = tensor2img ) Notice how similar to the CAM output they are. To better compore our three models, below we plot the grad cam for each input with respect to each model modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], None , GradCam , 'Gradcam' , device , nrows = 4 , ncols = 3 , target_class = None , inputs = inputs , idx2label = imagenet2human , annotations = [ 'alexnet' , 'vgg16' , 'resnet34' , 'resnet152' ], postprocessing = image_net_postprocessing ) free ( modules ) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). The reader can immediately notice the difference across the models. Interesting region We talk before about interesting region localizations. Grad-cam can be also used to extract the class object out of the image. Easily, once the have the grad-cam image we can used it as mask to crop out form the input image what we want. The reader can play with the TR parameter to see different effects. TR = 0.3 alexnet_pretrained . eval () vis = GradCam ( alexnet_pretrained , device ) _ = vis ( inputs [ 0 ], None , postprocessing = image_net_postprocessing ) import cv2 def gradcam2crop ( cam , original_img ): b , c , w , h = inputs [ 0 ] . shape cam = cam . numpy () cam -= np . min ( cam ) cam /= np . max ( cam ) cam = cv2 . resize ( cam , ( w , h )) mask = cam > TR original_img = tensor2img ( image_net_postprocessing ( original_img [ 0 ] . squeeze ())) crop = original_img . copy () crop [ mask == 0 ] = 0 return crop crop = gradcam2crop ( vis . cam . cpu (), inputs [ 0 ] . cpu ()) fig = plt . figure () plt . imshow ( crop ) <matplotlib.image.AxesImage at 0x7f79debee048> et voil\u00e0 ! We can also change again class, and crop the interest region for that class. _ = vis ( inputs [ 0 ], None , target_class = 231 , postprocessing = image_net_postprocessing ) crop = gradcam2crop ( vis . cam . cpu (), inputs [ 0 ] . cpu ()) fig = plt . figure () plt . imshow ( crop ) <matplotlib.image.AxesImage at 0x7f79f40c4c18> Different models We have seen all these techniques used with classic classicification models trained on imagenet . What about use them on a different domain? I have ported this paper to Pytorch and retrain it. The model learn from the frontal camera's image of a robot to predict the local distance sensors in order to avoid obstacles. Let's see what if, by using those techniques, we can understand better what is going on inside the model. Learning Long-range Perception using Self-Supervision from Short-Range Sensors and Odometry The idea is to predict the future outputs of a short-range sensor (such as a proximity sensor) given the current outputs of a long-range sensor (such as a camera). They trained a very simple CNN from the robot's camera images to predict the proximity sensor values. If you are interested in their work, you can read the full paper here I have made a PyTorch implementation and retrain the model from scratch. Be awere that I did not fine-tune or try different sets of hyper-parameters so probably my model is not performing as well as the author's one. Let's import it from os import path LONG_RANGE_PERCEPTION_PATH = path . abspath ( './models/long_range_perception/model.pt' ) from models.long_range_perception.model import SimpleCNN from models.long_range_perception.utils import get_dl , H5_PATH , imshow , post_processing , pre_processing , MODEL_PATH free ([ module ]) module = torch . load ( LONG_RANGE_PERCEPTION_PATH , map_location = lambda storage , loc : storage ) module = module . to ( device ) module /home/francesco/Documents/A-journey-into-Convolutional-Neural-Network-visualization-/model.pt SimpleCNN( (encoder): Sequential( (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): ReLU() (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (6): Conv2d(10, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): ReLU() (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (decoder): Sequential( (0): Dropout(p=0.2) (1): Linear(in_features=640, out_features=256, bias=True) (2): ReLU() (3): Linear(in_features=256, out_features=325, bias=True) (4): Sigmoid() ) ) We know need some inputs to test the model, they are taken directly from the test set import os def make_and_show_inputs ( path , transform ): image_paths = glob . glob ( path ) image_paths = filter ( lambda x : os . path . isfile ( x ), image_paths ) images = list ( map ( lambda x : Image . open ( x ) . convert ( 'RGB' ), image_paths )) subplot ( images , title = 'inputs' , rows_titles = [ '1' , '2' , '3' , '4' ], nrows = 1 , ncols = 4 ) plt . show () inputs = [ pre_processing ( x ) . unsqueeze ( 0 ) . to ( device ) for x in images ] # add 1 dim for batch subplot ( inputs , parse = tensor2img , title = 'inputs' , rows_titles = [ '1' , '2' , '3' , '4' ], nrows = 1 , ncols = 4 ) return images , inputs images , inputs = make_and_show_inputs ( 'images/long_range_perception/*' , pre_processing ) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Then author normalize each image, this is done by callind pre_processing . For some reason the inpupts images are different on mac and ubuntu, they should not be like these if you run the notebook on mac the result is different. This is probably due to the warning message. We are going to use the SaliencyMap and the GradCam since those are the best from torch.autograd import Variable module . eval () def run_long_range_vis (): grad = GradCam ( module , device ) all_true = torch . ones ( 1 , 65 * 5 ) . float () . to ( device ) outs_grad = [ grad ( input , None , target_class = all_true , postprocessing = post_processing , regression = True )[ 0 ] for input in inputs ] sal = SaliencyMap ( module , device ) outs_saliency = [ sal ( input , None , guide = True , target_class = all_true , regression = True )[ 0 ] for input in inputs ] subplot ([ * outs_grad , * outs_saliency ], title = 'long_range' , cols_titles = [ '1' , '2' , '3' , '4' ], nrows = 2 , ncols = 4 , parse = tensor2img ) run_long_range_vis () We can clearly see that the model looks at the objects. In the GradCam row, on the second picture, the plan is basically segmented by the heatmap. There is one problem, if you look at the third picture, the white box in front of the camera is not clearly highlighted. This is probably due to the white color of the floor that is very similar to the box's color. Let's investigate this problem. In the second row, the SaliencyMaps highlights all the objects, including the white box. The reader can notice that the reflection in the first picture on the left seems to excite the network in that region. We should also investigate this case but due to time limitations, we will leave it as an exercise for the curious reader. For completeness, let's also print the predicted sensor output. The model tries to predict five frontal distance sensors give the image camera. import seaborn as sns module . eval () preds = module ( torch . stack ( inputs ) . squeeze ( 1 )) fig = plt . figure () sns . heatmap ( preds [ 2 ] . view ( - 1 , 5 ) . detach () . cpu () . numpy ()) <matplotlib.axes._subplots.AxesSubplot at 0x7f79d26dc240> If you compare with the authors pictures, my prediction are worse. This is due to the fact that to speed up everything I did not used all the training set and I did not perform any hyper paramater optimisation. All the code con be found here . Let's now investigate the first problem, object with a similar color to the ground. Similar colors To test if the model has a problem with obstacles with a the same color of the ground, we created in blender four different scenarios with an obstacle. They are showed in the picture below. image_paths = [ * sorted ( glob . glob ( 'images/long_range_perception/equal_color/*' )), * sorted ( glob . glob ( 'images/long_range_perception/different_color/*' ))] image_paths = filter ( lambda x : os . path . isfile ( x ), image_paths ) images = list ( map ( lambda x : Image . open ( x ) . convert ( 'RGB' ), image_paths )) subplot ( images , title = 'inputs' , nrows = 2 , ncols = 4 ) plt . show () There are four different lights configuration and two differents cube colors, one equal to the ground and the second different. The first column represents a realistic situation, while the second has a really strong light from behind that generates a shadow in front of the camera. The third column has a shadow on the left and the last one has a little shadow on the left. This is a perfect scenario to use gradcam to see what the model is looking in each image. In the picture below we plotted the gradcam results. inputs = [ pre_processing ( x ) . unsqueeze ( 0 ) . to ( device ) for x in images ] # add 1 dim for batch run_long_range_vis () The big black shadow in the second column definitly confuses the model. In the first and last column, the grad cam highlights better the corners of the red cube, especially in the first picture. We can definitely say that this model has some hard time with the object of the same colour as the ground. Thanks to this consideration, we could improve the number equal object/ground in the dataset, perform a better preprocessing, change the model structure etc and hopefully increase the robustness of the network. Conclusion In this article, we present different convolutional neural network visualization techniques. In the first section, we introduced each one by applying to a set of famous classification networks. We compared different networks on different inputs and highlight the similarities and difference between them. Then we apply them to a model adopted in robotics to test its robustness and we were able to successfully reveal a problem in the network. Moreover, as a side project, I developed an interactive convolutional neural network visualization application called mirro that receives in just a few days more than a hundred stars on GitHub reflecting the interest of the deep learning community on this topic. All these visualizations are implemented using a common interface and there are available as python module so they can be used in any other module. Thank for reading Francesco Saverio Zuppichini","title":"A journey into Convolutional Neural Network visualization"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/medium/notebook/#a-journey-into-convolutional-neural-network-visualization","text":"Francesco Saverio Zuppichini There is one famous urban legend about computer vision. Around the 80s, the US military wanted to use neural networks to automatically detect camouflaged enemy tanks. They took a number of pictures of trees without tanks and then pictures with the same trees with tanks behind them. The results were impressive. So impressive that the army wanted to be sure the net had correctly generalized. They took new pictures of woods with and without tanks and they showed them again to the network. This time, the model performed terribly, it was not able to discriminate between pictures with tanks behind woods and just trees.It turned out that all the pictures without tanks were taken on a cloudy day while the ones with tanks on a sunny day! In reality, the network learn to recognize the weather, not the enemy tanks.","title":"A journey into Convolutional Neural Network visualization"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/medium/notebook/#nosce-te-ipsum","text":"With this article, we are going to see different techniques to understand what it is going on inside a Convolutional Neural Network to avoid making the same US' army mistake. We are going to use Pytorch . All the code can be found here . Most of the visualizations were developed from scratch, however, some inspiration and parts were taken from here . We will first introduce each technique by briefly explain it and making some example and comparison between different classic computer vision models, alexnet , vgg16 and resnet . Then we will try to better understand a model used in robotics to predict the local distance sensor using only the frontal camera's images. Our goal is not to explain in detail how each technique works since this is already done extremely well by each paper, but to use them to help the reader visualize different model with different inputs to better understand and highlight what and how different models react to a given input. Later on, we show a workflow in which we utilize some of the techniques you will learn in this journey to test the robustness of a model, this is extremely useful to understand and fix its limitations. The curios reader could further improve is understand by looking and the source code for each visulisation and by reading the references.","title":"Nosce te ipsum"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/medium/notebook/#preambula","text":"Disclaimer I am not a fan of jupyter. So apologize in advance if there are some warnings in the outputs and some figures are not well made Let's start our journey by selecting a network. Our first model will be the old school alexnet . It is already available in the torchvision.models package from Pytorch % load_ext autoreload % autoreload 2 from torchvision.models import * from visualisation.core.utils import device model = alexnet ( pretrained = True ) . to ( device ) print ( model ) AlexNet( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)) (1): ReLU(inplace) (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (4): ReLU(inplace) (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): ReLU(inplace) (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (9): ReLU(inplace) (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace) (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) ) (classifier): Sequential( (0): Dropout(p=0.5) (1): Linear(in_features=9216, out_features=4096, bias=True) (2): ReLU(inplace) (3): Dropout(p=0.5) (4): Linear(in_features=4096, out_features=4096, bias=True) (5): ReLU(inplace) (6): Linear(in_features=4096, out_features=1000, bias=True) ) ) Now we need some inputs # %matplotlib notebook Now we need some inputs images. We are going to use three pictures, a cat, the beautiful Basilica di San Pietro and an image with a dog and a cat. import glob import matplotlib.pyplot as plt import numpy as np import torch from utils import * from PIL import Image plt . rcParams [ \"figure.figsize\" ] = 16 , 8 We loaded a few packages. In utils there are several utility function to creates the plots. import glob import matplotlib.pyplot as plt import numpy as np from visualisation.core.utils import device from PIL import Image image_paths = glob . glob ( './images/*.*' ) images = list ( map ( lambda x : Image . open ( x ), image_paths )) subplot ( images , title = 'inputs' , rows_titles = [ 'cat' , 'san pietro' , 'dog_cat' ], nrows = 1 , ncols = 3 ) Since all of our models were trained on imagenet , a huge dataset with 1000 different classes, we need to parse and normalize them. In Pytorch, we have to manually send the data to a device. In this case the device if the fist gpu if you have one, otherwise cpu is selected. Be aware that jupyter have not a garbage collected so we will need to manually free the gpu memory. from torchvision.transforms import ToTensor , Resize , Compose , ToPILImage from visualisation.core import * from visualisation.core.utils import image_net_preprocessing inputs = [ Compose ([ Resize (( 224 , 224 )), ToTensor (), image_net_preprocessing ])( x ) . unsqueeze ( 0 ) for x in images ] # add 1 dim for batch inputs = [ i . to ( device ) for i in inputs ] We also define an utility function to clean the gpu cache def free ( modules ): for m in modules : del m torch . cuda . empty_cache () As we said, imagenet is a huge dataset with 1000 classes, represented by an integer not very human interpetable. We can associate each class id to its label by loading the imaganet2human.txt and create a python dictionary. imagenet2human = {} with open ( 'imaganet2human.txt' ) as f : for line in f . readlines (): key , value = line . split ( ':' ) key = key . replace ( '{' , '' ) . replace ( '}' , '' ) # I forget how regex works :) value = value . replace ( \"'\" , '' ) . replace ( \",\" , '' ) imagenet2human [ int ( key . strip ())] = str ( value . strip ()) list ( imagenet2human . items ())[: 2 ] [(0, 'tench Tinca tinca'), (1, 'goldfish Carassius auratus')]","title":"Preambula"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/medium/notebook/#weights-visualization","text":"The first straightforward visualization is to just plot the weights of a target Layer. Obviously, the deeper we go the smaller each image becomes while the channels number increases. We are going to show each channel as a grey array image. Unfortunately, each Pytorch module can be nested and nested, so to make our code as general as possible we first need to trace each sub-module that the input traverse and then store each layer in order. We first need to trace our model to get a list of all the layers so we can select a target layer without following the nested structure of a model. In PyTorch models can be infinitely nested. In other words, we are flattering the model's layers, this is implemented in the module2traced function. model_traced = module2traced ( model , inputs [ 0 ]) model_traced [Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)), ReLU(inplace), MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)), ReLU(inplace), MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace), Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace), MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False), Dropout(p=0.5), Linear(in_features=9216, out_features=4096, bias=True), ReLU(inplace), Dropout(p=0.5), Linear(in_features=4096, out_features=4096, bias=True), ReLU(inplace), Linear(in_features=4096, out_features=1000, bias=True)] Let's plot the first layer's weight. We also print the shape of the weight to give a correct idea to the reader of the dimensional reduction. vis = Weights ( model , device ) first_layer = model_traced [ 0 ] plt . rcParams [ \"figure.figsize\" ] = 16 , 16 run_vis_plot ( vis , inputs [ 0 ], first_layer , ncols = 4 , nrows = 4 ) torch.Size([1, 55, 55]) Let's stop for a minute to explain what those images represent. We traced the input through the computational graph in order to find out all the layers of our models, in this case, alexnet . Then we instantiate the Weights class implemented in visualisation.core and we call it by passing the current input, the cat image and a target layer . As outputs, we get all the current layer's weights as grey images. Then, we plot 16 of them. We can notice that they, in some way, makes sense; for example, some pixels are brighter in the edges of the images. Let's plot the first MaxPool layer to better see this effect, dimensional reduction and higher brightness pixels in some interesting areas. If you are wondering what the maxpolling operations is doing, check this awesome repo first_maxpool_layer = model_traced [ 2 ] run_vis_plot ( vis , inputs [ 0 ], first_maxpool_layer , ncols = 4 , nrows = 4 ) torch.Size([1, 27, 27]) Let's try with an other input, the San Pietro Basilica run_vis_plot ( vis , inputs [ 1 ], first_maxpool_layer , ncols = 4 , nrows = 4 ) torch.Size([1, 27, 27]) By looking at them, these images make somehow sense; they highlight the basilica layout but it is hard to understand what the model is actually doing. We got the idea that is computing something correctly but we could ask some questions, for example: is it looking at the cupola? Which are the most important features of the Basilica? Moreover, the deeper we go the harder it becomes to even recognize the input. deeper_layer = model_traced [ 6 ] run_vis_plot ( vis , inputs [ 1 ], deeper_layer , ncols = 4 , nrows = 4 ) torch.Size([1, 13, 13]) In this case, we have no idea of what is going on. It can be argued that weights visualization does not carry any useful information about the model, even if this is almost true, there is one nice reason of plotting the weights especially at the first layer. When a model is poorly trained or not trained at all, the first weights have lots of noise, since they are just randomly initialized, and they are a lot more similar to the inputs images than the trained ones. This feature can be useful to understand on the fly is a model is trained or not. However, except for this, weights visualization is not the way to go to understand what your black box is thinking. Below we plot the first layer's weight first for the untraind version of alexnet and the for the trained one. alexnet_not_pretrained = alexnet ( pretrained = False ) . to ( device ) run_vis_plot ( Weights ( alexnet_not_pretrained , device ), inputs [ 0 ], module2traced ( alexnet_not_pretrained , inputs [ 0 ])[ 0 ], ncols = 4 , nrows = 4 ) alexnet_pretrained = alexnet ( pretrained = True ) . to ( device ) run_vis_plot ( Weights ( alexnet_pretrained , device ), inputs [ 0 ], module2traced ( alexnet_pretrained , inputs [ 0 ])[ 0 ], ncols = 4 , nrows = 4 ) del alexnet_not_pretrained torch.Size([1, 55, 55]) torch.Size([1, 55, 55]) You can notice that in the first image is simpler to see the input image. Hoewer, this is not a general rule, but in some cases it can help.","title":"Weights Visualization"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/medium/notebook/#similarities-with-other-models","text":"We have seen alexnet 's weights, but are they similar across models? Below we plot the first 4 channel of each first layer's weight for alexnet , vgg and resnet modules_instances = [ alexnet , vgg16 , resnet34 ] modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], 0 , Weights , 'weights' , device , ncols = 4 ) free ( modules ) The resnet and vgg weights looks more similar to the input images than alexnet . But, again, what does it mean? Remember that at least resnet is initialized in a different way than the other two models.","title":"Similarities with other models"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/medium/notebook/#saliency-visualisation","text":"One idea proposed by Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps is to back-prop the output of the network with respect to a target class until the input and plot the computed gradient. This will highligh the part of the image responsible for that class. Let's start with alexnet. Let's first print the prediction of the network (this could change if you re-run the cell) model . eval () pred = model ( inputs [ 0 ]) _ , id = torch . max ( pred , 1 ) print ( 'predicted class {} ' . format ( imagenet2human [ id . item ()])) predicted class tiger cat Each visualisation is implemented in its own class. You can find the code here . It will backproprop the output with respect to the one hot encoding representation of the number corresponding to class tiger cat from visualisation.core.utils import image_net_postprocessing model . eval () model = model . to ( device ) vis = SaliencyMap ( model , device ) out , info = vis ( inputs [ 0 ], first_layer ) subplot ([ image_net_postprocessing ( inputs [ 0 ] . squeeze () . cpu ()), out ], rows_titles = [ 'original' , 'saliency map' ], parse = tensor2img , nrows = 1 , ncols = 2 ) We can see that alexnet gets exited on the cat. We can even do better! We can set to 0 each negative relu gradient when backprop. This is techinique is called guided . out , info = vis ( inputs [ 0 ], first_layer , guide = True ) subplot ([ image_net_postprocessing ( inputs [ 0 ] . squeeze () . cpu ()), out ], rows_titles = [ 'original' , 'guided saliency map' ], parse = tensor2img , nrows = 1 , ncols = 2 ) Now we can clearly see that the network is looking at the eyes and the nose of the cat. We can try to compare different models modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], 0 , SaliencyMap , 'Saliency' , device , nrows = 1 , ncols = 3 , target_class = 231 , guide = True ) free ( modules ) Alextnet seems more interested to the eyes, while VGG looks at the ears and resnet is similar to alexnet . Now we can clearly understand which part of the inputs help the network gives that prediction. While guiding yields a better human interpretable image, the vanilla implementation can be used for localizing an object of interest. In other words, we can find object of interest for free by cropping out of the input image the region corresponding to the gradient. Let's plot each input image for each model. modules_instances = [ alexnet , vgg16 , resnet34 , resnet152 ] modules = ( m ( pretrained = True ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], None , SaliencyMap , 'SaliencyMap' , device , nrows = 4 , ncols = 3 , inputs = inputs , idx2label = imagenet2human , annotations = [ 'alexnet' , 'vgg16' , 'resnet34' , 'resnet152' ], guide = True ) free ( modules ) The Basilica is very interesting, all four networks correctly classify it as a dome but only resnet152 is more interested in the sky than on the cupola. In the last column, we have an image with two classes, dog and cat . All the networks highlighted booths, like the eyes of the dog and the ears of the cat in vgg16 . What if we would like to discover only the region of the inputs that are related to a specific class? With this technique is impossible.","title":"Saliency Visualisation"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/medium/notebook/#class-activation-mapping","text":"Class Activation Mapping is a techniques presented in Learning Deep Features for Discriminative Localization . The idea is to use the last convolutional layer output and the neurons in the linear layer of the model responsable for a target class, the map is generated by taking the dot product of those. However, to make this work the model has to have some constrains. First of all, the output from the convolution must first go trought an global average polling and it requires feature maps to directly precede softmax layers. To make it works with other architecture, such as alexnet and vgg we have to change some layers in the model and retrain it. This is a major drawback that will be solved with the next section. For now, we can use it for free with resnet! Since its architecture is perfect. The implementation can be found here . We can pass to the visualisation a target_class parameter to get the relative weights from the fc layer. Notice that by changing the target class, we can see different part of the image highlighted. The first image uses the prediction class, while the second an other type of cat and the last one bookcase , just to see what the model will do with a wrong class. from visualisation.core.utils import imshow # we are using resnet 34 since the model has only one fc layer before the softmax and it is preceded by av avg pool # as required from the paper module = resnet34 ( True ) . to ( device ) module . eval () vis = ClassActivationMapping ( module , device ) classes = [ None , 285 , 453 ] def vis_outs2images_classes ( outs ): images = [ x [ 0 ] for x in outs ] classes = [ imagenet2human [ int ( x [ 1 ][ 'prediction' ])] for x in outs ] return images , classes outs = [ vis ( inputs [ 0 ], None , postprocessing = image_net_postprocessing , target_class = c , guide = True ) for c in classes ] images , classes = vis_outs2images_classes ( outs ) subplot ( images , rows_titles = classes , nrows = 1 , ncols = 3 , parse = tensor2img ) It makes sense, the only thing is that in the last row we still have some part of the cat highlighted for bookcase Let's plot the CAM on the cat images for different resnet architecture. For resnet > 34 the Bottleneck module is used modules_instances = [ resnet18 , resnet34 , resnet101 , resnet152 ] cat = inputs [ 2 ] modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , cat , None , ClassActivationMapping , 'ClassActivationMapping' , device , nrows = len ( modules_instances ), ncols = 1 , postprocessing = image_net_postprocessing , rows_name = [ 'resnet18' , 'resnet34' , 'resnet101' , 'resnet152' ], target_class = None ) free ( modules ) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). They are all very similar as expected. One big drawback of this technique is that force you to use a network with a specific architecture, global polling before the decoder part. The next technique generalize this approach by taking advantage of the gradient at one specific layer. Remember that with the class activation we are using the weights of the feature map as a scaling factor for the channels of the last layer. The features map must be before a softmax layer and right after the average pooling. The next technique propose a more general approach.","title":"Class Activation Mapping"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/medium/notebook/#grad-cam","text":"Grad Cam was introduced by Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization . The idea is actually simple, we backprop the output with respect to a target class while storing the gradient and the output at a given layer, in our case the last convolution. Then we perform a global average of the saved gradient keeping the channel dimension in order to get a 1-d tensor, this will represent the importance of each channel in the target convolutional layer. We then multiply each element of the convolutional layer outputs by the averaged gradients to create the grad cam. This whole procedure is fast and it is architecture independent. Interesting, the authors show that is a generalization of the previous technique. The code is here We can use it to higlight what different models are looking at. modules_instances = [ alexnet , vgg16 , resnet34 , resnet152 ] modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], None , GradCam , 'Gradcam' , device , nrows = 1 , ncols = 4 , target_class = None , postprocessing = image_net_postprocessing ) free ( modules ) It is really interesting to see how alexnet looks at the nose, while vgg at the ears and resnet at the whole cat. It is interesting to see that the two resnet version looks at different part of the cat. Below we plot the same input for resnet34 but we change the target class in each column to show the reader how the grad cam change accordingly. from visualisation.core.utils import imshow module = module . to ( device ) vis = GradCam ( module , device ) classes = [ None , 285 , 453 ] outs = [ vis ( inputs [ 0 ], None , postprocessing = image_net_postprocessing , target_class = c ) for c in classes ] images , classes = vis_outs2images_classes ( outs ) subplot ( images , title = 'resnet34' , rows_titles = classes , nrows = 1 , ncols = len ( outs ), parse = tensor2img ) Notice how similar to the CAM output they are. To better compore our three models, below we plot the grad cam for each input with respect to each model modules = ( m ( pretrained = True ) . to ( device ) for m in modules_instances ) # make a generator, we don't want to store in memory all of them at once run_vis_plot_across_models ( modules , inputs [ 0 ], None , GradCam , 'Gradcam' , device , nrows = 4 , ncols = 3 , target_class = None , inputs = inputs , idx2label = imagenet2human , annotations = [ 'alexnet' , 'vgg16' , 'resnet34' , 'resnet152' ], postprocessing = image_net_postprocessing ) free ( modules ) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). The reader can immediately notice the difference across the models.","title":"Grad Cam"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/medium/notebook/#interesting-region","text":"We talk before about interesting region localizations. Grad-cam can be also used to extract the class object out of the image. Easily, once the have the grad-cam image we can used it as mask to crop out form the input image what we want. The reader can play with the TR parameter to see different effects. TR = 0.3 alexnet_pretrained . eval () vis = GradCam ( alexnet_pretrained , device ) _ = vis ( inputs [ 0 ], None , postprocessing = image_net_postprocessing ) import cv2 def gradcam2crop ( cam , original_img ): b , c , w , h = inputs [ 0 ] . shape cam = cam . numpy () cam -= np . min ( cam ) cam /= np . max ( cam ) cam = cv2 . resize ( cam , ( w , h )) mask = cam > TR original_img = tensor2img ( image_net_postprocessing ( original_img [ 0 ] . squeeze ())) crop = original_img . copy () crop [ mask == 0 ] = 0 return crop crop = gradcam2crop ( vis . cam . cpu (), inputs [ 0 ] . cpu ()) fig = plt . figure () plt . imshow ( crop ) <matplotlib.image.AxesImage at 0x7f79debee048> et voil\u00e0 ! We can also change again class, and crop the interest region for that class. _ = vis ( inputs [ 0 ], None , target_class = 231 , postprocessing = image_net_postprocessing ) crop = gradcam2crop ( vis . cam . cpu (), inputs [ 0 ] . cpu ()) fig = plt . figure () plt . imshow ( crop ) <matplotlib.image.AxesImage at 0x7f79f40c4c18>","title":"Interesting region"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/medium/notebook/#different-models","text":"We have seen all these techniques used with classic classicification models trained on imagenet . What about use them on a different domain? I have ported this paper to Pytorch and retrain it. The model learn from the frontal camera's image of a robot to predict the local distance sensors in order to avoid obstacles. Let's see what if, by using those techniques, we can understand better what is going on inside the model.","title":"Different models"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/medium/notebook/#learning-long-range-perception-using-self-supervision-from-short-range-sensors-and-odometry","text":"The idea is to predict the future outputs of a short-range sensor (such as a proximity sensor) given the current outputs of a long-range sensor (such as a camera). They trained a very simple CNN from the robot's camera images to predict the proximity sensor values. If you are interested in their work, you can read the full paper here I have made a PyTorch implementation and retrain the model from scratch. Be awere that I did not fine-tune or try different sets of hyper-parameters so probably my model is not performing as well as the author's one. Let's import it from os import path LONG_RANGE_PERCEPTION_PATH = path . abspath ( './models/long_range_perception/model.pt' ) from models.long_range_perception.model import SimpleCNN from models.long_range_perception.utils import get_dl , H5_PATH , imshow , post_processing , pre_processing , MODEL_PATH free ([ module ]) module = torch . load ( LONG_RANGE_PERCEPTION_PATH , map_location = lambda storage , loc : storage ) module = module . to ( device ) module /home/francesco/Documents/A-journey-into-Convolutional-Neural-Network-visualization-/model.pt SimpleCNN( (encoder): Sequential( (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): ReLU() (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (6): Conv2d(10, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): ReLU() (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (decoder): Sequential( (0): Dropout(p=0.2) (1): Linear(in_features=640, out_features=256, bias=True) (2): ReLU() (3): Linear(in_features=256, out_features=325, bias=True) (4): Sigmoid() ) ) We know need some inputs to test the model, they are taken directly from the test set import os def make_and_show_inputs ( path , transform ): image_paths = glob . glob ( path ) image_paths = filter ( lambda x : os . path . isfile ( x ), image_paths ) images = list ( map ( lambda x : Image . open ( x ) . convert ( 'RGB' ), image_paths )) subplot ( images , title = 'inputs' , rows_titles = [ '1' , '2' , '3' , '4' ], nrows = 1 , ncols = 4 ) plt . show () inputs = [ pre_processing ( x ) . unsqueeze ( 0 ) . to ( device ) for x in images ] # add 1 dim for batch subplot ( inputs , parse = tensor2img , title = 'inputs' , rows_titles = [ '1' , '2' , '3' , '4' ], nrows = 1 , ncols = 4 ) return images , inputs images , inputs = make_and_show_inputs ( 'images/long_range_perception/*' , pre_processing ) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Then author normalize each image, this is done by callind pre_processing . For some reason the inpupts images are different on mac and ubuntu, they should not be like these if you run the notebook on mac the result is different. This is probably due to the warning message. We are going to use the SaliencyMap and the GradCam since those are the best from torch.autograd import Variable module . eval () def run_long_range_vis (): grad = GradCam ( module , device ) all_true = torch . ones ( 1 , 65 * 5 ) . float () . to ( device ) outs_grad = [ grad ( input , None , target_class = all_true , postprocessing = post_processing , regression = True )[ 0 ] for input in inputs ] sal = SaliencyMap ( module , device ) outs_saliency = [ sal ( input , None , guide = True , target_class = all_true , regression = True )[ 0 ] for input in inputs ] subplot ([ * outs_grad , * outs_saliency ], title = 'long_range' , cols_titles = [ '1' , '2' , '3' , '4' ], nrows = 2 , ncols = 4 , parse = tensor2img ) run_long_range_vis () We can clearly see that the model looks at the objects. In the GradCam row, on the second picture, the plan is basically segmented by the heatmap. There is one problem, if you look at the third picture, the white box in front of the camera is not clearly highlighted. This is probably due to the white color of the floor that is very similar to the box's color. Let's investigate this problem. In the second row, the SaliencyMaps highlights all the objects, including the white box. The reader can notice that the reflection in the first picture on the left seems to excite the network in that region. We should also investigate this case but due to time limitations, we will leave it as an exercise for the curious reader. For completeness, let's also print the predicted sensor output. The model tries to predict five frontal distance sensors give the image camera. import seaborn as sns module . eval () preds = module ( torch . stack ( inputs ) . squeeze ( 1 )) fig = plt . figure () sns . heatmap ( preds [ 2 ] . view ( - 1 , 5 ) . detach () . cpu () . numpy ()) <matplotlib.axes._subplots.AxesSubplot at 0x7f79d26dc240> If you compare with the authors pictures, my prediction are worse. This is due to the fact that to speed up everything I did not used all the training set and I did not perform any hyper paramater optimisation. All the code con be found here . Let's now investigate the first problem, object with a similar color to the ground.","title":"Learning Long-range Perception using Self-Supervision from Short-Range Sensors and Odometry"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/medium/notebook/#similar-colors","text":"To test if the model has a problem with obstacles with a the same color of the ground, we created in blender four different scenarios with an obstacle. They are showed in the picture below. image_paths = [ * sorted ( glob . glob ( 'images/long_range_perception/equal_color/*' )), * sorted ( glob . glob ( 'images/long_range_perception/different_color/*' ))] image_paths = filter ( lambda x : os . path . isfile ( x ), image_paths ) images = list ( map ( lambda x : Image . open ( x ) . convert ( 'RGB' ), image_paths )) subplot ( images , title = 'inputs' , nrows = 2 , ncols = 4 ) plt . show () There are four different lights configuration and two differents cube colors, one equal to the ground and the second different. The first column represents a realistic situation, while the second has a really strong light from behind that generates a shadow in front of the camera. The third column has a shadow on the left and the last one has a little shadow on the left. This is a perfect scenario to use gradcam to see what the model is looking in each image. In the picture below we plotted the gradcam results. inputs = [ pre_processing ( x ) . unsqueeze ( 0 ) . to ( device ) for x in images ] # add 1 dim for batch run_long_range_vis () The big black shadow in the second column definitly confuses the model. In the first and last column, the grad cam highlights better the corners of the red cube, especially in the first picture. We can definitely say that this model has some hard time with the object of the same colour as the ground. Thanks to this consideration, we could improve the number equal object/ground in the dataset, perform a better preprocessing, change the model structure etc and hopefully increase the robustness of the network.","title":"Similar colors"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/medium/notebook/#conclusion","text":"In this article, we present different convolutional neural network visualization techniques. In the first section, we introduced each one by applying to a set of famous classification networks. We compared different networks on different inputs and highlight the similarities and difference between them. Then we apply them to a model adopted in robotics to test its robustness and we were able to successfully reveal a problem in the network. Moreover, as a side project, I developed an interactive convolutional neural network visualization application called mirro that receives in just a few days more than a hundred stars on GitHub reflecting the interest of the deep learning community on this topic. All these visualizations are implemented using a common interface and there are available as python module so they can be used in any other module. Thank for reading Francesco Saverio Zuppichini","title":"Conclusion"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/models/long_range_perception/visualisation/","text":"import torch import numpy as np from utils import get_dl , H5_PATH , imshow , post_processing from model import SimpleCNN /usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters _ , ds = get_dl ( np . arange ( 9 , 10 ), H5_PATH , test = True ) x = ds [ 0 ] img , target = x imshow ( img ) imshow ( post_processing ( img )) for i in range ( 0 , 100 , 5 ): x = ds [ i ] img , _ = x imshow ( post_processing ( img )) model = torch . load ( './model.pt' , map_location = lambda storage , loc : storage ) /usr/local/lib/python3.6/site-packages/torch/serialization.py:434: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes. warnings.warn(msg, SourceChangeWarning) /usr/local/lib/python3.6/site-packages/torch/serialization.py:434: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes. warnings.warn(msg, SourceChangeWarning) /usr/local/lib/python3.6/site-packages/torch/serialization.py:434: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes. warnings.warn(msg, SourceChangeWarning) /usr/local/lib/python3.6/site-packages/torch/serialization.py:434: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.MaxPool2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes. warnings.warn(msg, SourceChangeWarning) /usr/local/lib/python3.6/site-packages/torch/serialization.py:434: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes. warnings.warn(msg, SourceChangeWarning) /usr/local/lib/python3.6/site-packages/torch/serialization.py:434: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes. warnings.warn(msg, SourceChangeWarning) /usr/local/lib/python3.6/site-packages/torch/serialization.py:434: SourceChangeWarning: source code of class 'torch.nn.modules.activation.Sigmoid' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes. warnings.warn(msg, SourceChangeWarning) from mirror import mirror from mirror.visualisations import * mirror ( ds [ 30 ][ 0 ] . unsqueeze ( 0 ), model , visualisations = [ DeepDreamVis , BackPropVis , GradCamVis ]) * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit) * Restarting with stat An exception has occurred, use %tb to see the full traceback. SystemExit: 1 /usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D. warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)","title":"Visualisation"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/visualisation/core/","text":"cnn-visualisations","title":"cnn-visualisations"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/neural_network_interpretation/references/cnn_visual/visualisation/core/#cnn-visualisations","text":"","title":"cnn-visualisations"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/object_detection/bounding_boxes/","text":"import matplotlib.pyplot as plt import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import torch import torchvision % matplotlib inline import glob import os from math import ceil import random import cv2 import PIL from IPython.core.interactiveshell import InteractiveShell InteractiveShell . ast_node_interactivity = \"all\" from typing import * # importing modules import urllib.request from urllib.request import urlopen from PIL import Image import bounding_boxes cat_dog_p = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/deep_learning/computer_vision/data/misc/catdog.jpg\" # plot cat and dog with title using PIL plt . figure ( figsize = ( 10 , 10 )) cat_dog = PIL . Image . open ( urlopen ( cat_dog_p )) plt . imshow ( cat_dog ) plt . title ( \"Cat and Dog\" ) plt . show (); height , width , channel = np . asarray ( cat_dog ) . shape print ( f \"The height is { height } and the width is { width } and the channel is { channel } \" ) The height is 561 and the width is 728 and the channel is 3 Bounding Boxes In object detection, we usually use a bounding box to describe the spatial location of an object. The bounding box is rectangular, which is determined by the \\(x\\) and \\(y\\) coordinates of the upper-left corner of the rectangle and the such coordinates of the lower-right corner. Another commonly used bounding box representation is the \\((x, y)\\) -axis coordinates of the bounding box center, and the width and height of the box. [ Here we define functions to convert between ] these ( two representations ): box_corner_to_center converts from the two-corner representation to the center-width-height presentation, and box_center_to_corner vice versa. The input argument boxes should be a two-dimensional tensor of shape ( \\(n\\) , 4), where \\(n\\) is the number of bounding boxes. Different conversions https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/ https://github.com/awsaf49/bbox/blob/main/bbox/utils.py Pascal https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/ pascal_voc is a format used by the Pascal VOC dataset . Coordinates of a bounding box are encoded with four values in pixels: [x_min, y_min, x_max, y_max] . x_min and y_min are coordinates of the top-left corner of the bounding box. x_max and y_max are coordinates of bottom-right corner of the bounding box. Coordinates of the example bounding box in this format are [98, 345, 420, 462] . COCO coco is a format used by the Common Objects in Context \\(COCO\\) dataset. In coco , a bounding box is defined by four values in pixels [x_min, y_min, width, height] . They are coordinates of the top-left corner along with the width and height of the bounding box. Coordinates of the example bounding box in this format are [98, 345, 322, 117] . YOLO In yolo , a bounding box is represented by four values [x_center, y_center, width, height] . x_center and y_center are the normalized coordinates of the center of the bounding box. To make coordinates normalized, we take pixel values of x and y, which marks the center of the bounding box on the x- and y-axis. Then we divide the value of x by the width of the image and value of y by the height of the image. width and height represent the width and the height of the bounding box. They are normalized as well. Coordinates of the example bounding box in this format are [((420 + 98) / 2) / 640, ((462 + 345) / 2) / 480, 322 / 640, 117 / 480] which are [0.4046875, 0.840625, 0.503125, 0.24375] . def voc2coco ( bboxes : Union [ np . ndarray , torch . Tensor ], return_tensor : bool = False ) -> np . ndarray : \"\"\"Convert pascal_voc to coco format. voc => [xmin, ymin, xmax, ymax] coco => [xmin, ymin, w, h] Args: bboxes (torch.Tensor): Shape of (N, 4) where N is the number of samples and 4 is the coordinates [xmin, ymin, xmax, ymax]. Returns: coco_bboxes (torch.Tensor): Shape of (N, 4) where N is the number of samples and 4 is the coordinates [xmin, ymin, w, h]. \"\"\" # don't perform in place to avoid mutation if isinstance ( bboxes , torch . Tensor ): bboxes = bboxes . cpu () . detach () . numpy () coco_bboxes = bboxes . copy () for index , each_bbox in enumerate ( bboxes ): xmin , ymin , xmax , ymax = each_bbox w , h = xmax - xmin , ymax - ymin if return_tensor : coco_bboxes [ index ] = torch . tensor ([ xmin , ymin , w , h ]) else : coco_bboxes [ index ] = np . asarray ([ xmin , ymin , w , h ]) return coco_bboxes def coco2voc ( bboxes : Union [ np . ndarray , torch . Tensor ], return_tensor : bool = False ) -> np . ndarray : \"\"\"Convert coco to pascal_voc format. coco => [xmin, ymin, w, h] voc => [xmin, ymin, xmax, ymax] Args: bboxes (torch.Tensor): Shape of (N, 4) where N is the number of samples and 4 is the coordinates [xmin, ymin, w, h]. Returns: voc_bboxes (torch.Tensor): Shape of (N, 4) where N is the number of samples and 4 is the coordinates [xmin, ymin, xmax, ymax]. \"\"\" # don't perform in place to avoid mutation if isinstance ( bboxes , torch . Tensor ): bboxes = bboxes . cpu () . detach () . numpy () voc_bboxes = bboxes . copy () for index , each_bbox in enumerate ( bboxes ): xmin , ymin , w , h = each_bbox xmax , ymax = xmin + w , ymin + h if return_tensor : voc_bboxes [ index ] = torch . tensor ([ xmin , ymin , xmax , ymax ]) else : voc_bboxes [ index ] = np . asarray ([ xmin , ymin , xmax , ymax ]) return voc_bboxes from numba import jit import cv2 import numpy as np import random __all__ = [ 'coco2yolo' , 'yolo2coco' , 'voc2coco' , 'coco2voc' , 'yolo2voc' , 'voc2yolo' , 'bbox_iou' , 'draw_bboxes' , 'load_image' ] def voc2yolo ( bboxes , height = 720 , width = 1280 ): \"\"\" voc => [x1, y1, x2, y1] yolo => [xmid, ymid, w, h] (normalized) \"\"\" if isinstance ( bboxes , torch . Tensor ): bboxes = bboxes . cpu () . detach () . numpy () bboxes = bboxes . copy () . astype ( float ) # otherwise all value will be 0 as voc_pascal dtype is np.int bboxes [ ... , 0 :: 2 ] /= width bboxes [ ... , 1 :: 2 ] /= height bboxes [ ... , 2 ] -= bboxes [ ... , 0 ] bboxes [ ... , 3 ] -= bboxes [ ... , 1 ] bboxes [ ... , 0 ] += bboxes [ ... , 2 ] / 2 bboxes [ ... , 1 ] += bboxes [ ... , 3 ] / 2 return bboxes def yolo2voc ( bboxes , height = 720 , width = 1280 ): \"\"\" yolo => [xmid, ymid, w, h] (normalized) voc => [x1, y1, x2, y1] \"\"\" if isinstance ( bboxes , torch . Tensor ): bboxes = bboxes . cpu () . detach () . numpy () bboxes = bboxes . copy () . astype ( float ) # otherwise all value will be 0 as voc_pascal dtype is np.int bboxes [ ... , 0 :: 2 ] *= width bboxes [ ... , 1 :: 2 ] *= height bboxes [ ... , 0 : 2 ] -= bboxes [ ... , 2 : 4 ] / 2 bboxes [ ... , 2 : 4 ] += bboxes [ ... , 0 : 2 ] return bboxes def coco2yolo ( bboxes , height = 720 , width = 1280 ): \"\"\" https://github.com/awsaf49/bbox/blob/main/bbox/utils.py coco => [xmin, ymin, w, h] yolo => [xmid, ymid, w, h] (normalized) \"\"\" # bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int # normolizinig bboxes [ ... , 0 :: 2 ] /= width bboxes [ ... , 1 :: 2 ] /= height # converstion (xmin, ymin) => (xmid, ymid) bboxes [ ... , 0 : 2 ] += bboxes [ ... , 2 : 4 ] / 2 return bboxes def yolo2coco ( bboxes , height = 720 , width = 1280 ): \"\"\" https://github.com/awsaf49/bbox/blob/main/bbox/utils.py yolo => [xmid, ymid, w, h] (normalized) coco => [xmin, ymin, w, h] \"\"\" # bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int # denormalizing bboxes [ ... , 0 :: 2 ] *= width bboxes [ ... , 1 :: 2 ] *= height # converstion (xmid, ymid) => (xmin, ymin) bboxes [ ... , 0 : 2 ] -= bboxes [ ... , 2 : 4 ] / 2 return bboxes dog_bbox , cat_bbox = [ 60.0 , 45.0 , 378.0 , 516.0 ], [ 400.0 , 112.0 , 655.0 , 493.0 ] bboxes = torch . tensor (( dog_bbox , cat_bbox )) bboxes tensor([[ 60., 45., 378., 516.], [400., 112., 655., 493.]]) VOC-COCO Test np . testing . assert_array_equal ( bboxes , coco2voc ( voc2coco ( bboxes ))) np . array_equal ( bboxes , coco2voc ( voc2coco ( bboxes ))) True VOC-YOLO Test coco2voc ( voc2coco ( bboxes )) array([[ 60., 45., 378., 516.], [400., 112., 655., 493.]], dtype=float32) yolo2voc ( voc2yolo ( bboxes , height = 561 , width = 768 ), height = 561 , width = 768 ) array([[ 60., 45., 378., 516.], [400., 112., 655., 493.]]) yolo2voc ( voc2yolo ( bboxes , height = 561 , width = 768 ), height = 561 , width = 768 ) array([[ 60., 45., 378., 516.], [400., 112., 655., 493.]]) a = voc2coco ( bboxes ) b = coco2voc ( a ) a , b (array([[ 60., 45., 318., 471.], [400., 112., 255., 381.]], dtype=float32), array([[ 60., 45., 378., 516.], [400., 112., 655., 493.]], dtype=float32)) We measure the bounding box coordinates for the cat and dog respectively. Here bbox is the abbreviation for bounding box. The bbox below represents dog_bbox , cat_bbox = [ 60.0 , 45.0 , 378.0 , 516.0 ], [ 400.0 , 112.0 , 655.0 , 493.0 ] def draw_bboxes_on_single_image ( image : Union [ torch . Tensor , np . ndarray ], bboxes : Union [ torch . Tensor , np . ndarray ], labels : Union [ torch . Tensor , np . ndarray , List [ int ]], labels_map : Dict [ int , str ], colors : Dict [ int , Tuple [ int , int , int ]], ) -> np . ndarray : \"\"\"Draws bounding boxes on a single image. Args: image (Union[torch.Tensor, np.ndarray]): The original image to draw on. bboxes (Union[torch.Tensor, np.ndarray]): The bounding boxes of the original image. This should be in voc_pascal format of (xmin, ymin, xmax, ymax). labels (Union[torch.Tensor, np.ndarray, List[int]]): The labels of the bounding boxes corresponding to the bounding boxes. Note they are needed to be in the same order as the bounding boxes. colors (Dict[int, Tuple[int, int, int]]): The color mapping of the labels. Returns: image (np.ndarray): The image with bounding boxes drawn on it. Shapes: - Input: - image: :math:`(H, W, C)` or :math:`(C, H, W)` depending on whether the channels are the first or last dimension. - bboxes: :math:`(N, 4)` where the bounding boxes are in the format of (xmin, ymin, xmax, ymax) and N is the number of bounding boxes. - labels: :math:`(N,)` where N is the number of bounding boxes. - Output: - image: :math:`(H, W, C)` Example: >>> import torch >>> import numpy as np >>> cat_dog_p = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/deep_learning/computer_vision/data/misc/catdog.jpg\" >>> cat_dog = PIL.Image.open(urlopen(cat_dog_p)) >>> dog_bbox, cat_bbox = [60.0, 45.0, 378.0, 516.0], [400.0, 112.0, 655.0, 493.0] >>> bboxes = torch.tensor((dog_bbox, cat_bbox)) # convert to (N, 2) >>> labels = [0, 1] >>> labels_map = {0: \"dog\", 1: \"cat\"} >>> colors = {0: (255, 0, 255), 1: (0, 255, 255)} >>> bbox_img = draw_bboxes_on_single_image(cat_dog, bboxes, labels=labels, labels_map=labels_map, colors=colors) >>> plt.imshow(bbox_img); \"\"\" if isinstance ( image , torch . Tensor ): image = image . cpu () . detach () . numpy () if isinstance ( bboxes , torch . Tensor ): bboxes = bboxes . cpu () . detach () . numpy () if isinstance ( labels , torch . Tensor ): labels = labels . cpu () . detach () . numpy () # make sure it is in np.ndarray format if read as PIL image = np . asarray ( image . copy ()) labels = np . asarray ( labels . copy ()) # if labels is like [[0], [1], [0]] then flatten if len ( labels . shape ) > 1 : labels = labels . flatten () # line/font thickness line_thickness = round ( 0.002 * ( image . shape [ 0 ] + image . shape [ 1 ]) / 2 ) + 1 for i , ( bbox , label ) in enumerate ( zip ( bboxes , labels )): color = colors [ label ] pt1 , pt2 = ( bbox [ 0 ], bbox [ 1 ]), ( bbox [ 2 ], bbox [ 3 ]) pt1 = int ( pt1 [ 0 ]), int ( pt1 [ 1 ]) pt2 = int ( pt2 [ 0 ]), int ( pt2 [ 1 ]) image = cv2 . rectangle ( image . copy (), pt1 , pt2 , color , int ( max ( image . shape [: 2 ]) / 200 ) ) # annotate text label2str = labels_map [ label ] cv2 . putText ( image , label2str , pt1 , cv2 . FONT_HERSHEY_SIMPLEX , line_thickness / 3 , color = [ 0 , 0 , 0 ], thickness = line_thickness , lineType = cv2 . LINE_AA , ) return image >>> import torch >>> import numpy as np >>> cat_dog_p = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/deep_learning/computer_vision/data/misc/catdog.jpg\" >>> cat_dog = PIL . Image . open ( urlopen ( cat_dog_p )) >>> dog_bbox , cat_bbox = [ 60.0 , 45.0 , 378.0 , 516.0 ], [ 400.0 , 112.0 , 655.0 , 493.0 ] >>> bboxes = torch . tensor (( dog_bbox , cat_bbox )) >>> labels = [ 0 , 1 ] >>> labels_map = { 0 : \"dog\" , 1 : \"cat\" } >>> colors = { 0 : ( 255 , 0 , 255 ), 1 : ( 0 , 255 , 255 )} >>> bbox_img = draw_bboxes_on_single_image ( cat_dog , bboxes , labels = labels , labels_map = labels_map , colors = colors ) >>> plt . imshow ( bbox_img ); Anchor Boxes def multibox_prior ( data , sizes , ratios ): \"\"\"Generate anchor boxes with different shapes centered on each pixel.\"\"\" in_height , in_width = data . shape [ - 2 :] device , num_sizes , num_ratios = data . device , len ( sizes ), len ( ratios ) boxes_per_pixel = ( num_sizes + num_ratios - 1 ) size_tensor = torch . tensor ( sizes , device = device ) ratio_tensor = torch . tensor ( ratios , device = device ) # Offsets are required to move the anchor to the center of a pixel. Since # a pixel has height=1 and width=1, we choose to offset our centers by 0.5 offset_h , offset_w = 0.5 , 0.5 steps_h = 1.0 / in_height # Scaled steps in y axis steps_w = 1.0 / in_width # Scaled steps in x axis # Generate all center points for the anchor boxes center_h = ( torch . arange ( in_height , device = device ) + offset_h ) * steps_h center_w = ( torch . arange ( in_width , device = device ) + offset_w ) * steps_w shift_y , shift_x = torch . meshgrid ( center_h , center_w ) shift_y , shift_x = shift_y . reshape ( - 1 ), shift_x . reshape ( - 1 ) # Generate `boxes_per_pixel` number of heights and widths that are later # used to create anchor box corner coordinates (xmin, xmax, ymin, ymax) w = torch . cat (( size_tensor * torch . sqrt ( ratio_tensor [ 0 ]), sizes [ 0 ] * torch . sqrt ( ratio_tensor [ 1 :]))) \\ * in_height / in_width # Handle rectangular inputs h = torch . cat (( size_tensor / torch . sqrt ( ratio_tensor [ 0 ]), sizes [ 0 ] / torch . sqrt ( ratio_tensor [ 1 :]))) # Divide by 2 to get half height and half width anchor_manipulations = torch . stack (( - w , - h , w , h )) . T . repeat ( in_height * in_width , 1 ) / 2 # Each center point will have `boxes_per_pixel` number of anchor boxes, so # generate a grid of all anchor box centers with `boxes_per_pixel` repeats out_grid = torch . stack ([ shift_x , shift_y , shift_x , shift_y ], dim = 1 ) . repeat_interleave ( boxes_per_pixel , dim = 0 ) output = out_grid + anchor_manipulations return output . unsqueeze ( 0 )","title":"Bounding boxes"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/object_detection/bounding_boxes/#bounding-boxes","text":"In object detection, we usually use a bounding box to describe the spatial location of an object. The bounding box is rectangular, which is determined by the \\(x\\) and \\(y\\) coordinates of the upper-left corner of the rectangle and the such coordinates of the lower-right corner. Another commonly used bounding box representation is the \\((x, y)\\) -axis coordinates of the bounding box center, and the width and height of the box. [ Here we define functions to convert between ] these ( two representations ): box_corner_to_center converts from the two-corner representation to the center-width-height presentation, and box_center_to_corner vice versa. The input argument boxes should be a two-dimensional tensor of shape ( \\(n\\) , 4), where \\(n\\) is the number of bounding boxes. Different conversions https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/ https://github.com/awsaf49/bbox/blob/main/bbox/utils.py","title":"Bounding Boxes"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/object_detection/bounding_boxes/#pascal","text":"https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/ pascal_voc is a format used by the Pascal VOC dataset . Coordinates of a bounding box are encoded with four values in pixels: [x_min, y_min, x_max, y_max] . x_min and y_min are coordinates of the top-left corner of the bounding box. x_max and y_max are coordinates of bottom-right corner of the bounding box. Coordinates of the example bounding box in this format are [98, 345, 420, 462] .","title":"Pascal"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/object_detection/bounding_boxes/#coco","text":"coco is a format used by the Common Objects in Context \\(COCO\\) dataset. In coco , a bounding box is defined by four values in pixels [x_min, y_min, width, height] . They are coordinates of the top-left corner along with the width and height of the bounding box. Coordinates of the example bounding box in this format are [98, 345, 322, 117] .","title":"COCO"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/object_detection/bounding_boxes/#yolo","text":"In yolo , a bounding box is represented by four values [x_center, y_center, width, height] . x_center and y_center are the normalized coordinates of the center of the bounding box. To make coordinates normalized, we take pixel values of x and y, which marks the center of the bounding box on the x- and y-axis. Then we divide the value of x by the width of the image and value of y by the height of the image. width and height represent the width and the height of the bounding box. They are normalized as well. Coordinates of the example bounding box in this format are [((420 + 98) / 2) / 640, ((462 + 345) / 2) / 480, 322 / 640, 117 / 480] which are [0.4046875, 0.840625, 0.503125, 0.24375] . def voc2coco ( bboxes : Union [ np . ndarray , torch . Tensor ], return_tensor : bool = False ) -> np . ndarray : \"\"\"Convert pascal_voc to coco format. voc => [xmin, ymin, xmax, ymax] coco => [xmin, ymin, w, h] Args: bboxes (torch.Tensor): Shape of (N, 4) where N is the number of samples and 4 is the coordinates [xmin, ymin, xmax, ymax]. Returns: coco_bboxes (torch.Tensor): Shape of (N, 4) where N is the number of samples and 4 is the coordinates [xmin, ymin, w, h]. \"\"\" # don't perform in place to avoid mutation if isinstance ( bboxes , torch . Tensor ): bboxes = bboxes . cpu () . detach () . numpy () coco_bboxes = bboxes . copy () for index , each_bbox in enumerate ( bboxes ): xmin , ymin , xmax , ymax = each_bbox w , h = xmax - xmin , ymax - ymin if return_tensor : coco_bboxes [ index ] = torch . tensor ([ xmin , ymin , w , h ]) else : coco_bboxes [ index ] = np . asarray ([ xmin , ymin , w , h ]) return coco_bboxes def coco2voc ( bboxes : Union [ np . ndarray , torch . Tensor ], return_tensor : bool = False ) -> np . ndarray : \"\"\"Convert coco to pascal_voc format. coco => [xmin, ymin, w, h] voc => [xmin, ymin, xmax, ymax] Args: bboxes (torch.Tensor): Shape of (N, 4) where N is the number of samples and 4 is the coordinates [xmin, ymin, w, h]. Returns: voc_bboxes (torch.Tensor): Shape of (N, 4) where N is the number of samples and 4 is the coordinates [xmin, ymin, xmax, ymax]. \"\"\" # don't perform in place to avoid mutation if isinstance ( bboxes , torch . Tensor ): bboxes = bboxes . cpu () . detach () . numpy () voc_bboxes = bboxes . copy () for index , each_bbox in enumerate ( bboxes ): xmin , ymin , w , h = each_bbox xmax , ymax = xmin + w , ymin + h if return_tensor : voc_bboxes [ index ] = torch . tensor ([ xmin , ymin , xmax , ymax ]) else : voc_bboxes [ index ] = np . asarray ([ xmin , ymin , xmax , ymax ]) return voc_bboxes from numba import jit import cv2 import numpy as np import random __all__ = [ 'coco2yolo' , 'yolo2coco' , 'voc2coco' , 'coco2voc' , 'yolo2voc' , 'voc2yolo' , 'bbox_iou' , 'draw_bboxes' , 'load_image' ] def voc2yolo ( bboxes , height = 720 , width = 1280 ): \"\"\" voc => [x1, y1, x2, y1] yolo => [xmid, ymid, w, h] (normalized) \"\"\" if isinstance ( bboxes , torch . Tensor ): bboxes = bboxes . cpu () . detach () . numpy () bboxes = bboxes . copy () . astype ( float ) # otherwise all value will be 0 as voc_pascal dtype is np.int bboxes [ ... , 0 :: 2 ] /= width bboxes [ ... , 1 :: 2 ] /= height bboxes [ ... , 2 ] -= bboxes [ ... , 0 ] bboxes [ ... , 3 ] -= bboxes [ ... , 1 ] bboxes [ ... , 0 ] += bboxes [ ... , 2 ] / 2 bboxes [ ... , 1 ] += bboxes [ ... , 3 ] / 2 return bboxes def yolo2voc ( bboxes , height = 720 , width = 1280 ): \"\"\" yolo => [xmid, ymid, w, h] (normalized) voc => [x1, y1, x2, y1] \"\"\" if isinstance ( bboxes , torch . Tensor ): bboxes = bboxes . cpu () . detach () . numpy () bboxes = bboxes . copy () . astype ( float ) # otherwise all value will be 0 as voc_pascal dtype is np.int bboxes [ ... , 0 :: 2 ] *= width bboxes [ ... , 1 :: 2 ] *= height bboxes [ ... , 0 : 2 ] -= bboxes [ ... , 2 : 4 ] / 2 bboxes [ ... , 2 : 4 ] += bboxes [ ... , 0 : 2 ] return bboxes def coco2yolo ( bboxes , height = 720 , width = 1280 ): \"\"\" https://github.com/awsaf49/bbox/blob/main/bbox/utils.py coco => [xmin, ymin, w, h] yolo => [xmid, ymid, w, h] (normalized) \"\"\" # bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int # normolizinig bboxes [ ... , 0 :: 2 ] /= width bboxes [ ... , 1 :: 2 ] /= height # converstion (xmin, ymin) => (xmid, ymid) bboxes [ ... , 0 : 2 ] += bboxes [ ... , 2 : 4 ] / 2 return bboxes def yolo2coco ( bboxes , height = 720 , width = 1280 ): \"\"\" https://github.com/awsaf49/bbox/blob/main/bbox/utils.py yolo => [xmid, ymid, w, h] (normalized) coco => [xmin, ymin, w, h] \"\"\" # bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int # denormalizing bboxes [ ... , 0 :: 2 ] *= width bboxes [ ... , 1 :: 2 ] *= height # converstion (xmid, ymid) => (xmin, ymin) bboxes [ ... , 0 : 2 ] -= bboxes [ ... , 2 : 4 ] / 2 return bboxes dog_bbox , cat_bbox = [ 60.0 , 45.0 , 378.0 , 516.0 ], [ 400.0 , 112.0 , 655.0 , 493.0 ] bboxes = torch . tensor (( dog_bbox , cat_bbox )) bboxes tensor([[ 60., 45., 378., 516.], [400., 112., 655., 493.]])","title":"YOLO"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/object_detection/bounding_boxes/#voc-coco-test","text":"np . testing . assert_array_equal ( bboxes , coco2voc ( voc2coco ( bboxes ))) np . array_equal ( bboxes , coco2voc ( voc2coco ( bboxes ))) True","title":"VOC-COCO Test"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/object_detection/bounding_boxes/#voc-yolo-test","text":"coco2voc ( voc2coco ( bboxes )) array([[ 60., 45., 378., 516.], [400., 112., 655., 493.]], dtype=float32) yolo2voc ( voc2yolo ( bboxes , height = 561 , width = 768 ), height = 561 , width = 768 ) array([[ 60., 45., 378., 516.], [400., 112., 655., 493.]]) yolo2voc ( voc2yolo ( bboxes , height = 561 , width = 768 ), height = 561 , width = 768 ) array([[ 60., 45., 378., 516.], [400., 112., 655., 493.]]) a = voc2coco ( bboxes ) b = coco2voc ( a ) a , b (array([[ 60., 45., 318., 471.], [400., 112., 255., 381.]], dtype=float32), array([[ 60., 45., 378., 516.], [400., 112., 655., 493.]], dtype=float32)) We measure the bounding box coordinates for the cat and dog respectively. Here bbox is the abbreviation for bounding box. The bbox below represents dog_bbox , cat_bbox = [ 60.0 , 45.0 , 378.0 , 516.0 ], [ 400.0 , 112.0 , 655.0 , 493.0 ] def draw_bboxes_on_single_image ( image : Union [ torch . Tensor , np . ndarray ], bboxes : Union [ torch . Tensor , np . ndarray ], labels : Union [ torch . Tensor , np . ndarray , List [ int ]], labels_map : Dict [ int , str ], colors : Dict [ int , Tuple [ int , int , int ]], ) -> np . ndarray : \"\"\"Draws bounding boxes on a single image. Args: image (Union[torch.Tensor, np.ndarray]): The original image to draw on. bboxes (Union[torch.Tensor, np.ndarray]): The bounding boxes of the original image. This should be in voc_pascal format of (xmin, ymin, xmax, ymax). labels (Union[torch.Tensor, np.ndarray, List[int]]): The labels of the bounding boxes corresponding to the bounding boxes. Note they are needed to be in the same order as the bounding boxes. colors (Dict[int, Tuple[int, int, int]]): The color mapping of the labels. Returns: image (np.ndarray): The image with bounding boxes drawn on it. Shapes: - Input: - image: :math:`(H, W, C)` or :math:`(C, H, W)` depending on whether the channels are the first or last dimension. - bboxes: :math:`(N, 4)` where the bounding boxes are in the format of (xmin, ymin, xmax, ymax) and N is the number of bounding boxes. - labels: :math:`(N,)` where N is the number of bounding boxes. - Output: - image: :math:`(H, W, C)` Example: >>> import torch >>> import numpy as np >>> cat_dog_p = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/deep_learning/computer_vision/data/misc/catdog.jpg\" >>> cat_dog = PIL.Image.open(urlopen(cat_dog_p)) >>> dog_bbox, cat_bbox = [60.0, 45.0, 378.0, 516.0], [400.0, 112.0, 655.0, 493.0] >>> bboxes = torch.tensor((dog_bbox, cat_bbox)) # convert to (N, 2) >>> labels = [0, 1] >>> labels_map = {0: \"dog\", 1: \"cat\"} >>> colors = {0: (255, 0, 255), 1: (0, 255, 255)} >>> bbox_img = draw_bboxes_on_single_image(cat_dog, bboxes, labels=labels, labels_map=labels_map, colors=colors) >>> plt.imshow(bbox_img); \"\"\" if isinstance ( image , torch . Tensor ): image = image . cpu () . detach () . numpy () if isinstance ( bboxes , torch . Tensor ): bboxes = bboxes . cpu () . detach () . numpy () if isinstance ( labels , torch . Tensor ): labels = labels . cpu () . detach () . numpy () # make sure it is in np.ndarray format if read as PIL image = np . asarray ( image . copy ()) labels = np . asarray ( labels . copy ()) # if labels is like [[0], [1], [0]] then flatten if len ( labels . shape ) > 1 : labels = labels . flatten () # line/font thickness line_thickness = round ( 0.002 * ( image . shape [ 0 ] + image . shape [ 1 ]) / 2 ) + 1 for i , ( bbox , label ) in enumerate ( zip ( bboxes , labels )): color = colors [ label ] pt1 , pt2 = ( bbox [ 0 ], bbox [ 1 ]), ( bbox [ 2 ], bbox [ 3 ]) pt1 = int ( pt1 [ 0 ]), int ( pt1 [ 1 ]) pt2 = int ( pt2 [ 0 ]), int ( pt2 [ 1 ]) image = cv2 . rectangle ( image . copy (), pt1 , pt2 , color , int ( max ( image . shape [: 2 ]) / 200 ) ) # annotate text label2str = labels_map [ label ] cv2 . putText ( image , label2str , pt1 , cv2 . FONT_HERSHEY_SIMPLEX , line_thickness / 3 , color = [ 0 , 0 , 0 ], thickness = line_thickness , lineType = cv2 . LINE_AA , ) return image >>> import torch >>> import numpy as np >>> cat_dog_p = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/deep_learning/computer_vision/data/misc/catdog.jpg\" >>> cat_dog = PIL . Image . open ( urlopen ( cat_dog_p )) >>> dog_bbox , cat_bbox = [ 60.0 , 45.0 , 378.0 , 516.0 ], [ 400.0 , 112.0 , 655.0 , 493.0 ] >>> bboxes = torch . tensor (( dog_bbox , cat_bbox )) >>> labels = [ 0 , 1 ] >>> labels_map = { 0 : \"dog\" , 1 : \"cat\" } >>> colors = { 0 : ( 255 , 0 , 255 ), 1 : ( 0 , 255 , 255 )} >>> bbox_img = draw_bboxes_on_single_image ( cat_dog , bboxes , labels = labels , labels_map = labels_map , colors = colors ) >>> plt . imshow ( bbox_img );","title":"VOC-YOLO Test"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/object_detection/bounding_boxes/#anchor-boxes","text":"def multibox_prior ( data , sizes , ratios ): \"\"\"Generate anchor boxes with different shapes centered on each pixel.\"\"\" in_height , in_width = data . shape [ - 2 :] device , num_sizes , num_ratios = data . device , len ( sizes ), len ( ratios ) boxes_per_pixel = ( num_sizes + num_ratios - 1 ) size_tensor = torch . tensor ( sizes , device = device ) ratio_tensor = torch . tensor ( ratios , device = device ) # Offsets are required to move the anchor to the center of a pixel. Since # a pixel has height=1 and width=1, we choose to offset our centers by 0.5 offset_h , offset_w = 0.5 , 0.5 steps_h = 1.0 / in_height # Scaled steps in y axis steps_w = 1.0 / in_width # Scaled steps in x axis # Generate all center points for the anchor boxes center_h = ( torch . arange ( in_height , device = device ) + offset_h ) * steps_h center_w = ( torch . arange ( in_width , device = device ) + offset_w ) * steps_w shift_y , shift_x = torch . meshgrid ( center_h , center_w ) shift_y , shift_x = shift_y . reshape ( - 1 ), shift_x . reshape ( - 1 ) # Generate `boxes_per_pixel` number of heights and widths that are later # used to create anchor box corner coordinates (xmin, xmax, ymin, ymax) w = torch . cat (( size_tensor * torch . sqrt ( ratio_tensor [ 0 ]), sizes [ 0 ] * torch . sqrt ( ratio_tensor [ 1 :]))) \\ * in_height / in_width # Handle rectangular inputs h = torch . cat (( size_tensor / torch . sqrt ( ratio_tensor [ 0 ]), sizes [ 0 ] / torch . sqrt ( ratio_tensor [ 1 :]))) # Divide by 2 to get half height and half width anchor_manipulations = torch . stack (( - w , - h , w , h )) . T . repeat ( in_height * in_width , 1 ) / 2 # Each center point will have `boxes_per_pixel` number of anchor boxes, so # generate a grid of all anchor box centers with `boxes_per_pixel` repeats out_grid = torch . stack ([ shift_x , shift_y , shift_x , shift_y ], dim = 1 ) . repeat_interleave ( boxes_per_pixel , dim = 0 ) output = out_grid + anchor_manipulations return output . unsqueeze ( 0 )","title":"Anchor Boxes"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/object_detection/faster_rcnn/","text":"https://everitt257.github.io/post/2018/08/10/object_detection.html https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9 Follow This for basic implementation Simple RCNN Implementation Imports import matplotlib.pyplot as plt import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import torch.nn.functional as F import torch import torchvision import torch.nn as nn from torchvision import transforms % matplotlib inline import glob import os from math import ceil import random import cv2 import PIL from IPython.core.interactiveshell import InteractiveShell from torchinfo import summary InteractiveShell . ast_node_interactivity = \"all\" from typing import * # importing modules import urllib.request from urllib.request import urlopen from PIL import Image import bounding_boxes if ( torch . cuda . is_available ()): device = torch . device ( \"cuda\" ) print ( device , torch . cuda . get_device_name ( 0 )) else : device = torch . device ( \"cpu\" ) print ( device ) cpu cat_dog_p = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/deep_learning/computer_vision/data/misc/catdog.jpg\" # plot cat and dog with title using PIL plt . figure ( figsize = ( 6 , 6 )) cat_dog = PIL . Image . open ( urlopen ( cat_dog_p )) plt . imshow ( cat_dog ) plt . title ( \"Cat and Dog\" ) plt . show (); Load the image and bbox in the correct format in np array and plot them; # Object information: a set of bounding boxes [ymin, xmin, ymax, xmax] and their labels # dog_bbox, cat_bbox = [60.0, 45.0, 378.0, 516.0], [400.0, 112.0, 655.0, 493.0] # unsure why is not xmin ymin format tho. dog_bbox , cat_bbox = [ 45.0 , 60.0 , 516.0 , 378.0 ], [ 112.0 , 400.0 , 493.0 , 655.0 ] img = np . asarray ( PIL . Image . open ( urlopen ( cat_dog_p ))) bboxes = np . asarray (( dog_bbox , cat_bbox )) . astype ( int ) labels = np . array ([ 1 , 2 ]) # 0: background, 1: dog, 2: cat for i in range ( len ( bboxes )): cv2 . rectangle ( img , ( bboxes [ i ][ 1 ], bboxes [ i ][ 0 ]), ( bboxes [ i ][ 3 ], bboxes [ i ][ 2 ]), color = ( 0 , 255 , 0 ), thickness = 3 , ) cv2 . putText ( img , str ( int ( labels [ i ])), ( bboxes [ i ][ 3 ], bboxes [ i ][ 2 ]), cv2 . FONT_HERSHEY_SIMPLEX , 3 , ( 0 , 0 , 255 ), thickness = 3 , ) plt . imshow ( img ) plt . show (); Our input to the Faster-RCNN should always be 800x800x3, so need to resize! img = np . asarray ( PIL . Image . open ( urlopen ( cat_dog_p ))) img_resized = cv2 . resize ( img , dsize = ( 800 , 800 ), interpolation = cv2 . INTER_CUBIC ) plt . imshow ( img_resized ) plt . show () <matplotlib.image.AxesImage at 0x1dee17471c0> After resizing the image, the bbox must also be resized to reflect the new coordinates, we do it below; # change the bounding box coordinates Wratio = 800 / img . shape [ 1 ] Hratio = 800 / img . shape [ 0 ] ratioLst = [ Hratio , Wratio , Hratio , Wratio ] bboxes_resized = [] for box in bboxes : box = [ int ( a * b ) for a , b in zip ( box , ratioLst )] bboxes_resized . append ( box ) bboxes_resized = np . asarray ( bboxes_resized ) print ( bboxes_resized ) [[ 64 65 735 415] [159 439 703 719]] for i in range ( len ( bboxes )): cv2 . rectangle ( img_resized , ( bboxes_resized [ i ][ 1 ], bboxes_resized [ i ][ 0 ]), ( bboxes_resized [ i ][ 3 ], bboxes_resized [ i ][ 2 ]), color = ( 0 , 255 , 0 ), thickness = 3 , ) cv2 . putText ( img_resized , str ( int ( labels [ i ])), ( bboxes_resized [ i ][ 3 ], bboxes_resized [ i ][ 2 ]), cv2 . FONT_HERSHEY_SIMPLEX , 3 , ( 0 , 0 , 255 ), thickness = 3 , ) plt . imshow ( img_resized ) plt . show (); Feature Extraction Step Use VGG16 to extract feature maps of the input image. This means we transform (1, 3, 800, 800) to (1, 512, 50, 50) # List all the layers of VGG16 vgg16 = torchvision . models . vgg16 ( pretrained = True ) . to ( device ) vgg16 # vgg16.features VGG( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace=True) (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU(inplace=True) (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (6): ReLU(inplace=True) (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (8): ReLU(inplace=True) (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace=True) (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (13): ReLU(inplace=True) (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (15): ReLU(inplace=True) (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (18): ReLU(inplace=True) (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (20): ReLU(inplace=True) (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (22): ReLU(inplace=True) (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (25): ReLU(inplace=True) (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (27): ReLU(inplace=True) (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (29): ReLU(inplace=True) (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (avgpool): AdaptiveAvgPool2d(output_size=(7, 7)) (classifier): Sequential( (0): Linear(in_features=25088, out_features=4096, bias=True) (1): ReLU(inplace=True) (2): Dropout(p=0.5, inplace=False) (3): Linear(in_features=4096, out_features=4096, bias=True) (4): ReLU(inplace=True) (5): Dropout(p=0.5, inplace=False) (6): Linear(in_features=4096, out_features=1000, bias=True) ) ) batch_size = 1 input_image = torch . rand ( size = ( 3 , 800 , 800 )) print ( summary ( vgg16 , ( batch_size , * input_image . shape ))) ========================================================================================== Layer (type:depth-idx) Output Shape Param # ========================================================================================== VGG -- -- \u251c\u2500Sequential: 1-1 [1, 512, 25, 25] -- \u2502 \u2514\u2500Conv2d: 2-1 [1, 64, 800, 800] 1,792 \u2502 \u2514\u2500ReLU: 2-2 [1, 64, 800, 800] -- \u2502 \u2514\u2500Conv2d: 2-3 [1, 64, 800, 800] 36,928 \u2502 \u2514\u2500ReLU: 2-4 [1, 64, 800, 800] -- \u2502 \u2514\u2500MaxPool2d: 2-5 [1, 64, 400, 400] -- \u2502 \u2514\u2500Conv2d: 2-6 [1, 128, 400, 400] 73,856 \u2502 \u2514\u2500ReLU: 2-7 [1, 128, 400, 400] -- \u2502 \u2514\u2500Conv2d: 2-8 [1, 128, 400, 400] 147,584 \u2502 \u2514\u2500ReLU: 2-9 [1, 128, 400, 400] -- \u2502 \u2514\u2500MaxPool2d: 2-10 [1, 128, 200, 200] -- \u2502 \u2514\u2500Conv2d: 2-11 [1, 256, 200, 200] 295,168 \u2502 \u2514\u2500ReLU: 2-12 [1, 256, 200, 200] -- \u2502 \u2514\u2500Conv2d: 2-13 [1, 256, 200, 200] 590,080 \u2502 \u2514\u2500ReLU: 2-14 [1, 256, 200, 200] -- \u2502 \u2514\u2500Conv2d: 2-15 [1, 256, 200, 200] 590,080 \u2502 \u2514\u2500ReLU: 2-16 [1, 256, 200, 200] -- \u2502 \u2514\u2500MaxPool2d: 2-17 [1, 256, 100, 100] -- \u2502 \u2514\u2500Conv2d: 2-18 [1, 512, 100, 100] 1,180,160 \u2502 \u2514\u2500ReLU: 2-19 [1, 512, 100, 100] -- \u2502 \u2514\u2500Conv2d: 2-20 [1, 512, 100, 100] 2,359,808 \u2502 \u2514\u2500ReLU: 2-21 [1, 512, 100, 100] -- \u2502 \u2514\u2500Conv2d: 2-22 [1, 512, 100, 100] 2,359,808 \u2502 \u2514\u2500ReLU: 2-23 [1, 512, 100, 100] -- \u2502 \u2514\u2500MaxPool2d: 2-24 [1, 512, 50, 50] -- \u2502 \u2514\u2500Conv2d: 2-25 [1, 512, 50, 50] 2,359,808 \u2502 \u2514\u2500ReLU: 2-26 [1, 512, 50, 50] -- \u2502 \u2514\u2500Conv2d: 2-27 [1, 512, 50, 50] 2,359,808 \u2502 \u2514\u2500ReLU: 2-28 [1, 512, 50, 50] -- \u2502 \u2514\u2500Conv2d: 2-29 [1, 512, 50, 50] 2,359,808 \u2502 \u2514\u2500ReLU: 2-30 [1, 512, 50, 50] -- \u2502 \u2514\u2500MaxPool2d: 2-31 [1, 512, 25, 25] -- \u251c\u2500AdaptiveAvgPool2d: 1-2 [1, 512, 7, 7] -- \u251c\u2500Sequential: 1-3 [1, 1000] -- \u2502 \u2514\u2500Linear: 2-32 [1, 4096] 102,764,544 \u2502 \u2514\u2500ReLU: 2-33 [1, 4096] -- \u2502 \u2514\u2500Dropout: 2-34 [1, 4096] -- \u2502 \u2514\u2500Linear: 2-35 [1, 4096] 16,781,312 \u2502 \u2514\u2500ReLU: 2-36 [1, 4096] -- \u2502 \u2514\u2500Dropout: 2-37 [1, 4096] -- \u2502 \u2514\u2500Linear: 2-38 [1, 1000] 4,097,000 ========================================================================================== Total params: 138,357,544 Trainable params: 138,357,544 Non-trainable params: 0 Total mult-adds (G): 196.04 ========================================================================================== Input size (MB): 7.68 Forward/backward pass size (MB): 1382.47 Params size (MB): 553.43 Estimated Total Size (MB): 1943.58 ========================================================================================== Basically we are taking the feature maps up until the last conv layer (exclude max pool). This is a typical step where by we cut off at the final conv layer to get the feature maps! We just subset all features from conv2d 2-1 to relu 2-30 which is just subset [0:-1] from vgg16 features method. req_features = vgg16 . features [ 0 : - 1 ] # Convert this list into a Sequential module, faster_rcnn_fe_extractor = torch . nn . Sequential ( * req_features ) # \u6e2c\u8a66\u770b\u770b input image \u901a\u904e feature extractor \u7684\u7d50\u679c transform = transforms . Compose ([ transforms . ToTensor ()]) # Defing PyTorch Transform imgTensor = transform ( img_resized ) . to ( device ) imgTensor = imgTensor . unsqueeze ( 0 ) out_map = faster_rcnn_fe_extractor ( imgTensor ) print ( out_map . size ()) torch.Size([1, 512, 50, 50]) # visualize the first 5 channels of the 50*50*512 feature maps imgArray = out_map . data . cpu () . numpy () . squeeze ( 0 ) fig = plt . figure ( figsize = ( 12 , 4 )) figNo = 1 for i in range ( 2 ): fig . add_subplot ( 1 , 2 , figNo ) plt . imshow ( imgArray [ i ]) figNo += 1 plt . show (); Generate Anchor Boxes on Each Input Image 50x50=2500 anchors, each anchor generate 9 anchor boxes, Total = 50x50x9=22,500 # x, y intervals to generate anchor box center fe_size = 800 // 16 # 50 ctr_x = np . arange ( 16 , ( fe_size + 1 ) * 16 , 16 ) ctr_y = np . arange ( 16 , ( fe_size + 1 ) * 16 , 16 ) print ( len ( ctr_x ), ctr_x ) print ( len ( ctr_y ), ctr_y ) 50 [ 16 32 48 64 80 96 112 128 144 160 176 192 208 224 240 256 272 288 304 320 336 352 368 384 400 416 432 448 464 480 496 512 528 544 560 576 592 608 624 640 656 672 688 704 720 736 752 768 784 800] 50 [ 16 32 48 64 80 96 112 128 144 160 176 192 208 224 240 256 272 288 304 320 336 352 368 384 400 416 432 448 464 480 496 512 528 544 560 576 592 608 624 640 656 672 688 704 720 736 752 768 784 800] # coordinates of the 2500 center points to generate anchor boxes index = 0 ctr = np . zeros (( 2500 , 2 )) for x in range ( len ( ctr_x )): for y in range ( len ( ctr_y )): ctr [ index , 1 ] = ctr_x [ x ] - 8 ctr [ index , 0 ] = ctr_y [ y ] - 8 index += 1 print ( ctr . shape ) (2500, 2) # display the 2500 anchors plt . figure ( figsize = ( 9 , 6 )) for i in range ( ctr . shape [ 0 ]): cv2 . circle ( img_resized , ( int ( ctr [ i ][ 0 ]), int ( ctr [ i ][ 1 ])), radius = 1 , color = ( 255 , 0 , 0 ), thickness = 1 ) plt . imshow ( img_resized ) plt . show (); # for each of the 2500 anchors, generate 9 anchor boxes # 2500*9 = 22500 anchor boxes ratios = [ 0.5 , 1 , 2 ] scales = [ 8 , 16 , 32 ] sub_sample = 16 anchor_boxes = np . zeros ( (( fe_size * fe_size * 9 ), 4 )) index = 0 for c in ctr : ctr_y , ctr_x = c for i in range ( len ( ratios )): for j in range ( len ( scales )): h = sub_sample * scales [ j ] * np . sqrt ( ratios [ i ]) w = sub_sample * scales [ j ] * np . sqrt ( 1. / ratios [ i ]) anchor_boxes [ index , 0 ] = ctr_y - h / 2. anchor_boxes [ index , 1 ] = ctr_x - w / 2. anchor_boxes [ index , 2 ] = ctr_y + h / 2. anchor_boxes [ index , 3 ] = ctr_x + w / 2. index += 1 print ( anchor_boxes . shape ) (22500, 4) def plot_anchor_boxes ( anchor_boxes , bboxes , image , anchor_bboxes_range : Callable ): image_clone = np . copy ( image ) # display the 9 anchor boxes of one anchor and the ground truth bbox plt . figure ( figsize = ( 9 , 6 )) for i in anchor_bboxes_range : # 9*1225=11025 x0 = int ( anchor_boxes [ i ][ 1 ]) y0 = int ( anchor_boxes [ i ][ 0 ]) x1 = int ( anchor_boxes [ i ][ 3 ]) y1 = int ( anchor_boxes [ i ][ 2 ]) cv2 . rectangle ( image_clone , ( x0 , y0 ), ( x1 , y1 ), color = ( 0 , 0 , 0 ), thickness = 3 ) for i in range ( len ( bboxes )): cv2 . rectangle ( image_clone , ( bboxes [ i ][ 1 ], bboxes [ i ][ 0 ]), ( bboxes [ i ][ 3 ], bboxes [ i ][ 2 ]), color = ( 0 , 255 , 0 ), thickness = 3 , ) # Draw Rectangle plt . imshow ( image_clone ) plt . show () anchor_bboxes_range = range ( 0 , 9 ) plot_anchor_boxes ( anchor_boxes , bboxes_resized , img_resized , anchor_bboxes_range ) anchor_bboxes_range = range ( 9 , 18 ) plot_anchor_boxes ( anchor_boxes , bboxes_resized , img_resized , anchor_bboxes_range ) This 9 anchor boxes are the centered around the center pixel. anchor_bboxes_range = range ( 11025 , 11034 ) #9*1225=11025 middle pixel's anchor box plot_anchor_boxes ( anchor_boxes , bboxes_resized , img_resized , anchor_bboxes_range ) Out of 22500 anchor boxes, we need to discard some anchor boxes, because due to the generating algorithm, some anchor boxes might be outside of the 800 by 800 image. After discarding we have 8940 anchor boxes left. I believe if you tune the ratios = [0.5, 1, 2] and scales = [8, 16, 32] you will get different results for valid anchor boxes. # Ignore cross-boundary anchor boxes # valid anchor boxes with (y1, x1)>0 and (y2, x2)<=800 index_inside = np . where ( ( anchor_boxes [:, 0 ] >= 0 ) & ( anchor_boxes [:, 1 ] >= 0 ) & ( anchor_boxes [:, 2 ] <= 800 ) & ( anchor_boxes [:, 3 ] <= 800 ) )[ 0 ] print ( index_inside . shape ) valid_anchor_boxes = anchor_boxes [ index_inside ] print ( valid_anchor_boxes . shape ) (8940,) (8940, 4) IOU of Anchor Box and True Bounding Boxes! Calculate iou of the valid anchor boxes with each ground truth bbox. Since there are 8940 valid anchor boxes and only 2 ground truth bbox per this image, then Our final ious should be an array of (8940, 2) where column 1 is the ious of each anchor box with bbox 1 (dog gt bbox) and column 2 is the ious of each anchor box with bbox 2 (the cat gt bbox) Note this is batch size of 1, so it is easy to see, but in reality each image has different number of gt bboxes! So take note the shape won't always be (8940, 2) # Calculate iou of the valid anchor boxes # Since we have 8940 anchor boxes and 2 ground truth objects, we should get an array with (8490, 2) as the output. ious = np . empty (( len ( valid_anchor_boxes ), 2 ), dtype = np . float32 ) ious . fill ( 0 ) for num1 , i in enumerate ( valid_anchor_boxes ): ya1 , xa1 , ya2 , xa2 = i anchor_area = ( ya2 - ya1 ) * ( xa2 - xa1 ) for num2 , j in enumerate ( bboxes_resized ): yb1 , xb1 , yb2 , xb2 = j box_area = ( yb2 - yb1 ) * ( xb2 - xb1 ) inter_x1 = max ([ xb1 , xa1 ]) inter_y1 = max ([ yb1 , ya1 ]) inter_x2 = min ([ xb2 , xa2 ]) inter_y2 = min ([ yb2 , ya2 ]) if ( inter_x1 < inter_x2 ) and ( inter_y1 < inter_y2 ): iter_area = ( inter_y2 - inter_y1 ) * ( inter_x2 - inter_x1 ) iou = iter_area / ( anchor_area + box_area - iter_area ) else : iou = 0. ious [ num1 , num2 ] = iou print ( ious . shape ) (8940, 2) For eg, [0.01919497, 0] means the cat bbox and the anchor bbox has 0 intersection. ious array([[0.01919497, 0. ], [0.02159903, 0. ], [0.02401446, 0. ], ..., [0. , 0.0157947 ], [0. , 0.01381642], [0. , 0.01184583]], dtype=float32) gt_argmax_ious: at row 1850 and 6846, we note that they have the highest IOU. We used argmax on the axis 0 column wise, means for each column, we find out which index has the highest IOU score. gt_max_ious: We put these two max ious (1 for dog 1 for cat) into an array [0.886.., 0.581..] gt_argmax_ious: We further see if there's other indexes with the highest IOUs because there can be tie when you use argmax. # What anchor box has max iou with the ground truth bbox gt_argmax_ious = ious . argmax ( axis = 0 ) print ( gt_argmax_ious ) gt_max_ious = ious [ gt_argmax_ious , np . arange ( ious . shape [ 1 ])] print ( gt_max_ious ) gt_argmax_ious = np . where ( ious == gt_max_ious )[ 0 ] print ( gt_argmax_ious ) [1850 6846] [0.8863183 0.5810547] [1850 1857 1864 1871 2078 2085 2092 2099 6846 6853 6860 6867 7074 7081 7088 7095 7302 7309 7316 7323 7530 7537 7544 7551 7758 7765 7772 7779] argmax_ious: this means for each row, we compare the two IOUs and take the largest IOU, then assign to the index: For example the first 3 rows of ious are ([[ 0.01919497 , 0. ], [ 0.02159903 , 0. ], [ 0.02401446 , 0. ], note each row is the anchor box IOU with the 2 gt, so for 1st row, the anchor bbox has some iou with dog as compared to the iou of it with the cat bbox, so we take the index to be 0, else index is 1. max_ious: take the highest IOU for each row and reduce it to 1 column. # What ground truth bbox is associated with each anchor box argmax_ious = ious . argmax ( axis = 1 ) print ( argmax_ious . shape ) print ( argmax_ious ) max_ious = ious [ np . arange ( len ( index_inside )), argmax_ious ] print ( max_ious ) (8940,) [0 0 0 ... 1 1 1] [0.01919497 0.02159903 0.02401446 ... 0.0157947 0.01381642 0.01184583] Let us plot some of these anchor boxes!! Note we change the input argument to valid_anchor_boxes now since we are looking at the valid ones. Let us plot 1850's anchor box by specifying the range from 1850 - 1851. Lo and behold the dog highest bbox is indeed quite good. anchor_bboxes_range = range ( 1850 , 1851 ) #9*1225=11025 middle pixel's anchor box plot_anchor_boxes ( valid_anchor_boxes , bboxes_resized , img_resized , anchor_bboxes_range ) Let us plot some of these anchor boxes!! Note we change the input argument to valid_anchor_boxes now since we are looking at the valid ones. Let us plot 6846's anchor box by specifying the range from 6846 - 6847. Lo and behold! The cat highest bbox isnt as good but ok. anchor_bboxes_range = range ( 6846 , 6847 ) #9*1225=11025 middle pixel's anchor box plot_anchor_boxes ( valid_anchor_boxes , bboxes_resized , img_resized , anchor_bboxes_range ) Use the IOU value to assign LABELS TO EACH VALID ANCHOR BOX ( IMPORTANT ) See D2L: Using the c (512) length- feature vector at the center of each anchor box, predict the binary class (background or objects) and bounding box for this anchor box. So, if IOU is more than a threshold say 0.7, then it is an object which we assign 1, if less than say 0.3, then assign it as background 0, and if between 0.3 and 0.7 we ignore and assign -1. This step is important and clears confusion of how labels were assigned to each anchor box very important! # \u628a 8940 \u500b valid anchor boxes \u7684\u6a19\u7c64\u5148\u7d71\u4e00\u8a2d\u70ba -1 (ignore) label = np . empty (( len ( index_inside ), ), dtype = np . int32 ) label . fill ( - 1 ) print ( label . shape ) (8940,) # Use iou to assign 1 (objects) to two kind of anchors # a) The anchors with the highest iou overlap with a ground-truth-box # b) An anchor that has an IoU overlap higher than 0.7 with ground-truth box # Assign 0 (background) to an anchor if its IoU ratio is lower than 0.3 for all ground-truth boxes pos_iou_threshold = 0.7 neg_iou_threshold = 0.3 label [ gt_argmax_ious ] = 1 label [ max_ious >= pos_iou_threshold ] = 1 label [ max_ious < neg_iou_threshold ] = 0 We see that there are only a few anchor boxes left that really has an object, the rest is background or ignore. np . where ( label == 1 ) (array([1394, 1401, 1408, 1415, 1622, 1629, 1636, 1643, 1850, 1857, 1864, 1871, 2078, 2085, 2092, 2099, 2314, 2322, 2330, 2338, 2560, 2568, 2576, 2584, 6846, 6853, 6860, 6867, 7074, 7081, 7088, 7095, 7302, 7309, 7316, 7323, 7530, 7537, 7544, 7551, 7758, 7765, 7772, 7779], dtype=int64),) Mini-Batch 256 Batch size 256 this is not the batch size of the image but batch size of the anchor boxes So there are 8940 valid anchor boxes, we take 256 from them randomly and such that 128 of them are object of 1 and 128 to be background 0 and the remaining is -1 Note that I only have 44 valid anchor boxes with \"object\" 1 that's ok, we just label all others to be -1. n_sample = 256 pos_ratio = 0.5 n_pos = pos_ratio * n_sample pos_index = np . where ( label == 1 )[ 0 ] if len ( pos_index ) > n_pos : disable_index = np . random . choice ( pos_index , size = ( len ( pos_index ) - n_pos ), replace = False ) label [ disable_index ] = - 1 n_neg = n_sample * np . sum ( label == 1 ) neg_index = np . where ( label == 0 )[ 0 ] print ( neg_index . shape ) if len ( neg_index ) > n_neg : disable_index = np . random . choice ( neg_index , size = ( len ( neg_index ) - n_neg ), replace = False ) label [ disable_index ] = - 1 (7842,) Transform Valid Anchor Boxes Format from (y1, x1, y2, x2) to the correct format # For each valid anchor box, find the groundtruth object which has max_iou max_iou_bbox = bboxes_resized [ argmax_ious ] print ( max_iou_bbox . shape ) # valid anchor boxes \u7684 h, w, cx, cy height = valid_anchor_boxes [:, 2 ] - valid_anchor_boxes [:, 0 ] width = valid_anchor_boxes [:, 3 ] - valid_anchor_boxes [:, 1 ] ctr_y = valid_anchor_boxes [:, 0 ] + 0.5 * height ctr_x = valid_anchor_boxes [:, 1 ] + 0.5 * width # valid anchor box \u7684 max iou \u7684 bbox \u7684 h, w, cx, cy base_height = max_iou_bbox [:, 2 ] - max_iou_bbox [:, 0 ] base_width = max_iou_bbox [:, 3 ] - max_iou_bbox [:, 1 ] base_ctr_y = max_iou_bbox [:, 0 ] + 0.5 * base_height base_ctr_x = max_iou_bbox [:, 1 ] + 0.5 * base_width # valid anchor boxes \u7684 loc = (y-ya/ha), (x-xa/wa), log(h/ha), log(w/wa) eps = np . finfo ( height . dtype ) . eps height = np . maximum ( height , eps ) #\u8b93 height !=0, \u6700\u5c0f\u503c\u70ba eps width = np . maximum ( width , eps ) dy = ( base_ctr_y - ctr_y ) / height dx = ( base_ctr_x - ctr_x ) / width dh = np . log ( base_height / height ) dw = np . log ( base_width / width ) anchor_locs = np . vstack (( dy , dx , dh , dw )) . transpose () print ( anchor_locs . shape ) (8940, 4) (8940, 4) Region Proposal Network So now we must be clear, we have labelled 8940 valid anchor boxes with the TRUE ANSWER Y labels: i.e. some have object we give it a binary class of 1 some dont have we give it 0, some we want to ignore we give it -1 So now back to our 22500 anchor boxes, we label all the \"invalid anchor bboxes\" to be -1 also cause we want to ignore them as well. Note that we are in a mini-batch of 256 anchor boxes so our RPN focuses on this 256 for now. See image below the \\(y\\) and \\(\\hat{y}\\) indicates the gt and predicted. NOTICE: \u6bcf\u500b training epoch, \u6211\u5011\u5f9e 8940 \u500b valid anchor boxes \u96a8\u6a5f\u9078 128 \u500b positive + 128\u500b negative, \u5176\u4ed6\u90fd\u6a19-1 anchor_labels = np . empty (( len ( anchor_boxes ),), dtype = label . dtype ) anchor_labels . fill ( - 1 ) anchor_labels [ index_inside ] = label print ( anchor_labels . shape ) anchor_locations = np . empty (( len ( anchor_boxes ),) + anchor_boxes . shape [ 1 :], dtype = anchor_locs . dtype ) anchor_locations . fill ( 0 ) anchor_locations [ index_inside , :] = anchor_locs print ( anchor_locations . shape ) (22500,) (22500, 4) Send Feature Maps to RPN to produce 22500 Regions of Interests (ROIs) Send the feature maps to the RPN Recall out_map = faster_rcnn_fe_extractor(imgTensor) where we already send the image tensor through the feature extractor and has shape of [1, 512, 50, 50] . So now we will concat some layers after the feature map out_map . out_map = faster_rcnn_fe_extractor ( imgTensor ) out_map . shape torch.Size([1, 512, 50, 50]) in_channels = 512 # depends on the output feature map. in vgg 16 it is equal to 512 mid_channels = 512 n_anchor = 9 # Number of anchors at each location conv1 = nn . Conv2d ( in_channels , mid_channels , 3 , 1 , 1 ) . to ( device ) conv1 . weight . data . normal_ ( 0 , 0.01 ) conv1 . bias . data . zero_ () reg_layer = nn . Conv2d ( mid_channels , n_anchor * 4 , 1 , 1 , 0 ) . to ( device ) reg_layer . weight . data . normal_ ( 0 , 0.01 ) reg_layer . bias . data . zero_ () cls_layer = nn . Conv2d ( mid_channels , n_anchor * 2 , 1 , 1 , 0 ) . to ( device ) # I will be going to use softmax here. you can equally use sigmoid if u replace 2 with 1. cls_layer . weight . data . normal_ ( 0 , 0.01 ) cls_layer . bias . data . zero_ (); x = conv1 ( out_map . to ( device )) # out_map = faster_rcnn_fe_extractor(imgTensor) print ( x . shape ) pred_anchor_locs = reg_layer ( x ) pred_cls_scores = cls_layer ( x ) print ( pred_anchor_locs . shape , pred_cls_scores . shape ) torch.Size([1, 512, 50, 50]) torch.Size([1, 36, 50, 50]) torch.Size([1, 18, 50, 50]) reg_layer : has 9*4 = 36 filters in this layer and therefore outputs [1, 36, 50, 50] shape since padding is same with stride 1. because recall there are 2500 anchors points (not boxes), and there are 2500 * 9 = 22500 anchor boxes, so each anchor box has 4 values so this can be reshaped from (36, 50, 50) to (22500, 4), which is exactly what we want for the regression layer to calculate the loss of localization of the predicted anchor boxes from RPN vs true anchor boxes. cls_layer : in turn this has only [1, 18, 50, 50] shape because it can be reshaped to [22500, 2] since we only want binary classification from RPN to predict whether there exist an object 1 or not 0. # \u8f49\u63db RPN \u9810\u6e2c anchor box \u7684\u4f4d\u7f6e\u8207\u5206\u985e\u4e4b format # \u4f4d\u7f6e: [1, 36(9*4), 50, 50] => [1, 22500(50*50*9), 4] (dy, dx, dh, dw) # \u5206\u985e: [1, 18(9*2), 50, 50] => [1, 22500, 2] (1, 0) pred_anchor_locs = pred_anchor_locs . permute ( 0 , 2 , 3 , 1 ) . contiguous () . view ( 1 , - 1 , 4 ) print ( pred_anchor_locs . shape ) pred_cls_scores = pred_cls_scores . permute ( 0 , 2 , 3 , 1 ) . contiguous () print ( pred_cls_scores . shape ) objectness_score = pred_cls_scores . view ( 1 , 50 , 50 , 9 , 2 )[:, :, :, :, 1 ] . contiguous () . view ( 1 , - 1 ) print ( objectness_score . shape ) pred_cls_scores = pred_cls_scores . view ( 1 , - 1 , 2 ) print ( pred_cls_scores . shape ) torch.Size([1, 22500, 4]) torch.Size([1, 50, 50, 18]) torch.Size([1, 22500]) torch.Size([1, 22500, 2]) RPN LOSS Localization loss uses regression with smooth L1 Loss Objectness Classification uses CE Loss print ( pred_anchor_locs . shape ) print ( pred_cls_scores . shape ) print ( anchor_locations . shape ) print ( anchor_labels . shape ) torch.Size([1, 22500, 4]) torch.Size([1, 22500, 2]) (22500, 4) (22500,) rpn_loc = pred_anchor_locs [ 0 ] rpn_score = pred_cls_scores [ 0 ] gt_rpn_loc = torch . from_numpy ( anchor_locations ) gt_rpn_score = torch . from_numpy ( anchor_labels ) print ( rpn_loc . shape , rpn_score . shape , gt_rpn_loc . shape , gt_rpn_score . shape ) torch.Size([22500, 4]) torch.Size([22500, 2]) torch.Size([22500, 4]) torch.Size([22500]) # For classification we use cross-entropy loss rpn_cls_loss = F . cross_entropy ( rpn_score , gt_rpn_score . long () . to ( device ), ignore_index = - 1 ) print ( rpn_cls_loss ) tensor(0.7012, grad_fn=<NllLossBackward0>) # For Regression we use smooth L1 loss as defined in the Fast RCNN paper pos = gt_rpn_score > 0 mask = pos . unsqueeze ( 1 ) . expand_as ( rpn_loc ) print ( mask . shape ) # take those bounding boxes which have positve labels mask_loc_preds = rpn_loc [ mask ] . view ( - 1 , 4 ) mask_loc_targets = gt_rpn_loc [ mask ] . view ( - 1 , 4 ) print ( mask_loc_preds . shape , mask_loc_targets . shape ) x = torch . abs ( mask_loc_targets . cpu () - mask_loc_preds . cpu ()) rpn_loc_loss = (( x < 1 ) . float () * 0.5 * x ** 2 ) + (( x >= 1 ) . float () * ( x - 0.5 )) print ( rpn_loc_loss . sum ()) torch.Size([22500, 4]) torch.Size([44, 4]) torch.Size([44, 4]) tensor(1.2859, dtype=torch.float64, grad_fn=<SumBackward0>) # Combining both the rpn_cls_loss and rpn_reg_loss rpn_lambda = 10. N_reg = ( gt_rpn_score > 0 ) . float () . sum () rpn_loc_loss = rpn_loc_loss . sum () / N_reg rpn_loss = rpn_cls_loss + ( rpn_lambda * rpn_loc_loss ) print ( rpn_loss ) tensor(0.9935, dtype=torch.float64, grad_fn=<AddBackward0>) NMS nms_thresh = 0.7 # non-maximum supression (NMS) n_train_pre_nms = 12000 # no. of train pre-NMS n_train_post_nms = 2000 # after nms, training Fast R-CNN using 2000 RPN proposals n_test_pre_nms = 6000 n_test_post_nms = 300 # During testing we evaluate 300 proposals, min_size = 16 # The labelled 22500 anchor boxes # format converted from [y1, x1, y2, x2] to [ctr_x, ctr_y, h, w] anc_height = anchor_boxes [:, 2 ] - anchor_boxes [:, 0 ] anc_width = anchor_boxes [:, 3 ] - anchor_boxes [:, 1 ] anc_ctr_y = anchor_boxes [:, 0 ] + 0.5 * anc_height anc_ctr_x = anchor_boxes [:, 1 ] + 0.5 * anc_width print ( anc_ctr_x . shape ) # The 22500 anchor boxes location and labels predicted by RPN (convert to numpy) # format = (dy, dx, dh, dw) pred_anchor_locs_numpy = pred_anchor_locs [ 0 ] . cpu () . data . numpy () objectness_score_numpy = objectness_score [ 0 ] . cpu () . data . numpy () dy = pred_anchor_locs_numpy [:, 0 :: 4 ] #\u6bcf\u500b anchor box \u7684 dy dx = pred_anchor_locs_numpy [:, 1 :: 4 ] # dx dh = pred_anchor_locs_numpy [:, 2 :: 4 ] # dh dw = pred_anchor_locs_numpy [:, 3 :: 4 ] # dw print ( dy . shape ) # ctr_y = dy predicted by RPN * anchor_h + anchor_cy # ctr_x similar # h = exp(dh predicted by RPN) * anchor_h # w similar ctr_y = dy * anc_height [:, np . newaxis ] + anc_ctr_y [:, np . newaxis ] ctr_x = dx * anc_width [:, np . newaxis ] + anc_ctr_x [:, np . newaxis ] h = np . exp ( dh ) * anc_height [:, np . newaxis ] w = np . exp ( dw ) * anc_width [:, np . newaxis ] print ( w . shape ) (22500,) (22500, 1) (22500, 1) # \u7528 labelled \u7684 anchor boxes \u8207 RPN \u9810\u6e2c\u7684 anchor boxes\u4f86\u8a08\u7b97 ROI = [y1, x1, y2, x2] roi = np . zeros ( pred_anchor_locs_numpy . shape , dtype = anchor_locs . dtype ) roi [:, 0 :: 4 ] = ctr_y - 0.5 * h roi [:, 1 :: 4 ] = ctr_x - 0.5 * w roi [:, 2 :: 4 ] = ctr_y + 0.5 * h roi [:, 3 :: 4 ] = ctr_x + 0.5 * w print ( roi . shape ) # clip the predicted boxes to the image img_size = ( 800 , 800 ) #Image size roi [:, slice ( 0 , 4 , 2 )] = np . clip ( roi [:, slice ( 0 , 4 , 2 )], 0 , img_size [ 0 ]) roi [:, slice ( 1 , 4 , 2 )] = np . clip ( roi [:, slice ( 1 , 4 , 2 )], 0 , img_size [ 1 ]) print ( roi . shape , np . max ( roi ), np . min ( roi )) (22500, 4) (22500, 4) 800.0 0.0 # Remove predicted boxes with either height or width < threshold. hs = roi [:, 2 ] - roi [:, 0 ] ws = roi [:, 3 ] - roi [:, 1 ] keep = np . where (( hs >= min_size ) & ( ws >= min_size ))[ 0 ] #min_size=16 roi = roi [ keep , :] score = objectness_score_numpy [ keep ] print ( keep . shape , roi . shape , score . shape ) # Sort all (proposal, score) pairs by score from highest to lowest order = score . ravel () . argsort ()[:: - 1 ] print ( order . shape ) #Take top pre_nms_topN (e.g. 12000 while training and 300 while testing) order = order [: n_train_pre_nms ] roi = roi [ order , :] print ( order . shape , roi . shape , roi . shape ) (22500,) (22500, 4) (22500,) (22500,) (12000,) (12000, 4) (12000, 4) # Take all the roi boxes [roi_array] y1 = roi [:, 0 ] x1 = roi [:, 1 ] y2 = roi [:, 2 ] x2 = roi [:, 3 ] # Find the areas of all the boxes [roi_area] areas = ( x2 - x1 + 1 ) * ( y2 - y1 + 1 ) #Take the indexes of order the probability score in descending order order = order . argsort ()[:: - 1 ] keep = [] while ( order . size > 0 ): i = order [ 0 ] #take the 1st elt in order and append to keep keep . append ( i ) xx1 = np . maximum ( x1 [ i ], x1 [ order [ 1 :]]) yy1 = np . maximum ( y1 [ i ], y1 [ order [ 1 :]]) xx2 = np . minimum ( x2 [ i ], x2 [ order [ 1 :]]) yy2 = np . minimum ( y2 [ i ], y2 [ order [ 1 :]]) w = np . maximum ( 0.0 , xx2 - xx1 + 1 ) h = np . maximum ( 0.0 , yy2 - yy1 + 1 ) inter = w * h ovr = inter / ( areas [ i ] + areas [ order [ 1 :]] - inter ) inds = np . where ( ovr <= nms_thresh )[ 0 ] order = order [ inds + 1 ] keep = keep [: n_train_post_nms ] # while training/testing , use accordingly roi = roi [ keep ] # the final region proposals print ( len ( keep ), roi . shape ) 2000 (2000, 4) Sample 128 From NMS's 2000 ROIs n_sample = 128 # Number of samples from roi pos_ratio = 0.25 # Number of positive examples out of the n_samples pos_iou_thresh = 0.5 # Min iou of region proposal with any groundtruth object to consider it as positive label neg_iou_thresh_hi = 0.5 # iou 0~0.5 is considered as negative (0, background) neg_iou_thresh_lo = 0.0 # Find the iou of each ground truth object with the region proposals, ious = np . empty (( len ( roi ), 2 ), dtype = np . float32 ) ious . fill ( 0 ) for num1 , i in enumerate ( roi ): ya1 , xa1 , ya2 , xa2 = i anchor_area = ( ya2 - ya1 ) * ( xa2 - xa1 ) for num2 , j in enumerate ( bboxes_resized ): yb1 , xb1 , yb2 , xb2 = j box_area = ( yb2 - yb1 ) * ( xb2 - xb1 ) inter_x1 = max ([ xb1 , xa1 ]) inter_y1 = max ([ yb1 , ya1 ]) inter_x2 = min ([ xb2 , xa2 ]) inter_y2 = min ([ yb2 , ya2 ]) if ( inter_x1 < inter_x2 ) and ( inter_y1 < inter_y2 ): iter_area = ( inter_y2 - inter_y1 ) * ( inter_x2 - inter_x1 ) iou = iter_area / ( anchor_area + box_area - iter_area ) else : iou = 0. ious [ num1 , num2 ] = iou print ( ious . shape ) (2000, 2) # Find out which ground truth has high IoU for each region proposal, Also find the maximum IoU gt_assignment = ious . argmax ( axis = 1 ) max_iou = ious . max ( axis = 1 ) print ( gt_assignment ) print ( max_iou ) # Assign the labels to each proposal gt_roi_label = labels [ gt_assignment ] print ( gt_roi_label ) [1 0 1 ... 0 0 0] [0.16893381 0. 0.02414654 ... 0.280571 0.06312128 0.07249717] [2 1 2 ... 1 1 1] # Select the foreground rois as per the pos_iou_thesh and # n_sample x pos_ratio (128 x 0.25 = 32) foreground samples. pos_roi_per_image = 32 pos_index = np . where ( max_iou >= pos_iou_thresh )[ 0 ] pos_roi_per_this_image = int ( min ( pos_roi_per_image , pos_index . size )) if pos_index . size > 0 : pos_index = np . random . choice ( pos_index , size = pos_roi_per_this_image , replace = False ) print ( pos_roi_per_this_image ) print ( pos_index ) # Similarly we do for negitive (background) region proposals neg_index = np . where (( max_iou < neg_iou_thresh_hi ) & ( max_iou >= neg_iou_thresh_lo ))[ 0 ] neg_roi_per_this_image = n_sample - pos_roi_per_this_image neg_roi_per_this_image = int ( min ( neg_roi_per_this_image , neg_index . size )) if neg_index . size > 0 : neg_index = np . random . choice ( neg_index , size = neg_roi_per_this_image , replace = False ) print ( neg_roi_per_this_image ) print ( neg_index ) 25 [1586 1678 1453 1905 715 1738 395 1342 122 19 561 672 1591 136 1640 347 1574 568 255 1779 199 1957 1736 1405 1609] 103 [ 534 1181 1987 1226 441 808 397 43 406 1372 1133 1332 1783 744 824 939 562 1178 1089 595 558 1353 1470 1995 538 491 477 1224 1208 1359 743 1799 674 290 1944 962 1806 681 192 1824 249 1452 914 547 69 107 303 1304 1711 1986 911 1197 489 1033 94 1334 806 187 1996 708 1657 1659 1688 1011 1196 225 403 509 282 401 409 440 95 892 1321 133 1096 334 153 1923 1873 770 1291 734 143 1680 475 418 1555 1950 1039 1382 298 1067 1965 1525 1255 330 832 1485 212 1114 1487] Here displays ROI samples with positive class 1; meaning there is object. # display ROI samples with postive img_clone = np . copy ( img_resized ) plt . figure ( figsize = ( 9 , 6 )) for i in range ( pos_roi_per_this_image ): y0 , x0 , y1 , x1 = roi [ pos_index [ i ]] . astype ( int ) cv2 . rectangle ( img_clone , ( x0 , y0 ), ( x1 , y1 ), color = ( 0 , 0 , 0 ), thickness = 3 ) for i in range ( len ( bboxes_resized )): cv2 . rectangle ( img_clone , ( bboxes_resized [ i ][ 1 ], bboxes_resized [ i ][ 0 ]), ( bboxes_resized [ i ][ 3 ], bboxes_resized [ i ][ 2 ]), color = ( 0 , 255 , 0 ), thickness = 3 ) # Draw Rectangle plt . imshow ( img_clone ) plt . show (); Here displays ROI samples with negative class 0; meaning there is no object, background # display ROI samples with negative img_clone = np . copy ( img_resized ) plt . figure ( figsize = ( 9 , 6 )) for i in range ( neg_roi_per_this_image ): y0 , x0 , y1 , x1 = roi [ neg_index [ i ]] . astype ( int ) cv2 . rectangle ( img_clone , ( x0 , y0 ), ( x1 , y1 ), color = ( 0 , 0 , 0 ), thickness = 3 ) for i in range ( len ( bboxes_resized )): cv2 . rectangle ( img_clone , ( bboxes_resized [ i ][ 1 ], bboxes_resized [ i ][ 0 ]), ( bboxes_resized [ i ][ 3 ], bboxes_resized [ i ][ 2 ]), color = ( 0 , 255 , 0 ), thickness = 3 ) # Draw Rectangle plt . imshow ( img_clone ) plt . show (); # Now we gather positve samples index and negitive samples index, # their respective labels and region proposals keep_index = np . append ( pos_index , neg_index ) gt_roi_labels = gt_roi_label [ keep_index ] gt_roi_labels [ pos_roi_per_this_image :] = 0 # negative labels --> 0 sample_roi = roi [ keep_index ] print ( sample_roi . shape ) # Pick the ground truth objects for these sample_roi and # later parameterize as we have done while assigning locations to anchor boxes in section 2. bbox_for_sampled_roi = bboxes_resized [ gt_assignment [ keep_index ]] print ( bbox_for_sampled_roi . shape ) height = sample_roi [:, 2 ] - sample_roi [:, 0 ] width = sample_roi [:, 3 ] - sample_roi [:, 1 ] ctr_y = sample_roi [:, 0 ] + 0.5 * height ctr_x = sample_roi [:, 1 ] + 0.5 * width base_height = bbox_for_sampled_roi [:, 2 ] - bbox_for_sampled_roi [:, 0 ] base_width = bbox_for_sampled_roi [:, 3 ] - bbox_for_sampled_roi [:, 1 ] base_ctr_y = bbox_for_sampled_roi [:, 0 ] + 0.5 * base_height base_ctr_x = bbox_for_sampled_roi [:, 1 ] + 0.5 * base_width (128, 4) (128, 4) eps = np . finfo ( height . dtype ) . eps height = np . maximum ( height , eps ) width = np . maximum ( width , eps ) dy = ( base_ctr_y - ctr_y ) / height dx = ( base_ctr_x - ctr_x ) / width dh = np . log ( base_height / height ) dw = np . log ( base_width / width ) gt_roi_locs = np . vstack (( dy , dx , dh , dw )) . transpose () print ( gt_roi_locs . shape ) (128, 4) 128 ROI Samples' Features and Pool them rois = torch . from_numpy ( sample_roi ) . float () roi_indices = 0 * np . ones (( len ( rois ),), dtype = np . int32 ) roi_indices = torch . from_numpy ( roi_indices ) . float () print ( rois . shape , roi_indices . shape ) indices_and_rois = torch . cat ([ roi_indices [:, None ], rois ], dim = 1 ) xy_indices_and_rois = indices_and_rois [:, [ 0 , 2 , 1 , 4 , 3 ]] indices_and_rois = xy_indices_and_rois . contiguous () print ( xy_indices_and_rois . shape ) torch.Size([128, 4]) torch.Size([128]) torch.Size([128, 5]) size = ( 7 , 7 ) adaptive_max_pool = nn . AdaptiveMaxPool2d ( size [ 0 ], size [ 1 ]) output = [] rois = indices_and_rois . data . float () rois [:, 1 :] . mul_ ( 1 / 16.0 ) # Subsampling ratio rois = rois . long () num_rois = rois . size ( 0 ) for i in range ( num_rois ): roi = rois [ i ] im_idx = roi [ 0 ] im = out_map . narrow ( 0 , im_idx , 1 )[ ... , roi [ 2 ]:( roi [ 4 ] + 1 ), roi [ 1 ]:( roi [ 3 ] + 1 )] tmp = adaptive_max_pool ( im ) output . append ( tmp [ 0 ]) output = torch . cat ( output , 0 ) print ( output . size ()); torch.Size([128, 512, 7, 7]) # Visualize the first 5 ROI's feature map (for each feature map, only show the 1st channel of d=512) fig = plt . figure ( figsize = ( 12 , 4 )) figNo = 1 for i in range ( 5 ): roi = rois [ i ] im_idx = roi [ 0 ] im = out_map . narrow ( 0 , im_idx , 1 )[ ... , roi [ 2 ]:( roi [ 4 ] + 1 ), roi [ 1 ]:( roi [ 3 ] + 1 )] tmp = im [ 0 ][ 0 ] . detach () . cpu () . numpy () fig . add_subplot ( 1 , 5 , figNo ) plt . imshow ( tmp ) figNo += 1 plt . show (); # Visualize the first 5 ROI's feature maps after ROI pooling (for each feature map, only show the 1st channel of d=512) fig = plt . figure ( figsize = ( 12 , 4 )) figNo = 1 for i in range ( 5 ): roi = rois [ i ] im_idx = roi [ 0 ] im = out_map . narrow ( 0 , im_idx , 1 )[ ... , roi [ 2 ]:( roi [ 4 ] + 1 ), roi [ 1 ]:( roi [ 3 ] + 1 )] tmp = adaptive_max_pool ( im )[ 0 ] tmp = tmp [ 0 ][ 0 ] . detach () . cpu () . numpy () fig . add_subplot ( 1 , 5 , figNo ) plt . imshow ( tmp ) figNo += 1 plt . show (); # Reshape the tensor so that we can pass it through the feed forward layer. k = output . view ( output . size ( 0 ), - 1 ) print ( k . shape ) # 25088 = 7*7*512 torch.Size([128, 25088]) PyTorch Pre-trained model model = torchvision . models . detection . fasterrcnn_resnet50_fpn ( pretrained = True ) model FasterRCNN( (transform): GeneralizedRCNNTransform( Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) Resize(min_size=(800,), max_size=1333, mode='bilinear') ) (backbone): BackboneWithFPN( (body): IntermediateLayerGetter( (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): FrozenBatchNorm2d(64, eps=0.0) (relu): ReLU(inplace=True) (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (layer1): Sequential( (0): Bottleneck( (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): FrozenBatchNorm2d(64, eps=0.0) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): FrozenBatchNorm2d(64, eps=0.0) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): FrozenBatchNorm2d(256, eps=0.0) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): FrozenBatchNorm2d(256, eps=0.0) ) ) (1): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): FrozenBatchNorm2d(64, eps=0.0) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): FrozenBatchNorm2d(64, eps=0.0) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): FrozenBatchNorm2d(256, eps=0.0) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): FrozenBatchNorm2d(64, eps=0.0) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): FrozenBatchNorm2d(64, eps=0.0) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): FrozenBatchNorm2d(256, eps=0.0) (relu): ReLU(inplace=True) ) ) (layer2): Sequential( (0): Bottleneck( (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): FrozenBatchNorm2d(128, eps=0.0) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): FrozenBatchNorm2d(128, eps=0.0) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): FrozenBatchNorm2d(512, eps=0.0) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): FrozenBatchNorm2d(512, eps=0.0) ) ) (1): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): FrozenBatchNorm2d(128, eps=0.0) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): FrozenBatchNorm2d(128, eps=0.0) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): FrozenBatchNorm2d(512, eps=0.0) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): FrozenBatchNorm2d(128, eps=0.0) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): FrozenBatchNorm2d(128, eps=0.0) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): FrozenBatchNorm2d(512, eps=0.0) (relu): ReLU(inplace=True) ) (3): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): FrozenBatchNorm2d(128, eps=0.0) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): FrozenBatchNorm2d(128, eps=0.0) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): FrozenBatchNorm2d(512, eps=0.0) (relu): ReLU(inplace=True) ) ) (layer3): Sequential( (0): Bottleneck( (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): FrozenBatchNorm2d(256, eps=0.0) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): FrozenBatchNorm2d(256, eps=0.0) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): FrozenBatchNorm2d(1024, eps=0.0) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): FrozenBatchNorm2d(1024, eps=0.0) ) ) (1): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): FrozenBatchNorm2d(256, eps=0.0) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): FrozenBatchNorm2d(256, eps=0.0) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): FrozenBatchNorm2d(1024, eps=0.0) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): FrozenBatchNorm2d(256, eps=0.0) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): FrozenBatchNorm2d(256, eps=0.0) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): FrozenBatchNorm2d(1024, eps=0.0) (relu): ReLU(inplace=True) ) (3): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): FrozenBatchNorm2d(256, eps=0.0) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): FrozenBatchNorm2d(256, eps=0.0) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): FrozenBatchNorm2d(1024, eps=0.0) (relu): ReLU(inplace=True) ) (4): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): FrozenBatchNorm2d(256, eps=0.0) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): FrozenBatchNorm2d(256, eps=0.0) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): FrozenBatchNorm2d(1024, eps=0.0) (relu): ReLU(inplace=True) ) (5): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): FrozenBatchNorm2d(256, eps=0.0) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): FrozenBatchNorm2d(256, eps=0.0) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): FrozenBatchNorm2d(1024, eps=0.0) (relu): ReLU(inplace=True) ) ) (layer4): Sequential( (0): Bottleneck( (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): FrozenBatchNorm2d(512, eps=0.0) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): FrozenBatchNorm2d(512, eps=0.0) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): FrozenBatchNorm2d(2048, eps=0.0) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): FrozenBatchNorm2d(2048, eps=0.0) ) ) (1): Bottleneck( (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): FrozenBatchNorm2d(512, eps=0.0) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): FrozenBatchNorm2d(512, eps=0.0) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): FrozenBatchNorm2d(2048, eps=0.0) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): FrozenBatchNorm2d(512, eps=0.0) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): FrozenBatchNorm2d(512, eps=0.0) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): FrozenBatchNorm2d(2048, eps=0.0) (relu): ReLU(inplace=True) ) ) ) (fpn): FeaturePyramidNetwork( (inner_blocks): ModuleList( (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1)) (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1)) (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1)) (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1)) ) (layer_blocks): ModuleList( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) ) (extra_blocks): LastLevelMaxPool() ) ) (rpn): RegionProposalNetwork( (anchor_generator): AnchorGenerator() (head): RPNHead( (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1)) (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1)) ) ) (roi_heads): RoIHeads( (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2) (box_head): TwoMLPHead( (fc6): Linear(in_features=12544, out_features=1024, bias=True) (fc7): Linear(in_features=1024, out_features=1024, bias=True) ) (box_predictor): FastRCNNPredictor( (cls_score): Linear(in_features=1024, out_features=91, bias=True) (bbox_pred): Linear(in_features=1024, out_features=364, bias=True) ) ) ) # For training images , boxes = torch . rand ( 4 , 3 , 600 , 1200 ), torch . rand ( 4 , 11 , 4 ) boxes [:, :, 2 : 4 ] = boxes [:, :, 0 : 2 ] + boxes [:, :, 2 : 4 ] labels = torch . randint ( 1 , 91 , ( 4 , 11 )) images = list ( image for image in images ) print ( labels . shape ) torch.Size([4, 11]) >>> targets = [] >>> for i in range ( len ( images )): >>> d = {} >>> d [ 'boxes' ] = boxes [ i ] >>> d [ 'labels' ] = labels [ i ] >>> targets . append ( d ) >>> output = model ( images , targets ) >>> # For inference >>> model . eval () >>> x = [ torch . rand ( 3 , 300 , 400 ), torch . rand ( 3 , 500 , 400 )] >>> predictions = model ( x )","title":"Faster rcnn"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/object_detection/faster_rcnn/#imports","text":"import matplotlib.pyplot as plt import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import torch.nn.functional as F import torch import torchvision import torch.nn as nn from torchvision import transforms % matplotlib inline import glob import os from math import ceil import random import cv2 import PIL from IPython.core.interactiveshell import InteractiveShell from torchinfo import summary InteractiveShell . ast_node_interactivity = \"all\" from typing import * # importing modules import urllib.request from urllib.request import urlopen from PIL import Image import bounding_boxes if ( torch . cuda . is_available ()): device = torch . device ( \"cuda\" ) print ( device , torch . cuda . get_device_name ( 0 )) else : device = torch . device ( \"cpu\" ) print ( device ) cpu cat_dog_p = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/deep_learning/computer_vision/data/misc/catdog.jpg\" # plot cat and dog with title using PIL plt . figure ( figsize = ( 6 , 6 )) cat_dog = PIL . Image . open ( urlopen ( cat_dog_p )) plt . imshow ( cat_dog ) plt . title ( \"Cat and Dog\" ) plt . show (); Load the image and bbox in the correct format in np array and plot them; # Object information: a set of bounding boxes [ymin, xmin, ymax, xmax] and their labels # dog_bbox, cat_bbox = [60.0, 45.0, 378.0, 516.0], [400.0, 112.0, 655.0, 493.0] # unsure why is not xmin ymin format tho. dog_bbox , cat_bbox = [ 45.0 , 60.0 , 516.0 , 378.0 ], [ 112.0 , 400.0 , 493.0 , 655.0 ] img = np . asarray ( PIL . Image . open ( urlopen ( cat_dog_p ))) bboxes = np . asarray (( dog_bbox , cat_bbox )) . astype ( int ) labels = np . array ([ 1 , 2 ]) # 0: background, 1: dog, 2: cat for i in range ( len ( bboxes )): cv2 . rectangle ( img , ( bboxes [ i ][ 1 ], bboxes [ i ][ 0 ]), ( bboxes [ i ][ 3 ], bboxes [ i ][ 2 ]), color = ( 0 , 255 , 0 ), thickness = 3 , ) cv2 . putText ( img , str ( int ( labels [ i ])), ( bboxes [ i ][ 3 ], bboxes [ i ][ 2 ]), cv2 . FONT_HERSHEY_SIMPLEX , 3 , ( 0 , 0 , 255 ), thickness = 3 , ) plt . imshow ( img ) plt . show (); Our input to the Faster-RCNN should always be 800x800x3, so need to resize! img = np . asarray ( PIL . Image . open ( urlopen ( cat_dog_p ))) img_resized = cv2 . resize ( img , dsize = ( 800 , 800 ), interpolation = cv2 . INTER_CUBIC ) plt . imshow ( img_resized ) plt . show () <matplotlib.image.AxesImage at 0x1dee17471c0> After resizing the image, the bbox must also be resized to reflect the new coordinates, we do it below; # change the bounding box coordinates Wratio = 800 / img . shape [ 1 ] Hratio = 800 / img . shape [ 0 ] ratioLst = [ Hratio , Wratio , Hratio , Wratio ] bboxes_resized = [] for box in bboxes : box = [ int ( a * b ) for a , b in zip ( box , ratioLst )] bboxes_resized . append ( box ) bboxes_resized = np . asarray ( bboxes_resized ) print ( bboxes_resized ) [[ 64 65 735 415] [159 439 703 719]] for i in range ( len ( bboxes )): cv2 . rectangle ( img_resized , ( bboxes_resized [ i ][ 1 ], bboxes_resized [ i ][ 0 ]), ( bboxes_resized [ i ][ 3 ], bboxes_resized [ i ][ 2 ]), color = ( 0 , 255 , 0 ), thickness = 3 , ) cv2 . putText ( img_resized , str ( int ( labels [ i ])), ( bboxes_resized [ i ][ 3 ], bboxes_resized [ i ][ 2 ]), cv2 . FONT_HERSHEY_SIMPLEX , 3 , ( 0 , 0 , 255 ), thickness = 3 , ) plt . imshow ( img_resized ) plt . show ();","title":"Imports"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/object_detection/faster_rcnn/#feature-extraction-step","text":"Use VGG16 to extract feature maps of the input image. This means we transform (1, 3, 800, 800) to (1, 512, 50, 50) # List all the layers of VGG16 vgg16 = torchvision . models . vgg16 ( pretrained = True ) . to ( device ) vgg16 # vgg16.features VGG( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace=True) (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU(inplace=True) (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (6): ReLU(inplace=True) (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (8): ReLU(inplace=True) (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace=True) (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (13): ReLU(inplace=True) (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (15): ReLU(inplace=True) (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (18): ReLU(inplace=True) (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (20): ReLU(inplace=True) (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (22): ReLU(inplace=True) (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (25): ReLU(inplace=True) (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (27): ReLU(inplace=True) (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (29): ReLU(inplace=True) (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (avgpool): AdaptiveAvgPool2d(output_size=(7, 7)) (classifier): Sequential( (0): Linear(in_features=25088, out_features=4096, bias=True) (1): ReLU(inplace=True) (2): Dropout(p=0.5, inplace=False) (3): Linear(in_features=4096, out_features=4096, bias=True) (4): ReLU(inplace=True) (5): Dropout(p=0.5, inplace=False) (6): Linear(in_features=4096, out_features=1000, bias=True) ) ) batch_size = 1 input_image = torch . rand ( size = ( 3 , 800 , 800 )) print ( summary ( vgg16 , ( batch_size , * input_image . shape ))) ========================================================================================== Layer (type:depth-idx) Output Shape Param # ========================================================================================== VGG -- -- \u251c\u2500Sequential: 1-1 [1, 512, 25, 25] -- \u2502 \u2514\u2500Conv2d: 2-1 [1, 64, 800, 800] 1,792 \u2502 \u2514\u2500ReLU: 2-2 [1, 64, 800, 800] -- \u2502 \u2514\u2500Conv2d: 2-3 [1, 64, 800, 800] 36,928 \u2502 \u2514\u2500ReLU: 2-4 [1, 64, 800, 800] -- \u2502 \u2514\u2500MaxPool2d: 2-5 [1, 64, 400, 400] -- \u2502 \u2514\u2500Conv2d: 2-6 [1, 128, 400, 400] 73,856 \u2502 \u2514\u2500ReLU: 2-7 [1, 128, 400, 400] -- \u2502 \u2514\u2500Conv2d: 2-8 [1, 128, 400, 400] 147,584 \u2502 \u2514\u2500ReLU: 2-9 [1, 128, 400, 400] -- \u2502 \u2514\u2500MaxPool2d: 2-10 [1, 128, 200, 200] -- \u2502 \u2514\u2500Conv2d: 2-11 [1, 256, 200, 200] 295,168 \u2502 \u2514\u2500ReLU: 2-12 [1, 256, 200, 200] -- \u2502 \u2514\u2500Conv2d: 2-13 [1, 256, 200, 200] 590,080 \u2502 \u2514\u2500ReLU: 2-14 [1, 256, 200, 200] -- \u2502 \u2514\u2500Conv2d: 2-15 [1, 256, 200, 200] 590,080 \u2502 \u2514\u2500ReLU: 2-16 [1, 256, 200, 200] -- \u2502 \u2514\u2500MaxPool2d: 2-17 [1, 256, 100, 100] -- \u2502 \u2514\u2500Conv2d: 2-18 [1, 512, 100, 100] 1,180,160 \u2502 \u2514\u2500ReLU: 2-19 [1, 512, 100, 100] -- \u2502 \u2514\u2500Conv2d: 2-20 [1, 512, 100, 100] 2,359,808 \u2502 \u2514\u2500ReLU: 2-21 [1, 512, 100, 100] -- \u2502 \u2514\u2500Conv2d: 2-22 [1, 512, 100, 100] 2,359,808 \u2502 \u2514\u2500ReLU: 2-23 [1, 512, 100, 100] -- \u2502 \u2514\u2500MaxPool2d: 2-24 [1, 512, 50, 50] -- \u2502 \u2514\u2500Conv2d: 2-25 [1, 512, 50, 50] 2,359,808 \u2502 \u2514\u2500ReLU: 2-26 [1, 512, 50, 50] -- \u2502 \u2514\u2500Conv2d: 2-27 [1, 512, 50, 50] 2,359,808 \u2502 \u2514\u2500ReLU: 2-28 [1, 512, 50, 50] -- \u2502 \u2514\u2500Conv2d: 2-29 [1, 512, 50, 50] 2,359,808 \u2502 \u2514\u2500ReLU: 2-30 [1, 512, 50, 50] -- \u2502 \u2514\u2500MaxPool2d: 2-31 [1, 512, 25, 25] -- \u251c\u2500AdaptiveAvgPool2d: 1-2 [1, 512, 7, 7] -- \u251c\u2500Sequential: 1-3 [1, 1000] -- \u2502 \u2514\u2500Linear: 2-32 [1, 4096] 102,764,544 \u2502 \u2514\u2500ReLU: 2-33 [1, 4096] -- \u2502 \u2514\u2500Dropout: 2-34 [1, 4096] -- \u2502 \u2514\u2500Linear: 2-35 [1, 4096] 16,781,312 \u2502 \u2514\u2500ReLU: 2-36 [1, 4096] -- \u2502 \u2514\u2500Dropout: 2-37 [1, 4096] -- \u2502 \u2514\u2500Linear: 2-38 [1, 1000] 4,097,000 ========================================================================================== Total params: 138,357,544 Trainable params: 138,357,544 Non-trainable params: 0 Total mult-adds (G): 196.04 ========================================================================================== Input size (MB): 7.68 Forward/backward pass size (MB): 1382.47 Params size (MB): 553.43 Estimated Total Size (MB): 1943.58 ========================================================================================== Basically we are taking the feature maps up until the last conv layer (exclude max pool). This is a typical step where by we cut off at the final conv layer to get the feature maps! We just subset all features from conv2d 2-1 to relu 2-30 which is just subset [0:-1] from vgg16 features method. req_features = vgg16 . features [ 0 : - 1 ] # Convert this list into a Sequential module, faster_rcnn_fe_extractor = torch . nn . Sequential ( * req_features ) # \u6e2c\u8a66\u770b\u770b input image \u901a\u904e feature extractor \u7684\u7d50\u679c transform = transforms . Compose ([ transforms . ToTensor ()]) # Defing PyTorch Transform imgTensor = transform ( img_resized ) . to ( device ) imgTensor = imgTensor . unsqueeze ( 0 ) out_map = faster_rcnn_fe_extractor ( imgTensor ) print ( out_map . size ()) torch.Size([1, 512, 50, 50]) # visualize the first 5 channels of the 50*50*512 feature maps imgArray = out_map . data . cpu () . numpy () . squeeze ( 0 ) fig = plt . figure ( figsize = ( 12 , 4 )) figNo = 1 for i in range ( 2 ): fig . add_subplot ( 1 , 2 , figNo ) plt . imshow ( imgArray [ i ]) figNo += 1 plt . show ();","title":"Feature Extraction Step"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/object_detection/faster_rcnn/#generate-anchor-boxes-on-each-input-image","text":"50x50=2500 anchors, each anchor generate 9 anchor boxes, Total = 50x50x9=22,500 # x, y intervals to generate anchor box center fe_size = 800 // 16 # 50 ctr_x = np . arange ( 16 , ( fe_size + 1 ) * 16 , 16 ) ctr_y = np . arange ( 16 , ( fe_size + 1 ) * 16 , 16 ) print ( len ( ctr_x ), ctr_x ) print ( len ( ctr_y ), ctr_y ) 50 [ 16 32 48 64 80 96 112 128 144 160 176 192 208 224 240 256 272 288 304 320 336 352 368 384 400 416 432 448 464 480 496 512 528 544 560 576 592 608 624 640 656 672 688 704 720 736 752 768 784 800] 50 [ 16 32 48 64 80 96 112 128 144 160 176 192 208 224 240 256 272 288 304 320 336 352 368 384 400 416 432 448 464 480 496 512 528 544 560 576 592 608 624 640 656 672 688 704 720 736 752 768 784 800] # coordinates of the 2500 center points to generate anchor boxes index = 0 ctr = np . zeros (( 2500 , 2 )) for x in range ( len ( ctr_x )): for y in range ( len ( ctr_y )): ctr [ index , 1 ] = ctr_x [ x ] - 8 ctr [ index , 0 ] = ctr_y [ y ] - 8 index += 1 print ( ctr . shape ) (2500, 2) # display the 2500 anchors plt . figure ( figsize = ( 9 , 6 )) for i in range ( ctr . shape [ 0 ]): cv2 . circle ( img_resized , ( int ( ctr [ i ][ 0 ]), int ( ctr [ i ][ 1 ])), radius = 1 , color = ( 255 , 0 , 0 ), thickness = 1 ) plt . imshow ( img_resized ) plt . show (); # for each of the 2500 anchors, generate 9 anchor boxes # 2500*9 = 22500 anchor boxes ratios = [ 0.5 , 1 , 2 ] scales = [ 8 , 16 , 32 ] sub_sample = 16 anchor_boxes = np . zeros ( (( fe_size * fe_size * 9 ), 4 )) index = 0 for c in ctr : ctr_y , ctr_x = c for i in range ( len ( ratios )): for j in range ( len ( scales )): h = sub_sample * scales [ j ] * np . sqrt ( ratios [ i ]) w = sub_sample * scales [ j ] * np . sqrt ( 1. / ratios [ i ]) anchor_boxes [ index , 0 ] = ctr_y - h / 2. anchor_boxes [ index , 1 ] = ctr_x - w / 2. anchor_boxes [ index , 2 ] = ctr_y + h / 2. anchor_boxes [ index , 3 ] = ctr_x + w / 2. index += 1 print ( anchor_boxes . shape ) (22500, 4) def plot_anchor_boxes ( anchor_boxes , bboxes , image , anchor_bboxes_range : Callable ): image_clone = np . copy ( image ) # display the 9 anchor boxes of one anchor and the ground truth bbox plt . figure ( figsize = ( 9 , 6 )) for i in anchor_bboxes_range : # 9*1225=11025 x0 = int ( anchor_boxes [ i ][ 1 ]) y0 = int ( anchor_boxes [ i ][ 0 ]) x1 = int ( anchor_boxes [ i ][ 3 ]) y1 = int ( anchor_boxes [ i ][ 2 ]) cv2 . rectangle ( image_clone , ( x0 , y0 ), ( x1 , y1 ), color = ( 0 , 0 , 0 ), thickness = 3 ) for i in range ( len ( bboxes )): cv2 . rectangle ( image_clone , ( bboxes [ i ][ 1 ], bboxes [ i ][ 0 ]), ( bboxes [ i ][ 3 ], bboxes [ i ][ 2 ]), color = ( 0 , 255 , 0 ), thickness = 3 , ) # Draw Rectangle plt . imshow ( image_clone ) plt . show () anchor_bboxes_range = range ( 0 , 9 ) plot_anchor_boxes ( anchor_boxes , bboxes_resized , img_resized , anchor_bboxes_range ) anchor_bboxes_range = range ( 9 , 18 ) plot_anchor_boxes ( anchor_boxes , bboxes_resized , img_resized , anchor_bboxes_range ) This 9 anchor boxes are the centered around the center pixel. anchor_bboxes_range = range ( 11025 , 11034 ) #9*1225=11025 middle pixel's anchor box plot_anchor_boxes ( anchor_boxes , bboxes_resized , img_resized , anchor_bboxes_range ) Out of 22500 anchor boxes, we need to discard some anchor boxes, because due to the generating algorithm, some anchor boxes might be outside of the 800 by 800 image. After discarding we have 8940 anchor boxes left. I believe if you tune the ratios = [0.5, 1, 2] and scales = [8, 16, 32] you will get different results for valid anchor boxes. # Ignore cross-boundary anchor boxes # valid anchor boxes with (y1, x1)>0 and (y2, x2)<=800 index_inside = np . where ( ( anchor_boxes [:, 0 ] >= 0 ) & ( anchor_boxes [:, 1 ] >= 0 ) & ( anchor_boxes [:, 2 ] <= 800 ) & ( anchor_boxes [:, 3 ] <= 800 ) )[ 0 ] print ( index_inside . shape ) valid_anchor_boxes = anchor_boxes [ index_inside ] print ( valid_anchor_boxes . shape ) (8940,) (8940, 4)","title":"Generate Anchor Boxes on Each Input Image"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/object_detection/faster_rcnn/#iou-of-anchor-box-and-true-bounding-boxes","text":"Calculate iou of the valid anchor boxes with each ground truth bbox. Since there are 8940 valid anchor boxes and only 2 ground truth bbox per this image, then Our final ious should be an array of (8940, 2) where column 1 is the ious of each anchor box with bbox 1 (dog gt bbox) and column 2 is the ious of each anchor box with bbox 2 (the cat gt bbox) Note this is batch size of 1, so it is easy to see, but in reality each image has different number of gt bboxes! So take note the shape won't always be (8940, 2) # Calculate iou of the valid anchor boxes # Since we have 8940 anchor boxes and 2 ground truth objects, we should get an array with (8490, 2) as the output. ious = np . empty (( len ( valid_anchor_boxes ), 2 ), dtype = np . float32 ) ious . fill ( 0 ) for num1 , i in enumerate ( valid_anchor_boxes ): ya1 , xa1 , ya2 , xa2 = i anchor_area = ( ya2 - ya1 ) * ( xa2 - xa1 ) for num2 , j in enumerate ( bboxes_resized ): yb1 , xb1 , yb2 , xb2 = j box_area = ( yb2 - yb1 ) * ( xb2 - xb1 ) inter_x1 = max ([ xb1 , xa1 ]) inter_y1 = max ([ yb1 , ya1 ]) inter_x2 = min ([ xb2 , xa2 ]) inter_y2 = min ([ yb2 , ya2 ]) if ( inter_x1 < inter_x2 ) and ( inter_y1 < inter_y2 ): iter_area = ( inter_y2 - inter_y1 ) * ( inter_x2 - inter_x1 ) iou = iter_area / ( anchor_area + box_area - iter_area ) else : iou = 0. ious [ num1 , num2 ] = iou print ( ious . shape ) (8940, 2) For eg, [0.01919497, 0] means the cat bbox and the anchor bbox has 0 intersection. ious array([[0.01919497, 0. ], [0.02159903, 0. ], [0.02401446, 0. ], ..., [0. , 0.0157947 ], [0. , 0.01381642], [0. , 0.01184583]], dtype=float32) gt_argmax_ious: at row 1850 and 6846, we note that they have the highest IOU. We used argmax on the axis 0 column wise, means for each column, we find out which index has the highest IOU score. gt_max_ious: We put these two max ious (1 for dog 1 for cat) into an array [0.886.., 0.581..] gt_argmax_ious: We further see if there's other indexes with the highest IOUs because there can be tie when you use argmax. # What anchor box has max iou with the ground truth bbox gt_argmax_ious = ious . argmax ( axis = 0 ) print ( gt_argmax_ious ) gt_max_ious = ious [ gt_argmax_ious , np . arange ( ious . shape [ 1 ])] print ( gt_max_ious ) gt_argmax_ious = np . where ( ious == gt_max_ious )[ 0 ] print ( gt_argmax_ious ) [1850 6846] [0.8863183 0.5810547] [1850 1857 1864 1871 2078 2085 2092 2099 6846 6853 6860 6867 7074 7081 7088 7095 7302 7309 7316 7323 7530 7537 7544 7551 7758 7765 7772 7779] argmax_ious: this means for each row, we compare the two IOUs and take the largest IOU, then assign to the index: For example the first 3 rows of ious are ([[ 0.01919497 , 0. ], [ 0.02159903 , 0. ], [ 0.02401446 , 0. ], note each row is the anchor box IOU with the 2 gt, so for 1st row, the anchor bbox has some iou with dog as compared to the iou of it with the cat bbox, so we take the index to be 0, else index is 1. max_ious: take the highest IOU for each row and reduce it to 1 column. # What ground truth bbox is associated with each anchor box argmax_ious = ious . argmax ( axis = 1 ) print ( argmax_ious . shape ) print ( argmax_ious ) max_ious = ious [ np . arange ( len ( index_inside )), argmax_ious ] print ( max_ious ) (8940,) [0 0 0 ... 1 1 1] [0.01919497 0.02159903 0.02401446 ... 0.0157947 0.01381642 0.01184583] Let us plot some of these anchor boxes!! Note we change the input argument to valid_anchor_boxes now since we are looking at the valid ones. Let us plot 1850's anchor box by specifying the range from 1850 - 1851. Lo and behold the dog highest bbox is indeed quite good. anchor_bboxes_range = range ( 1850 , 1851 ) #9*1225=11025 middle pixel's anchor box plot_anchor_boxes ( valid_anchor_boxes , bboxes_resized , img_resized , anchor_bboxes_range ) Let us plot some of these anchor boxes!! Note we change the input argument to valid_anchor_boxes now since we are looking at the valid ones. Let us plot 6846's anchor box by specifying the range from 6846 - 6847. Lo and behold! The cat highest bbox isnt as good but ok. anchor_bboxes_range = range ( 6846 , 6847 ) #9*1225=11025 middle pixel's anchor box plot_anchor_boxes ( valid_anchor_boxes , bboxes_resized , img_resized , anchor_bboxes_range )","title":"IOU of Anchor Box and True Bounding Boxes!"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/object_detection/faster_rcnn/#use-the-iou-value-to-assign-labels-to-each-valid-anchor-box-important","text":"See D2L: Using the c (512) length- feature vector at the center of each anchor box, predict the binary class (background or objects) and bounding box for this anchor box. So, if IOU is more than a threshold say 0.7, then it is an object which we assign 1, if less than say 0.3, then assign it as background 0, and if between 0.3 and 0.7 we ignore and assign -1. This step is important and clears confusion of how labels were assigned to each anchor box very important! # \u628a 8940 \u500b valid anchor boxes \u7684\u6a19\u7c64\u5148\u7d71\u4e00\u8a2d\u70ba -1 (ignore) label = np . empty (( len ( index_inside ), ), dtype = np . int32 ) label . fill ( - 1 ) print ( label . shape ) (8940,) # Use iou to assign 1 (objects) to two kind of anchors # a) The anchors with the highest iou overlap with a ground-truth-box # b) An anchor that has an IoU overlap higher than 0.7 with ground-truth box # Assign 0 (background) to an anchor if its IoU ratio is lower than 0.3 for all ground-truth boxes pos_iou_threshold = 0.7 neg_iou_threshold = 0.3 label [ gt_argmax_ious ] = 1 label [ max_ious >= pos_iou_threshold ] = 1 label [ max_ious < neg_iou_threshold ] = 0 We see that there are only a few anchor boxes left that really has an object, the rest is background or ignore. np . where ( label == 1 ) (array([1394, 1401, 1408, 1415, 1622, 1629, 1636, 1643, 1850, 1857, 1864, 1871, 2078, 2085, 2092, 2099, 2314, 2322, 2330, 2338, 2560, 2568, 2576, 2584, 6846, 6853, 6860, 6867, 7074, 7081, 7088, 7095, 7302, 7309, 7316, 7323, 7530, 7537, 7544, 7551, 7758, 7765, 7772, 7779], dtype=int64),)","title":"Use the IOU value to assign LABELS TO EACH VALID ANCHOR BOX (IMPORTANT)"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/object_detection/faster_rcnn/#mini-batch-256","text":"Batch size 256 this is not the batch size of the image but batch size of the anchor boxes So there are 8940 valid anchor boxes, we take 256 from them randomly and such that 128 of them are object of 1 and 128 to be background 0 and the remaining is -1 Note that I only have 44 valid anchor boxes with \"object\" 1 that's ok, we just label all others to be -1. n_sample = 256 pos_ratio = 0.5 n_pos = pos_ratio * n_sample pos_index = np . where ( label == 1 )[ 0 ] if len ( pos_index ) > n_pos : disable_index = np . random . choice ( pos_index , size = ( len ( pos_index ) - n_pos ), replace = False ) label [ disable_index ] = - 1 n_neg = n_sample * np . sum ( label == 1 ) neg_index = np . where ( label == 0 )[ 0 ] print ( neg_index . shape ) if len ( neg_index ) > n_neg : disable_index = np . random . choice ( neg_index , size = ( len ( neg_index ) - n_neg ), replace = False ) label [ disable_index ] = - 1 (7842,)","title":"Mini-Batch 256"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/object_detection/faster_rcnn/#transform-valid-anchor-boxes-format-from-y1-x1-y2-x2-to-the-correct-format","text":"# For each valid anchor box, find the groundtruth object which has max_iou max_iou_bbox = bboxes_resized [ argmax_ious ] print ( max_iou_bbox . shape ) # valid anchor boxes \u7684 h, w, cx, cy height = valid_anchor_boxes [:, 2 ] - valid_anchor_boxes [:, 0 ] width = valid_anchor_boxes [:, 3 ] - valid_anchor_boxes [:, 1 ] ctr_y = valid_anchor_boxes [:, 0 ] + 0.5 * height ctr_x = valid_anchor_boxes [:, 1 ] + 0.5 * width # valid anchor box \u7684 max iou \u7684 bbox \u7684 h, w, cx, cy base_height = max_iou_bbox [:, 2 ] - max_iou_bbox [:, 0 ] base_width = max_iou_bbox [:, 3 ] - max_iou_bbox [:, 1 ] base_ctr_y = max_iou_bbox [:, 0 ] + 0.5 * base_height base_ctr_x = max_iou_bbox [:, 1 ] + 0.5 * base_width # valid anchor boxes \u7684 loc = (y-ya/ha), (x-xa/wa), log(h/ha), log(w/wa) eps = np . finfo ( height . dtype ) . eps height = np . maximum ( height , eps ) #\u8b93 height !=0, \u6700\u5c0f\u503c\u70ba eps width = np . maximum ( width , eps ) dy = ( base_ctr_y - ctr_y ) / height dx = ( base_ctr_x - ctr_x ) / width dh = np . log ( base_height / height ) dw = np . log ( base_width / width ) anchor_locs = np . vstack (( dy , dx , dh , dw )) . transpose () print ( anchor_locs . shape ) (8940, 4) (8940, 4)","title":"Transform Valid Anchor Boxes Format from (y1, x1, y2, x2) to the correct format"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/object_detection/faster_rcnn/#region-proposal-network","text":"So now we must be clear, we have labelled 8940 valid anchor boxes with the TRUE ANSWER Y labels: i.e. some have object we give it a binary class of 1 some dont have we give it 0, some we want to ignore we give it -1 So now back to our 22500 anchor boxes, we label all the \"invalid anchor bboxes\" to be -1 also cause we want to ignore them as well. Note that we are in a mini-batch of 256 anchor boxes so our RPN focuses on this 256 for now. See image below the \\(y\\) and \\(\\hat{y}\\) indicates the gt and predicted. NOTICE: \u6bcf\u500b training epoch, \u6211\u5011\u5f9e 8940 \u500b valid anchor boxes \u96a8\u6a5f\u9078 128 \u500b positive + 128\u500b negative, \u5176\u4ed6\u90fd\u6a19-1 anchor_labels = np . empty (( len ( anchor_boxes ),), dtype = label . dtype ) anchor_labels . fill ( - 1 ) anchor_labels [ index_inside ] = label print ( anchor_labels . shape ) anchor_locations = np . empty (( len ( anchor_boxes ),) + anchor_boxes . shape [ 1 :], dtype = anchor_locs . dtype ) anchor_locations . fill ( 0 ) anchor_locations [ index_inside , :] = anchor_locs print ( anchor_locations . shape ) (22500,) (22500, 4)","title":"Region Proposal Network"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/object_detection/faster_rcnn/#send-feature-maps-to-rpn-to-produce-22500-regions-of-interests-rois","text":"Send the feature maps to the RPN Recall out_map = faster_rcnn_fe_extractor(imgTensor) where we already send the image tensor through the feature extractor and has shape of [1, 512, 50, 50] . So now we will concat some layers after the feature map out_map . out_map = faster_rcnn_fe_extractor ( imgTensor ) out_map . shape torch.Size([1, 512, 50, 50]) in_channels = 512 # depends on the output feature map. in vgg 16 it is equal to 512 mid_channels = 512 n_anchor = 9 # Number of anchors at each location conv1 = nn . Conv2d ( in_channels , mid_channels , 3 , 1 , 1 ) . to ( device ) conv1 . weight . data . normal_ ( 0 , 0.01 ) conv1 . bias . data . zero_ () reg_layer = nn . Conv2d ( mid_channels , n_anchor * 4 , 1 , 1 , 0 ) . to ( device ) reg_layer . weight . data . normal_ ( 0 , 0.01 ) reg_layer . bias . data . zero_ () cls_layer = nn . Conv2d ( mid_channels , n_anchor * 2 , 1 , 1 , 0 ) . to ( device ) # I will be going to use softmax here. you can equally use sigmoid if u replace 2 with 1. cls_layer . weight . data . normal_ ( 0 , 0.01 ) cls_layer . bias . data . zero_ (); x = conv1 ( out_map . to ( device )) # out_map = faster_rcnn_fe_extractor(imgTensor) print ( x . shape ) pred_anchor_locs = reg_layer ( x ) pred_cls_scores = cls_layer ( x ) print ( pred_anchor_locs . shape , pred_cls_scores . shape ) torch.Size([1, 512, 50, 50]) torch.Size([1, 36, 50, 50]) torch.Size([1, 18, 50, 50]) reg_layer : has 9*4 = 36 filters in this layer and therefore outputs [1, 36, 50, 50] shape since padding is same with stride 1. because recall there are 2500 anchors points (not boxes), and there are 2500 * 9 = 22500 anchor boxes, so each anchor box has 4 values so this can be reshaped from (36, 50, 50) to (22500, 4), which is exactly what we want for the regression layer to calculate the loss of localization of the predicted anchor boxes from RPN vs true anchor boxes. cls_layer : in turn this has only [1, 18, 50, 50] shape because it can be reshaped to [22500, 2] since we only want binary classification from RPN to predict whether there exist an object 1 or not 0. # \u8f49\u63db RPN \u9810\u6e2c anchor box \u7684\u4f4d\u7f6e\u8207\u5206\u985e\u4e4b format # \u4f4d\u7f6e: [1, 36(9*4), 50, 50] => [1, 22500(50*50*9), 4] (dy, dx, dh, dw) # \u5206\u985e: [1, 18(9*2), 50, 50] => [1, 22500, 2] (1, 0) pred_anchor_locs = pred_anchor_locs . permute ( 0 , 2 , 3 , 1 ) . contiguous () . view ( 1 , - 1 , 4 ) print ( pred_anchor_locs . shape ) pred_cls_scores = pred_cls_scores . permute ( 0 , 2 , 3 , 1 ) . contiguous () print ( pred_cls_scores . shape ) objectness_score = pred_cls_scores . view ( 1 , 50 , 50 , 9 , 2 )[:, :, :, :, 1 ] . contiguous () . view ( 1 , - 1 ) print ( objectness_score . shape ) pred_cls_scores = pred_cls_scores . view ( 1 , - 1 , 2 ) print ( pred_cls_scores . shape ) torch.Size([1, 22500, 4]) torch.Size([1, 50, 50, 18]) torch.Size([1, 22500]) torch.Size([1, 22500, 2])","title":"Send Feature Maps to RPN to produce 22500 Regions of Interests (ROIs)"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/object_detection/faster_rcnn/#rpn-loss","text":"Localization loss uses regression with smooth L1 Loss Objectness Classification uses CE Loss print ( pred_anchor_locs . shape ) print ( pred_cls_scores . shape ) print ( anchor_locations . shape ) print ( anchor_labels . shape ) torch.Size([1, 22500, 4]) torch.Size([1, 22500, 2]) (22500, 4) (22500,) rpn_loc = pred_anchor_locs [ 0 ] rpn_score = pred_cls_scores [ 0 ] gt_rpn_loc = torch . from_numpy ( anchor_locations ) gt_rpn_score = torch . from_numpy ( anchor_labels ) print ( rpn_loc . shape , rpn_score . shape , gt_rpn_loc . shape , gt_rpn_score . shape ) torch.Size([22500, 4]) torch.Size([22500, 2]) torch.Size([22500, 4]) torch.Size([22500]) # For classification we use cross-entropy loss rpn_cls_loss = F . cross_entropy ( rpn_score , gt_rpn_score . long () . to ( device ), ignore_index = - 1 ) print ( rpn_cls_loss ) tensor(0.7012, grad_fn=<NllLossBackward0>) # For Regression we use smooth L1 loss as defined in the Fast RCNN paper pos = gt_rpn_score > 0 mask = pos . unsqueeze ( 1 ) . expand_as ( rpn_loc ) print ( mask . shape ) # take those bounding boxes which have positve labels mask_loc_preds = rpn_loc [ mask ] . view ( - 1 , 4 ) mask_loc_targets = gt_rpn_loc [ mask ] . view ( - 1 , 4 ) print ( mask_loc_preds . shape , mask_loc_targets . shape ) x = torch . abs ( mask_loc_targets . cpu () - mask_loc_preds . cpu ()) rpn_loc_loss = (( x < 1 ) . float () * 0.5 * x ** 2 ) + (( x >= 1 ) . float () * ( x - 0.5 )) print ( rpn_loc_loss . sum ()) torch.Size([22500, 4]) torch.Size([44, 4]) torch.Size([44, 4]) tensor(1.2859, dtype=torch.float64, grad_fn=<SumBackward0>) # Combining both the rpn_cls_loss and rpn_reg_loss rpn_lambda = 10. N_reg = ( gt_rpn_score > 0 ) . float () . sum () rpn_loc_loss = rpn_loc_loss . sum () / N_reg rpn_loss = rpn_cls_loss + ( rpn_lambda * rpn_loc_loss ) print ( rpn_loss ) tensor(0.9935, dtype=torch.float64, grad_fn=<AddBackward0>)","title":"RPN LOSS"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/object_detection/faster_rcnn/#nms","text":"nms_thresh = 0.7 # non-maximum supression (NMS) n_train_pre_nms = 12000 # no. of train pre-NMS n_train_post_nms = 2000 # after nms, training Fast R-CNN using 2000 RPN proposals n_test_pre_nms = 6000 n_test_post_nms = 300 # During testing we evaluate 300 proposals, min_size = 16 # The labelled 22500 anchor boxes # format converted from [y1, x1, y2, x2] to [ctr_x, ctr_y, h, w] anc_height = anchor_boxes [:, 2 ] - anchor_boxes [:, 0 ] anc_width = anchor_boxes [:, 3 ] - anchor_boxes [:, 1 ] anc_ctr_y = anchor_boxes [:, 0 ] + 0.5 * anc_height anc_ctr_x = anchor_boxes [:, 1 ] + 0.5 * anc_width print ( anc_ctr_x . shape ) # The 22500 anchor boxes location and labels predicted by RPN (convert to numpy) # format = (dy, dx, dh, dw) pred_anchor_locs_numpy = pred_anchor_locs [ 0 ] . cpu () . data . numpy () objectness_score_numpy = objectness_score [ 0 ] . cpu () . data . numpy () dy = pred_anchor_locs_numpy [:, 0 :: 4 ] #\u6bcf\u500b anchor box \u7684 dy dx = pred_anchor_locs_numpy [:, 1 :: 4 ] # dx dh = pred_anchor_locs_numpy [:, 2 :: 4 ] # dh dw = pred_anchor_locs_numpy [:, 3 :: 4 ] # dw print ( dy . shape ) # ctr_y = dy predicted by RPN * anchor_h + anchor_cy # ctr_x similar # h = exp(dh predicted by RPN) * anchor_h # w similar ctr_y = dy * anc_height [:, np . newaxis ] + anc_ctr_y [:, np . newaxis ] ctr_x = dx * anc_width [:, np . newaxis ] + anc_ctr_x [:, np . newaxis ] h = np . exp ( dh ) * anc_height [:, np . newaxis ] w = np . exp ( dw ) * anc_width [:, np . newaxis ] print ( w . shape ) (22500,) (22500, 1) (22500, 1) # \u7528 labelled \u7684 anchor boxes \u8207 RPN \u9810\u6e2c\u7684 anchor boxes\u4f86\u8a08\u7b97 ROI = [y1, x1, y2, x2] roi = np . zeros ( pred_anchor_locs_numpy . shape , dtype = anchor_locs . dtype ) roi [:, 0 :: 4 ] = ctr_y - 0.5 * h roi [:, 1 :: 4 ] = ctr_x - 0.5 * w roi [:, 2 :: 4 ] = ctr_y + 0.5 * h roi [:, 3 :: 4 ] = ctr_x + 0.5 * w print ( roi . shape ) # clip the predicted boxes to the image img_size = ( 800 , 800 ) #Image size roi [:, slice ( 0 , 4 , 2 )] = np . clip ( roi [:, slice ( 0 , 4 , 2 )], 0 , img_size [ 0 ]) roi [:, slice ( 1 , 4 , 2 )] = np . clip ( roi [:, slice ( 1 , 4 , 2 )], 0 , img_size [ 1 ]) print ( roi . shape , np . max ( roi ), np . min ( roi )) (22500, 4) (22500, 4) 800.0 0.0 # Remove predicted boxes with either height or width < threshold. hs = roi [:, 2 ] - roi [:, 0 ] ws = roi [:, 3 ] - roi [:, 1 ] keep = np . where (( hs >= min_size ) & ( ws >= min_size ))[ 0 ] #min_size=16 roi = roi [ keep , :] score = objectness_score_numpy [ keep ] print ( keep . shape , roi . shape , score . shape ) # Sort all (proposal, score) pairs by score from highest to lowest order = score . ravel () . argsort ()[:: - 1 ] print ( order . shape ) #Take top pre_nms_topN (e.g. 12000 while training and 300 while testing) order = order [: n_train_pre_nms ] roi = roi [ order , :] print ( order . shape , roi . shape , roi . shape ) (22500,) (22500, 4) (22500,) (22500,) (12000,) (12000, 4) (12000, 4) # Take all the roi boxes [roi_array] y1 = roi [:, 0 ] x1 = roi [:, 1 ] y2 = roi [:, 2 ] x2 = roi [:, 3 ] # Find the areas of all the boxes [roi_area] areas = ( x2 - x1 + 1 ) * ( y2 - y1 + 1 ) #Take the indexes of order the probability score in descending order order = order . argsort ()[:: - 1 ] keep = [] while ( order . size > 0 ): i = order [ 0 ] #take the 1st elt in order and append to keep keep . append ( i ) xx1 = np . maximum ( x1 [ i ], x1 [ order [ 1 :]]) yy1 = np . maximum ( y1 [ i ], y1 [ order [ 1 :]]) xx2 = np . minimum ( x2 [ i ], x2 [ order [ 1 :]]) yy2 = np . minimum ( y2 [ i ], y2 [ order [ 1 :]]) w = np . maximum ( 0.0 , xx2 - xx1 + 1 ) h = np . maximum ( 0.0 , yy2 - yy1 + 1 ) inter = w * h ovr = inter / ( areas [ i ] + areas [ order [ 1 :]] - inter ) inds = np . where ( ovr <= nms_thresh )[ 0 ] order = order [ inds + 1 ] keep = keep [: n_train_post_nms ] # while training/testing , use accordingly roi = roi [ keep ] # the final region proposals print ( len ( keep ), roi . shape ) 2000 (2000, 4)","title":"NMS"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/object_detection/faster_rcnn/#sample-128-from-nmss-2000-rois","text":"n_sample = 128 # Number of samples from roi pos_ratio = 0.25 # Number of positive examples out of the n_samples pos_iou_thresh = 0.5 # Min iou of region proposal with any groundtruth object to consider it as positive label neg_iou_thresh_hi = 0.5 # iou 0~0.5 is considered as negative (0, background) neg_iou_thresh_lo = 0.0 # Find the iou of each ground truth object with the region proposals, ious = np . empty (( len ( roi ), 2 ), dtype = np . float32 ) ious . fill ( 0 ) for num1 , i in enumerate ( roi ): ya1 , xa1 , ya2 , xa2 = i anchor_area = ( ya2 - ya1 ) * ( xa2 - xa1 ) for num2 , j in enumerate ( bboxes_resized ): yb1 , xb1 , yb2 , xb2 = j box_area = ( yb2 - yb1 ) * ( xb2 - xb1 ) inter_x1 = max ([ xb1 , xa1 ]) inter_y1 = max ([ yb1 , ya1 ]) inter_x2 = min ([ xb2 , xa2 ]) inter_y2 = min ([ yb2 , ya2 ]) if ( inter_x1 < inter_x2 ) and ( inter_y1 < inter_y2 ): iter_area = ( inter_y2 - inter_y1 ) * ( inter_x2 - inter_x1 ) iou = iter_area / ( anchor_area + box_area - iter_area ) else : iou = 0. ious [ num1 , num2 ] = iou print ( ious . shape ) (2000, 2) # Find out which ground truth has high IoU for each region proposal, Also find the maximum IoU gt_assignment = ious . argmax ( axis = 1 ) max_iou = ious . max ( axis = 1 ) print ( gt_assignment ) print ( max_iou ) # Assign the labels to each proposal gt_roi_label = labels [ gt_assignment ] print ( gt_roi_label ) [1 0 1 ... 0 0 0] [0.16893381 0. 0.02414654 ... 0.280571 0.06312128 0.07249717] [2 1 2 ... 1 1 1] # Select the foreground rois as per the pos_iou_thesh and # n_sample x pos_ratio (128 x 0.25 = 32) foreground samples. pos_roi_per_image = 32 pos_index = np . where ( max_iou >= pos_iou_thresh )[ 0 ] pos_roi_per_this_image = int ( min ( pos_roi_per_image , pos_index . size )) if pos_index . size > 0 : pos_index = np . random . choice ( pos_index , size = pos_roi_per_this_image , replace = False ) print ( pos_roi_per_this_image ) print ( pos_index ) # Similarly we do for negitive (background) region proposals neg_index = np . where (( max_iou < neg_iou_thresh_hi ) & ( max_iou >= neg_iou_thresh_lo ))[ 0 ] neg_roi_per_this_image = n_sample - pos_roi_per_this_image neg_roi_per_this_image = int ( min ( neg_roi_per_this_image , neg_index . size )) if neg_index . size > 0 : neg_index = np . random . choice ( neg_index , size = neg_roi_per_this_image , replace = False ) print ( neg_roi_per_this_image ) print ( neg_index ) 25 [1586 1678 1453 1905 715 1738 395 1342 122 19 561 672 1591 136 1640 347 1574 568 255 1779 199 1957 1736 1405 1609] 103 [ 534 1181 1987 1226 441 808 397 43 406 1372 1133 1332 1783 744 824 939 562 1178 1089 595 558 1353 1470 1995 538 491 477 1224 1208 1359 743 1799 674 290 1944 962 1806 681 192 1824 249 1452 914 547 69 107 303 1304 1711 1986 911 1197 489 1033 94 1334 806 187 1996 708 1657 1659 1688 1011 1196 225 403 509 282 401 409 440 95 892 1321 133 1096 334 153 1923 1873 770 1291 734 143 1680 475 418 1555 1950 1039 1382 298 1067 1965 1525 1255 330 832 1485 212 1114 1487] Here displays ROI samples with positive class 1; meaning there is object. # display ROI samples with postive img_clone = np . copy ( img_resized ) plt . figure ( figsize = ( 9 , 6 )) for i in range ( pos_roi_per_this_image ): y0 , x0 , y1 , x1 = roi [ pos_index [ i ]] . astype ( int ) cv2 . rectangle ( img_clone , ( x0 , y0 ), ( x1 , y1 ), color = ( 0 , 0 , 0 ), thickness = 3 ) for i in range ( len ( bboxes_resized )): cv2 . rectangle ( img_clone , ( bboxes_resized [ i ][ 1 ], bboxes_resized [ i ][ 0 ]), ( bboxes_resized [ i ][ 3 ], bboxes_resized [ i ][ 2 ]), color = ( 0 , 255 , 0 ), thickness = 3 ) # Draw Rectangle plt . imshow ( img_clone ) plt . show (); Here displays ROI samples with negative class 0; meaning there is no object, background # display ROI samples with negative img_clone = np . copy ( img_resized ) plt . figure ( figsize = ( 9 , 6 )) for i in range ( neg_roi_per_this_image ): y0 , x0 , y1 , x1 = roi [ neg_index [ i ]] . astype ( int ) cv2 . rectangle ( img_clone , ( x0 , y0 ), ( x1 , y1 ), color = ( 0 , 0 , 0 ), thickness = 3 ) for i in range ( len ( bboxes_resized )): cv2 . rectangle ( img_clone , ( bboxes_resized [ i ][ 1 ], bboxes_resized [ i ][ 0 ]), ( bboxes_resized [ i ][ 3 ], bboxes_resized [ i ][ 2 ]), color = ( 0 , 255 , 0 ), thickness = 3 ) # Draw Rectangle plt . imshow ( img_clone ) plt . show (); # Now we gather positve samples index and negitive samples index, # their respective labels and region proposals keep_index = np . append ( pos_index , neg_index ) gt_roi_labels = gt_roi_label [ keep_index ] gt_roi_labels [ pos_roi_per_this_image :] = 0 # negative labels --> 0 sample_roi = roi [ keep_index ] print ( sample_roi . shape ) # Pick the ground truth objects for these sample_roi and # later parameterize as we have done while assigning locations to anchor boxes in section 2. bbox_for_sampled_roi = bboxes_resized [ gt_assignment [ keep_index ]] print ( bbox_for_sampled_roi . shape ) height = sample_roi [:, 2 ] - sample_roi [:, 0 ] width = sample_roi [:, 3 ] - sample_roi [:, 1 ] ctr_y = sample_roi [:, 0 ] + 0.5 * height ctr_x = sample_roi [:, 1 ] + 0.5 * width base_height = bbox_for_sampled_roi [:, 2 ] - bbox_for_sampled_roi [:, 0 ] base_width = bbox_for_sampled_roi [:, 3 ] - bbox_for_sampled_roi [:, 1 ] base_ctr_y = bbox_for_sampled_roi [:, 0 ] + 0.5 * base_height base_ctr_x = bbox_for_sampled_roi [:, 1 ] + 0.5 * base_width (128, 4) (128, 4) eps = np . finfo ( height . dtype ) . eps height = np . maximum ( height , eps ) width = np . maximum ( width , eps ) dy = ( base_ctr_y - ctr_y ) / height dx = ( base_ctr_x - ctr_x ) / width dh = np . log ( base_height / height ) dw = np . log ( base_width / width ) gt_roi_locs = np . vstack (( dy , dx , dh , dw )) . transpose () print ( gt_roi_locs . shape ) (128, 4)","title":"Sample 128 From NMS's 2000 ROIs"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/object_detection/faster_rcnn/#128-roi-samples-features-and-pool-them","text":"rois = torch . from_numpy ( sample_roi ) . float () roi_indices = 0 * np . ones (( len ( rois ),), dtype = np . int32 ) roi_indices = torch . from_numpy ( roi_indices ) . float () print ( rois . shape , roi_indices . shape ) indices_and_rois = torch . cat ([ roi_indices [:, None ], rois ], dim = 1 ) xy_indices_and_rois = indices_and_rois [:, [ 0 , 2 , 1 , 4 , 3 ]] indices_and_rois = xy_indices_and_rois . contiguous () print ( xy_indices_and_rois . shape ) torch.Size([128, 4]) torch.Size([128]) torch.Size([128, 5]) size = ( 7 , 7 ) adaptive_max_pool = nn . AdaptiveMaxPool2d ( size [ 0 ], size [ 1 ]) output = [] rois = indices_and_rois . data . float () rois [:, 1 :] . mul_ ( 1 / 16.0 ) # Subsampling ratio rois = rois . long () num_rois = rois . size ( 0 ) for i in range ( num_rois ): roi = rois [ i ] im_idx = roi [ 0 ] im = out_map . narrow ( 0 , im_idx , 1 )[ ... , roi [ 2 ]:( roi [ 4 ] + 1 ), roi [ 1 ]:( roi [ 3 ] + 1 )] tmp = adaptive_max_pool ( im ) output . append ( tmp [ 0 ]) output = torch . cat ( output , 0 ) print ( output . size ()); torch.Size([128, 512, 7, 7]) # Visualize the first 5 ROI's feature map (for each feature map, only show the 1st channel of d=512) fig = plt . figure ( figsize = ( 12 , 4 )) figNo = 1 for i in range ( 5 ): roi = rois [ i ] im_idx = roi [ 0 ] im = out_map . narrow ( 0 , im_idx , 1 )[ ... , roi [ 2 ]:( roi [ 4 ] + 1 ), roi [ 1 ]:( roi [ 3 ] + 1 )] tmp = im [ 0 ][ 0 ] . detach () . cpu () . numpy () fig . add_subplot ( 1 , 5 , figNo ) plt . imshow ( tmp ) figNo += 1 plt . show (); # Visualize the first 5 ROI's feature maps after ROI pooling (for each feature map, only show the 1st channel of d=512) fig = plt . figure ( figsize = ( 12 , 4 )) figNo = 1 for i in range ( 5 ): roi = rois [ i ] im_idx = roi [ 0 ] im = out_map . narrow ( 0 , im_idx , 1 )[ ... , roi [ 2 ]:( roi [ 4 ] + 1 ), roi [ 1 ]:( roi [ 3 ] + 1 )] tmp = adaptive_max_pool ( im )[ 0 ] tmp = tmp [ 0 ][ 0 ] . detach () . cpu () . numpy () fig . add_subplot ( 1 , 5 , figNo ) plt . imshow ( tmp ) figNo += 1 plt . show (); # Reshape the tensor so that we can pass it through the feed forward layer. k = output . view ( output . size ( 0 ), - 1 ) print ( k . shape ) # 25088 = 7*7*512 torch.Size([128, 25088])","title":"128 ROI Samples' Features and Pool them"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/object_detection/faster_rcnn/#pytorch-pre-trained-model","text":"model = torchvision . models . detection . fasterrcnn_resnet50_fpn ( pretrained = True ) model FasterRCNN( (transform): GeneralizedRCNNTransform( Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) Resize(min_size=(800,), max_size=1333, mode='bilinear') ) (backbone): BackboneWithFPN( (body): IntermediateLayerGetter( (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): FrozenBatchNorm2d(64, eps=0.0) (relu): ReLU(inplace=True) (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (layer1): Sequential( (0): Bottleneck( (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): FrozenBatchNorm2d(64, eps=0.0) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): FrozenBatchNorm2d(64, eps=0.0) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): FrozenBatchNorm2d(256, eps=0.0) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): FrozenBatchNorm2d(256, eps=0.0) ) ) (1): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): FrozenBatchNorm2d(64, eps=0.0) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): FrozenBatchNorm2d(64, eps=0.0) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): FrozenBatchNorm2d(256, eps=0.0) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): FrozenBatchNorm2d(64, eps=0.0) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): FrozenBatchNorm2d(64, eps=0.0) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): FrozenBatchNorm2d(256, eps=0.0) (relu): ReLU(inplace=True) ) ) (layer2): Sequential( (0): Bottleneck( (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): FrozenBatchNorm2d(128, eps=0.0) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): FrozenBatchNorm2d(128, eps=0.0) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): FrozenBatchNorm2d(512, eps=0.0) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): FrozenBatchNorm2d(512, eps=0.0) ) ) (1): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): FrozenBatchNorm2d(128, eps=0.0) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): FrozenBatchNorm2d(128, eps=0.0) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): FrozenBatchNorm2d(512, eps=0.0) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): FrozenBatchNorm2d(128, eps=0.0) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): FrozenBatchNorm2d(128, eps=0.0) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): FrozenBatchNorm2d(512, eps=0.0) (relu): ReLU(inplace=True) ) (3): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): FrozenBatchNorm2d(128, eps=0.0) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): FrozenBatchNorm2d(128, eps=0.0) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): FrozenBatchNorm2d(512, eps=0.0) (relu): ReLU(inplace=True) ) ) (layer3): Sequential( (0): Bottleneck( (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): FrozenBatchNorm2d(256, eps=0.0) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): FrozenBatchNorm2d(256, eps=0.0) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): FrozenBatchNorm2d(1024, eps=0.0) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): FrozenBatchNorm2d(1024, eps=0.0) ) ) (1): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): FrozenBatchNorm2d(256, eps=0.0) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): FrozenBatchNorm2d(256, eps=0.0) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): FrozenBatchNorm2d(1024, eps=0.0) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): FrozenBatchNorm2d(256, eps=0.0) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): FrozenBatchNorm2d(256, eps=0.0) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): FrozenBatchNorm2d(1024, eps=0.0) (relu): ReLU(inplace=True) ) (3): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): FrozenBatchNorm2d(256, eps=0.0) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): FrozenBatchNorm2d(256, eps=0.0) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): FrozenBatchNorm2d(1024, eps=0.0) (relu): ReLU(inplace=True) ) (4): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): FrozenBatchNorm2d(256, eps=0.0) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): FrozenBatchNorm2d(256, eps=0.0) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): FrozenBatchNorm2d(1024, eps=0.0) (relu): ReLU(inplace=True) ) (5): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): FrozenBatchNorm2d(256, eps=0.0) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): FrozenBatchNorm2d(256, eps=0.0) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): FrozenBatchNorm2d(1024, eps=0.0) (relu): ReLU(inplace=True) ) ) (layer4): Sequential( (0): Bottleneck( (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): FrozenBatchNorm2d(512, eps=0.0) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): FrozenBatchNorm2d(512, eps=0.0) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): FrozenBatchNorm2d(2048, eps=0.0) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): FrozenBatchNorm2d(2048, eps=0.0) ) ) (1): Bottleneck( (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): FrozenBatchNorm2d(512, eps=0.0) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): FrozenBatchNorm2d(512, eps=0.0) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): FrozenBatchNorm2d(2048, eps=0.0) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): FrozenBatchNorm2d(512, eps=0.0) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): FrozenBatchNorm2d(512, eps=0.0) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): FrozenBatchNorm2d(2048, eps=0.0) (relu): ReLU(inplace=True) ) ) ) (fpn): FeaturePyramidNetwork( (inner_blocks): ModuleList( (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1)) (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1)) (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1)) (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1)) ) (layer_blocks): ModuleList( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) ) (extra_blocks): LastLevelMaxPool() ) ) (rpn): RegionProposalNetwork( (anchor_generator): AnchorGenerator() (head): RPNHead( (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1)) (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1)) ) ) (roi_heads): RoIHeads( (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2) (box_head): TwoMLPHead( (fc6): Linear(in_features=12544, out_features=1024, bias=True) (fc7): Linear(in_features=1024, out_features=1024, bias=True) ) (box_predictor): FastRCNNPredictor( (cls_score): Linear(in_features=1024, out_features=91, bias=True) (bbox_pred): Linear(in_features=1024, out_features=364, bias=True) ) ) ) # For training images , boxes = torch . rand ( 4 , 3 , 600 , 1200 ), torch . rand ( 4 , 11 , 4 ) boxes [:, :, 2 : 4 ] = boxes [:, :, 0 : 2 ] + boxes [:, :, 2 : 4 ] labels = torch . randint ( 1 , 91 , ( 4 , 11 )) images = list ( image for image in images ) print ( labels . shape ) torch.Size([4, 11]) >>> targets = [] >>> for i in range ( len ( images )): >>> d = {} >>> d [ 'boxes' ] = boxes [ i ] >>> d [ 'labels' ] = labels [ i ] >>> targets . append ( d ) >>> output = model ( images , targets ) >>> # For inference >>> model . eval () >>> x = [ torch . rand ( 3 , 300 , 400 ), torch . rand ( 3 , 500 , 400 )] >>> predictions = model ( x )","title":"PyTorch Pre-trained model"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/object_detection/non_maximum_suppression/","text":"The NMS Algorithm Intuition See diagram below of a human and dog. There should be 2 gt bbox but model not smart predicted 4 bboxes, 2 around human 2 around dog. Let's say bbox A has the highest confidence score amongst the 4. We pick A; We compare IOU of A with B, C and D and get say 0.9, 0 and 0.1 respectively. Set a threshold of \\(t = 0.5\\) for example. We discard all bboxes with IOU more than \\(t\\) , which in this case B is discarded immediately. At this point you should realize that for the human object we really want 1 bbox only, so B being removed is a result of the algorithm detecting that B is too close to A and hence it is related to the human bbox. Next, we get the highest confidence score in C, D, let's say it is D. Here is a repeat of step 1 already. We compare IOU of D and C and find them to be 0.6, we discard it too. Now no more bbox left. We are left with A and D, as desired. Implementation From https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch/#:~:text=Non%20Maximum%20Suppression%20(NMS)%20is,arrive%20at%20the%20desired%20results. Let us get to the nitty-gritty of this post, the actual algorithm. I will divide this into three parts, what we need as input, what we get after applying the algorithm and the actual algorithm itself. Input We get a list P of prediction BBoxes of the form (x1,y1,x2,y2,c), where (x1,y1) and (x2,y2) are the ends of the BBox and c is the predicted confidence score of the model. We also get overlap threshold IoU thresh_iou. Output We return a list keep of filtered prediction BBoxes. Algorithm Step 1 : Select the prediction S with highest confidence score and remove it from P and add it to the final prediction list keep. (keep is empty initially). Step 2 : Now compare this prediction S with all the predictions present in P. Calculate the IoU of this prediction S with every other predictions in P. If the IoU is greater than the threshold thresh_iou for any prediction T present in P, remove prediction T from P. Step 3 : If there are still predictions left in P, then go to Step 1 again, else return the list keep containing the filtered predictions. In layman terms, we select the predictions with the maximum confidence and suppress all the other predictions having overlap with the selected predictions greater than a threshold. In other words, we take the maximum and suppress the non-maximum ones, hence the name non-maximum suppression. If you observe the algorithm above, the whole filtering process depends on a single threshold value thresh_iou. So selection of threshold value is vital for the performance of the model. Usually, we take its value as 0.5, but it depends on the experiment you are doing.As discussed in the NMS algorithm above, we extract the BBox of highest confidence score and remove it from P. Now that we have a good grasp of how NMS works, let us implement it in PyTorch so that you can use it in your future Object Detection pipelines \ud83d\ude42 https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch/#:~:text=Non%20Maximum%20Suppression%20(NMS)%20is,arrive%20at%20the%20desired%20results. https://www.analyticsvidhya.com/blog/2020/08/selecting-the-right-bounding-box-using-non-max-suppression-with-implementation/","title":"Non maximum suppression"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/object_detection/non_maximum_suppression/#the-nms-algorithm","text":"","title":"The NMS Algorithm"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/object_detection/non_maximum_suppression/#intuition","text":"See diagram below of a human and dog. There should be 2 gt bbox but model not smart predicted 4 bboxes, 2 around human 2 around dog. Let's say bbox A has the highest confidence score amongst the 4. We pick A; We compare IOU of A with B, C and D and get say 0.9, 0 and 0.1 respectively. Set a threshold of \\(t = 0.5\\) for example. We discard all bboxes with IOU more than \\(t\\) , which in this case B is discarded immediately. At this point you should realize that for the human object we really want 1 bbox only, so B being removed is a result of the algorithm detecting that B is too close to A and hence it is related to the human bbox. Next, we get the highest confidence score in C, D, let's say it is D. Here is a repeat of step 1 already. We compare IOU of D and C and find them to be 0.6, we discard it too. Now no more bbox left. We are left with A and D, as desired.","title":"Intuition"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/object_detection/non_maximum_suppression/#implementation","text":"From https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch/#:~:text=Non%20Maximum%20Suppression%20(NMS)%20is,arrive%20at%20the%20desired%20results. Let us get to the nitty-gritty of this post, the actual algorithm. I will divide this into three parts, what we need as input, what we get after applying the algorithm and the actual algorithm itself. Input We get a list P of prediction BBoxes of the form (x1,y1,x2,y2,c), where (x1,y1) and (x2,y2) are the ends of the BBox and c is the predicted confidence score of the model. We also get overlap threshold IoU thresh_iou. Output We return a list keep of filtered prediction BBoxes. Algorithm Step 1 : Select the prediction S with highest confidence score and remove it from P and add it to the final prediction list keep. (keep is empty initially). Step 2 : Now compare this prediction S with all the predictions present in P. Calculate the IoU of this prediction S with every other predictions in P. If the IoU is greater than the threshold thresh_iou for any prediction T present in P, remove prediction T from P. Step 3 : If there are still predictions left in P, then go to Step 1 again, else return the list keep containing the filtered predictions. In layman terms, we select the predictions with the maximum confidence and suppress all the other predictions having overlap with the selected predictions greater than a threshold. In other words, we take the maximum and suppress the non-maximum ones, hence the name non-maximum suppression. If you observe the algorithm above, the whole filtering process depends on a single threshold value thresh_iou. So selection of threshold value is vital for the performance of the model. Usually, we take its value as 0.5, but it depends on the experiment you are doing.As discussed in the NMS algorithm above, we extract the BBox of highest confidence score and remove it from P. Now that we have a good grasp of how NMS works, let us implement it in PyTorch so that you can use it in your future Object Detection pipelines \ud83d\ude42 https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch/#:~:text=Non%20Maximum%20Suppression%20(NMS)%20is,arrive%20at%20the%20desired%20results. https://www.analyticsvidhya.com/blog/2020/08/selecting-the-right-bounding-box-using-non-max-suppression-with-implementation/","title":"Implementation"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/object_detection/yolo/","text":"Exercises 1 Introduce YOLO and explain why it is fast? YOLO It is the pioneering work of single-stage detection algorithm, it is built upon a CNN backbone and an input image can be passed in its entirety and the input image will be classified and regressed , returning the results in one pass. Although in YOLOv1 I believe the loss function is considered a regression problem. Based on the concept of a single-stage predictor, it is much faster than RCNN families whom need two stage. The core idea of is to pass the input image through backbone and after feature extraction, the characteristic graph of is divided into \\(S \\times S\\) and the grid in which the center of the object fall in will be responsible for predicting the confidence of the object, class label and bounding box coordinates. How would you deploy YOLO? (Rephrase): At present, most deep learning algorithm models require high computational power to be implemented and if it's on the server and have access to GPU Accelerate. But at the edge or on a development board where computing power is scarce, The model has to be further compressed or improved; you can also use the existing reasoning optimization acceleration framework on the market for specific scenarios. At present, several common deployment schemes are \uff1a nvidia GPU\uff1apytorch->onnx->TensorRT intel CPU\uff1a pytorch->onnx->openvino Mobile \uff08 mobile phone \u3001 Development board, etc \uff09\uff1apytorch->onnx->MNN\u3001NCNN\u3001TNN\u3001TF-lite\u3001Paddle-lite\u3001RKNN etc. https://chowdera.com/2021/12/20211207053926327c.html \u21a9","title":"Yolo"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/object_detection/yolo/#exercises-1","text":"","title":"Exercises 1"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/object_detection/yolo/#introduce-yolo-and-explain-why-it-is-fast","text":"YOLO It is the pioneering work of single-stage detection algorithm, it is built upon a CNN backbone and an input image can be passed in its entirety and the input image will be classified and regressed , returning the results in one pass. Although in YOLOv1 I believe the loss function is considered a regression problem. Based on the concept of a single-stage predictor, it is much faster than RCNN families whom need two stage. The core idea of is to pass the input image through backbone and after feature extraction, the characteristic graph of is divided into \\(S \\times S\\) and the grid in which the center of the object fall in will be responsible for predicting the confidence of the object, class label and bounding box coordinates.","title":"Introduce YOLO and explain why it is fast?"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/object_detection/yolo/#how-would-you-deploy-yolo","text":"(Rephrase): At present, most deep learning algorithm models require high computational power to be implemented and if it's on the server and have access to GPU Accelerate. But at the edge or on a development board where computing power is scarce, The model has to be further compressed or improved; you can also use the existing reasoning optimization acceleration framework on the market for specific scenarios. At present, several common deployment schemes are \uff1a nvidia GPU\uff1apytorch->onnx->TensorRT intel CPU\uff1a pytorch->onnx->openvino Mobile \uff08 mobile phone \u3001 Development board, etc \uff09\uff1apytorch->onnx->MNN\u3001NCNN\u3001TNN\u3001TF-lite\u3001Paddle-lite\u3001RKNN etc. https://chowdera.com/2021/12/20211207053926327c.html \u21a9","title":"How would you deploy YOLO?"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/opencv/Interpolation/","text":"Interpolation 1 Read this for examples to visualize. cv2.INTER_AREA (ZOOM) We use cv2.INTER_AREA to resize, when we are resizing a smaller image into a bigger image then it is the zoom method which in their documentation says it is a nearest neighbour method. A simple example below reveals how it works. import numpy as np import cv2 img = np . array ( [ [ 86 , 83 , 101 , 142 ], [ 162 , 103 , 144 , 151 ], [ 125 , 154 , 189 , 67 ], [ 138 , 116 , 124 , 43 ], ], dtype = np . uint8 , ) enlarged = cv2 . resize ( img , ( 8 , 8 ), interpolation = cv2 . INTER_AREA ) print ( enlarged ) [[ 86 86 83 83 101 101 142 142] [ 86 86 83 83 101 101 142 142] [162 162 103 103 144 144 151 151] [162 162 103 103 144 144 151 151] [125 125 154 154 189 189 67 67] [125 125 154 154 189 189 67 67] [138 138 116 116 124 124 43 43] [138 138 116 116 124 124 43 43]] https://zhuanlan.zhihu.com/p/38493205 \u21a9","title":"Interpolation"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/opencv/Interpolation/#interpolation-1","text":"Read this for examples to visualize.","title":"Interpolation 1"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/computer_vision/opencv/Interpolation/#cv2inter_area-zoom","text":"We use cv2.INTER_AREA to resize, when we are resizing a smaller image into a bigger image then it is the zoom method which in their documentation says it is a nearest neighbour method. A simple example below reveals how it works. import numpy as np import cv2 img = np . array ( [ [ 86 , 83 , 101 , 142 ], [ 162 , 103 , 144 , 151 ], [ 125 , 154 , 189 , 67 ], [ 138 , 116 , 124 , 43 ], ], dtype = np . uint8 , ) enlarged = cv2 . resize ( img , ( 8 , 8 ), interpolation = cv2 . INTER_AREA ) print ( enlarged ) [[ 86 86 83 83 101 101 142 142] [ 86 86 83 83 101 101 142 142] [162 162 103 103 144 144 151 151] [162 162 103 103 144 144 151 151] [125 125 154 154 189 189 67 67] [125 125 154 154 189 189 67 67] [138 138 116 116 124 124 43 43] [138 138 116 116 124 124 43 43]] https://zhuanlan.zhihu.com/p/38493205 \u21a9","title":"cv2.INTER_AREA (ZOOM)"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/ensemble_theory/forward_ensemble/","text":"import pandas as pd import numpy as np import os import matplotlib.pyplot as plt from typing import List , Tuple , Callable from sklearn.metrics import roc_auc_score Convert OOFs and Ground Truth to NumPy First, we define a function return_list_of_dataframes to return either the OOFs or the SUBs. At this point, one should be clear what are OOFs, if not, please read the post here . def return_list_of_dataframes ( path : str , is_oof : bool = True ) -> Tuple [ List [ pd . DataFrame ], int ]: \"\"\"Return a list of dataframes from a directory of files. The boolean is_oof is used to determine whether the list of dataframes contains oof or subs. Args: path (str): The path to the directory containing the files. is_oof (bool, optional): Determine whether the list of dataframes contains oof or subs. Defaults to True. Returns: List[pd.DataFrame]: The list of dataframes for either oof or subs. int: The number of files in the directory. \"\"\" oof_and_subs_files = os . listdir ( path ) if is_oof : oof_files_sorted = np . sort ( [ f for f in oof_and_subs_files if \"oof\" in f ] ) return [ pd . read_csv ( os . path . join ( path , k )) for k in oof_files_sorted ], len ( oof_files_sorted ) else : sub_files_sorted = np . sort ( [ f for f in oof_and_subs_files if \"sub\" in f ] ) return [ pd . read_csv ( os . path . join ( path , k )) for k in sub_files_sorted ], len ( sub_files_sorted ) This function is first applied to is_oof=True . For now, we just want all our OOFs converted to a pandas dataframe, and stored in a list. Note, we also conveniently returned the number of files in the director for OOFs and SUBs respectively, it should be clear in this context that the number of files for OOF is the same as the number of files for SUB. oof_and_subs_path = \"./oof_and_subs/toy_examples\" oof_dfs_list , num_oofs = return_list_of_dataframes ( path = oof_and_subs_path , is_oof = True ) display ( oof_dfs_list [ 0 ]) display ( oof_dfs_list [ 1 ]) display ( oof_dfs_list [ 2 ]) print ( f \"We have { num_oofs } oof files. \\n \" ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } image_id y_trues class_0_oof class_1_oof 0 1 1 0.70 0.30 1 2 0 0.70 0.30 2 3 0 0.65 0.35 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } image_id y_trues class_0_oof class_1_oof 0 1 1 0.6 0.4 1 2 0 0.8 0.2 2 3 0 0.4 0.6 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } image_id y_trues class_0_oof class_1_oof 0 1 1 0.1 0.8 1 2 0 0.4 0.8 2 3 0 0.9 0.7 We have 3 oof files. At this junction, we need to be clear of a few things. Let us use an example to illustrate. Let us say we trained our model over 5 folds and get our OOF predictions in terms of the Macro-AUROC with respect to the positive class 1 . We then need to calculate our OOF scores with our corresponding y_trues. Just by eyeballing, we can deduce the Macro-AUROC score for the positive class as follows: oof_1_auroc = roc_auc_score ([ 1 , 0 , 0 ], [ 0.3 , 0.3 , 0.35 ]) -> 0.25 oof_2_auroc = roc_auc_score ([ 1 , 0 , 0 ], [ 0.4 , 0.2 , 0.6 ]) -> 0.5 oof_3_auroc = roc_auc_score ([ 1 , 0 , 0 ], [ 0.8 , 0.8 , 0.7 ]) -> 0.75 To compute the above, we can manually hardcode them, but for larger data, we will have to maintain a better data structure. We will store them into matrices and vectors (numpy) to compute the OOF scores efficiently. To do so, we define two variables: ground_truth_column_name = [ \"y_trues\" ] positive_class_oof_column_name = [ \"class_1_oof\" ] where the first variable is the name of the column(s) for the ground truth, while the second is the name of the column(s) that we will be using to compute the OOF scores. We will then create a function stack_oofs to convert the list of OOF dataframes into an array. def stack_oofs ( oof_dfs : List [ pd . DataFrame ], pred_column_names : List [ str ] ) -> np . ndarray : \"\"\"Stack all oof predictions horziontally. Args: oof_dfs (List[pd.DataFrame]): The list of oof predictions in dataframes. pred_column_names (List[str]): The list of prediction column names. Returns: all_oof_preds (np.ndarray): The stacked oof predictions of shape (num_samples, num_oofs * num_pred_columns). Example: >>> oof_1 = pd.DataFrame([1,2,3], columns=['class_1_oof']) >>> oof_2 = pd.DataFrame([4,5,6], columns=['class_1_oof']) >>> all_oof_preds = stack_oofs([oof_1, oof_2], ['class_1_oof']) >>> all_oof_preds = np.array([1, 4], [2, 5], [3, 6]) \"\"\" num_oofs = len ( oof_dfs ) num_samples = len ( oof_dfs [ 0 ]) num_target_cols = len ( pred_column_names ) all_oof_preds = np . zeros (( num_samples , num_oofs * num_target_cols )) if num_target_cols == 1 : for index , oof_df in enumerate ( oof_dfs ): all_oof_preds [:, index : index + 1 ] = oof_df [ pred_column_names ] . values elif num_target_cols > 1 : # Used in RANZCR where there are 11 target columns for index , oof_df in enumerate ( oof_dfs ): all_oof_preds [ :, index * num_target_cols : ( index + 1 ) * num_target_cols ] = oof_df [ pred_column_names ] . values return all_oof_preds ground_truth_column_name = [ \"y_trues\" ] positive_class_oof_column_name = [ \"class_1_oof\" ] In my oof files, I also saved the corresponding y_trues as a column, we thus take the first oof_df from oof_dfs_list and use it to get the y_trues , assuming they are the same for all oof files. Note of caution, if you use different resampling methods, you will need to change the y_trues accordingly. y_trues = oof_dfs_list [ 0 ][ ground_truth_column_name ] . values all_oof_preds = stack_oofs ( oof_dfs_list , positive_class_oof_column_name ) print ( f \"y_trues shape: { y_trues . shape } \\n This variable is global and holds all ground truth. \\n \" ) print ( f \"all_oof_preds shape: { all_oof_preds . shape } \\n This variable is global and holds all oof predictions stacked horizontally. \\n \" ) y_trues shape: (3, 1) This variable is global and holds all ground truth. all_oof_preds shape: (3, 3) This variable is global and holds all oof predictions stacked horizontally. After converting to numpy, we should have this following representation: \\[ \\textbf{y_true} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} \\in \\mathbb{R}^3 \\quad \\textbf{all_oof_preds} = \\begin{bmatrix} 0.3 & 0.4 & 0.8 \\\\ 0.3 & 0.2 & 0.8 \\\\ 0.35 & 0.6 & 0.7 \\end{bmatrix} \\in \\mathbb{R}^{3 \\times 3} \\] where y_true is a \\(3 \\times 1\\) column vector, each row entry is the corresponding ground truth for sample \\(i\\) (i.e y_true[0] is ground truth for sample 1). For all_oof_preds , this is a \\(3 \\times 3\\) matrix, where each column \\(i\\) represents the OOF predictions made by model \\(i\\) . In other words, the first column \\(\\begin{bmatrix} 0.3 \\\\ 0.35 \\\\ 0.3 \\end{bmatrix}\\) is the OOF predictions made by the first model. It is important to note that we are dealing with classification (binary or multiclass), so it is usually the case that both our target and predicted columns are just \\(1\\) . In multi-label however, we will have multiple target columns, and will be the focus in part II. Compute Scores of OOFs and Find the Best Score This part is very crucial, when we apply our Hill Climbing (Forward Ensembling) technique here, we want to initialize with the best models first and iteratively blend with the rest . In other words def compute_best_oof ( all_oof_preds : np . ndarray , y_trues : np . ndarray , num_oofs : int , performance_metric : Callable , ) -> Tuple [ float , int ]: \"\"\"Compute the oof score of all models using a performance metric and return the best model index and score. Args: all_oof_preds (np.ndarray): The stacked oof predictions of shape (num_samples, num_oofs * num_pred_columns). Taken from stack_oofs. y_trues (np.ndarray): The true labels of shape (num_samples, num_target_cols). num_oofs (int): The number of oof predictions. performance_metric (Callable): The performance metric to use, this is a function. Returns: best_oof_metric_score (float): The best oof score. best_model_index (int): The index of the best model. \"\"\" all_oof_scores = [] for k in range ( num_oofs ): oof_k = all_oof_preds [:, k ] . reshape ( - 1 , 1 ) metric_score = performance_metric ( y_trues , oof_k , num_target_cols = 1 , multilabel = False , ) all_oof_scores . append ( metric_score ) print ( f \"Model { k } has OOF AUC = { metric_score } \" ) best_oof_metric_score , best_oof_index = np . max ( all_oof_scores ), np . argmax ( all_oof_scores ) return best_oof_metric_score , best_oof_index def macro_multilabel_auc ( label , pred , num_target_cols : int = 1 , multilabel : bool = False ): \"\"\"Also works for binary AUC like Melanoma\"\"\" if not multilabel : return roc_auc_score ( label , pred ) else : aucs = [] for i in range ( num_target_cols ): print ( label [:, i ]) print () print ( pred [:, i ]) print ( roc_auc_score ( label [:, i ], pred [:, i ])) aucs . append ( roc_auc_score ( label , pred )) return np . mean ( aucs ) best_oof_metric_score , best_oof_index = compute_best_oof ( all_oof_preds = all_oof_preds , y_trues = y_trues , num_oofs = num_oofs , performance_metric = macro_multilabel_auc , ) print ( f \" \\n ### Computing Best OOF scores among all models ### \\n The best OOF AUC score is { best_oof_metric_score } and the best model index is { best_oof_index } corresponding to the oof file { best_oof_index } \" ) Model 0 has OOF AUC = 0.25 Model 1 has OOF AUC = 0.5 Model 2 has OOF AUC = 0.75 ### Computing Best OOF scores among all models ### The best OOF AUC score is 0.75 and the best model index is 2 corresponding to the oof file 2 The Algorithm Now the logic flows as follows: Start with the model with the best OOF score: we get that from compute_best_oof which yields us the key variables best_oof_index . best_oof_index : 2 indicates that our oof_2 (model 2) has the best OOF score amongst the 3 OOFs (models), as we can see from above, it is indeed the case since Model 2 has OOF AUC of \\(0.75\\) . We now need to blend Model 2 with Model 0 and Model 1 respectively to find out which weight combination gives a better OOF score. We need to define a variable weight_interval , which tells us the weights to sample from. For example, if weight_interval is \\(3\\) , then we will uniformly sample the weights \\(\\frac{0}{3}, \\frac{1}{3}, \\frac{2}{3}\\) . Let us see a concrete example. weight_interval = 3 patience_counter = 0 model_0_oof = all_oof_preds [:, 0 ] . reshape ( y_trues . shape ) model_1_oof = all_oof_preds [:, 1 ] . reshape ( y_trues . shape ) model_2_oof = all_oof_preds [:, best_oof_index ] . reshape ( y_trues . shape ) # best_oof_index=2 We define best_oof_index_list = [ best_oof_index ] best_weights_list = [] which keeps track two lists which holds the best OOFs to blend with and their corresponding weights. We use a naive example to illustrate: best_oof_index_list = [ 2 , 0 , 1 ] best_weights_list = [ 0.3 , 0.8 ] all_oof_preds_matrix = [[ 0.3 , 0.4 , 0.8 ], [ 0.3 , 0.2 , 0.8 ], [ 0.35 , 0.6 , 0.7 ]] initial_best_vector = A_matrix [:, 2 ] In other words, if after our Hill Climbing algorithm, we have the above variables and to calculate the final blended OOF using the above optimal weights, we have: Let \\(\\mathbf{c_i}\\) be the columns of the matrix all_oof_preds_matrix , you can think \\(\\mathbf{c_i}\\) as the Model \\(i\\) 's OOFs predictions, since in this naive example, we see that our initial best model is Model 2. \\[ ((1-0.3) \\times \\mathbf{c_2} + 0.3 \\times \\mathbf{c_0}) \\times (1-0.8) + 0.8 \\times \\mathbf{c_1} \\] To dissect clearly, we start with the first index, which is Model 2, and blend with the next index, which is Model 0, they will be blended with weights of \\(0.3\\) , which means \\((1-0.3) \\times \\mathbf{c_2} + 0.3 \\times \\mathbf{c_0}\\) . We call it oof_blend_1 which indicates the linear combination of the weights of the first blend. Note that this results in a new and better \"OOF predictions\". The next blend will be of oof_blend_1 with Model 1, with a weight of \\(0.8\\) and the assignment is \\((1-0.8) \\times \\textbf{oof_blend_1} + 0.8 \\times \\mathbf{c_0}\\) . best_oof_index_list = [ best_oof_index ] best_weights_list = [] print ( f \"Current Tracked Model List: { best_oof_index_list } \" ) print ( f \"Current Weights List: { best_weights_list } \" ) Current Tracked Model List: [2] Current Weights List: [] First Round - Blending Model 2 with Model 0 We check the blending results of Model 2 and Model 0 over 3 weights: \\(0, \\frac{1}{3}, \\frac{2}{3}\\) . We also assign 3 variables running_best_score , running_best_weight , running_best_oof_index = best_oof_metric_score , 0 , 0 such that whenever a blend of \\(w_1 \\times \\textbf{model_i_oof} + (1-w_2) \\times \\textbf{model_j_oof}\\) produces a better OOF score, then we will assign if temp_ensemble_oof_score >= running_best_score : print ( f \"The blend of weight { temp_weight } of model 2 and model 0 led to a greater or equals to OOF score = { temp_ensemble_oof_score } \\n \" ) running_best_score = temp_ensemble_oof_score running_best_weight = temp_weight running_best_oof_index = 0 Notice that we hardcoded the running_best_oof_index to be \\(0\\) since we know we are only looking at the interaction of Model 2 and Model 0. In proper code, this part should not be hardcoded and you can refer to my full code for clarity. running_best_score , running_best_weight , running_best_oof_index = best_oof_metric_score , 0 , 0 for weight in range ( weight_interval ): temp_weight = weight / weight_interval print ( f \"weight = { temp_weight } \" ) temp_ensemble_oof_preds = ( temp_weight * model_0_oof + ( 1 - temp_weight ) * model_2_oof ) temp_ensemble_oof_score = macro_multilabel_auc ( y_trues , temp_ensemble_oof_preds , num_target_cols = 1 , multilabel = False , ) print ( f \"blended OOF score with model_0_oof = { temp_ensemble_oof_score } \" ) if temp_ensemble_oof_score >= running_best_score : running_best_score = temp_ensemble_oof_score running_best_weight = temp_weight running_best_oof_index = 0 print ( f \"The blend of weight { temp_weight } of model 2 and model { running_best_oof_index } led to a greater or equals to OOF score = { temp_ensemble_oof_score } \\n \" ) weight = 0.0 blended OOF score with model_0_oof = 0.75 The blend of weight 0.0 of model 2 and model 0 led to a greater or equals to OOF score = 0.75 weight = 0.3333333333333333 blended OOF score with model_0_oof = 0.75 The blend of weight 0.3333333333333333 of model 2 and model 0 led to a greater or equals to OOF score = 0.75 weight = 0.6666666666666666 blended OOF score with model_0_oof = 0.5 So now we finished blending Model 2 and Model 0 and we have: running_best_score = 0.75 running_best_weight = 0.333 ... We need to blend Model 2 and Model 1 now to see if Model 2 and Model 1 can give better OOF scores when blended! We repeat the exact same loop as above, but bear in mind that the running_best_score and running_best_weight is already updated to the ones we got in the blend of Model 2 and Model 0 because we want to check if Model 2 and Model 1's blend can give better results than the previous running_best_score . First Round - Blending Model 2 and Model 1 for weight in range ( weight_interval ): temp_weight = weight / weight_interval print ( f \"weight = { temp_weight } \" ) temp_ensemble_oof_preds = ( temp_weight * model_1_oof + ( 1 - temp_weight ) * model_2_oof ) temp_ensemble_oof_score = macro_multilabel_auc ( y_trues , temp_ensemble_oof_preds , num_target_cols = 1 , multilabel = False , ) print ( f \"blended OOF score with model_1_oof = { temp_ensemble_oof_score } \" ) if temp_ensemble_oof_score >= running_best_score : running_best_score = temp_ensemble_oof_score running_best_weight = temp_weight running_best_oof_index = 1 print ( f \"The blend of weight { temp_weight } of model 2 and model { running_best_oof_index } led to a greater or equals to OOF score = { temp_ensemble_oof_score } \\n \" ) weight = 0.0 blended OOF score with model_1_oof = 0.75 The blend of weight 0.0 of model 2 and model 1 led to a greater or equals to OOF score = 0.75 weight = 0.3333333333333333 blended OOF score with model_1_oof = 1.0 The blend of weight 0.3333333333333333 of model 2 and model 1 led to a greater or equals to OOF score = 1.0 weight = 0.6666666666666666 blended OOF score with model_1_oof = 0.5 So now we finished blending Model 2 and Model 1 and we have: running_best_score = 1 running_best_weight = 0.333 ... and we have a new winner in town! After our first round of iterating our initial best OOF Model 2 with the rest (Model 0 and 1), we found out that if we take \\(w_1 = 1 - \\frac{2}{3}\\) and \\(w_2 = \\frac{1}{3}\\) , Model 2 and 1 gives us a better overall score. That is to say: \\[ w_1 * \\textbf{OOF_2} + w_2 \\times \\textbf{OOF_1} \\] leads to the greatest increase in our OOF score! Notice that we hardcoded the running_best_oof_index to be \\(1\\) since we know we are only looking at the interaction of Model 2 and Model 1. In proper code, this part should not be hardcoded and you can refer to my full code for clarity. Technically, we can stop the algorithm now since the metric Macro-AUROC is capped at \\(1\\) , but for the sake of explanation, let us continue. First Round - Save Results for Loop 1 We then append the best OOF index and the corresponding weight to the best_oof_index_list and best_weights_list respectively. best_oof_index_list . append ( running_best_oof_index ) best_weights_list . append ( running_best_weight ) print ( f \"Current Tracked Model List: { best_oof_index_list } \" ) print ( f \"Current Weights List: { best_weights_list } \" ) Current Tracked Model List: [2, 1] Current Weights List: [0.3333333333333333] Second Round - Blend OOF Now we have a brand new OOF after blending Model 2 and Model 1, we call it blended_oof_1 and note that this is our new best OOF! # blended_oof_1 = all_oof_preds[:, best_oof_index_list[0]].reshape(-1, 1) * (1 - best_weights_list[0]) + all_oof_preds[:, best_oof_index_list[1]].reshape(-1, 1) * best_weights_list[0] blended_oof_1 = model_2_oof * ( 1 - 1 / 3 ) + model_1_oof * ( 1 / 3 ) assert macro_multilabel_auc ( y_trues , blended_oof_1 ) == 1 Second Round - Blending blended_oof_1 with Model 0 We continue to try out blended_oof_1 with the rest of the models that were not selected. This means we have to check our Current Tracked Model List best_oof_index_list and see that we already have \\([2, 1]\\) being used up, in our simple example here, there only left with Model 0 to try! So make sure in your code you do not try blended_oof_1 with Model 0, 1 and 2 again since the blended_oof_1 is already made up with Model 1 and 2! for weight in range ( weight_interval ): temp_weight = weight / weight_interval print ( f \" \\n weight = { temp_weight } \" ) temp_ensemble_oof_preds = ( temp_weight * model_0_oof + ( 1 - temp_weight ) * blended_oof_1 ) temp_ensemble_oof_score = macro_multilabel_auc ( y_trues , temp_ensemble_oof_preds , num_target_cols = 1 , multilabel = False , ) print ( f \"blended OOF score with model_0_oof = { temp_ensemble_oof_score } \" ) if temp_ensemble_oof_score >= running_best_score : running_best_score = temp_ensemble_oof_score running_best_weight = temp_weight running_best_oof_index = 0 print ( f \"The blend of weight { temp_weight } of model 2 and model { running_best_oof_index } led to a greater or equals to OOF score = { temp_ensemble_oof_score } \\n \" ) weight = 0.0 blended OOF score with model_0_oof = 1.0 The blend of weight 0.0 of model 2 and model 0 led to a greater or equals to OOF score = 1.0 weight = 0.3333333333333333 blended OOF score with model_0_oof = 0.5 weight = 0.6666666666666666 blended OOF score with model_0_oof = 0.5 Since we have done checking blended_oof_1 with the last remaining Model 0 and found that blending Model 0 with a weight of 0 (what a surprise haha!) yields the best result, we once again update the running metrics and also append to our global lists below. best_oof_index_list . append ( running_best_oof_index ) best_weights_list . append ( running_best_weight ) print ( f \"Current Tracked Model List: { best_oof_index_list } \" ) print ( f \"Current Weights List: { best_weights_list } \" ) Current Tracked Model List: [2, 1, 0] Current Weights List: [0.3333333333333333, 0.0] # blended_oof_2 = blended_oof_1 * (1 - best_weights_list[1]) + all_oof_preds[:, best_oof_index_list[2]].reshape(-1, 1) * best_weights_list[1] blended_oof_2 = blended_oof_1 * ( 1 - 0.0 ) + model_0_oof . reshape ( - 1 , 1 ) * 0.0 assert macro_multilabel_auc ( y_trues , blended_oof_2 ) == 1 Ensembling Model Predictions with the Found Optimal Weights So we end the discussion with what to do with the weights we got. We already established to the readers that our Initial Best OOF is Model 2 with a Macro-AUROC score of \\(0.75\\) , and by way of Hill Climbing, we found out that we can blend the 3 Models with some weights such that their new OOF produces a Macro-AUROC score of \\(1.0\\) , a huge improvement. We aren't done yet! We want to apply these optimal weights to our test set predictions as well. Note that our test set predictions are unseen and our usual ensemble methods can be as simple as mean averaging. More concretely, let us check out the example below. oof_and_subs_path = \"./oof_and_subs/toy_examples\" sub_dfs_list , num_subs = return_list_of_dataframes ( path = oof_and_subs_path , is_oof = False ) display ( sub_dfs_list [ 0 ]) display ( sub_dfs_list [ 1 ]) display ( sub_dfs_list [ 2 ]) print ( f \"We have { num_subs } sub files. \\n \" ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } image_id class_0_preds class_1_preds 0 4 0.2 0.8 1 5 0.3 0.7 2 6 0.6 0.4 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } image_id class_0_preds class_1_preds 0 4 0.3 0.7 1 5 0.9 0.1 2 6 0.2 0.8 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } image_id class_0_preds class_1_preds 0 4 0.3 0.7 1 5 0.7 0.3 2 6 0.8 0.2 We have 3 sub files. The stack_subs function does the same thing as stack_oofs so they can be combined into one function for code clarity. def stack_subs ( sub_dfs : List [ pd . DataFrame ], pred_column_names : List [ str ] ) -> np . ndarray : \"\"\"Stack all sub predictions horziontally. Args: sub_dfs (List[pd.DataFrame]): The list of sub predictions in dataframes. pred_column_names (List[str]): The list of prediction column names. Returns: all_sub_preds (np.ndarray): The stacked oof predictions of shape (num_samples, num_subs * num_pred_columns). \"\"\" num_subs = len ( sub_dfs ) num_samples = len ( sub_dfs [ 0 ]) num_target_cols = len ( pred_column_names ) all_sub_preds = np . zeros (( num_samples , num_subs * num_target_cols )) if num_target_cols == 1 : for index , sub_df in enumerate ( sub_dfs ): all_sub_preds [:, index : index + 1 ] = sub_df [ pred_column_names ] . values elif num_target_cols > 1 : # Used in RANZCR where there are 11 target columns for index , sub_df in enumerate ( sub_dfs ): all_sub_preds [ :, index * num_target_cols : ( index + 1 ) * num_target_cols ] = sub_df [ pred_column_names ] . values return all_sub_preds test_set_target_column_name = [ \"class_1_preds\" ] all_subs_preds = stack_subs ( sub_dfs_list , test_set_target_column_name ) model_0_sub = all_subs_preds [:, 0 ] . reshape ( y_trues . shape ) model_1_sub = all_subs_preds [:, 1 ] . reshape ( y_trues . shape ) model_2_sub = all_subs_preds [:, 2 ] . reshape ( y_trues . shape ) Recall the optimal weights earlier: Current Tracked Model List : [ 2 , 1 , 0 ] Current Weights List : [ 0.3333333333333333 , 0.0 ] and we now have a way to ensemble our model subs accordingly. # blended_oof_1 = all_oof_preds[:, best_oof_index_list[0]].reshape(-1, 1) * (1 - best_weights_list[0]) + all_oof_preds[:, best_oof_index_list[1]].reshape(-1, 1) * best_weights_list[0] blended_sub_1 = model_2_sub * ( 1 - 1 / 3 ) + model_1_sub * ( 1 / 3 ) blended_sub_2 = blended_sub_1 * ( 1 - 0.0 ) + model_0_sub * 0.0 blended_sub_2 array([[0.7 ], [0.23333333], [0.4 ]]) Then blended_sub_2 should be our final test set predictions ! Forward Ensemble with SIIM-ISIC Melanoma Classification This is taken from my repo. import pandas as pd import numpy as np import os import matplotlib.pyplot as plt from typing import List , Tuple , Callable from sklearn.metrics import roc_auc_score def stack_oofs ( oof_dfs : List [ pd . DataFrame ], pred_column_names : List [ str ] ) -> np . ndarray : \"\"\"Stack all oof predictions horziontally. Args: oof_dfs (List[pd.DataFrame]): The list of oof predictions in dataframes. pred_column_names (List[str]): The list of prediction column names. Returns: all_oof_preds (np.ndarray): The stacked oof predictions of shape (num_samples, num_oofs * num_pred_columns). Example: >>> oof_1 = pd.DataFrame([1,2,3], columns=['class_1_oof']) >>> oof_2 = pd.DataFrame([4,5,6], columns=['class_1_oof']) >>> all_oof_preds = stack_oofs([oof_1, oof_2], ['class_1_oof']) >>> all_oof_preds = np.array([1, 4], [2, 5], [3, 6]) \"\"\" num_oofs = len ( oof_dfs ) num_samples = len ( oof_dfs [ 0 ]) num_target_cols = len ( pred_column_names ) all_oof_preds = np . zeros (( num_samples , num_oofs * num_target_cols )) if num_target_cols == 1 : for index , oof_df in enumerate ( oof_dfs ): all_oof_preds [:, index : index + 1 ] = oof_df [ pred_column_names ] . values elif num_target_cols > 1 : # Used in RANZCR where there are 11 target columns for index , oof_df in enumerate ( oof_dfs ): all_oof_preds [ :, index * num_target_cols : ( index + 1 ) * num_target_cols ] = oof_df [ pred_column_names ] . values return all_oof_preds def macro_multilabel_auc ( label , pred , num_target_cols : int = 1 , multilabel : bool = False ): \"\"\"Also works for binary AUC like Melanoma\"\"\" if not multilabel : return roc_auc_score ( label , pred ) else : aucs = [] for i in range ( num_target_cols ): print ( label [:, i ]) print () print ( pred [:, i ]) print ( roc_auc_score ( label [:, i ], pred [:, i ])) aucs . append ( roc_auc_score ( label , pred )) return np . mean ( aucs ) def compute_best_oof ( all_oof_preds : np . ndarray , y_trues : np . ndarray , num_oofs : int , performance_metric : Callable , ) -> Tuple [ float , int ]: \"\"\"Compute the oof score of all models using a performance metric and return the best model index and score. Args: all_oof_preds (np.ndarray): The stacked oof predictions of shape (num_samples, num_oofs * num_pred_columns). Taken from stack_oofs. y_trues (np.ndarray): The true labels of shape (num_samples, num_target_cols). num_oofs (int): The number of oof predictions. performance_metric (Callable): The performance metric to use, this is a function. Returns: best_oof_metric_score (float): The best oof score. best_model_index (int): The index of the best model. \"\"\" all_oof_scores = [] for k in range ( num_oofs ): metric_score = performance_metric ( y_trues , all_oof_preds [:, k ], num_target_cols = 1 , multilabel = False , ) all_oof_scores . append ( metric_score ) print ( f \"Model { k } has OOF AUC = { metric_score } \" ) best_oof_metric_score , best_oof_index = np . max ( all_oof_scores ), np . argmax ( all_oof_scores ) return best_oof_metric_score , best_oof_index def calculate_best_score_over_weight_interval ( weight_interval : float , model_i_oof : np . ndarray , model_j_oof : np . ndarray , y_trues : np . ndarray , performance_metric : Callable , running_best_score : float , running_best_weight : float , patience : int , ) -> Tuple [ float , float ]: \"\"\"Calculate the best score over a weight interval. Args: weight_interval (float): _description_ model_i_oof (np.ndarray): _description_ model_j_oof (np.ndarray): _description_ y_trues (np.ndarray): _description_ performance_metric (Callable): _description_ running_best_score (float): _description_ running_best_weight (float): _description_ patience (int): _description_ Returns: Tuple[float, float]: _description_ \"\"\" patience_counter = 0 for weight in range ( weight_interval ): temp_weight = weight / weight_interval temp_ensemble_oof_preds = ( temp_weight * model_j_oof + ( 1 - temp_weight ) * model_i_oof ) temp_ensemble_oof_score = performance_metric ( y_trues , temp_ensemble_oof_preds , num_target_cols = 1 , multilabel = False , ) # in the first loop, if any of the blending is more than best_oof_metric_score, we will assign it to running_best_score. if temp_ensemble_oof_score > running_best_score : running_best_score = temp_ensemble_oof_score running_best_weight = temp_weight else : patience_counter += 1 if patience_counter > patience : break return running_best_score , running_best_weight def get_blended_oof ( initial_best_model_oof , best_oof_index_list , best_weights_list ): # can be used on both oof and subs curr_model_oof = initial_best_model_oof for index , _ in enumerate ( best_oof_index_list [ 1 :]): model_j_index = best_oof_index_list [ index + 1 ] curr_model_oof = ( 1 - best_weights_list [ index ]) * curr_model_oof + ( best_weights_list [ index ] ) * all_oof_preds [:, model_j_index ] . reshape ( - 1 , 1 ) return curr_model_oof def return_list_of_dataframes ( path : str , is_oof : bool = True ) -> Tuple [ List [ pd . DataFrame ], int ]: \"\"\"Return a list of dataframes from a directory of files. The boolean is_oof is used to determine whether the list of dataframes contains oof or subs. Args: path (str): The path to the directory containing the files. is_oof (bool, optional): Determine whether the list of dataframes contains oof or subs. Defaults to True. Returns: List[pd.DataFrame]: The list of dataframes for either oof or subs. int: The number of files in the directory. \"\"\" oof_and_subs_files = os . listdir ( path ) if is_oof : oof_files_sorted = np . sort ( [ f for f in oof_and_subs_files if \"oof\" in f ] ) return [ pd . read_csv ( os . path . join ( path , k )) for k in oof_files_sorted ], len ( oof_files_sorted ) else : sub_files_sorted = np . sort ( [ f for f in oof_and_subs_files if \"sub\" in f ] ) return [ pd . read_csv ( os . path . join ( path , k )) for k in sub_files_sorted ], len ( sub_files_sorted ) if __name__ == \"__main__\" : oof_and_subs_path = \"./oof_and_subs/melanoma\" # [\"oof_1.csv\", \"sub_1.csv\", \"oof_2.csv\", \"sub_2.csv\"] oof_and_subs_files = os . listdir ( oof_and_subs_path ) # [\"oof_1.csv\", \"oof_2.csv\", \"sub_1.csv\", \"sub_2.csv\"] sorted oof_files_sorted = np . sort ([ f for f in oof_and_subs_files if \"oof\" in f ]) # [oof_1_df, oof_2_df, sub_1_df, sub_2_df] in dataframe oof_dfs_list = [ pd . read_csv ( os . path . join ( oof_and_subs_path , k )) for k in oof_files_sorted ] num_oofs = len ( oof_dfs_list ) # in my oof files, I also saved the corresponding y_trues, we thus take the first oof_df and use it to get the y_trues, assuming they are the same for all oof files. # note of caution, if you use different resampling methods, you will need to change the y_trues accordingly. y_trues_df = oof_dfs_list [ 0 ][[ \"oof_trues\" ]] y_trues = y_trues_df . values print ( f \"We have { len ( oof_files_sorted ) } oof files. \\n \" ) target_cols = [ \"oof_trues\" ] pred_cols = [ \"class_1_oof\" ] num_target_cols = len ( target_cols ) all_oof_preds = stack_oofs ( oof_dfs = oof_dfs_list , pred_column_names = pred_cols ) print ( f \"all_oof_preds shape: { all_oof_preds . shape } \\n This variable is global and holds all oof predictions stacked horizontally. \\n \" ) best_oof_metric_score , best_oof_index = compute_best_oof ( all_oof_preds = all_oof_preds , y_trues = y_trues , num_oofs = num_oofs , performance_metric = macro_multilabel_auc , ) print ( f \" \\n ### Computing Best OOF scores among all models ### \\n The best OOF AUC score is { best_oof_metric_score } and the best model index is { best_oof_index } corresponding to the oof file { oof_files_sorted [ best_oof_index ] } \" ) weight_interval = 1000 # 200 patience = 20 # 10 min_increase = 0.0003 # 0.00003 print ( f \" \\n ### HyperParameters ### \\n weight_interval = { weight_interval } \\n patience = { patience } \\n min_increase = { min_increase } \\n \" ) # keep track of oof index that are blended best_oof_index_list = [ best_oof_index ] best_weights_list = [] print ( f \"Current Tracked Model List: { best_oof_index_list } \" ) print ( f \"Current Weights List: { best_weights_list } \" ) counter = 0 # Initially, this curr_model_oof is the single best model that we got above from [oof_1, oof_2,...] initial_best_model_oof = all_oof_preds [:, best_oof_index ] . reshape ( - 1 , 1 ) old_best_score = best_oof_metric_score model_i_best_score , model_i_index , model_i_weights = 0 , 0 , 0 print ( \"Denote model i as the current model, and model j as the model that we are blending with.\" ) for outer_oof_index in range ( num_oofs ): # basically in the first loop, we already know the current model's oof and we assign it by subsetting the all_oof_preds with the best oof index. curr_model_oof = initial_best_model_oof if counter > 0 : curr_model_oof = get_blended_oof ( initial_best_model_oof , best_oof_index_list , best_weights_list ) print ( curr_model_oof ) for inner_oof_index in range ( num_oofs ): # If we have [oof_1, oof_2] and best_oof_index = 1 (oof_2), then we do not need to blend oof_2 and itself. if inner_oof_index in best_oof_index_list : continue # in the first loop, our running_best_score is the best_oof_metric_score # also our old_best_score is the best_oof_metric_score in the first loop ( running_best_score , running_best_weight , patience_counter ,) = ( 0 , 0 , 0 , ) # what we are doing here is to find the best oof score among all models that we have not blended yet. # for example, if we have [oof_1, oof_2, oof_3], and we know oof_2 is our initial_best_model_oof, # then we need to blend oof_2 with oof_1, then oof_2 with oof_3 to find out which of them yields the best overall oof when blended. ( running_best_score , running_best_weight , ) = calculate_best_score_over_weight_interval ( weight_interval , curr_model_oof , all_oof_preds [:, inner_oof_index ] . reshape ( - 1 , 1 ), y_trues , macro_multilabel_auc , running_best_score , running_best_weight , patience , ) if running_best_score > model_i_best_score : model_i_index = inner_oof_index model_i_best_score = running_best_score model_i_weights = running_best_weight increment = model_i_best_score - old_best_score if increment <= min_increase : print ( \"Increment is too small, stop blending\" ) break # DISPLAY RESULTS print () print ( \"Ensemble AUC = %.4f after adding model %i with weight %.3f . Increase of %.4f \" % ( model_i_best_score , model_i_index , model_i_weights , increment , ) ) print () old_best_score = model_i_best_score best_oof_index_list . append ( model_i_index ) best_weights_list . append ( model_i_weights ) print ( f \"Current Tracked Model List: { best_oof_index_list } \" ) print ( f \"Current Weights List: { best_weights_list } \" ) print ( f \"Current Best Score: { model_i_best_score } \" ) counter += 1 plt . hist ( curr_model_oof , bins = 100 ) plt . title ( \"Ensemble OOF predictions\" ) plt . show () # apply on submission sub_files_sorted = np . sort ([ f for f in oof_and_subs_files if \"sub\" in f ]) sub_dfs_list = [ pd . read_csv ( os . path . join ( oof_and_subs_path , k )) for k in sub_files_sorted ] print ( f \" \\n We have { len ( sub_files_sorted ) } submission files...\" ) print () print ( sub_files_sorted ) y = np . zeros (( len ( sub_dfs_list [ 0 ]), len ( sub_files_sorted ) * len ( pred_cols ))) print ( y . shape ) for k in range ( len ( sub_files_sorted )): y [ :, int ( k * len ( pred_cols )) : int (( k + 1 ) * len ( pred_cols )) ] = sub_dfs_list [ k ][ \"target\" ] . values . reshape ( - 1 , 1 ) print ( y ) md2 = y [ :, int ( best_oof_index_list [ 0 ] * len ( pred_cols )) : int ( ( best_oof_index_list [ 0 ] + 1 ) * len ( pred_cols ) ), ] print ( md2 ) for i , k in enumerate ( best_oof_index_list [ 1 :]): md2 = ( best_weights_list [ i ] * y [:, int ( k * len ( pred_cols )) : int (( k + 1 ) * len ( pred_cols ))] + ( 1 - best_weights_list [ i ]) * md2 ) plt . hist ( md2 , bins = 100 ) plt . show () df = sub_dfs_list [ 0 ] . copy () df [[ \"target\" ]] = md2 df . to_csv ( \"submission.csv\" , index = False ) df . head () We have 6 oof files. all_oof_preds shape: (33126, 6) This variable is global and holds all oof predictions stacked horizontally. Model 0 has OOF AUC = 0.8967598406021975 Model 1 has OOF AUC = 0.897338308007439 Model 2 has OOF AUC = 0.8969099101014242 Model 3 has OOF AUC = 0.8997782002268091 Model 4 has OOF AUC = 0.9021602904318382 Model 5 has OOF AUC = 0.9035525112752076 ### Computing Best OOF scores among all models ### The best OOF AUC score is 0.9035525112752076 and the best model index is 5 corresponding to the oof file oof_tf_efficientnet_b2_ns_tf_efficientnet_b2_ns_5_folds_3c0odinh.csv ### HyperParameters ### weight_interval = 1000 patience = 20 min_increase = 0.0003 Current Tracked Model List: [5] Current Weights List: [] Denote model i as the current model, and model j as the model that we are blending with. Ensemble AUC = 0.9141 after adding model 2 with weight 0.452. Increase of 0.0106 Current Tracked Model List: [5, 2] Current Weights List: [0.452] Current Best Score: 0.9141475652539226 [[4.50905440e-03] [3.01906102e-05] [1.43342172e-03] ... [6.48897682e-05] [1.05257292e-02] [9.18150437e-03]] Ensemble AUC = 0.9175 after adding model 3 with weight 0.290. Increase of 0.0034 Current Tracked Model List: [5, 2, 3] Current Weights List: [0.452, 0.29] Current Best Score: 0.9175226556534317 [[3.58108299e-03] [2.45987573e-05] [1.03199503e-03] ... [4.75034868e-05] [9.02447451e-03] [6.67390628e-03]] Ensemble AUC = 0.9193 after adding model 4 with weight 0.273. Increase of 0.0018 Current Tracked Model List: [5, 2, 3, 4] Current Weights List: [0.452, 0.29, 0.273] Current Best Score: 0.9193341713090691 [[2.65875346e-03] [2.51985486e-05] [8.82538860e-04] ... [4.77394006e-05] [1.76098008e-02] [5.30504798e-03]] Ensemble AUC = 0.9201 after adding model 0 with weight 0.161. Increase of 0.0008 Current Tracked Model List: [5, 2, 3, 4, 0] Current Weights List: [0.452, 0.29, 0.273, 0.161] Current Best Score: 0.9201217730848145 [[2.68508770e-03] [3.33216799e-05] [7.54874316e-04] ... [1.12230140e-04] [2.00920893e-02] [1.02248122e-02]] Increment is too small, stop blending We have 6 submission files... ['sub_tf_efficientnet_b0_ns_tf_efficientnet_b0_ns_5_folds_2dcluilj.csv' 'sub_tf_efficientnet_b0_ns_tf_efficientnet_b0_ns_5_folds_3725vib5.csv' 'sub_tf_efficientnet_b0_ns_tf_efficientnet_b0_ns_5_folds_kh6lm0mc.csv' 'sub_tf_efficientnet_b1_ns_tf_efficientnet_b1_ns_5_folds_9qhxwbbq.csv' 'sub_tf_efficientnet_b2_ns_tf_efficientnet_b2_ns_5_folds_1lvjyja0.csv' 'sub_tf_efficientnet_b2_ns_tf_efficientnet_b2_ns_5_folds_3c0odinh.csv'] (10982, 6) [[1.10712990e-03 4.36813570e-04 5.27769700e-04 5.15609340e-04 7.12666400e-04 4.17358470e-04] [2.20565100e-04 1.78343150e-04 9.28863160e-05 1.78823610e-05 1.48618330e-04 3.16100000e-05] [1.81065010e-04 7.23641860e-05 2.92848130e-04 7.86881400e-05 9.41391550e-05 9.77370900e-05] ... [6.67065500e-02 1.14237880e-01 1.26132040e-01 1.13317326e-01 6.87497260e-02 7.28674750e-02] [6.35771550e-03 6.06310670e-04 8.84217040e-04 9.36840800e-04 5.04250900e-04 5.87228570e-05] [1.41756660e-02 2.95386670e-02 1.08320400e-02 2.21859530e-02 1.42423960e-02 9.72992800e-02]] [[4.1735847e-04] [3.1610000e-05] [9.7737090e-05] ... [7.2867475e-02] [5.8722857e-05] [9.7299280e-02]]","title":"Forward Ensemble (Hill Climbing)"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/ensemble_theory/forward_ensemble/#convert-oofs-and-ground-truth-to-numpy","text":"First, we define a function return_list_of_dataframes to return either the OOFs or the SUBs. At this point, one should be clear what are OOFs, if not, please read the post here . def return_list_of_dataframes ( path : str , is_oof : bool = True ) -> Tuple [ List [ pd . DataFrame ], int ]: \"\"\"Return a list of dataframes from a directory of files. The boolean is_oof is used to determine whether the list of dataframes contains oof or subs. Args: path (str): The path to the directory containing the files. is_oof (bool, optional): Determine whether the list of dataframes contains oof or subs. Defaults to True. Returns: List[pd.DataFrame]: The list of dataframes for either oof or subs. int: The number of files in the directory. \"\"\" oof_and_subs_files = os . listdir ( path ) if is_oof : oof_files_sorted = np . sort ( [ f for f in oof_and_subs_files if \"oof\" in f ] ) return [ pd . read_csv ( os . path . join ( path , k )) for k in oof_files_sorted ], len ( oof_files_sorted ) else : sub_files_sorted = np . sort ( [ f for f in oof_and_subs_files if \"sub\" in f ] ) return [ pd . read_csv ( os . path . join ( path , k )) for k in sub_files_sorted ], len ( sub_files_sorted ) This function is first applied to is_oof=True . For now, we just want all our OOFs converted to a pandas dataframe, and stored in a list. Note, we also conveniently returned the number of files in the director for OOFs and SUBs respectively, it should be clear in this context that the number of files for OOF is the same as the number of files for SUB. oof_and_subs_path = \"./oof_and_subs/toy_examples\" oof_dfs_list , num_oofs = return_list_of_dataframes ( path = oof_and_subs_path , is_oof = True ) display ( oof_dfs_list [ 0 ]) display ( oof_dfs_list [ 1 ]) display ( oof_dfs_list [ 2 ]) print ( f \"We have { num_oofs } oof files. \\n \" ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } image_id y_trues class_0_oof class_1_oof 0 1 1 0.70 0.30 1 2 0 0.70 0.30 2 3 0 0.65 0.35 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } image_id y_trues class_0_oof class_1_oof 0 1 1 0.6 0.4 1 2 0 0.8 0.2 2 3 0 0.4 0.6 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } image_id y_trues class_0_oof class_1_oof 0 1 1 0.1 0.8 1 2 0 0.4 0.8 2 3 0 0.9 0.7 We have 3 oof files. At this junction, we need to be clear of a few things. Let us use an example to illustrate. Let us say we trained our model over 5 folds and get our OOF predictions in terms of the Macro-AUROC with respect to the positive class 1 . We then need to calculate our OOF scores with our corresponding y_trues. Just by eyeballing, we can deduce the Macro-AUROC score for the positive class as follows: oof_1_auroc = roc_auc_score ([ 1 , 0 , 0 ], [ 0.3 , 0.3 , 0.35 ]) -> 0.25 oof_2_auroc = roc_auc_score ([ 1 , 0 , 0 ], [ 0.4 , 0.2 , 0.6 ]) -> 0.5 oof_3_auroc = roc_auc_score ([ 1 , 0 , 0 ], [ 0.8 , 0.8 , 0.7 ]) -> 0.75 To compute the above, we can manually hardcode them, but for larger data, we will have to maintain a better data structure. We will store them into matrices and vectors (numpy) to compute the OOF scores efficiently. To do so, we define two variables: ground_truth_column_name = [ \"y_trues\" ] positive_class_oof_column_name = [ \"class_1_oof\" ] where the first variable is the name of the column(s) for the ground truth, while the second is the name of the column(s) that we will be using to compute the OOF scores. We will then create a function stack_oofs to convert the list of OOF dataframes into an array. def stack_oofs ( oof_dfs : List [ pd . DataFrame ], pred_column_names : List [ str ] ) -> np . ndarray : \"\"\"Stack all oof predictions horziontally. Args: oof_dfs (List[pd.DataFrame]): The list of oof predictions in dataframes. pred_column_names (List[str]): The list of prediction column names. Returns: all_oof_preds (np.ndarray): The stacked oof predictions of shape (num_samples, num_oofs * num_pred_columns). Example: >>> oof_1 = pd.DataFrame([1,2,3], columns=['class_1_oof']) >>> oof_2 = pd.DataFrame([4,5,6], columns=['class_1_oof']) >>> all_oof_preds = stack_oofs([oof_1, oof_2], ['class_1_oof']) >>> all_oof_preds = np.array([1, 4], [2, 5], [3, 6]) \"\"\" num_oofs = len ( oof_dfs ) num_samples = len ( oof_dfs [ 0 ]) num_target_cols = len ( pred_column_names ) all_oof_preds = np . zeros (( num_samples , num_oofs * num_target_cols )) if num_target_cols == 1 : for index , oof_df in enumerate ( oof_dfs ): all_oof_preds [:, index : index + 1 ] = oof_df [ pred_column_names ] . values elif num_target_cols > 1 : # Used in RANZCR where there are 11 target columns for index , oof_df in enumerate ( oof_dfs ): all_oof_preds [ :, index * num_target_cols : ( index + 1 ) * num_target_cols ] = oof_df [ pred_column_names ] . values return all_oof_preds ground_truth_column_name = [ \"y_trues\" ] positive_class_oof_column_name = [ \"class_1_oof\" ] In my oof files, I also saved the corresponding y_trues as a column, we thus take the first oof_df from oof_dfs_list and use it to get the y_trues , assuming they are the same for all oof files. Note of caution, if you use different resampling methods, you will need to change the y_trues accordingly. y_trues = oof_dfs_list [ 0 ][ ground_truth_column_name ] . values all_oof_preds = stack_oofs ( oof_dfs_list , positive_class_oof_column_name ) print ( f \"y_trues shape: { y_trues . shape } \\n This variable is global and holds all ground truth. \\n \" ) print ( f \"all_oof_preds shape: { all_oof_preds . shape } \\n This variable is global and holds all oof predictions stacked horizontally. \\n \" ) y_trues shape: (3, 1) This variable is global and holds all ground truth. all_oof_preds shape: (3, 3) This variable is global and holds all oof predictions stacked horizontally. After converting to numpy, we should have this following representation: \\[ \\textbf{y_true} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} \\in \\mathbb{R}^3 \\quad \\textbf{all_oof_preds} = \\begin{bmatrix} 0.3 & 0.4 & 0.8 \\\\ 0.3 & 0.2 & 0.8 \\\\ 0.35 & 0.6 & 0.7 \\end{bmatrix} \\in \\mathbb{R}^{3 \\times 3} \\] where y_true is a \\(3 \\times 1\\) column vector, each row entry is the corresponding ground truth for sample \\(i\\) (i.e y_true[0] is ground truth for sample 1). For all_oof_preds , this is a \\(3 \\times 3\\) matrix, where each column \\(i\\) represents the OOF predictions made by model \\(i\\) . In other words, the first column \\(\\begin{bmatrix} 0.3 \\\\ 0.35 \\\\ 0.3 \\end{bmatrix}\\) is the OOF predictions made by the first model. It is important to note that we are dealing with classification (binary or multiclass), so it is usually the case that both our target and predicted columns are just \\(1\\) . In multi-label however, we will have multiple target columns, and will be the focus in part II.","title":"Convert OOFs and Ground Truth to NumPy"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/ensemble_theory/forward_ensemble/#compute-scores-of-oofs-and-find-the-best-score","text":"This part is very crucial, when we apply our Hill Climbing (Forward Ensembling) technique here, we want to initialize with the best models first and iteratively blend with the rest . In other words def compute_best_oof ( all_oof_preds : np . ndarray , y_trues : np . ndarray , num_oofs : int , performance_metric : Callable , ) -> Tuple [ float , int ]: \"\"\"Compute the oof score of all models using a performance metric and return the best model index and score. Args: all_oof_preds (np.ndarray): The stacked oof predictions of shape (num_samples, num_oofs * num_pred_columns). Taken from stack_oofs. y_trues (np.ndarray): The true labels of shape (num_samples, num_target_cols). num_oofs (int): The number of oof predictions. performance_metric (Callable): The performance metric to use, this is a function. Returns: best_oof_metric_score (float): The best oof score. best_model_index (int): The index of the best model. \"\"\" all_oof_scores = [] for k in range ( num_oofs ): oof_k = all_oof_preds [:, k ] . reshape ( - 1 , 1 ) metric_score = performance_metric ( y_trues , oof_k , num_target_cols = 1 , multilabel = False , ) all_oof_scores . append ( metric_score ) print ( f \"Model { k } has OOF AUC = { metric_score } \" ) best_oof_metric_score , best_oof_index = np . max ( all_oof_scores ), np . argmax ( all_oof_scores ) return best_oof_metric_score , best_oof_index def macro_multilabel_auc ( label , pred , num_target_cols : int = 1 , multilabel : bool = False ): \"\"\"Also works for binary AUC like Melanoma\"\"\" if not multilabel : return roc_auc_score ( label , pred ) else : aucs = [] for i in range ( num_target_cols ): print ( label [:, i ]) print () print ( pred [:, i ]) print ( roc_auc_score ( label [:, i ], pred [:, i ])) aucs . append ( roc_auc_score ( label , pred )) return np . mean ( aucs ) best_oof_metric_score , best_oof_index = compute_best_oof ( all_oof_preds = all_oof_preds , y_trues = y_trues , num_oofs = num_oofs , performance_metric = macro_multilabel_auc , ) print ( f \" \\n ### Computing Best OOF scores among all models ### \\n The best OOF AUC score is { best_oof_metric_score } and the best model index is { best_oof_index } corresponding to the oof file { best_oof_index } \" ) Model 0 has OOF AUC = 0.25 Model 1 has OOF AUC = 0.5 Model 2 has OOF AUC = 0.75 ### Computing Best OOF scores among all models ### The best OOF AUC score is 0.75 and the best model index is 2 corresponding to the oof file 2","title":"Compute Scores of OOFs and Find the Best Score"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/ensemble_theory/forward_ensemble/#the-algorithm","text":"Now the logic flows as follows: Start with the model with the best OOF score: we get that from compute_best_oof which yields us the key variables best_oof_index . best_oof_index : 2 indicates that our oof_2 (model 2) has the best OOF score amongst the 3 OOFs (models), as we can see from above, it is indeed the case since Model 2 has OOF AUC of \\(0.75\\) . We now need to blend Model 2 with Model 0 and Model 1 respectively to find out which weight combination gives a better OOF score. We need to define a variable weight_interval , which tells us the weights to sample from. For example, if weight_interval is \\(3\\) , then we will uniformly sample the weights \\(\\frac{0}{3}, \\frac{1}{3}, \\frac{2}{3}\\) . Let us see a concrete example. weight_interval = 3 patience_counter = 0 model_0_oof = all_oof_preds [:, 0 ] . reshape ( y_trues . shape ) model_1_oof = all_oof_preds [:, 1 ] . reshape ( y_trues . shape ) model_2_oof = all_oof_preds [:, best_oof_index ] . reshape ( y_trues . shape ) # best_oof_index=2 We define best_oof_index_list = [ best_oof_index ] best_weights_list = [] which keeps track two lists which holds the best OOFs to blend with and their corresponding weights. We use a naive example to illustrate: best_oof_index_list = [ 2 , 0 , 1 ] best_weights_list = [ 0.3 , 0.8 ] all_oof_preds_matrix = [[ 0.3 , 0.4 , 0.8 ], [ 0.3 , 0.2 , 0.8 ], [ 0.35 , 0.6 , 0.7 ]] initial_best_vector = A_matrix [:, 2 ] In other words, if after our Hill Climbing algorithm, we have the above variables and to calculate the final blended OOF using the above optimal weights, we have: Let \\(\\mathbf{c_i}\\) be the columns of the matrix all_oof_preds_matrix , you can think \\(\\mathbf{c_i}\\) as the Model \\(i\\) 's OOFs predictions, since in this naive example, we see that our initial best model is Model 2. \\[ ((1-0.3) \\times \\mathbf{c_2} + 0.3 \\times \\mathbf{c_0}) \\times (1-0.8) + 0.8 \\times \\mathbf{c_1} \\] To dissect clearly, we start with the first index, which is Model 2, and blend with the next index, which is Model 0, they will be blended with weights of \\(0.3\\) , which means \\((1-0.3) \\times \\mathbf{c_2} + 0.3 \\times \\mathbf{c_0}\\) . We call it oof_blend_1 which indicates the linear combination of the weights of the first blend. Note that this results in a new and better \"OOF predictions\". The next blend will be of oof_blend_1 with Model 1, with a weight of \\(0.8\\) and the assignment is \\((1-0.8) \\times \\textbf{oof_blend_1} + 0.8 \\times \\mathbf{c_0}\\) . best_oof_index_list = [ best_oof_index ] best_weights_list = [] print ( f \"Current Tracked Model List: { best_oof_index_list } \" ) print ( f \"Current Weights List: { best_weights_list } \" ) Current Tracked Model List: [2] Current Weights List: []","title":"The Algorithm"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/ensemble_theory/forward_ensemble/#first-round-blending-model-2-with-model-0","text":"We check the blending results of Model 2 and Model 0 over 3 weights: \\(0, \\frac{1}{3}, \\frac{2}{3}\\) . We also assign 3 variables running_best_score , running_best_weight , running_best_oof_index = best_oof_metric_score , 0 , 0 such that whenever a blend of \\(w_1 \\times \\textbf{model_i_oof} + (1-w_2) \\times \\textbf{model_j_oof}\\) produces a better OOF score, then we will assign if temp_ensemble_oof_score >= running_best_score : print ( f \"The blend of weight { temp_weight } of model 2 and model 0 led to a greater or equals to OOF score = { temp_ensemble_oof_score } \\n \" ) running_best_score = temp_ensemble_oof_score running_best_weight = temp_weight running_best_oof_index = 0 Notice that we hardcoded the running_best_oof_index to be \\(0\\) since we know we are only looking at the interaction of Model 2 and Model 0. In proper code, this part should not be hardcoded and you can refer to my full code for clarity. running_best_score , running_best_weight , running_best_oof_index = best_oof_metric_score , 0 , 0 for weight in range ( weight_interval ): temp_weight = weight / weight_interval print ( f \"weight = { temp_weight } \" ) temp_ensemble_oof_preds = ( temp_weight * model_0_oof + ( 1 - temp_weight ) * model_2_oof ) temp_ensemble_oof_score = macro_multilabel_auc ( y_trues , temp_ensemble_oof_preds , num_target_cols = 1 , multilabel = False , ) print ( f \"blended OOF score with model_0_oof = { temp_ensemble_oof_score } \" ) if temp_ensemble_oof_score >= running_best_score : running_best_score = temp_ensemble_oof_score running_best_weight = temp_weight running_best_oof_index = 0 print ( f \"The blend of weight { temp_weight } of model 2 and model { running_best_oof_index } led to a greater or equals to OOF score = { temp_ensemble_oof_score } \\n \" ) weight = 0.0 blended OOF score with model_0_oof = 0.75 The blend of weight 0.0 of model 2 and model 0 led to a greater or equals to OOF score = 0.75 weight = 0.3333333333333333 blended OOF score with model_0_oof = 0.75 The blend of weight 0.3333333333333333 of model 2 and model 0 led to a greater or equals to OOF score = 0.75 weight = 0.6666666666666666 blended OOF score with model_0_oof = 0.5 So now we finished blending Model 2 and Model 0 and we have: running_best_score = 0.75 running_best_weight = 0.333 ... We need to blend Model 2 and Model 1 now to see if Model 2 and Model 1 can give better OOF scores when blended! We repeat the exact same loop as above, but bear in mind that the running_best_score and running_best_weight is already updated to the ones we got in the blend of Model 2 and Model 0 because we want to check if Model 2 and Model 1's blend can give better results than the previous running_best_score .","title":"First Round - Blending Model 2 with Model 0"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/ensemble_theory/forward_ensemble/#first-round-blending-model-2-and-model-1","text":"for weight in range ( weight_interval ): temp_weight = weight / weight_interval print ( f \"weight = { temp_weight } \" ) temp_ensemble_oof_preds = ( temp_weight * model_1_oof + ( 1 - temp_weight ) * model_2_oof ) temp_ensemble_oof_score = macro_multilabel_auc ( y_trues , temp_ensemble_oof_preds , num_target_cols = 1 , multilabel = False , ) print ( f \"blended OOF score with model_1_oof = { temp_ensemble_oof_score } \" ) if temp_ensemble_oof_score >= running_best_score : running_best_score = temp_ensemble_oof_score running_best_weight = temp_weight running_best_oof_index = 1 print ( f \"The blend of weight { temp_weight } of model 2 and model { running_best_oof_index } led to a greater or equals to OOF score = { temp_ensemble_oof_score } \\n \" ) weight = 0.0 blended OOF score with model_1_oof = 0.75 The blend of weight 0.0 of model 2 and model 1 led to a greater or equals to OOF score = 0.75 weight = 0.3333333333333333 blended OOF score with model_1_oof = 1.0 The blend of weight 0.3333333333333333 of model 2 and model 1 led to a greater or equals to OOF score = 1.0 weight = 0.6666666666666666 blended OOF score with model_1_oof = 0.5 So now we finished blending Model 2 and Model 1 and we have: running_best_score = 1 running_best_weight = 0.333 ... and we have a new winner in town! After our first round of iterating our initial best OOF Model 2 with the rest (Model 0 and 1), we found out that if we take \\(w_1 = 1 - \\frac{2}{3}\\) and \\(w_2 = \\frac{1}{3}\\) , Model 2 and 1 gives us a better overall score. That is to say: \\[ w_1 * \\textbf{OOF_2} + w_2 \\times \\textbf{OOF_1} \\] leads to the greatest increase in our OOF score! Notice that we hardcoded the running_best_oof_index to be \\(1\\) since we know we are only looking at the interaction of Model 2 and Model 1. In proper code, this part should not be hardcoded and you can refer to my full code for clarity. Technically, we can stop the algorithm now since the metric Macro-AUROC is capped at \\(1\\) , but for the sake of explanation, let us continue.","title":"First Round - Blending Model 2 and Model 1"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/ensemble_theory/forward_ensemble/#first-round-save-results-for-loop-1","text":"We then append the best OOF index and the corresponding weight to the best_oof_index_list and best_weights_list respectively. best_oof_index_list . append ( running_best_oof_index ) best_weights_list . append ( running_best_weight ) print ( f \"Current Tracked Model List: { best_oof_index_list } \" ) print ( f \"Current Weights List: { best_weights_list } \" ) Current Tracked Model List: [2, 1] Current Weights List: [0.3333333333333333]","title":"First Round - Save Results for Loop 1"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/ensemble_theory/forward_ensemble/#second-round-blend-oof","text":"Now we have a brand new OOF after blending Model 2 and Model 1, we call it blended_oof_1 and note that this is our new best OOF! # blended_oof_1 = all_oof_preds[:, best_oof_index_list[0]].reshape(-1, 1) * (1 - best_weights_list[0]) + all_oof_preds[:, best_oof_index_list[1]].reshape(-1, 1) * best_weights_list[0] blended_oof_1 = model_2_oof * ( 1 - 1 / 3 ) + model_1_oof * ( 1 / 3 ) assert macro_multilabel_auc ( y_trues , blended_oof_1 ) == 1","title":"Second Round - Blend OOF"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/ensemble_theory/forward_ensemble/#second-round-blending-blended_oof_1-with-model-0","text":"We continue to try out blended_oof_1 with the rest of the models that were not selected. This means we have to check our Current Tracked Model List best_oof_index_list and see that we already have \\([2, 1]\\) being used up, in our simple example here, there only left with Model 0 to try! So make sure in your code you do not try blended_oof_1 with Model 0, 1 and 2 again since the blended_oof_1 is already made up with Model 1 and 2! for weight in range ( weight_interval ): temp_weight = weight / weight_interval print ( f \" \\n weight = { temp_weight } \" ) temp_ensemble_oof_preds = ( temp_weight * model_0_oof + ( 1 - temp_weight ) * blended_oof_1 ) temp_ensemble_oof_score = macro_multilabel_auc ( y_trues , temp_ensemble_oof_preds , num_target_cols = 1 , multilabel = False , ) print ( f \"blended OOF score with model_0_oof = { temp_ensemble_oof_score } \" ) if temp_ensemble_oof_score >= running_best_score : running_best_score = temp_ensemble_oof_score running_best_weight = temp_weight running_best_oof_index = 0 print ( f \"The blend of weight { temp_weight } of model 2 and model { running_best_oof_index } led to a greater or equals to OOF score = { temp_ensemble_oof_score } \\n \" ) weight = 0.0 blended OOF score with model_0_oof = 1.0 The blend of weight 0.0 of model 2 and model 0 led to a greater or equals to OOF score = 1.0 weight = 0.3333333333333333 blended OOF score with model_0_oof = 0.5 weight = 0.6666666666666666 blended OOF score with model_0_oof = 0.5 Since we have done checking blended_oof_1 with the last remaining Model 0 and found that blending Model 0 with a weight of 0 (what a surprise haha!) yields the best result, we once again update the running metrics and also append to our global lists below. best_oof_index_list . append ( running_best_oof_index ) best_weights_list . append ( running_best_weight ) print ( f \"Current Tracked Model List: { best_oof_index_list } \" ) print ( f \"Current Weights List: { best_weights_list } \" ) Current Tracked Model List: [2, 1, 0] Current Weights List: [0.3333333333333333, 0.0] # blended_oof_2 = blended_oof_1 * (1 - best_weights_list[1]) + all_oof_preds[:, best_oof_index_list[2]].reshape(-1, 1) * best_weights_list[1] blended_oof_2 = blended_oof_1 * ( 1 - 0.0 ) + model_0_oof . reshape ( - 1 , 1 ) * 0.0 assert macro_multilabel_auc ( y_trues , blended_oof_2 ) == 1","title":"Second Round - Blending blended_oof_1 with Model 0"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/ensemble_theory/forward_ensemble/#ensembling-model-predictions-with-the-found-optimal-weights","text":"So we end the discussion with what to do with the weights we got. We already established to the readers that our Initial Best OOF is Model 2 with a Macro-AUROC score of \\(0.75\\) , and by way of Hill Climbing, we found out that we can blend the 3 Models with some weights such that their new OOF produces a Macro-AUROC score of \\(1.0\\) , a huge improvement. We aren't done yet! We want to apply these optimal weights to our test set predictions as well. Note that our test set predictions are unseen and our usual ensemble methods can be as simple as mean averaging. More concretely, let us check out the example below. oof_and_subs_path = \"./oof_and_subs/toy_examples\" sub_dfs_list , num_subs = return_list_of_dataframes ( path = oof_and_subs_path , is_oof = False ) display ( sub_dfs_list [ 0 ]) display ( sub_dfs_list [ 1 ]) display ( sub_dfs_list [ 2 ]) print ( f \"We have { num_subs } sub files. \\n \" ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } image_id class_0_preds class_1_preds 0 4 0.2 0.8 1 5 0.3 0.7 2 6 0.6 0.4 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } image_id class_0_preds class_1_preds 0 4 0.3 0.7 1 5 0.9 0.1 2 6 0.2 0.8 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } image_id class_0_preds class_1_preds 0 4 0.3 0.7 1 5 0.7 0.3 2 6 0.8 0.2 We have 3 sub files. The stack_subs function does the same thing as stack_oofs so they can be combined into one function for code clarity. def stack_subs ( sub_dfs : List [ pd . DataFrame ], pred_column_names : List [ str ] ) -> np . ndarray : \"\"\"Stack all sub predictions horziontally. Args: sub_dfs (List[pd.DataFrame]): The list of sub predictions in dataframes. pred_column_names (List[str]): The list of prediction column names. Returns: all_sub_preds (np.ndarray): The stacked oof predictions of shape (num_samples, num_subs * num_pred_columns). \"\"\" num_subs = len ( sub_dfs ) num_samples = len ( sub_dfs [ 0 ]) num_target_cols = len ( pred_column_names ) all_sub_preds = np . zeros (( num_samples , num_subs * num_target_cols )) if num_target_cols == 1 : for index , sub_df in enumerate ( sub_dfs ): all_sub_preds [:, index : index + 1 ] = sub_df [ pred_column_names ] . values elif num_target_cols > 1 : # Used in RANZCR where there are 11 target columns for index , sub_df in enumerate ( sub_dfs ): all_sub_preds [ :, index * num_target_cols : ( index + 1 ) * num_target_cols ] = sub_df [ pred_column_names ] . values return all_sub_preds test_set_target_column_name = [ \"class_1_preds\" ] all_subs_preds = stack_subs ( sub_dfs_list , test_set_target_column_name ) model_0_sub = all_subs_preds [:, 0 ] . reshape ( y_trues . shape ) model_1_sub = all_subs_preds [:, 1 ] . reshape ( y_trues . shape ) model_2_sub = all_subs_preds [:, 2 ] . reshape ( y_trues . shape ) Recall the optimal weights earlier: Current Tracked Model List : [ 2 , 1 , 0 ] Current Weights List : [ 0.3333333333333333 , 0.0 ] and we now have a way to ensemble our model subs accordingly. # blended_oof_1 = all_oof_preds[:, best_oof_index_list[0]].reshape(-1, 1) * (1 - best_weights_list[0]) + all_oof_preds[:, best_oof_index_list[1]].reshape(-1, 1) * best_weights_list[0] blended_sub_1 = model_2_sub * ( 1 - 1 / 3 ) + model_1_sub * ( 1 / 3 ) blended_sub_2 = blended_sub_1 * ( 1 - 0.0 ) + model_0_sub * 0.0 blended_sub_2 array([[0.7 ], [0.23333333], [0.4 ]]) Then blended_sub_2 should be our final test set predictions !","title":"Ensembling Model Predictions with the Found Optimal Weights"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/ensemble_theory/forward_ensemble/#forward-ensemble-with-siim-isic-melanoma-classification","text":"This is taken from my repo. import pandas as pd import numpy as np import os import matplotlib.pyplot as plt from typing import List , Tuple , Callable from sklearn.metrics import roc_auc_score def stack_oofs ( oof_dfs : List [ pd . DataFrame ], pred_column_names : List [ str ] ) -> np . ndarray : \"\"\"Stack all oof predictions horziontally. Args: oof_dfs (List[pd.DataFrame]): The list of oof predictions in dataframes. pred_column_names (List[str]): The list of prediction column names. Returns: all_oof_preds (np.ndarray): The stacked oof predictions of shape (num_samples, num_oofs * num_pred_columns). Example: >>> oof_1 = pd.DataFrame([1,2,3], columns=['class_1_oof']) >>> oof_2 = pd.DataFrame([4,5,6], columns=['class_1_oof']) >>> all_oof_preds = stack_oofs([oof_1, oof_2], ['class_1_oof']) >>> all_oof_preds = np.array([1, 4], [2, 5], [3, 6]) \"\"\" num_oofs = len ( oof_dfs ) num_samples = len ( oof_dfs [ 0 ]) num_target_cols = len ( pred_column_names ) all_oof_preds = np . zeros (( num_samples , num_oofs * num_target_cols )) if num_target_cols == 1 : for index , oof_df in enumerate ( oof_dfs ): all_oof_preds [:, index : index + 1 ] = oof_df [ pred_column_names ] . values elif num_target_cols > 1 : # Used in RANZCR where there are 11 target columns for index , oof_df in enumerate ( oof_dfs ): all_oof_preds [ :, index * num_target_cols : ( index + 1 ) * num_target_cols ] = oof_df [ pred_column_names ] . values return all_oof_preds def macro_multilabel_auc ( label , pred , num_target_cols : int = 1 , multilabel : bool = False ): \"\"\"Also works for binary AUC like Melanoma\"\"\" if not multilabel : return roc_auc_score ( label , pred ) else : aucs = [] for i in range ( num_target_cols ): print ( label [:, i ]) print () print ( pred [:, i ]) print ( roc_auc_score ( label [:, i ], pred [:, i ])) aucs . append ( roc_auc_score ( label , pred )) return np . mean ( aucs ) def compute_best_oof ( all_oof_preds : np . ndarray , y_trues : np . ndarray , num_oofs : int , performance_metric : Callable , ) -> Tuple [ float , int ]: \"\"\"Compute the oof score of all models using a performance metric and return the best model index and score. Args: all_oof_preds (np.ndarray): The stacked oof predictions of shape (num_samples, num_oofs * num_pred_columns). Taken from stack_oofs. y_trues (np.ndarray): The true labels of shape (num_samples, num_target_cols). num_oofs (int): The number of oof predictions. performance_metric (Callable): The performance metric to use, this is a function. Returns: best_oof_metric_score (float): The best oof score. best_model_index (int): The index of the best model. \"\"\" all_oof_scores = [] for k in range ( num_oofs ): metric_score = performance_metric ( y_trues , all_oof_preds [:, k ], num_target_cols = 1 , multilabel = False , ) all_oof_scores . append ( metric_score ) print ( f \"Model { k } has OOF AUC = { metric_score } \" ) best_oof_metric_score , best_oof_index = np . max ( all_oof_scores ), np . argmax ( all_oof_scores ) return best_oof_metric_score , best_oof_index def calculate_best_score_over_weight_interval ( weight_interval : float , model_i_oof : np . ndarray , model_j_oof : np . ndarray , y_trues : np . ndarray , performance_metric : Callable , running_best_score : float , running_best_weight : float , patience : int , ) -> Tuple [ float , float ]: \"\"\"Calculate the best score over a weight interval. Args: weight_interval (float): _description_ model_i_oof (np.ndarray): _description_ model_j_oof (np.ndarray): _description_ y_trues (np.ndarray): _description_ performance_metric (Callable): _description_ running_best_score (float): _description_ running_best_weight (float): _description_ patience (int): _description_ Returns: Tuple[float, float]: _description_ \"\"\" patience_counter = 0 for weight in range ( weight_interval ): temp_weight = weight / weight_interval temp_ensemble_oof_preds = ( temp_weight * model_j_oof + ( 1 - temp_weight ) * model_i_oof ) temp_ensemble_oof_score = performance_metric ( y_trues , temp_ensemble_oof_preds , num_target_cols = 1 , multilabel = False , ) # in the first loop, if any of the blending is more than best_oof_metric_score, we will assign it to running_best_score. if temp_ensemble_oof_score > running_best_score : running_best_score = temp_ensemble_oof_score running_best_weight = temp_weight else : patience_counter += 1 if patience_counter > patience : break return running_best_score , running_best_weight def get_blended_oof ( initial_best_model_oof , best_oof_index_list , best_weights_list ): # can be used on both oof and subs curr_model_oof = initial_best_model_oof for index , _ in enumerate ( best_oof_index_list [ 1 :]): model_j_index = best_oof_index_list [ index + 1 ] curr_model_oof = ( 1 - best_weights_list [ index ]) * curr_model_oof + ( best_weights_list [ index ] ) * all_oof_preds [:, model_j_index ] . reshape ( - 1 , 1 ) return curr_model_oof def return_list_of_dataframes ( path : str , is_oof : bool = True ) -> Tuple [ List [ pd . DataFrame ], int ]: \"\"\"Return a list of dataframes from a directory of files. The boolean is_oof is used to determine whether the list of dataframes contains oof or subs. Args: path (str): The path to the directory containing the files. is_oof (bool, optional): Determine whether the list of dataframes contains oof or subs. Defaults to True. Returns: List[pd.DataFrame]: The list of dataframes for either oof or subs. int: The number of files in the directory. \"\"\" oof_and_subs_files = os . listdir ( path ) if is_oof : oof_files_sorted = np . sort ( [ f for f in oof_and_subs_files if \"oof\" in f ] ) return [ pd . read_csv ( os . path . join ( path , k )) for k in oof_files_sorted ], len ( oof_files_sorted ) else : sub_files_sorted = np . sort ( [ f for f in oof_and_subs_files if \"sub\" in f ] ) return [ pd . read_csv ( os . path . join ( path , k )) for k in sub_files_sorted ], len ( sub_files_sorted ) if __name__ == \"__main__\" : oof_and_subs_path = \"./oof_and_subs/melanoma\" # [\"oof_1.csv\", \"sub_1.csv\", \"oof_2.csv\", \"sub_2.csv\"] oof_and_subs_files = os . listdir ( oof_and_subs_path ) # [\"oof_1.csv\", \"oof_2.csv\", \"sub_1.csv\", \"sub_2.csv\"] sorted oof_files_sorted = np . sort ([ f for f in oof_and_subs_files if \"oof\" in f ]) # [oof_1_df, oof_2_df, sub_1_df, sub_2_df] in dataframe oof_dfs_list = [ pd . read_csv ( os . path . join ( oof_and_subs_path , k )) for k in oof_files_sorted ] num_oofs = len ( oof_dfs_list ) # in my oof files, I also saved the corresponding y_trues, we thus take the first oof_df and use it to get the y_trues, assuming they are the same for all oof files. # note of caution, if you use different resampling methods, you will need to change the y_trues accordingly. y_trues_df = oof_dfs_list [ 0 ][[ \"oof_trues\" ]] y_trues = y_trues_df . values print ( f \"We have { len ( oof_files_sorted ) } oof files. \\n \" ) target_cols = [ \"oof_trues\" ] pred_cols = [ \"class_1_oof\" ] num_target_cols = len ( target_cols ) all_oof_preds = stack_oofs ( oof_dfs = oof_dfs_list , pred_column_names = pred_cols ) print ( f \"all_oof_preds shape: { all_oof_preds . shape } \\n This variable is global and holds all oof predictions stacked horizontally. \\n \" ) best_oof_metric_score , best_oof_index = compute_best_oof ( all_oof_preds = all_oof_preds , y_trues = y_trues , num_oofs = num_oofs , performance_metric = macro_multilabel_auc , ) print ( f \" \\n ### Computing Best OOF scores among all models ### \\n The best OOF AUC score is { best_oof_metric_score } and the best model index is { best_oof_index } corresponding to the oof file { oof_files_sorted [ best_oof_index ] } \" ) weight_interval = 1000 # 200 patience = 20 # 10 min_increase = 0.0003 # 0.00003 print ( f \" \\n ### HyperParameters ### \\n weight_interval = { weight_interval } \\n patience = { patience } \\n min_increase = { min_increase } \\n \" ) # keep track of oof index that are blended best_oof_index_list = [ best_oof_index ] best_weights_list = [] print ( f \"Current Tracked Model List: { best_oof_index_list } \" ) print ( f \"Current Weights List: { best_weights_list } \" ) counter = 0 # Initially, this curr_model_oof is the single best model that we got above from [oof_1, oof_2,...] initial_best_model_oof = all_oof_preds [:, best_oof_index ] . reshape ( - 1 , 1 ) old_best_score = best_oof_metric_score model_i_best_score , model_i_index , model_i_weights = 0 , 0 , 0 print ( \"Denote model i as the current model, and model j as the model that we are blending with.\" ) for outer_oof_index in range ( num_oofs ): # basically in the first loop, we already know the current model's oof and we assign it by subsetting the all_oof_preds with the best oof index. curr_model_oof = initial_best_model_oof if counter > 0 : curr_model_oof = get_blended_oof ( initial_best_model_oof , best_oof_index_list , best_weights_list ) print ( curr_model_oof ) for inner_oof_index in range ( num_oofs ): # If we have [oof_1, oof_2] and best_oof_index = 1 (oof_2), then we do not need to blend oof_2 and itself. if inner_oof_index in best_oof_index_list : continue # in the first loop, our running_best_score is the best_oof_metric_score # also our old_best_score is the best_oof_metric_score in the first loop ( running_best_score , running_best_weight , patience_counter ,) = ( 0 , 0 , 0 , ) # what we are doing here is to find the best oof score among all models that we have not blended yet. # for example, if we have [oof_1, oof_2, oof_3], and we know oof_2 is our initial_best_model_oof, # then we need to blend oof_2 with oof_1, then oof_2 with oof_3 to find out which of them yields the best overall oof when blended. ( running_best_score , running_best_weight , ) = calculate_best_score_over_weight_interval ( weight_interval , curr_model_oof , all_oof_preds [:, inner_oof_index ] . reshape ( - 1 , 1 ), y_trues , macro_multilabel_auc , running_best_score , running_best_weight , patience , ) if running_best_score > model_i_best_score : model_i_index = inner_oof_index model_i_best_score = running_best_score model_i_weights = running_best_weight increment = model_i_best_score - old_best_score if increment <= min_increase : print ( \"Increment is too small, stop blending\" ) break # DISPLAY RESULTS print () print ( \"Ensemble AUC = %.4f after adding model %i with weight %.3f . Increase of %.4f \" % ( model_i_best_score , model_i_index , model_i_weights , increment , ) ) print () old_best_score = model_i_best_score best_oof_index_list . append ( model_i_index ) best_weights_list . append ( model_i_weights ) print ( f \"Current Tracked Model List: { best_oof_index_list } \" ) print ( f \"Current Weights List: { best_weights_list } \" ) print ( f \"Current Best Score: { model_i_best_score } \" ) counter += 1 plt . hist ( curr_model_oof , bins = 100 ) plt . title ( \"Ensemble OOF predictions\" ) plt . show () # apply on submission sub_files_sorted = np . sort ([ f for f in oof_and_subs_files if \"sub\" in f ]) sub_dfs_list = [ pd . read_csv ( os . path . join ( oof_and_subs_path , k )) for k in sub_files_sorted ] print ( f \" \\n We have { len ( sub_files_sorted ) } submission files...\" ) print () print ( sub_files_sorted ) y = np . zeros (( len ( sub_dfs_list [ 0 ]), len ( sub_files_sorted ) * len ( pred_cols ))) print ( y . shape ) for k in range ( len ( sub_files_sorted )): y [ :, int ( k * len ( pred_cols )) : int (( k + 1 ) * len ( pred_cols )) ] = sub_dfs_list [ k ][ \"target\" ] . values . reshape ( - 1 , 1 ) print ( y ) md2 = y [ :, int ( best_oof_index_list [ 0 ] * len ( pred_cols )) : int ( ( best_oof_index_list [ 0 ] + 1 ) * len ( pred_cols ) ), ] print ( md2 ) for i , k in enumerate ( best_oof_index_list [ 1 :]): md2 = ( best_weights_list [ i ] * y [:, int ( k * len ( pred_cols )) : int (( k + 1 ) * len ( pred_cols ))] + ( 1 - best_weights_list [ i ]) * md2 ) plt . hist ( md2 , bins = 100 ) plt . show () df = sub_dfs_list [ 0 ] . copy () df [[ \"target\" ]] = md2 df . to_csv ( \"submission.csv\" , index = False ) df . head () We have 6 oof files. all_oof_preds shape: (33126, 6) This variable is global and holds all oof predictions stacked horizontally. Model 0 has OOF AUC = 0.8967598406021975 Model 1 has OOF AUC = 0.897338308007439 Model 2 has OOF AUC = 0.8969099101014242 Model 3 has OOF AUC = 0.8997782002268091 Model 4 has OOF AUC = 0.9021602904318382 Model 5 has OOF AUC = 0.9035525112752076 ### Computing Best OOF scores among all models ### The best OOF AUC score is 0.9035525112752076 and the best model index is 5 corresponding to the oof file oof_tf_efficientnet_b2_ns_tf_efficientnet_b2_ns_5_folds_3c0odinh.csv ### HyperParameters ### weight_interval = 1000 patience = 20 min_increase = 0.0003 Current Tracked Model List: [5] Current Weights List: [] Denote model i as the current model, and model j as the model that we are blending with. Ensemble AUC = 0.9141 after adding model 2 with weight 0.452. Increase of 0.0106 Current Tracked Model List: [5, 2] Current Weights List: [0.452] Current Best Score: 0.9141475652539226 [[4.50905440e-03] [3.01906102e-05] [1.43342172e-03] ... [6.48897682e-05] [1.05257292e-02] [9.18150437e-03]] Ensemble AUC = 0.9175 after adding model 3 with weight 0.290. Increase of 0.0034 Current Tracked Model List: [5, 2, 3] Current Weights List: [0.452, 0.29] Current Best Score: 0.9175226556534317 [[3.58108299e-03] [2.45987573e-05] [1.03199503e-03] ... [4.75034868e-05] [9.02447451e-03] [6.67390628e-03]] Ensemble AUC = 0.9193 after adding model 4 with weight 0.273. Increase of 0.0018 Current Tracked Model List: [5, 2, 3, 4] Current Weights List: [0.452, 0.29, 0.273] Current Best Score: 0.9193341713090691 [[2.65875346e-03] [2.51985486e-05] [8.82538860e-04] ... [4.77394006e-05] [1.76098008e-02] [5.30504798e-03]] Ensemble AUC = 0.9201 after adding model 0 with weight 0.161. Increase of 0.0008 Current Tracked Model List: [5, 2, 3, 4, 0] Current Weights List: [0.452, 0.29, 0.273, 0.161] Current Best Score: 0.9201217730848145 [[2.68508770e-03] [3.33216799e-05] [7.54874316e-04] ... [1.12230140e-04] [2.00920893e-02] [1.02248122e-02]] Increment is too small, stop blending We have 6 submission files... ['sub_tf_efficientnet_b0_ns_tf_efficientnet_b0_ns_5_folds_2dcluilj.csv' 'sub_tf_efficientnet_b0_ns_tf_efficientnet_b0_ns_5_folds_3725vib5.csv' 'sub_tf_efficientnet_b0_ns_tf_efficientnet_b0_ns_5_folds_kh6lm0mc.csv' 'sub_tf_efficientnet_b1_ns_tf_efficientnet_b1_ns_5_folds_9qhxwbbq.csv' 'sub_tf_efficientnet_b2_ns_tf_efficientnet_b2_ns_5_folds_1lvjyja0.csv' 'sub_tf_efficientnet_b2_ns_tf_efficientnet_b2_ns_5_folds_3c0odinh.csv'] (10982, 6) [[1.10712990e-03 4.36813570e-04 5.27769700e-04 5.15609340e-04 7.12666400e-04 4.17358470e-04] [2.20565100e-04 1.78343150e-04 9.28863160e-05 1.78823610e-05 1.48618330e-04 3.16100000e-05] [1.81065010e-04 7.23641860e-05 2.92848130e-04 7.86881400e-05 9.41391550e-05 9.77370900e-05] ... [6.67065500e-02 1.14237880e-01 1.26132040e-01 1.13317326e-01 6.87497260e-02 7.28674750e-02] [6.35771550e-03 6.06310670e-04 8.84217040e-04 9.36840800e-04 5.04250900e-04 5.87228570e-05] [1.41756660e-02 2.95386670e-02 1.08320400e-02 2.21859530e-02 1.42423960e-02 9.72992800e-02]] [[4.1735847e-04] [3.1610000e-05] [9.7737090e-05] ... [7.2867475e-02] [5.8722857e-05] [9.7299280e-02]]","title":"Forward Ensemble with SIIM-ISIC Melanoma Classification"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/ensemble_theory/out_of_folds_predictions/","text":"","title":"Out of folds predictions"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/fundamentals/activation_functions/Activation_Functions/","text":"Regularization https://cs231n.github.io/neural-networks-1/ https://machinelearningmastery.com/activation-regularization-for-reducing-generalization-error-in-deep-learning-neural-networks/ CS231N: In the diagram above, we can see that Neural Networks with more neurons can express more complicated functions. However, this is both a blessing (since we can learn to classify more complicated data) and a curse (since it is easier to overfit the training data). Overfitting occurs when a model with high capacity fits the noise in the data instead of the (assumed) underlying relationship. For example, the model with 20 hidden neurons fits all the training data but at the cost of segmenting the space into many disjoint red and green decision regions. The model with 3 hidden neurons only has the representational power to classify the data in broad strokes. It models the data as two blobs and interprets the few red points inside the green cluster as outliers (noise). In practice, this could lead to better generalization on the test set. Softmax The softmax function takes as input a vector \\(z\\) of \\(K\\) real numbers, and normalizes it into a probability distribution consisting of \\(K\\) probabilities proportional to the exponentials of the input numbers. That is, prior to applying softmax, some vector components could be negative, or greater than one; and might not sum to 1; but after applying softmax, each component will be in the interval \\([0, 1]\\) and the components will add up to 1, so that they can be interpreted as probabilities. Furthermore, the larger input components will correspond to larger probabilities. One key difference is softmax takes in a vector of inputs, while sigmoid can only take in one real value. Sigmoid The sigmoid function takes as input a real value and output one real value as well. In Binary classification case, with class 0 and 1, we only need one output neuron (positive class neuron), and when applied sigmoid will get a number between 0 and 1, say \\(p^{+}\\) , then \\(p^{-} = 1 - p^{+}\\) . However the catch is that sigmoid in Binarcy Classification setting works just like softmax, but not when in multi-label! One key difference is softmax takes in a vector of inputs, while sigmoid can only take in one real value. Softmax vs Sigmoid Sigmoid vs Softmax I've noticed people often get directed to this question when searching whether to use sigmoid vs softmax in neural networks. If you are one of those people building a neural network classifier, here is how to decide whether to apply sigmoid or softmax to the raw output values from your network: If you have a multi-label classification problem = there is more than one \"right answer\" = the outputs are NOT mutually exclusive, then use a sigmoid function on each raw output independently. The sigmoid will allow you to have high probability for all of your classes, some of them, or none of them. Example: classifying diseases in a chest x-ray image. The image might contain pneumonia, emphysema, and/or cancer, or none of those findings. If you have a multi-class classification problem = there is only one \"right answer\" = the outputs are mutually exclusive, then use a softmax function. The softmax will enforce that the sum of the probabilities of your output classes are equal to one, so in order to increase the probability of a particular class, your model must correspondingly decrease the probability of at least one of the other classes. Example: classifying images from the MNIST data set of handwritten digits. A single picture of a digit has only one true identity - the picture cannot be a 7 and an 8 at the same time. More reading ReLU Properties https://medium.com/@kanchansarkar/relu-not-a-differentiable-function-why-used-in-gradient-based-optimization-7fef3a4cecec \\(g(z) = \\max(0,z)\\) Differentiable over all points except \\(z = 0\\) . Swish https://stats.stackexchange.com/questions/544739/why-does-being-bounded-below-in-swish-reduces-overfitting Indeed relu is also bounded below, they didn't claim otherwise. The difference is, that swish allows small negative values for small negative inputs, which according to them, increases expressivity and improve gradient flow. The reason behind improving generalization is that, as in regularization, small, approaching zero, weights improve generalization as the function become more smooth and it reduces the effect of fitting the noise. They claim that by bounding large negative vales in the activation function, the effect is that the network \"forgets\" large negative inputs and thus helping the weights to approach to zero. See the image they added, large negative values, which are common before training are forgotten and after training the negative scale is much smaller. There is a tradeoff between bounded which improve generaliztion and unbounded that avoids saturation of gradients, and help the network to stay in the linear regime. Properties When we design or choose an activation function, we need to ensure the follows: (Smoothness) Differentiable and Continuous: For example, the sigmoid function is continuous and hence differentiable. If the property is not fulfilled, we might face issues as backpropagation may not be performed properly since we cannot differentiate it.If you notice, the heaviside function is not. We cant perform GD using the HF as we cannot compute gradients but for the logistic function we can. The gradient of sigmoid function g is g(1-g) conveniently Monotonic: This helps the model to converge faster. But spoiler alert, Swish is not monotonic. The properties of Swish are as follows: Bounded below: It is claimed in the paper it serves as a strong regularization. Smoothness: More smooth than ReLU which allows the model to optimize better, the error landscape, when smoothed, is easier to traverse in order to find a minima. An intuitive idea is the hill again, imagine you traverse down Bukit Timah Hill, vs traversing down Mount Himalaya LOL!!! Let us see how swish looks like when plotted. # Import matplotlib, numpy and math import matplotlib.pyplot as plt import numpy as np import math def swish ( x ): sigmoid = 1 / ( 1 + np . exp ( - x )) swish = x * sigmoid return swish epsilon = 1e-20 x = np . linspace ( - 10 , 10 , 10 ) z = swish ( x ) print ( f \"x= { x } \" ) print ( f \" \\n z=swish(x)= { z } \" ) print ( f \" \\n min z = { min ( z ) } \" ) x=[-10. -7.77777778 -5.55555556 -3.33333333 -1.11111111 1.11111111 3.33333333 5.55555556 7.77777778 10. ] z=swish(x)=[-4.53978687e-04 -3.25707421e-03 -2.13946242e-02 -1.14817319e-01 -2.75182001e-01 8.35929110e-01 3.21851601e+00 5.53416093e+00 7.77452070e+00 9.99954602e+00] min z = -0.27518200126563513 plt . plot ( x , z ) plt . xlabel ( \"x\" ) plt . ylabel ( \"Swish(X)\" ) plt . show ();","title":"Regularization"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/fundamentals/activation_functions/Activation_Functions/#regularization","text":"https://cs231n.github.io/neural-networks-1/ https://machinelearningmastery.com/activation-regularization-for-reducing-generalization-error-in-deep-learning-neural-networks/ CS231N: In the diagram above, we can see that Neural Networks with more neurons can express more complicated functions. However, this is both a blessing (since we can learn to classify more complicated data) and a curse (since it is easier to overfit the training data). Overfitting occurs when a model with high capacity fits the noise in the data instead of the (assumed) underlying relationship. For example, the model with 20 hidden neurons fits all the training data but at the cost of segmenting the space into many disjoint red and green decision regions. The model with 3 hidden neurons only has the representational power to classify the data in broad strokes. It models the data as two blobs and interprets the few red points inside the green cluster as outliers (noise). In practice, this could lead to better generalization on the test set.","title":"Regularization"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/fundamentals/activation_functions/Activation_Functions/#softmax","text":"The softmax function takes as input a vector \\(z\\) of \\(K\\) real numbers, and normalizes it into a probability distribution consisting of \\(K\\) probabilities proportional to the exponentials of the input numbers. That is, prior to applying softmax, some vector components could be negative, or greater than one; and might not sum to 1; but after applying softmax, each component will be in the interval \\([0, 1]\\) and the components will add up to 1, so that they can be interpreted as probabilities. Furthermore, the larger input components will correspond to larger probabilities. One key difference is softmax takes in a vector of inputs, while sigmoid can only take in one real value.","title":"Softmax"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/fundamentals/activation_functions/Activation_Functions/#sigmoid","text":"The sigmoid function takes as input a real value and output one real value as well. In Binary classification case, with class 0 and 1, we only need one output neuron (positive class neuron), and when applied sigmoid will get a number between 0 and 1, say \\(p^{+}\\) , then \\(p^{-} = 1 - p^{+}\\) . However the catch is that sigmoid in Binarcy Classification setting works just like softmax, but not when in multi-label! One key difference is softmax takes in a vector of inputs, while sigmoid can only take in one real value.","title":"Sigmoid"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/fundamentals/activation_functions/Activation_Functions/#softmax-vs-sigmoid","text":"Sigmoid vs Softmax I've noticed people often get directed to this question when searching whether to use sigmoid vs softmax in neural networks. If you are one of those people building a neural network classifier, here is how to decide whether to apply sigmoid or softmax to the raw output values from your network: If you have a multi-label classification problem = there is more than one \"right answer\" = the outputs are NOT mutually exclusive, then use a sigmoid function on each raw output independently. The sigmoid will allow you to have high probability for all of your classes, some of them, or none of them. Example: classifying diseases in a chest x-ray image. The image might contain pneumonia, emphysema, and/or cancer, or none of those findings. If you have a multi-class classification problem = there is only one \"right answer\" = the outputs are mutually exclusive, then use a softmax function. The softmax will enforce that the sum of the probabilities of your output classes are equal to one, so in order to increase the probability of a particular class, your model must correspondingly decrease the probability of at least one of the other classes. Example: classifying images from the MNIST data set of handwritten digits. A single picture of a digit has only one true identity - the picture cannot be a 7 and an 8 at the same time. More reading","title":"Softmax vs Sigmoid"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/fundamentals/activation_functions/Activation_Functions/#relu","text":"","title":"ReLU"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/fundamentals/activation_functions/Activation_Functions/#properties","text":"https://medium.com/@kanchansarkar/relu-not-a-differentiable-function-why-used-in-gradient-based-optimization-7fef3a4cecec \\(g(z) = \\max(0,z)\\) Differentiable over all points except \\(z = 0\\) .","title":"Properties"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/fundamentals/activation_functions/Activation_Functions/#swish","text":"https://stats.stackexchange.com/questions/544739/why-does-being-bounded-below-in-swish-reduces-overfitting Indeed relu is also bounded below, they didn't claim otherwise. The difference is, that swish allows small negative values for small negative inputs, which according to them, increases expressivity and improve gradient flow. The reason behind improving generalization is that, as in regularization, small, approaching zero, weights improve generalization as the function become more smooth and it reduces the effect of fitting the noise. They claim that by bounding large negative vales in the activation function, the effect is that the network \"forgets\" large negative inputs and thus helping the weights to approach to zero. See the image they added, large negative values, which are common before training are forgotten and after training the negative scale is much smaller. There is a tradeoff between bounded which improve generaliztion and unbounded that avoids saturation of gradients, and help the network to stay in the linear regime.","title":"Swish"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/fundamentals/activation_functions/Activation_Functions/#properties_1","text":"When we design or choose an activation function, we need to ensure the follows: (Smoothness) Differentiable and Continuous: For example, the sigmoid function is continuous and hence differentiable. If the property is not fulfilled, we might face issues as backpropagation may not be performed properly since we cannot differentiate it.If you notice, the heaviside function is not. We cant perform GD using the HF as we cannot compute gradients but for the logistic function we can. The gradient of sigmoid function g is g(1-g) conveniently Monotonic: This helps the model to converge faster. But spoiler alert, Swish is not monotonic. The properties of Swish are as follows: Bounded below: It is claimed in the paper it serves as a strong regularization. Smoothness: More smooth than ReLU which allows the model to optimize better, the error landscape, when smoothed, is easier to traverse in order to find a minima. An intuitive idea is the hill again, imagine you traverse down Bukit Timah Hill, vs traversing down Mount Himalaya LOL!!! Let us see how swish looks like when plotted. # Import matplotlib, numpy and math import matplotlib.pyplot as plt import numpy as np import math def swish ( x ): sigmoid = 1 / ( 1 + np . exp ( - x )) swish = x * sigmoid return swish epsilon = 1e-20 x = np . linspace ( - 10 , 10 , 10 ) z = swish ( x ) print ( f \"x= { x } \" ) print ( f \" \\n z=swish(x)= { z } \" ) print ( f \" \\n min z = { min ( z ) } \" ) x=[-10. -7.77777778 -5.55555556 -3.33333333 -1.11111111 1.11111111 3.33333333 5.55555556 7.77777778 10. ] z=swish(x)=[-4.53978687e-04 -3.25707421e-03 -2.13946242e-02 -1.14817319e-01 -2.75182001e-01 8.35929110e-01 3.21851601e+00 5.53416093e+00 7.77452070e+00 9.99954602e+00] min z = -0.27518200126563513 plt . plot ( x , z ) plt . xlabel ( \"x\" ) plt . ylabel ( \"Swish(X)\" ) plt . show ();","title":"Properties"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/fundamentals/learning_rate_schedulers/pytorch_gradual_warmup_scheduler/notebook/guide-to-pytorch-learning-rate-scheduling/","text":"# This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # For example, here's several helpful packages to load import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the read-only \"../input/\" directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ): for filename in filenames : print ( os . path . join ( dirname , filename )) # You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session import torch import matplotlib.pyplot as plt 1. LAMBDA LR Sets the learning rate of each parameter group to the initial lr times a given function. When last_epoch=-1, sets initial lr as lr. \\[ l r_{\\text {epoch}} = l r_{\\text {initial}} * Lambda(epoch) \\] model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 100 ) lambda1 = lambda epoch : 0.65 ** epoch scheduler = torch . optim . lr_scheduler . LambdaLR ( optimizer , lr_lambda = lambda1 ) lrs = [] for i in range ( 10 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \", round(0.65 ** i,3),\" , Learning Rate = \",round(optimizer.param_groups[0][\"lr\"],3)) scheduler . step () plt . plot ( range ( 10 ), lrs ) [<matplotlib.lines.Line2D at 0x7f53451ba510>] 2. MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. When last_epoch=-1, sets initial lr as lr. \\[ l r_{\\text {epoch}} = l r_{\\text {epoch - 1}} * Lambda(epoch) \\] model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 100 ) lmbda = lambda epoch : 0.65 ** epoch scheduler = torch . optim . lr_scheduler . MultiplicativeLR ( optimizer , lr_lambda = lmbda ) lrs = [] for i in range ( 10 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",0.95,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( range ( 10 ), lrs ) [<matplotlib.lines.Line2D at 0x7f53450df590>] 3. StepLR Decays the learning rate of each parameter group by gamma every step_size epochs. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr. \\[ l r_{\\text {epoch}}=\\left\\{\\begin{array}{ll} Gamma * l r_{\\text {epoch - 1}}, & \\text { if } {\\text {epoch % step_size}}=0 \\\\ l r_{\\text {epoch - 1}}, & \\text { otherwise } \\end{array}\\right. \\] model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 100 ) scheduler = torch . optim . lr_scheduler . StepLR ( optimizer , step_size = 2 , gamma = 0.1 ) lrs = [] for i in range ( 10 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",0.1 if i!=0 and i%2!=0 else 1,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( range ( 10 ), lrs ) [<matplotlib.lines.Line2D at 0x7f534505af10>] 4. MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr. \\[ l r_{\\text {epoch}}=\\left\\{\\begin{array}{ll} Gamma * l r_{\\text {epoch - 1}}, & \\text { if } {\\text{ epoch in [milestones]}} \\\\ l r_{\\text {epoch - 1}}, & \\text { otherwise } \\end{array}\\right. \\] model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 100 ) scheduler = torch . optim . lr_scheduler . MultiStepLR ( optimizer , milestones = [ 6 , 8 , 9 ], gamma = 0.1 ) lrs = [] for i in range ( 10 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",0.1 if i in [6,8,9] else 1,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( range ( 10 ), lrs ) [<matplotlib.lines.Line2D at 0x7f5344fc5ad0>] 5. ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. When last_epoch=-1, sets initial lr as lr. \\[ l r_{\\text {epoch}}= Gamma * l r_{\\text {epoch - 1}} \\] model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 100 ) scheduler = torch . optim . lr_scheduler . ExponentialLR ( optimizer , gamma = 0.1 ) lrs = [] for i in range ( 10 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",0.1,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f5344fad990>] 6. CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule. When last_epoch=-1, sets initial lr as lr. Notice that because the schedule is defined recursively, the learning rate can be simultaneously modified outside this scheduler by other operators. If the learning rate is set solely by this scheduler, the learning rate at each step becomes: \\[ \\eta_{t}=\\eta_{\\min }+\\frac{1}{2}\\left(\\eta_{\\max }-\\eta_{\\min }\\right)\\left(1+\\cos \\left(\\frac{T_{c u r}}{T_{\\max }} \\pi\\right)\\right) \\] It has been proposed in SGDR: Stochastic Gradient Descent with Warm Restarts. Note that this only implements the cosine annealing part of SGDR, and not the restarts.https://arxiv.org/abs/1608.03983 model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 100 ) scheduler = torch . optim . lr_scheduler . CosineAnnealingLR ( optimizer , T_max = 10 , eta_min = 0 ) lrs = [] for i in range ( 100 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f534503ce90>] 7. CyclicLR - triangular model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 0.1 ) scheduler = torch . optim . lr_scheduler . CyclicLR ( optimizer , base_lr = 0.001 , max_lr = 0.1 , step_size_up = 5 , mode = \"triangular\" ) lrs = [] for i in range ( 100 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f5344e82a10>] 7. CyclicLR - triangular2 model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 0.1 ) scheduler = torch . optim . lr_scheduler . CyclicLR ( optimizer , base_lr = 0.001 , max_lr = 0.1 , step_size_up = 5 , mode = \"triangular2\" ) lrs = [] for i in range ( 100 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f5344e16690>] 7. CyclicLR - exp_range model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 100 ) scheduler = torch . optim . lr_scheduler . CyclicLR ( optimizer , base_lr = 0.001 , max_lr = 0.1 , step_size_up = 5 , mode = \"exp_range\" , gamma = 0.85 ) lrs = [] for i in range ( 100 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f5344dd4d90>] 8.OneCycleLR - cos Sets the learning rate of each parameter group according to the 1cycle learning rate policy. The 1cycle policy anneals the learning rate from an initial learning rate to some maximum learning rate and then from that maximum learning rate to some minimum learning rate much lower than the initial learning rate. This policy was initially described in the paper Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates. The 1cycle learning rate policy changes the learning rate after every batch. step should be called after a batch has been used for training. This scheduler is not chainable. model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 0.1 ) scheduler = torch . optim . lr_scheduler . OneCycleLR ( optimizer , max_lr = 0.1 , steps_per_epoch = 10 , epochs = 10 ) lrs = [] for i in range ( 100 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f5344d64c50>] 8.OneCycleLR - linear model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 0.1 ) scheduler = torch . optim . lr_scheduler . OneCycleLR ( optimizer , max_lr = 0.1 , steps_per_epoch = 10 , epochs = 10 , anneal_strategy = 'linear' ) lrs = [] for i in range ( 100 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f5344d283d0>] 9.CosineAnnealingWarmRestarts Set the learning rate of each parameter group using a cosine annealing schedule, and restarts after Ti epochs. \\[ \\eta_{t}=\\eta_{\\min }+\\frac{1}{2}\\left(\\eta_{\\max }-\\eta_{\\min }\\right)\\left(1+\\cos \\left(\\frac{T_{\\operatorname{cur}}}{T_{i}} \\pi\\right)\\right) \\] import torch import matplotlib.pyplot as plt model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 0.1 ) lr_sched = torch . optim . lr_scheduler . CosineAnnealingWarmRestarts ( optimizer , T_0 = 10 , T_mult = 1 , eta_min = 0.001 , last_epoch =- 1 ) lrs = [] for i in range ( 100 ): lr_sched . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ] ) plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f5344c93ad0>] import torch import matplotlib.pyplot as plt model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 0.1 ) lr_sched = torch . optim . lr_scheduler . CosineAnnealingWarmRestarts ( optimizer , T_0 = 10 , T_mult = 2 , eta_min = 0.01 , last_epoch =- 1 ) lrs = [] for i in range ( 300 ): lr_sched . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ] ) plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f5344bffa10>]","title":"Guide to pytorch learning rate scheduling"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/fundamentals/learning_rate_schedulers/pytorch_gradual_warmup_scheduler/notebook/guide-to-pytorch-learning-rate-scheduling/#1-lambda-lr","text":"Sets the learning rate of each parameter group to the initial lr times a given function. When last_epoch=-1, sets initial lr as lr. \\[ l r_{\\text {epoch}} = l r_{\\text {initial}} * Lambda(epoch) \\] model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 100 ) lambda1 = lambda epoch : 0.65 ** epoch scheduler = torch . optim . lr_scheduler . LambdaLR ( optimizer , lr_lambda = lambda1 ) lrs = [] for i in range ( 10 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \", round(0.65 ** i,3),\" , Learning Rate = \",round(optimizer.param_groups[0][\"lr\"],3)) scheduler . step () plt . plot ( range ( 10 ), lrs ) [<matplotlib.lines.Line2D at 0x7f53451ba510>]","title":"1. LAMBDA LR"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/fundamentals/learning_rate_schedulers/pytorch_gradual_warmup_scheduler/notebook/guide-to-pytorch-learning-rate-scheduling/#2-multiplicativelr","text":"Multiply the learning rate of each parameter group by the factor given in the specified function. When last_epoch=-1, sets initial lr as lr. \\[ l r_{\\text {epoch}} = l r_{\\text {epoch - 1}} * Lambda(epoch) \\] model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 100 ) lmbda = lambda epoch : 0.65 ** epoch scheduler = torch . optim . lr_scheduler . MultiplicativeLR ( optimizer , lr_lambda = lmbda ) lrs = [] for i in range ( 10 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",0.95,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( range ( 10 ), lrs ) [<matplotlib.lines.Line2D at 0x7f53450df590>]","title":"2. MultiplicativeLR"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/fundamentals/learning_rate_schedulers/pytorch_gradual_warmup_scheduler/notebook/guide-to-pytorch-learning-rate-scheduling/#3-steplr","text":"Decays the learning rate of each parameter group by gamma every step_size epochs. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr. \\[ l r_{\\text {epoch}}=\\left\\{\\begin{array}{ll} Gamma * l r_{\\text {epoch - 1}}, & \\text { if } {\\text {epoch % step_size}}=0 \\\\ l r_{\\text {epoch - 1}}, & \\text { otherwise } \\end{array}\\right. \\] model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 100 ) scheduler = torch . optim . lr_scheduler . StepLR ( optimizer , step_size = 2 , gamma = 0.1 ) lrs = [] for i in range ( 10 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",0.1 if i!=0 and i%2!=0 else 1,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( range ( 10 ), lrs ) [<matplotlib.lines.Line2D at 0x7f534505af10>]","title":"3. StepLR"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/fundamentals/learning_rate_schedulers/pytorch_gradual_warmup_scheduler/notebook/guide-to-pytorch-learning-rate-scheduling/#4-multisteplr","text":"Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr. \\[ l r_{\\text {epoch}}=\\left\\{\\begin{array}{ll} Gamma * l r_{\\text {epoch - 1}}, & \\text { if } {\\text{ epoch in [milestones]}} \\\\ l r_{\\text {epoch - 1}}, & \\text { otherwise } \\end{array}\\right. \\] model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 100 ) scheduler = torch . optim . lr_scheduler . MultiStepLR ( optimizer , milestones = [ 6 , 8 , 9 ], gamma = 0.1 ) lrs = [] for i in range ( 10 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",0.1 if i in [6,8,9] else 1,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( range ( 10 ), lrs ) [<matplotlib.lines.Line2D at 0x7f5344fc5ad0>]","title":"4. MultiStepLR"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/fundamentals/learning_rate_schedulers/pytorch_gradual_warmup_scheduler/notebook/guide-to-pytorch-learning-rate-scheduling/#5-exponentiallr","text":"Decays the learning rate of each parameter group by gamma every epoch. When last_epoch=-1, sets initial lr as lr. \\[ l r_{\\text {epoch}}= Gamma * l r_{\\text {epoch - 1}} \\] model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 100 ) scheduler = torch . optim . lr_scheduler . ExponentialLR ( optimizer , gamma = 0.1 ) lrs = [] for i in range ( 10 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",0.1,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f5344fad990>]","title":"5. ExponentialLR"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/fundamentals/learning_rate_schedulers/pytorch_gradual_warmup_scheduler/notebook/guide-to-pytorch-learning-rate-scheduling/#6-cosineannealinglr","text":"Set the learning rate of each parameter group using a cosine annealing schedule. When last_epoch=-1, sets initial lr as lr. Notice that because the schedule is defined recursively, the learning rate can be simultaneously modified outside this scheduler by other operators. If the learning rate is set solely by this scheduler, the learning rate at each step becomes: \\[ \\eta_{t}=\\eta_{\\min }+\\frac{1}{2}\\left(\\eta_{\\max }-\\eta_{\\min }\\right)\\left(1+\\cos \\left(\\frac{T_{c u r}}{T_{\\max }} \\pi\\right)\\right) \\] It has been proposed in SGDR: Stochastic Gradient Descent with Warm Restarts. Note that this only implements the cosine annealing part of SGDR, and not the restarts.https://arxiv.org/abs/1608.03983 model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 100 ) scheduler = torch . optim . lr_scheduler . CosineAnnealingLR ( optimizer , T_max = 10 , eta_min = 0 ) lrs = [] for i in range ( 100 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f534503ce90>]","title":"6. CosineAnnealingLR"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/fundamentals/learning_rate_schedulers/pytorch_gradual_warmup_scheduler/notebook/guide-to-pytorch-learning-rate-scheduling/#7-cycliclr-triangular","text":"model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 0.1 ) scheduler = torch . optim . lr_scheduler . CyclicLR ( optimizer , base_lr = 0.001 , max_lr = 0.1 , step_size_up = 5 , mode = \"triangular\" ) lrs = [] for i in range ( 100 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f5344e82a10>]","title":"7. CyclicLR - triangular"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/fundamentals/learning_rate_schedulers/pytorch_gradual_warmup_scheduler/notebook/guide-to-pytorch-learning-rate-scheduling/#7-cycliclr-triangular2","text":"model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 0.1 ) scheduler = torch . optim . lr_scheduler . CyclicLR ( optimizer , base_lr = 0.001 , max_lr = 0.1 , step_size_up = 5 , mode = \"triangular2\" ) lrs = [] for i in range ( 100 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f5344e16690>]","title":"7. CyclicLR - triangular2"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/fundamentals/learning_rate_schedulers/pytorch_gradual_warmup_scheduler/notebook/guide-to-pytorch-learning-rate-scheduling/#7-cycliclr-exp_range","text":"model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 100 ) scheduler = torch . optim . lr_scheduler . CyclicLR ( optimizer , base_lr = 0.001 , max_lr = 0.1 , step_size_up = 5 , mode = \"exp_range\" , gamma = 0.85 ) lrs = [] for i in range ( 100 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f5344dd4d90>]","title":"7. CyclicLR - exp_range"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/fundamentals/learning_rate_schedulers/pytorch_gradual_warmup_scheduler/notebook/guide-to-pytorch-learning-rate-scheduling/#8onecyclelr-cos","text":"Sets the learning rate of each parameter group according to the 1cycle learning rate policy. The 1cycle policy anneals the learning rate from an initial learning rate to some maximum learning rate and then from that maximum learning rate to some minimum learning rate much lower than the initial learning rate. This policy was initially described in the paper Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates. The 1cycle learning rate policy changes the learning rate after every batch. step should be called after a batch has been used for training. This scheduler is not chainable. model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 0.1 ) scheduler = torch . optim . lr_scheduler . OneCycleLR ( optimizer , max_lr = 0.1 , steps_per_epoch = 10 , epochs = 10 ) lrs = [] for i in range ( 100 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f5344d64c50>]","title":"8.OneCycleLR - cos"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/fundamentals/learning_rate_schedulers/pytorch_gradual_warmup_scheduler/notebook/guide-to-pytorch-learning-rate-scheduling/#8onecyclelr-linear","text":"model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 0.1 ) scheduler = torch . optim . lr_scheduler . OneCycleLR ( optimizer , max_lr = 0.1 , steps_per_epoch = 10 , epochs = 10 , anneal_strategy = 'linear' ) lrs = [] for i in range ( 100 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f5344d283d0>]","title":"8.OneCycleLR - linear"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/fundamentals/learning_rate_schedulers/pytorch_gradual_warmup_scheduler/notebook/guide-to-pytorch-learning-rate-scheduling/#9cosineannealingwarmrestarts","text":"Set the learning rate of each parameter group using a cosine annealing schedule, and restarts after Ti epochs. \\[ \\eta_{t}=\\eta_{\\min }+\\frac{1}{2}\\left(\\eta_{\\max }-\\eta_{\\min }\\right)\\left(1+\\cos \\left(\\frac{T_{\\operatorname{cur}}}{T_{i}} \\pi\\right)\\right) \\] import torch import matplotlib.pyplot as plt model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 0.1 ) lr_sched = torch . optim . lr_scheduler . CosineAnnealingWarmRestarts ( optimizer , T_0 = 10 , T_mult = 1 , eta_min = 0.001 , last_epoch =- 1 ) lrs = [] for i in range ( 100 ): lr_sched . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ] ) plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f5344c93ad0>] import torch import matplotlib.pyplot as plt model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 0.1 ) lr_sched = torch . optim . lr_scheduler . CosineAnnealingWarmRestarts ( optimizer , T_0 = 10 , T_mult = 2 , eta_min = 0.01 , last_epoch =- 1 ) lrs = [] for i in range ( 300 ): lr_sched . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ] ) plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f5344bffa10>]","title":"9.CosineAnnealingWarmRestarts"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/fundamentals/learning_rate_schedulers/pytorch_gradual_warmup_scheduler/src/scheduler/","text":"gradual warmup Paper Implementation Here are some parameters in the paper. n = minibatch size k = multiplier of minibatch size For eg, if n = 32, and k = 2, then the new batch size is 64. Intuition: In the first few steps, the LR is set to lower than the base LR, and increased gradually (linearly) to approach it in a number of epochs. Example: - base_lr = 1 - total_epoch (Warmup Epochs) = 10 - [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs] - [1 * (0/10) for [all base_lrs in which case here is just 1]] -> [0],[0.1], [0.2] etc Important Currently, if your warmup epochs is 10, then at both 10 and 11th epoch, the LR will be the target/base learning rate of the optimizer. Ideally, maybe we should start the 11th epoch with the \"stepped\" scheduler already. This is on my To-Do list. To read these known \"issues and confusions\": here , here and here . Intuition of gradual warmup is in the paper, to summarize, instead of going to the base learning rate set in the optimizer, we first have a linear warmup period. Say if your base/initial learning rate is 10, then you want to warmup 10 epochs, so during these first 10 epochs, it will be increasing in a linear fashion from 1, 2, 3... to 10. (I am unsure if we should start from 0 though). bug workaround: if epoch == scheduler.total_epoch: we step the scheduler once more. This is because if we use the settings in this code, then Epoch 10 and 11 will both be having LR = 10, what we want is at epoch 10 it is LR=10, then epoch 11 it already start to use the scheduler's decay, instead of repeating twice. Params last_epoch : This means the previous epoch and not the \"last epoch | not end of training\". Methods scheduler.get_last_lr()[0] to get the current learning rate before stepping. scheduler.get_lr()[0] not to be used to call the current learning rate. See here and here . The above two will cause some confusions, but won't impact the training process, only log file may be confusing. get_lr for StepLR: def get_lr(self): if not self._get_lr_called_within_step: warnings.warn(\"To get the last learning rate computed by the scheduler, \" \"please use `get_last_lr()`.\", UserWarning) if (self.last_epoch == 0) or (self.last_epoch % self.step_size != 0): return [group['lr'] for group in self.optimizer.param_groups] return [group['lr'] * self.gamma for group in self.optimizer.param_groups] This illustrates the issue mentioned here . As an example, since our last_epoch = -1 as default, 1 10 10 2 10 10 3 10 10 4 10 10 5 10 10 6 10 10 7 10 10 8 10 10 9 10 10 10 10 10 11 0.1 1.0 # See the problem? https://stackoverflow.com/questions/59599603/about-pytorch-learning-rate-scheduler 12 1.0 1.0 13 1.0 1.0 14 1.0 1.0 15 1.0 1.0 16 1.0 1.0 17 1.0 1.0 18 1.0 1.0 19 1.0 1.0 20 1.0 1.0 21 0.01 0.1 22 0.1 0.1 23 0.1 0.1 24 0.1 0.1 25 0.1 0.1 26 0.1 0.1 27 0.1 0.1 28 0.1 0.1 29 0.1 0.1 30 0.1 0.1","title":"Scheduler"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/fundamentals/learning_rate_schedulers/pytorch_gradual_warmup_scheduler/src/scheduler/#gradual-warmup","text":"","title":"gradual warmup"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/fundamentals/learning_rate_schedulers/pytorch_gradual_warmup_scheduler/src/scheduler/#paper-implementation","text":"Here are some parameters in the paper. n = minibatch size k = multiplier of minibatch size For eg, if n = 32, and k = 2, then the new batch size is 64. Intuition: In the first few steps, the LR is set to lower than the base LR, and increased gradually (linearly) to approach it in a number of epochs. Example: - base_lr = 1 - total_epoch (Warmup Epochs) = 10 - [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs] - [1 * (0/10) for [all base_lrs in which case here is just 1]] -> [0],[0.1], [0.2] etc Important Currently, if your warmup epochs is 10, then at both 10 and 11th epoch, the LR will be the target/base learning rate of the optimizer. Ideally, maybe we should start the 11th epoch with the \"stepped\" scheduler already. This is on my To-Do list. To read these known \"issues and confusions\": here , here and here . Intuition of gradual warmup is in the paper, to summarize, instead of going to the base learning rate set in the optimizer, we first have a linear warmup period. Say if your base/initial learning rate is 10, then you want to warmup 10 epochs, so during these first 10 epochs, it will be increasing in a linear fashion from 1, 2, 3... to 10. (I am unsure if we should start from 0 though). bug workaround: if epoch == scheduler.total_epoch: we step the scheduler once more. This is because if we use the settings in this code, then Epoch 10 and 11 will both be having LR = 10, what we want is at epoch 10 it is LR=10, then epoch 11 it already start to use the scheduler's decay, instead of repeating twice.","title":"Paper Implementation"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/fundamentals/learning_rate_schedulers/pytorch_gradual_warmup_scheduler/src/scheduler/#params","text":"last_epoch : This means the previous epoch and not the \"last epoch | not end of training\".","title":"Params"},{"location":"reighns_ml_journey/machine_learning_and_deep_learning/fundamentals/learning_rate_schedulers/pytorch_gradual_warmup_scheduler/src/scheduler/#methods","text":"scheduler.get_last_lr()[0] to get the current learning rate before stepping. scheduler.get_lr()[0] not to be used to call the current learning rate. See here and here . The above two will cause some confusions, but won't impact the training process, only log file may be confusing. get_lr for StepLR: def get_lr(self): if not self._get_lr_called_within_step: warnings.warn(\"To get the last learning rate computed by the scheduler, \" \"please use `get_last_lr()`.\", UserWarning) if (self.last_epoch == 0) or (self.last_epoch % self.step_size != 0): return [group['lr'] for group in self.optimizer.param_groups] return [group['lr'] * self.gamma for group in self.optimizer.param_groups] This illustrates the issue mentioned here . As an example, since our last_epoch = -1 as default, 1 10 10 2 10 10 3 10 10 4 10 10 5 10 10 6 10 10 7 10 10 8 10 10 9 10 10 10 10 10 11 0.1 1.0 # See the problem? https://stackoverflow.com/questions/59599603/about-pytorch-learning-rate-scheduler 12 1.0 1.0 13 1.0 1.0 14 1.0 1.0 15 1.0 1.0 16 1.0 1.0 17 1.0 1.0 18 1.0 1.0 19 1.0 1.0 20 1.0 1.0 21 0.01 0.1 22 0.1 0.1 23 0.1 0.1 24 0.1 0.1 25 0.1 0.1 26 0.1 0.1 27 0.1 0.1 28 0.1 0.1 29 0.1 0.1 30 0.1 0.1","title":"Methods"},{"location":"reighns_ml_journey/mathematics/linear_algebra/Linear%20Equations%20%28MacroAnalyst%29/","text":"! pip install vtk == 9.0.1 ipympl mayavi Requirement already satisfied: vtk==9.0.1 in /usr/local/lib/python3.7/dist-packages (9.0.1) Requirement already satisfied: ipympl in /usr/local/lib/python3.7/dist-packages (0.8.5) Requirement already satisfied: mayavi in /usr/local/lib/python3.7/dist-packages (4.7.4) Requirement already satisfied: traitlets<6 in /usr/local/lib/python3.7/dist-packages (from ipympl) (5.1.1) Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from ipympl) (0.2.0) Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ipympl) (1.19.5) Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from ipympl) (7.1.2) Requirement already satisfied: ipywidgets<8,>=7.6.0 in /usr/local/lib/python3.7/dist-packages (from ipympl) (7.6.5) Requirement already satisfied: matplotlib<4,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipympl) (3.2.2) Requirement already satisfied: ipython<8 in /usr/local/lib/python3.7/dist-packages (from ipympl) (5.5.0) Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython<8->ipympl) (57.4.0) Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython<8->ipympl) (1.0.18) Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython<8->ipympl) (4.4.2) Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython<8->ipympl) (0.8.1) Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython<8->ipympl) (0.7.5) Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython<8->ipympl) (4.8.0) Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython<8->ipympl) (2.6.1) Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7.6.0->ipympl) (1.0.2) Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7.6.0->ipympl) (5.1.3) Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7.6.0->ipympl) (3.5.2) Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7.6.0->ipympl) (4.10.1) Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7.6.0->ipympl) (5.3.5) Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7.6.0->ipympl) (5.1.1) Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.0.0->ipympl) (0.11.0) Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.0.0->ipympl) (1.3.2) Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.0.0->ipympl) (2.8.2) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.0.0->ipympl) (3.0.6) Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets<8,>=7.6.0->ipympl) (4.3.3) Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets<8,>=7.6.0->ipympl) (4.9.1) Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<8,>=7.6.0->ipympl) (21.4.0) Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<8,>=7.6.0->ipympl) (4.10.0) Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<8,>=7.6.0->ipympl) (5.4.0) Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<8,>=7.6.0->ipympl) (3.10.0.2) Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<8,>=7.6.0->ipympl) (0.18.0) Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<8,>=7.6.0->ipympl) (3.7.0) Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython<8->ipympl) (1.15.0) Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython<8->ipympl) (0.2.5) Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipympl) (5.3.1) Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipympl) (2.11.3) Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipympl) (5.6.1) Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipympl) (0.12.1) Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipympl) (1.8.0) Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets<8,>=7.6.0->ipympl) (22.3.0) Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipympl) (0.7.0) Requirement already satisfied: apptools in /usr/local/lib/python3.7/dist-packages (from mayavi) (5.1.0) Requirement already satisfied: envisage in /usr/local/lib/python3.7/dist-packages (from mayavi) (6.0.1) Requirement already satisfied: pyface>=6.1.1 in /usr/local/lib/python3.7/dist-packages (from mayavi) (7.3.0) Requirement already satisfied: traits>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from mayavi) (6.3.2) Requirement already satisfied: traitsui>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from mayavi) (7.2.1) Requirement already satisfied: configobj in /usr/local/lib/python3.7/dist-packages (from apptools->mayavi) (5.0.6) Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipympl) (2.0.1) Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipympl) (1.5.0) Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipympl) (0.5.0) Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipympl) (0.8.4) Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipympl) (4.1.0) Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipympl) (0.7.1) Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipympl) (0.3) Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipympl) (21.3) Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.6.0->ipympl) (0.5.1) ! apt - get install vtk6 ! apt - get install libvtk6 - dev python - vtk6 Reading package lists... Done Building dependency tree Reading state information... Done vtk6 is already the newest version (6.3.0+dfsg1-11build1). 0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded. Reading package lists... Done Building dependency tree Reading state information... Done libvtk6-dev is already the newest version (6.3.0+dfsg1-11build1). python-vtk6 is already the newest version (6.3.0+dfsg1-11build1). 0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded. ! pip install mayavi Requirement already satisfied: mayavi in /usr/local/lib/python3.7/dist-packages (4.7.4) Requirement already satisfied: apptools in /usr/local/lib/python3.7/dist-packages (from mayavi) (5.1.0) Requirement already satisfied: envisage in /usr/local/lib/python3.7/dist-packages (from mayavi) (6.0.1) Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mayavi) (1.19.5) Requirement already satisfied: pyface>=6.1.1 in /usr/local/lib/python3.7/dist-packages (from mayavi) (7.3.0) Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from mayavi) (2.6.1) Requirement already satisfied: traits>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from mayavi) (6.3.2) Requirement already satisfied: traitsui>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from mayavi) (7.2.1) Requirement already satisfied: vtk in /usr/local/lib/python3.7/dist-packages (from mayavi) (9.0.1) Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from pyface>=6.1.1->mayavi) (4.10.0) Requirement already satisfied: importlib-resources>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from pyface>=6.1.1->mayavi) (5.4.0) Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.1.0->pyface>=6.1.1->mayavi) (3.7.0) Requirement already satisfied: configobj in /usr/local/lib/python3.7/dist-packages (from apptools->mayavi) (5.0.6) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from configobj->apptools->mayavi) (1.15.0) Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from envisage->mayavi) (57.4.0) Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->pyface>=6.1.1->mayavi) (3.10.0.2) % matplotlib widget from mayavi import mlab /usr/local/lib/python3.7/dist-packages/traits/etsconfig/etsconfig.py:412: UserWarning: Environment variable \"HOME\" not set, setting home directory to /tmp % (environment_variable, parent_directory) ******************************************************************************** WARNING: Imported VTK version (9.0) does not match the one used to build the TVTK classes (9.1). This may cause problems. Please rebuild TVTK. ******************************************************************************** import matplotlib.pyplot as plt import numpy as np from mpl_toolkits.mplot3d import Axes3D import scipy as sp import scipy.linalg import sympy as sy sy . init_printing () np . set_printoptions ( precision = 3 ) np . set_printoptions ( suppress = True ) mlab . init_notebook ( backend = 'x3d' ) Notebook initialized with x3d backend. We start from plotting basics in Python environment, in the meanwhile refresh the system of linear equations. Visualisation of A System of Two Linear Equations Consider a linear system of two equations: \\begin{align} x+y&=6\\ x-y&=-4 \\end{align} Easy to solve: \\((x, y)^T = (1, 5)^T\\) . Let's plot the linear system. x = np . linspace ( - 5 , 5 , 100 ) y1 = - x + 6 y2 = x + 4 fig , ax = plt . subplots ( figsize = ( 12 , 7 )) ax . scatter ( 1 , 5 , s = 200 , zorder = 5 , color = \"r\" , alpha = 0.8 ) ax . plot ( x , y1 , lw = 3 , label = \"$x+y=6$\" ) ax . plot ( x , y2 , lw = 3 , label = \"$x-y=-4$\" ) ax . plot ([ 1 , 1 ], [ 0 , 5 ], ls = \"--\" , color = \"b\" , alpha = 0.5 ) ax . plot ([ - 5 , 1 ], [ 5 , 5 ], ls = \"--\" , color = \"b\" , alpha = 0.5 ) ax . set_xlim ([ - 5 , 5 ]) ax . set_ylim ([ 0 , 12 ]) ax . legend () s = \"$(1,5)$\" ax . text ( 1 , 5.5 , s , fontsize = 20 ) ax . set_title ( \"Solution of $x+y=6$, $x-y=-4$\" , size = 22 ) ax . grid () Figure from google.colab import output output . enable_custom_widget_manager () How to Draw a Plane Before drawing a plane, let's refresh the logic of Matplotlib 3D plotting. This should be familiar to you if you are a MATLAB user. First, create meshgrids. x , y = [ - 1 , 0 , 1 ], [ - 1 , 0 , 1 ] X , Y = np . meshgrid ( x , y ) Mathematically, meshgrids are the coordinates of Cartesian product . To illustrate, we can plot all the coordinates of these meshgrids. So the x, y = [-1, 0, 1], [-1, 0, 1] translates to \\(9\\) coordinates since it represent Cartesian Product . More concretely, np.meshgrid(x, y) will have 9 coordinates (-1, -1), (-1, 0), (-1, 1), (0, -1), (0, 0), (0, 1), (1, -1), (1, 0), (1, 1) . fig , ax = plt . subplots ( figsize = ( 12 , 7 )) ax . scatter ( X , Y , s = 200 , color = 'red' ) ax . axis ([ - 2 , 3.01 , - 2.01 , 2 ]) ax . spines [ 'left' ] . set_position ( 'zero' ) # alternative position is 'center' ax . spines [ 'right' ] . set_color ( 'none' ) ax . spines [ 'bottom' ] . set_position ( 'zero' ) ax . spines [ 'top' ] . set_color ( 'none' ) ax . grid () plt . show () Figure Unhandled message type set_device_pixel_ratio. {'type': 'set_device_pixel_ratio', 'device_pixel_ratio': 1.5} Unhandled message type set_device_pixel_ratio. {'type': 'set_device_pixel_ratio', 'device_pixel_ratio': 1.5} Try a more complicated meshgrid. x , y = np . arange ( - 3 , 4 , 1 ), np . arange ( - 3 , 4 , 1 ) X , Y = np . meshgrid ( x , y ) fig , ax = plt . subplots ( figsize = ( 12 , 12 )) ax . scatter ( X , Y , s = 200 , color = 'red' , zorder = 3 ) ax . axis ([ - 5 , 5 , - 5 , 5 ]) ax . spines [ 'left' ] . set_position ( 'zero' ) # alternative position is 'center' ax . spines [ 'right' ] . set_color ( 'none' ) ax . spines [ 'bottom' ] . set_position ( 'zero' ) ax . spines [ 'top' ] . set_color ( 'none' ) ax . grid () Figure Unhandled message type set_device_pixel_ratio. {'type': 'set_device_pixel_ratio', 'device_pixel_ratio': 1.5} Unhandled message type set_device_pixel_ratio. {'type': 'set_device_pixel_ratio', 'device_pixel_ratio': 1.5} Now consider the function \\(z = f(x, y)\\) , \\(z\\) is in the \\(3rd\\) dimension. Though Matplotlib is not meant for delicate plotting of 3D graphics, basic 3D plotting is still acceptable. For example, we define a simple plane as \\( \\(z= x + y\\) \\) Then plot \\(z\\) . Recall that we have already defined our \\(X, Y\\) plane through np.meshgrid . Thus, if a plane is defined by \\(z = x + y\\) , then the coordinates of each point on the plane is \\((x, y, z)\\) and we can use a 3d-scatter to mimic a plane. Z = X + Y fig = plt . figure ( figsize = ( 9 , 9 )) ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X , Y , Z , s = 100 , label = '$z=x+y$' ) ax . legend () plt . show () Figure Unhandled message type set_device_pixel_ratio. {'type': 'set_device_pixel_ratio', 'device_pixel_ratio': 1.5} Unhandled message type set_device_pixel_ratio. {'type': 'set_device_pixel_ratio', 'device_pixel_ratio': 1.5} Or we can plot it as a surface, Matplotlib will automatically interpolate values among the Cartesian coordinates such that the graph will look like a surface. fig = plt . figure ( figsize = ( 9 , 9 )) ax = fig . add_subplot ( 111 , projection = '3d' ) ax . plot_surface ( X , Y , Z , cmap = 'viridis' ) # MATLAB default color map ax . set_xlabel ( 'x-axis' ) ax . set_ylabel ( 'y-axis' ) ax . set_zlabel ( 'z-axis' ) ax . set_title ( '$z=x+y$' , size = 18 ) plt . show () Figure Unhandled message type set_device_pixel_ratio. {'type': 'set_device_pixel_ratio', 'device_pixel_ratio': 1.5} Visualisation of A System of Three Linear Equations We have reviewed on plotting planes, now we are ready to plot several planes all together. Consider this system of linear equations \\begin{align} x_1- 2x_2+x_3&=0\\ 2x_2-8x_3&=8\\ -4x_1+5x_2+9x_3&=-9 \\end{align} And solution is \\((x_1, x_2, x_3)^T = (29, 16, 3)^T\\) . Let's reproduce the system visually. x1 = np . linspace ( 25 , 35 , 20 ) x2 = np . linspace ( 10 , 20 , 20 ) X1 , X2 = np . meshgrid ( x1 , x2 ) fig = plt . figure ( figsize = ( 9 , 9 )) ax = fig . add_subplot ( 111 , projection = '3d' ) X3 = 2 * X2 - X1 ax . plot_surface ( X1 , X2 , X3 , cmap = 'viridis' , alpha = 1 ) X3 = .25 * X2 - 1 ax . plot_surface ( X1 , X2 , X3 , cmap = 'summer' , alpha = 1 ) X3 = - 5 / 9 * X2 + 4 / 9 * X1 - 1 ax . plot_surface ( X1 , X2 , X3 , cmap = 'spring' , alpha = 1 ) ax . scatter ( 29 , 16 , 3 , s = 200 , color = 'black' ) plt . show () Figure Unhandled message type set_device_pixel_ratio. {'type': 'set_device_pixel_ratio', 'device_pixel_ratio': 1.5} Unhandled message type set_device_pixel_ratio. {'type': 'set_device_pixel_ratio', 'device_pixel_ratio': 1.5} We are certain there is a solution, however the graph does not show the intersection of planes. The problem originates from Matplotlib's rendering algorithm, which is not designed for drawing genuine 3D graphics. It merely projects 3D objects onto 2D dimension to imitate 3D features. Mayavi is much professional in rendering 3D graphics, we give an example here. If not installed, run conda install -c anaconda mayavi . mlab . clf () X1 , X2 = np . mgrid [ - 10 : 10 : 21 * 1 j , - 5 : 10 : 21 * 1 j ] X3 = 6 - X1 - X2 mlab . mesh ( X1 , X2 , X3 , colormap = \"spring\" ) X3 = 3 - 2 * X1 + X2 mlab . mesh ( X1 , X2 , X3 , colormap = \"winter\" ) X3 = 3 * X1 + 2 * X2 - 4 mlab . mesh ( X1 , X2 , X3 , colormap = \"summer\" ) mlab . axes () mlab . outline () mlab . points3d ( 1 , 2 , 3 , color = ( .8 , 0.2 , .2 ), ) mlab . title ( 'A System of Linear Equations' ) Visualisation of An Inconsistent System Now let's visualise the linear system that does not have a solution. \\begin{align} x+y+z&=1\\ x-y-2z&=2\\ 2x-z&=1 \\end{align} Rearrange the system to solve for \\(z\\) : \\[\\begin{align} z&=1-x-y\\\\ z&=\\frac{x}{2}-\\frac{y}{2}+1\\\\ z&=2x-1 \\end{align}\\] mlab . clf () X , Y = np . mgrid [ - 5 : 5 : 21 * 1 j , - 5 : 5 : 21 * 1 j ] Z = 1 - X - Y mlab . mesh ( X , Y , Z , colormap = \"spring\" ) Z = X / 2 - Y / 2 + 1 mlab . mesh ( X , Y , Z , colormap = \"summer\" ) Z = 2 * X - 1 mlab . mesh ( X , Y , Z , colormap = \"autumn\" ) mlab . axes () mlab . outline () mlab . title ( 'A Inconsistent System of Linear Equations' ) Visualisation of A System With Infinite Numbers of Solutions Our system of equations is given \\[\\begin{align} y-z=&4\\\\ 2x+y+2z=&4\\\\ 2x+2y+z=&8 \\end{align}\\] Rearrange to solve for \\(z\\) \\[\\begin{align} z=&y-4\\\\ z=&2-x-\\frac{y}{2}\\\\ z=&8-2x-2y \\end{align}\\] mlab . clf () X , Y = np . mgrid [ - 2 : 2 : 21 * 1 j , 2 : 6 : 21 * 1 j ] Z = Y - 4 mlab . mesh ( X , Y , Z , colormap = \"spring\" ) Z = 2 - X - Y / 2 mlab . mesh ( X , Y , Z , colormap = \"summer\" ) Z = 8 - 2 * X - 2 * Y mlab . mesh ( X , Y , Z , colormap = \"autumn\" ) mlab . axes () mlab . outline () mlab . title ( 'A System of Linear Equations With Infinite Number of Solutions' ) The solution of the system is \\((x,y,z)=(-3z/2,z+4,z)^T\\) , where \\(z\\) is a free variable . The solution is an infinite line in \\(\\mathbb{R}^3\\) , to visualise the solution requires setting a range of \\(x\\) and \\(y\\) , for instance we can set \\[\\begin{align} -2 \\leq x \\leq 2\\\\ 2 \\leq y \\leq 6 \\end{align}\\] which means \\[\\begin{align} -2\\leq -\\frac32z\\leq 2\\\\ 2\\leq z+4 \\leq 6 \\end{align}\\] We can pick one inequality to set the range of \\(z\\) , e.g. second inequality: \\(-2 \\leq z \\leq 2\\) . Then plot the planes and the solutions together. mlab . clf () X , Y = np . mgrid [ - 2 : 2 : 21 * 1 j , 2 : 6 : 21 * 1 j ] Z = Y - 4 mlab . mesh ( X , Y , Z , colormap = \"spring\" ) Z = 2 - X - Y / 2 mlab . mesh ( X , Y , Z , colormap = \"summer\" ) Z = 8 - 2 * X - 2 * Y mlab . mesh ( X , Y , Z , colormap = \"autumn\" ) ZL = np . linspace ( - 2 , 2 , 20 ) # ZL means Z for line, we have chosen the range [-2, 2] X = - 3 * ZL / 2 Y = ZL + 4 mlab . plot3d ( X , Y , ZL ) mlab . axes () mlab . outline () mlab . title ( 'A System of Linear Equations With Infinite Number of Solutions' ) Reduced Row Echelon Form For easy demonstration, we will be using SymPy frequently in lectures. SymPy is a very power symbolic computation library, we will see its basic features as the lectures move forward. We define a SymPy matrix: M = sy . Matrix ([[ 5 , 0 , 11 , 3 ], [ 7 , 23 , - 3 , 7 ], [ 12 , 11 , 3 , - 4 ]]); M Think of it as an augmented matrix which combines coefficients of linear system. With row operations, we can solve the system quickly. Let's turn it into a row reduced echelon form . M_rref = M . rref (); M_rref # .rref() is the SymPy method for row reduced echelon form Take out the first element in the big parentheses, i.e. the rref matrix. M_rref = np . array ( M_rref [ 0 ]); M_rref If you don't like fractions, convert it into float type. M_rref . astype ( float ) The last column of the rref matrix is the solution of the system. Example: rref and Visualisation Let's use .rref() method to compute a solution of a system then visualise it. Consider the system: \\[\\begin{align} 3x+6y+2z&=-13\\\\ x+2y+z&=-5\\\\ -5x-10y-2z&=19 \\end{align}\\] Extract the augmented matrix into a SymPy matrix: A = sy . Matrix ([[ 3 , 6 , 2 , - 13 ], [ 1 , 2 , 1 , - 5 ], [ - 5 , - 10 , - 2 , 19 ]]); A A_rref = A . rref (); A_rref In case you are wondering what's \\((0, 2)\\) : they are the column number of pivot columns, in the augmented matrix above the pivot columns resides on the \\(0\\) th and \\(2\\) nd column. Because it's not a rank matrix, therefore solutions is in general form \\begin{align} x + 2y & = -3\\ z &= -2\\ y &= free \\end{align} Let's pick 3 different values of \\(y\\) , for instance \\((3, 5, 7)\\) , to calculate \\(3\\) special solutions: point1 = ( - 2 * 3 - 3 , 3 , - 2 ) point2 = ( - 2 * 5 - 3 , 5 , - 2 ) point3 = ( - 2 * 7 - 3 , 7 , - 2 ) special_solution = np . array ([ point1 , point2 , point3 ]); special_solution # each row is a special solution We can visualise the general solution, and the 3 specific solutions altogether. y = np . linspace ( 2 , 8 , 20 ) # y is the free variable x = - 3 - 2 * y z = np . full (( len ( y ), ), - 2 ) # z is a constant fig = plt . figure ( figsize = ( 12 , 9 )) ax = fig . add_subplot ( 111 , projection = '3d' ) ax . plot ( x , y , z , lw = 3 , color = 'red' ) ax . scatter ( special_solution [:, 0 ], special_solution [:, 1 ], special_solution [:, 2 ], s = 200 ) ax . set_title ( 'General Solution and Special Solution of the Linear Sytem' , size = 16 ) plt . show () Example: A Symbolic Solution Consider a system where all right-hand side values are indeterminate: \\[\\begin{align} x + 2y - 3z &= a\\\\ 4x - y + 8z &= b\\\\ 2x - 6y - 4z &= c \\end{align}\\] We define \\(a, b, c\\) as SymPy objects, then extract the augmented matrix a , b , c = sy . symbols ( 'a, b, c' , real = True ) A = sy . Matrix ([[ 1 , 2 , - 3 , a ], [ 4 , - 1 , 8 , b ], [ 2 , - 6 , - 4 , c ]]); A We can immediately achieve the symbolic solution by using .rref() method. A_rref = A . rref (); A_rref Of course, we can substitute values of \\(a\\) , \\(b\\) and \\(c\\) to get a specific solution. vDict = { a : 3 , b : 6 , c : 7 } A_rref = A_rref [ 0 ] . subs ( vDict ); A_rref # define a dictionary for special values to substitute in Example: Polynomials Consider this question : How to find a cubic polynomial that passes through each of these points \\((1,3)\\) , \\((2, -2)\\) , \\((3, -5)\\) , and \\((4, 0)\\) . The form of cubic polynomial is \\begin{align} y=a_0+a_1x+a_2x^2+a_3x^3 \\end{align} We substitute all the points: \\[\\begin{align} (x,y)&=(1,3)\\qquad\\longrightarrow\\qquad \\ 2=a_0+3a_1+9a_2 +27a_3 \\\\ (x,y)&=(2,-2)\\qquad\\longrightarrow\\qquad 3=a_0+a_1+a_2+a_3\\\\ (x,y)&=(3,-5)\\qquad\\longrightarrow\\qquad 2=a_0-4a_1+16a_2-64a_3\\\\ (x,y)&=(4,0)\\qquad\\longrightarrow\\qquad -2=a_0+2a_1+4a_2+8a_3 \\end{align}\\] It turns to be a linear system, the rest should be familiar already. The augmented matrix is A = sy . Matrix ([[ 1 , 1 , 1 , 1 , 3 ], [ 1 , 2 , 4 , 8 , - 2 ], [ 1 , 3 , 9 , 27 , - 5 ], [ 1 , 4 , 16 , 64 , 0 ]]); A A_rref = A . rref (); A_rref A_rref = np . array ( A_rref [ 0 ]); A_rref The last column is the solution, i.e. the coefficients of the cubic polynomial. poly_coef = A_rref . astype ( float )[:, - 1 ]; poly_coef Cubic polynomial form is: \\begin{align} y = 4 + 3x - 5x^2 + x^3 \\end{align} Since we have the specific form of the cubic polynomial, we can plot it x = np . linspace ( - 5 , 5 , 40 ) y = poly_coef [ 0 ] + poly_coef [ 1 ] * x + poly_coef [ 2 ] * x ** 2 + poly_coef [ 3 ] * x ** 3 fig , ax = plt . subplots ( figsize = ( 8 , 8 )) ax . plot ( x , y , lw = 3 , color = 'red' ) ax . scatter ([ 1 , 2 , 3 , 4 ], [ 3 , - 2 , - 5 , 0 ], s = 100 , color = 'blue' , zorder = 3 ) ax . grid () ax . set_xlim ([ 0 , 5 ]) ax . set_ylim ([ - 10 , 10 ]) ax . text ( 1 , 3.5 , '$(1, 3)$' , fontsize = 15 ) ax . text ( 1.5 , - 2.5 , '$(2, -2)$' , fontsize = 15 ) ax . text ( 2.7 , - 4 , '$(3, -5.5)$' , fontsize = 15 ) ax . text ( 4.1 , 0 , '$(4, .5)$' , fontsize = 15 ) plt . show () Now you know the trick, try another 5 points: \\((1,2)\\) , \\((2,5)\\) , \\((3,8)\\) , \\((4,6)\\) , \\((5, 9)\\) . And polynomial form is \\begin{align} y=a_0+a_1x+a_2x^2+a_3x^3+a_4x^4 \\end{align} The augmented matrix is A = sy . Matrix ([[ 1 , 1 , 1 , 1 , 1 , 2 ], [ 1 , 2 , 4 , 8 , 16 , 5 ], [ 1 , 3 , 9 , 27 , 81 , 8 ], [ 1 , 4 , 16 , 64 , 256 , 6 ], [ 1 , 5 , 25 , 125 , 625 , 9 ]]); A A_rref = A . rref () A_rref = np . array ( A_rref [ 0 ]) coef = A_rref . astype ( float )[:, - 1 ]; coef x = np . linspace ( 0 , 6 , 100 ) y = coef [ 0 ] + coef [ 1 ] * x + coef [ 2 ] * x ** 2 + coef [ 3 ] * x ** 3 + coef [ 4 ] * x ** 4 fig , ax = plt . subplots ( figsize = ( 8 , 8 )) ax . plot ( x , y , lw = 3 ) ax . scatter ([ 1 , 2 , 3 , 4 , 5 ], [ 2 , 5 , 8 , 6 , 9 ], s = 100 , color = 'red' , zorder = 3 ) ax . grid () Solving The System of Linear Equations By NumPy Set up the system \\(A x = b\\) , generate a random \\(A\\) and \\(b\\) A = np . round ( 10 * np . random . rand ( 5 , 5 )) b = np . round ( 10 * np . random . rand ( 5 ,)) x = np . linalg . solve ( A , b ); x Let's verify if $ Ax = b$ A @x - b They are technically zeros, due to some round-off errors omitted, that's why there is \\(-\\) in front \\(0\\) .","title":"Linear Equations (MacroAnalyst)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/Linear%20Equations%20%28MacroAnalyst%29/#visualisation-of-a-system-of-two-linear-equations","text":"Consider a linear system of two equations: \\begin{align} x+y&=6\\ x-y&=-4 \\end{align} Easy to solve: \\((x, y)^T = (1, 5)^T\\) . Let's plot the linear system. x = np . linspace ( - 5 , 5 , 100 ) y1 = - x + 6 y2 = x + 4 fig , ax = plt . subplots ( figsize = ( 12 , 7 )) ax . scatter ( 1 , 5 , s = 200 , zorder = 5 , color = \"r\" , alpha = 0.8 ) ax . plot ( x , y1 , lw = 3 , label = \"$x+y=6$\" ) ax . plot ( x , y2 , lw = 3 , label = \"$x-y=-4$\" ) ax . plot ([ 1 , 1 ], [ 0 , 5 ], ls = \"--\" , color = \"b\" , alpha = 0.5 ) ax . plot ([ - 5 , 1 ], [ 5 , 5 ], ls = \"--\" , color = \"b\" , alpha = 0.5 ) ax . set_xlim ([ - 5 , 5 ]) ax . set_ylim ([ 0 , 12 ]) ax . legend () s = \"$(1,5)$\" ax . text ( 1 , 5.5 , s , fontsize = 20 ) ax . set_title ( \"Solution of $x+y=6$, $x-y=-4$\" , size = 22 ) ax . grid () Figure from google.colab import output output . enable_custom_widget_manager ()","title":" Visualisation of A System of Two Linear Equations "},{"location":"reighns_ml_journey/mathematics/linear_algebra/Linear%20Equations%20%28MacroAnalyst%29/#how-to-draw-a-plane","text":"Before drawing a plane, let's refresh the logic of Matplotlib 3D plotting. This should be familiar to you if you are a MATLAB user. First, create meshgrids. x , y = [ - 1 , 0 , 1 ], [ - 1 , 0 , 1 ] X , Y = np . meshgrid ( x , y ) Mathematically, meshgrids are the coordinates of Cartesian product . To illustrate, we can plot all the coordinates of these meshgrids. So the x, y = [-1, 0, 1], [-1, 0, 1] translates to \\(9\\) coordinates since it represent Cartesian Product . More concretely, np.meshgrid(x, y) will have 9 coordinates (-1, -1), (-1, 0), (-1, 1), (0, -1), (0, 0), (0, 1), (1, -1), (1, 0), (1, 1) . fig , ax = plt . subplots ( figsize = ( 12 , 7 )) ax . scatter ( X , Y , s = 200 , color = 'red' ) ax . axis ([ - 2 , 3.01 , - 2.01 , 2 ]) ax . spines [ 'left' ] . set_position ( 'zero' ) # alternative position is 'center' ax . spines [ 'right' ] . set_color ( 'none' ) ax . spines [ 'bottom' ] . set_position ( 'zero' ) ax . spines [ 'top' ] . set_color ( 'none' ) ax . grid () plt . show () Figure Unhandled message type set_device_pixel_ratio. {'type': 'set_device_pixel_ratio', 'device_pixel_ratio': 1.5} Unhandled message type set_device_pixel_ratio. {'type': 'set_device_pixel_ratio', 'device_pixel_ratio': 1.5} Try a more complicated meshgrid. x , y = np . arange ( - 3 , 4 , 1 ), np . arange ( - 3 , 4 , 1 ) X , Y = np . meshgrid ( x , y ) fig , ax = plt . subplots ( figsize = ( 12 , 12 )) ax . scatter ( X , Y , s = 200 , color = 'red' , zorder = 3 ) ax . axis ([ - 5 , 5 , - 5 , 5 ]) ax . spines [ 'left' ] . set_position ( 'zero' ) # alternative position is 'center' ax . spines [ 'right' ] . set_color ( 'none' ) ax . spines [ 'bottom' ] . set_position ( 'zero' ) ax . spines [ 'top' ] . set_color ( 'none' ) ax . grid () Figure Unhandled message type set_device_pixel_ratio. {'type': 'set_device_pixel_ratio', 'device_pixel_ratio': 1.5} Unhandled message type set_device_pixel_ratio. {'type': 'set_device_pixel_ratio', 'device_pixel_ratio': 1.5} Now consider the function \\(z = f(x, y)\\) , \\(z\\) is in the \\(3rd\\) dimension. Though Matplotlib is not meant for delicate plotting of 3D graphics, basic 3D plotting is still acceptable. For example, we define a simple plane as \\( \\(z= x + y\\) \\) Then plot \\(z\\) . Recall that we have already defined our \\(X, Y\\) plane through np.meshgrid . Thus, if a plane is defined by \\(z = x + y\\) , then the coordinates of each point on the plane is \\((x, y, z)\\) and we can use a 3d-scatter to mimic a plane. Z = X + Y fig = plt . figure ( figsize = ( 9 , 9 )) ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X , Y , Z , s = 100 , label = '$z=x+y$' ) ax . legend () plt . show () Figure Unhandled message type set_device_pixel_ratio. {'type': 'set_device_pixel_ratio', 'device_pixel_ratio': 1.5} Unhandled message type set_device_pixel_ratio. {'type': 'set_device_pixel_ratio', 'device_pixel_ratio': 1.5} Or we can plot it as a surface, Matplotlib will automatically interpolate values among the Cartesian coordinates such that the graph will look like a surface. fig = plt . figure ( figsize = ( 9 , 9 )) ax = fig . add_subplot ( 111 , projection = '3d' ) ax . plot_surface ( X , Y , Z , cmap = 'viridis' ) # MATLAB default color map ax . set_xlabel ( 'x-axis' ) ax . set_ylabel ( 'y-axis' ) ax . set_zlabel ( 'z-axis' ) ax . set_title ( '$z=x+y$' , size = 18 ) plt . show () Figure Unhandled message type set_device_pixel_ratio. {'type': 'set_device_pixel_ratio', 'device_pixel_ratio': 1.5}","title":" How to Draw a Plane "},{"location":"reighns_ml_journey/mathematics/linear_algebra/Linear%20Equations%20%28MacroAnalyst%29/#visualisation-of-a-system-of-three-linear-equations","text":"We have reviewed on plotting planes, now we are ready to plot several planes all together. Consider this system of linear equations \\begin{align} x_1- 2x_2+x_3&=0\\ 2x_2-8x_3&=8\\ -4x_1+5x_2+9x_3&=-9 \\end{align} And solution is \\((x_1, x_2, x_3)^T = (29, 16, 3)^T\\) . Let's reproduce the system visually. x1 = np . linspace ( 25 , 35 , 20 ) x2 = np . linspace ( 10 , 20 , 20 ) X1 , X2 = np . meshgrid ( x1 , x2 ) fig = plt . figure ( figsize = ( 9 , 9 )) ax = fig . add_subplot ( 111 , projection = '3d' ) X3 = 2 * X2 - X1 ax . plot_surface ( X1 , X2 , X3 , cmap = 'viridis' , alpha = 1 ) X3 = .25 * X2 - 1 ax . plot_surface ( X1 , X2 , X3 , cmap = 'summer' , alpha = 1 ) X3 = - 5 / 9 * X2 + 4 / 9 * X1 - 1 ax . plot_surface ( X1 , X2 , X3 , cmap = 'spring' , alpha = 1 ) ax . scatter ( 29 , 16 , 3 , s = 200 , color = 'black' ) plt . show () Figure Unhandled message type set_device_pixel_ratio. {'type': 'set_device_pixel_ratio', 'device_pixel_ratio': 1.5} Unhandled message type set_device_pixel_ratio. {'type': 'set_device_pixel_ratio', 'device_pixel_ratio': 1.5} We are certain there is a solution, however the graph does not show the intersection of planes. The problem originates from Matplotlib's rendering algorithm, which is not designed for drawing genuine 3D graphics. It merely projects 3D objects onto 2D dimension to imitate 3D features. Mayavi is much professional in rendering 3D graphics, we give an example here. If not installed, run conda install -c anaconda mayavi . mlab . clf () X1 , X2 = np . mgrid [ - 10 : 10 : 21 * 1 j , - 5 : 10 : 21 * 1 j ] X3 = 6 - X1 - X2 mlab . mesh ( X1 , X2 , X3 , colormap = \"spring\" ) X3 = 3 - 2 * X1 + X2 mlab . mesh ( X1 , X2 , X3 , colormap = \"winter\" ) X3 = 3 * X1 + 2 * X2 - 4 mlab . mesh ( X1 , X2 , X3 , colormap = \"summer\" ) mlab . axes () mlab . outline () mlab . points3d ( 1 , 2 , 3 , color = ( .8 , 0.2 , .2 ), ) mlab . title ( 'A System of Linear Equations' )","title":" Visualisation of A System of Three Linear Equations  "},{"location":"reighns_ml_journey/mathematics/linear_algebra/Linear%20Equations%20%28MacroAnalyst%29/#visualisation-of-an-inconsistent-system","text":"Now let's visualise the linear system that does not have a solution. \\begin{align} x+y+z&=1\\ x-y-2z&=2\\ 2x-z&=1 \\end{align} Rearrange the system to solve for \\(z\\) : \\[\\begin{align} z&=1-x-y\\\\ z&=\\frac{x}{2}-\\frac{y}{2}+1\\\\ z&=2x-1 \\end{align}\\] mlab . clf () X , Y = np . mgrid [ - 5 : 5 : 21 * 1 j , - 5 : 5 : 21 * 1 j ] Z = 1 - X - Y mlab . mesh ( X , Y , Z , colormap = \"spring\" ) Z = X / 2 - Y / 2 + 1 mlab . mesh ( X , Y , Z , colormap = \"summer\" ) Z = 2 * X - 1 mlab . mesh ( X , Y , Z , colormap = \"autumn\" ) mlab . axes () mlab . outline () mlab . title ( 'A Inconsistent System of Linear Equations' )","title":" Visualisation of An Inconsistent System "},{"location":"reighns_ml_journey/mathematics/linear_algebra/Linear%20Equations%20%28MacroAnalyst%29/#visualisation-of-a-system-with-infinite-numbers-of-solutions","text":"Our system of equations is given \\[\\begin{align} y-z=&4\\\\ 2x+y+2z=&4\\\\ 2x+2y+z=&8 \\end{align}\\] Rearrange to solve for \\(z\\) \\[\\begin{align} z=&y-4\\\\ z=&2-x-\\frac{y}{2}\\\\ z=&8-2x-2y \\end{align}\\] mlab . clf () X , Y = np . mgrid [ - 2 : 2 : 21 * 1 j , 2 : 6 : 21 * 1 j ] Z = Y - 4 mlab . mesh ( X , Y , Z , colormap = \"spring\" ) Z = 2 - X - Y / 2 mlab . mesh ( X , Y , Z , colormap = \"summer\" ) Z = 8 - 2 * X - 2 * Y mlab . mesh ( X , Y , Z , colormap = \"autumn\" ) mlab . axes () mlab . outline () mlab . title ( 'A System of Linear Equations With Infinite Number of Solutions' ) The solution of the system is \\((x,y,z)=(-3z/2,z+4,z)^T\\) , where \\(z\\) is a free variable . The solution is an infinite line in \\(\\mathbb{R}^3\\) , to visualise the solution requires setting a range of \\(x\\) and \\(y\\) , for instance we can set \\[\\begin{align} -2 \\leq x \\leq 2\\\\ 2 \\leq y \\leq 6 \\end{align}\\] which means \\[\\begin{align} -2\\leq -\\frac32z\\leq 2\\\\ 2\\leq z+4 \\leq 6 \\end{align}\\] We can pick one inequality to set the range of \\(z\\) , e.g. second inequality: \\(-2 \\leq z \\leq 2\\) . Then plot the planes and the solutions together. mlab . clf () X , Y = np . mgrid [ - 2 : 2 : 21 * 1 j , 2 : 6 : 21 * 1 j ] Z = Y - 4 mlab . mesh ( X , Y , Z , colormap = \"spring\" ) Z = 2 - X - Y / 2 mlab . mesh ( X , Y , Z , colormap = \"summer\" ) Z = 8 - 2 * X - 2 * Y mlab . mesh ( X , Y , Z , colormap = \"autumn\" ) ZL = np . linspace ( - 2 , 2 , 20 ) # ZL means Z for line, we have chosen the range [-2, 2] X = - 3 * ZL / 2 Y = ZL + 4 mlab . plot3d ( X , Y , ZL ) mlab . axes () mlab . outline () mlab . title ( 'A System of Linear Equations With Infinite Number of Solutions' )","title":" Visualisation of A System With Infinite Numbers of Solutions "},{"location":"reighns_ml_journey/mathematics/linear_algebra/Linear%20Equations%20%28MacroAnalyst%29/#reduced-row-echelon-form","text":"For easy demonstration, we will be using SymPy frequently in lectures. SymPy is a very power symbolic computation library, we will see its basic features as the lectures move forward. We define a SymPy matrix: M = sy . Matrix ([[ 5 , 0 , 11 , 3 ], [ 7 , 23 , - 3 , 7 ], [ 12 , 11 , 3 , - 4 ]]); M Think of it as an augmented matrix which combines coefficients of linear system. With row operations, we can solve the system quickly. Let's turn it into a row reduced echelon form . M_rref = M . rref (); M_rref # .rref() is the SymPy method for row reduced echelon form Take out the first element in the big parentheses, i.e. the rref matrix. M_rref = np . array ( M_rref [ 0 ]); M_rref If you don't like fractions, convert it into float type. M_rref . astype ( float ) The last column of the rref matrix is the solution of the system.","title":" Reduced Row Echelon Form "},{"location":"reighns_ml_journey/mathematics/linear_algebra/Linear%20Equations%20%28MacroAnalyst%29/#example-rref-and-visualisation","text":"Let's use .rref() method to compute a solution of a system then visualise it. Consider the system: \\[\\begin{align} 3x+6y+2z&=-13\\\\ x+2y+z&=-5\\\\ -5x-10y-2z&=19 \\end{align}\\] Extract the augmented matrix into a SymPy matrix: A = sy . Matrix ([[ 3 , 6 , 2 , - 13 ], [ 1 , 2 , 1 , - 5 ], [ - 5 , - 10 , - 2 , 19 ]]); A A_rref = A . rref (); A_rref In case you are wondering what's \\((0, 2)\\) : they are the column number of pivot columns, in the augmented matrix above the pivot columns resides on the \\(0\\) th and \\(2\\) nd column. Because it's not a rank matrix, therefore solutions is in general form \\begin{align} x + 2y & = -3\\ z &= -2\\ y &= free \\end{align} Let's pick 3 different values of \\(y\\) , for instance \\((3, 5, 7)\\) , to calculate \\(3\\) special solutions: point1 = ( - 2 * 3 - 3 , 3 , - 2 ) point2 = ( - 2 * 5 - 3 , 5 , - 2 ) point3 = ( - 2 * 7 - 3 , 7 , - 2 ) special_solution = np . array ([ point1 , point2 , point3 ]); special_solution # each row is a special solution We can visualise the general solution, and the 3 specific solutions altogether. y = np . linspace ( 2 , 8 , 20 ) # y is the free variable x = - 3 - 2 * y z = np . full (( len ( y ), ), - 2 ) # z is a constant fig = plt . figure ( figsize = ( 12 , 9 )) ax = fig . add_subplot ( 111 , projection = '3d' ) ax . plot ( x , y , z , lw = 3 , color = 'red' ) ax . scatter ( special_solution [:, 0 ], special_solution [:, 1 ], special_solution [:, 2 ], s = 200 ) ax . set_title ( 'General Solution and Special Solution of the Linear Sytem' , size = 16 ) plt . show ()","title":" Example: rref and Visualisation "},{"location":"reighns_ml_journey/mathematics/linear_algebra/Linear%20Equations%20%28MacroAnalyst%29/#example-a-symbolic-solution","text":"Consider a system where all right-hand side values are indeterminate: \\[\\begin{align} x + 2y - 3z &= a\\\\ 4x - y + 8z &= b\\\\ 2x - 6y - 4z &= c \\end{align}\\] We define \\(a, b, c\\) as SymPy objects, then extract the augmented matrix a , b , c = sy . symbols ( 'a, b, c' , real = True ) A = sy . Matrix ([[ 1 , 2 , - 3 , a ], [ 4 , - 1 , 8 , b ], [ 2 , - 6 , - 4 , c ]]); A We can immediately achieve the symbolic solution by using .rref() method. A_rref = A . rref (); A_rref Of course, we can substitute values of \\(a\\) , \\(b\\) and \\(c\\) to get a specific solution. vDict = { a : 3 , b : 6 , c : 7 } A_rref = A_rref [ 0 ] . subs ( vDict ); A_rref # define a dictionary for special values to substitute in","title":" Example: A Symbolic Solution "},{"location":"reighns_ml_journey/mathematics/linear_algebra/Linear%20Equations%20%28MacroAnalyst%29/#example-polynomials","text":"Consider this question : How to find a cubic polynomial that passes through each of these points \\((1,3)\\) , \\((2, -2)\\) , \\((3, -5)\\) , and \\((4, 0)\\) . The form of cubic polynomial is \\begin{align} y=a_0+a_1x+a_2x^2+a_3x^3 \\end{align} We substitute all the points: \\[\\begin{align} (x,y)&=(1,3)\\qquad\\longrightarrow\\qquad \\ 2=a_0+3a_1+9a_2 +27a_3 \\\\ (x,y)&=(2,-2)\\qquad\\longrightarrow\\qquad 3=a_0+a_1+a_2+a_3\\\\ (x,y)&=(3,-5)\\qquad\\longrightarrow\\qquad 2=a_0-4a_1+16a_2-64a_3\\\\ (x,y)&=(4,0)\\qquad\\longrightarrow\\qquad -2=a_0+2a_1+4a_2+8a_3 \\end{align}\\] It turns to be a linear system, the rest should be familiar already. The augmented matrix is A = sy . Matrix ([[ 1 , 1 , 1 , 1 , 3 ], [ 1 , 2 , 4 , 8 , - 2 ], [ 1 , 3 , 9 , 27 , - 5 ], [ 1 , 4 , 16 , 64 , 0 ]]); A A_rref = A . rref (); A_rref A_rref = np . array ( A_rref [ 0 ]); A_rref The last column is the solution, i.e. the coefficients of the cubic polynomial. poly_coef = A_rref . astype ( float )[:, - 1 ]; poly_coef Cubic polynomial form is: \\begin{align} y = 4 + 3x - 5x^2 + x^3 \\end{align} Since we have the specific form of the cubic polynomial, we can plot it x = np . linspace ( - 5 , 5 , 40 ) y = poly_coef [ 0 ] + poly_coef [ 1 ] * x + poly_coef [ 2 ] * x ** 2 + poly_coef [ 3 ] * x ** 3 fig , ax = plt . subplots ( figsize = ( 8 , 8 )) ax . plot ( x , y , lw = 3 , color = 'red' ) ax . scatter ([ 1 , 2 , 3 , 4 ], [ 3 , - 2 , - 5 , 0 ], s = 100 , color = 'blue' , zorder = 3 ) ax . grid () ax . set_xlim ([ 0 , 5 ]) ax . set_ylim ([ - 10 , 10 ]) ax . text ( 1 , 3.5 , '$(1, 3)$' , fontsize = 15 ) ax . text ( 1.5 , - 2.5 , '$(2, -2)$' , fontsize = 15 ) ax . text ( 2.7 , - 4 , '$(3, -5.5)$' , fontsize = 15 ) ax . text ( 4.1 , 0 , '$(4, .5)$' , fontsize = 15 ) plt . show () Now you know the trick, try another 5 points: \\((1,2)\\) , \\((2,5)\\) , \\((3,8)\\) , \\((4,6)\\) , \\((5, 9)\\) . And polynomial form is \\begin{align} y=a_0+a_1x+a_2x^2+a_3x^3+a_4x^4 \\end{align} The augmented matrix is A = sy . Matrix ([[ 1 , 1 , 1 , 1 , 1 , 2 ], [ 1 , 2 , 4 , 8 , 16 , 5 ], [ 1 , 3 , 9 , 27 , 81 , 8 ], [ 1 , 4 , 16 , 64 , 256 , 6 ], [ 1 , 5 , 25 , 125 , 625 , 9 ]]); A A_rref = A . rref () A_rref = np . array ( A_rref [ 0 ]) coef = A_rref . astype ( float )[:, - 1 ]; coef x = np . linspace ( 0 , 6 , 100 ) y = coef [ 0 ] + coef [ 1 ] * x + coef [ 2 ] * x ** 2 + coef [ 3 ] * x ** 3 + coef [ 4 ] * x ** 4 fig , ax = plt . subplots ( figsize = ( 8 , 8 )) ax . plot ( x , y , lw = 3 ) ax . scatter ([ 1 , 2 , 3 , 4 , 5 ], [ 2 , 5 , 8 , 6 , 9 ], s = 100 , color = 'red' , zorder = 3 ) ax . grid ()","title":" Example: Polynomials "},{"location":"reighns_ml_journey/mathematics/linear_algebra/Linear%20Equations%20%28MacroAnalyst%29/#solving-the-system-of-linear-equations-by-numpy","text":"Set up the system \\(A x = b\\) , generate a random \\(A\\) and \\(b\\) A = np . round ( 10 * np . random . rand ( 5 , 5 )) b = np . round ( 10 * np . random . rand ( 5 ,)) x = np . linalg . solve ( A , b ); x Let's verify if $ Ax = b$ A @x - b They are technically zeros, due to some round-off errors omitted, that's why there is \\(-\\) in front \\(0\\) .","title":" Solving The System of Linear Equations By NumPy "},{"location":"reighns_ml_journey/mathematics/linear_algebra/characteristic%20polynomial/","text":"Characteristic Polynomial Definition (Characteristic Polynomial)","title":"Characteristic polynomial"},{"location":"reighns_ml_journey/mathematics/linear_algebra/characteristic%20polynomial/#characteristic-polynomial","text":"","title":"Characteristic Polynomial"},{"location":"reighns_ml_journey/mathematics/linear_algebra/characteristic%20polynomial/#definition-characteristic-polynomial","text":"","title":"Definition (Characteristic Polynomial)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/linear_algebra_interview_questions/","text":"\\[ \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\r}{\\mathbf{r}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\z}{\\mathbf{z}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\C}{\\mathbf{C}} \\newcommand{\\rank}{\\textbf{rank}} \\newcommand{\\det}{\\textbf{det}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\U}{\\mathrm{U}} \\newcommand{\\V}{\\mathrm{V}} \\newcommand{\\W}{\\mathrm{W}} \\newcommand{\\L}{\\mathcal{L}} \\] Linear Algebra Interview Questions Given matrix \\(\\A \\in \\R^{m \\times n}\\) . For what vectors \\(\\mathbf{b} \\in \\mathbb{R}^{m}\\) does \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) have a solution \\(\\mathbf{x} \\in \\mathbb{R}^{n}\\) ? We need to realize a few key points before answering the question. \\(\\A\\x = \\b\\) is equivalent to a system of equations, where there are \\(m\\) equations and \\(n\\) variables (unknowns) and we have a few cases to enumerate: All cases enumerated from the cartesian product \\(\\{\\textbf{Underdetermined, Exactly Determined, Overdetermined}\\} \\times \\{\\textbf{Consistent, Inconsistent}\\}\\) If one finds the definition in the previous point unfamiliar/vague, then please read here 1 . Since we know that right matrix-vector multiplication of \\(\\A\\) on \\(\\x\\) means: \\[ \\A\\x = x_1 \\begin{bmatrix} a_{11} \\\\ a_{21} \\\\ \\vdots \\\\ a_{m1} \\end{bmatrix} + x_2 \\begin{bmatrix} a_{12} \\\\ a_{22} \\\\ \\vdots \\\\ a_{m2} \\end{bmatrix} + \\cdots + x_n \\begin{bmatrix} a_{1n} \\\\ a_{2n} \\\\ \\vdots \\\\ a_{mn} \\end{bmatrix} \\] which is made up of linear combination of columns of \\(\\A\\) with elements of \\(\\x\\) as coefficients. Case 1: m > n We first need to realize that this is an overdetermined system because we have more equations than unknowns. Let us point back to the question, what kind of \\(\\b\\) allows us to have a solution \\(\\x\\) that solves this equation? Certainly not all kinds of \\(\\b\\) ! Why? We can illustrate with an easy example, if \\(m = 4, n = 3\\) , then \\(\\b\\) is made up of linear combination of columns of \\(\\A\\) , but we only have \\(3\\) columns, and at most we have something like \\(\\lambda_1 \\c_1 + \\lambda_2 \\c_2 + \\lambda_3 \\c_3\\) where \\(\\lambda \\in \\R, \\c_i \\in \\R^{4}\\) , \\(\\c_i\\) being the columns of \\(\\A\\) . However, we know from my previous chapter on Basis and Dimension 's Theorem (Equivalent Basis Definition) that we necessarily need \\(4\\) vectors in \\(\\R^{4}\\) to span this subspace. Consequently, \\(3\\) vectors (columns of \\(\\A\\) ) cannot possibly span the entire \\(\\R^4\\) ( column space of \\(\\A\\) here ) and hence there are vectors \\(\\b\\) that are not linear combinations of columns of \\(\\A\\) . Consistent With the above in mind, then if \\(\\b\\) is in the column space of \\(\\A\\) , then \\(\\b\\) is solvable and thus has a solution. Inconsistent If \\(\\b \\not \\in C(\\A)\\) , then no solution \\(\\x\\) exists. Case 2: m < n We first need to realize that this is an underdetermined system because we have more unknowns than equations. Let us point back to the question, what kind of \\(\\b\\) allows us to have a solution \\(\\x\\) that solves this equation? One would've thought that all kinds of \\(\\b\\) will have a solution \\(\\x\\) , as opposed to the overdetermined system , but this is not true! Why? - - We can illustrate with an easy example, if \\(m = 3, n = 4\\) , then \\(\\b\\) is made up of linear combination of columns of \\(\\A\\) , although we have \\(4\\) columns, that does not mean all columns span the entire \\(\\R^4\\) space. Consider the \\(4\\) columns, but \\(\\r_2, \\r_3, \\r_4\\) are multiples of \\(\\r_1\\) , and hence these \\(4\\) columns only managed to span the 1D-subspace in \\(\\R^4\\) , consequently, there are \"many\" \\(\\b\\) 's that cannot be represented by these \\(4\\) columns. We leave to the readers to construct and enumerate all examples. Case 3: m = n This is an exercise for the reader \ud83d\ude02\ud83d\ude02\ud83d\ude02\ud83d\ude02\ud83d\ude02\ud83d\ude02\ud83d\ude02\ud83d\ude02\ud83d\ude02 How are \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) and \\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\) related in the context of Machine Learning? Read Linear Algebra: Theory, Intuition, Code, 2021. (pp. 230-231) . Why do we say that matrices are linear transformations? 2 Naive Interpretation: Let \\(\\A = (\\a_{ij}) \\in \\F^{m \\times n}\\) be an \\(m \\times n\\) matrix with entries in a field \\(\\F\\) . Let us first define the matrix-vector multiplication \\(\\A\\x = \\b\\) where \\(\\x \\in \\F^{n}\\) and \\(\\b \\in \\F^{m}\\) , and specifically, let \\(\\F^n\\) and \\(\\F^m\\) be our vector spaces \\(\\V\\) and \\(\\W\\) respectively. Define the map \\[T_{\\A}: \\F_{c}^{n} \\rightarrow F_{c}^{m}\\] \\[\\x = \\begin{bmatrix} x_{1}\\\\ x_{2}\\\\ x_{3}\\\\ \\vdots\\\\ x_{n} \\end{bmatrix} \\mapsto \\A\\x \\] Then \\(T_{\\A}\\) is a linear transformation (associated with the matrix \\(\\A\\) ) where we are mapping (sending) the vector \\(\\x\\) from \\(n\\) dimensional space to the vector \\(\\b = \\A\\x\\) in the \\(m\\) dimensional space . Important for readers: \\(T_{\\A}(\\x) = \\A\\x\\) . What does the determinant of a matrix represent (geometrically)?[^chip_huyen_5.1.2.3] Geometrically, if we are given a 2 by 2 matrix \\(\\A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\) , then denote the columns as \\(\\v_1 = \\begin{bmatrix} a \\\\ b \\end{bmatrix}\\) and \\(\\v_2 = \\begin{bmatrix} c \\\\ d \\end{bmatrix}\\) . We can show that the \\(\\det(\\A) = ad - bc\\) is actually the area of the parallelogram spanned by the 2 vectors. This generalizes to \\(n\\) dimensional space (i.e. 3 by 3 matrix corresponds to a volume of a cuboid) https://math.stackexchange.com/questions/668/whats-an-intuitive-way-to-think-about-the-determinant https://www.khanacademy.org/math/linear-algebra/matrix-transformations/determinant-depth/v/linear-algebra-determinant-and-area-of-a-parallelogram https://www.khanacademy.org/math/linear-algebra/matrix-transformations/determinant-depth/v/linear-algebra-determinant-as-scaling-factor 3blue1brown: https://www.youtube.com/watch?v=Ip3X9LOh2dk Name some applications with examples of a determinant of a matrix. What is the direct sum decomposition? It makes sense that a vector \\(\\v \\in V\\) can be decomposed uniquely to \\(\\u + \\w\\) from two orthogonal subspaces that complement each other. After all, imagine a 2d R2, we already know that nullspace and row space of A form R2, then it follows that if we take one vector from V, then we take one vector from nullspace, then assume nullspace not empty, then it follows that there must exists another vector from row space of A that is linearly independent from the vector in nullspace, that makes up the https://www.khanacademy.org/math/linear-algebra/alternate-bases/orthogonal-projections/v/linear-algebra-projections-onto-subspaces?modal=1 Relationship of Covariance matrix and Gram Matrix?[^chip_huyen_5.1.2.7] https://stats.stackexchange.com/questions/164997/relationship-between-gram-and-covariance-matrices https://en.wikipedia.org/wiki/Consistent_and_inconsistent_equations \u21a9 Chip Huyen: ML Interviews Book, 2021. section 5.1.2, Q1 \u21a9","title":"Interview"},{"location":"reighns_ml_journey/mathematics/linear_algebra/linear_algebra_interview_questions/#linear-algebra-interview-questions","text":"","title":"Linear Algebra Interview Questions"},{"location":"reighns_ml_journey/mathematics/linear_algebra/linear_algebra_interview_questions/#given-matrix-a-in-rm-times-n-for-what-vectors-mathbfb-in-mathbbrm-does-mathbfamathbfx-mathbfb-have-a-solution-mathbfx-in-mathbbrn","text":"We need to realize a few key points before answering the question. \\(\\A\\x = \\b\\) is equivalent to a system of equations, where there are \\(m\\) equations and \\(n\\) variables (unknowns) and we have a few cases to enumerate: All cases enumerated from the cartesian product \\(\\{\\textbf{Underdetermined, Exactly Determined, Overdetermined}\\} \\times \\{\\textbf{Consistent, Inconsistent}\\}\\) If one finds the definition in the previous point unfamiliar/vague, then please read here 1 . Since we know that right matrix-vector multiplication of \\(\\A\\) on \\(\\x\\) means: \\[ \\A\\x = x_1 \\begin{bmatrix} a_{11} \\\\ a_{21} \\\\ \\vdots \\\\ a_{m1} \\end{bmatrix} + x_2 \\begin{bmatrix} a_{12} \\\\ a_{22} \\\\ \\vdots \\\\ a_{m2} \\end{bmatrix} + \\cdots + x_n \\begin{bmatrix} a_{1n} \\\\ a_{2n} \\\\ \\vdots \\\\ a_{mn} \\end{bmatrix} \\] which is made up of linear combination of columns of \\(\\A\\) with elements of \\(\\x\\) as coefficients.","title":"Given matrix \\(\\A \\in \\R^{m \\times n}\\). For what vectors \\(\\mathbf{b} \\in \\mathbb{R}^{m}\\) does \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) have a solution \\(\\mathbf{x} \\in \\mathbb{R}^{n}\\)?"},{"location":"reighns_ml_journey/mathematics/linear_algebra/linear_algebra_interview_questions/#case-1-m-n","text":"We first need to realize that this is an overdetermined system because we have more equations than unknowns. Let us point back to the question, what kind of \\(\\b\\) allows us to have a solution \\(\\x\\) that solves this equation? Certainly not all kinds of \\(\\b\\) ! Why? We can illustrate with an easy example, if \\(m = 4, n = 3\\) , then \\(\\b\\) is made up of linear combination of columns of \\(\\A\\) , but we only have \\(3\\) columns, and at most we have something like \\(\\lambda_1 \\c_1 + \\lambda_2 \\c_2 + \\lambda_3 \\c_3\\) where \\(\\lambda \\in \\R, \\c_i \\in \\R^{4}\\) , \\(\\c_i\\) being the columns of \\(\\A\\) . However, we know from my previous chapter on Basis and Dimension 's Theorem (Equivalent Basis Definition) that we necessarily need \\(4\\) vectors in \\(\\R^{4}\\) to span this subspace. Consequently, \\(3\\) vectors (columns of \\(\\A\\) ) cannot possibly span the entire \\(\\R^4\\) ( column space of \\(\\A\\) here ) and hence there are vectors \\(\\b\\) that are not linear combinations of columns of \\(\\A\\) .","title":"Case 1: m &gt; n"},{"location":"reighns_ml_journey/mathematics/linear_algebra/linear_algebra_interview_questions/#consistent","text":"With the above in mind, then if \\(\\b\\) is in the column space of \\(\\A\\) , then \\(\\b\\) is solvable and thus has a solution.","title":"Consistent"},{"location":"reighns_ml_journey/mathematics/linear_algebra/linear_algebra_interview_questions/#inconsistent","text":"If \\(\\b \\not \\in C(\\A)\\) , then no solution \\(\\x\\) exists.","title":"Inconsistent"},{"location":"reighns_ml_journey/mathematics/linear_algebra/linear_algebra_interview_questions/#case-2-m-n","text":"We first need to realize that this is an underdetermined system because we have more unknowns than equations. Let us point back to the question, what kind of \\(\\b\\) allows us to have a solution \\(\\x\\) that solves this equation? One would've thought that all kinds of \\(\\b\\) will have a solution \\(\\x\\) , as opposed to the overdetermined system , but this is not true! Why? - - We can illustrate with an easy example, if \\(m = 3, n = 4\\) , then \\(\\b\\) is made up of linear combination of columns of \\(\\A\\) , although we have \\(4\\) columns, that does not mean all columns span the entire \\(\\R^4\\) space. Consider the \\(4\\) columns, but \\(\\r_2, \\r_3, \\r_4\\) are multiples of \\(\\r_1\\) , and hence these \\(4\\) columns only managed to span the 1D-subspace in \\(\\R^4\\) , consequently, there are \"many\" \\(\\b\\) 's that cannot be represented by these \\(4\\) columns. We leave to the readers to construct and enumerate all examples.","title":"Case 2: m &lt; n"},{"location":"reighns_ml_journey/mathematics/linear_algebra/linear_algebra_interview_questions/#case-3-m-n","text":"This is an exercise for the reader \ud83d\ude02\ud83d\ude02\ud83d\ude02\ud83d\ude02\ud83d\ude02\ud83d\ude02\ud83d\ude02\ud83d\ude02\ud83d\ude02","title":"Case 3: m = n"},{"location":"reighns_ml_journey/mathematics/linear_algebra/linear_algebra_interview_questions/#how-are-mathbfamathbfx-mathbfb-and-mathbfamathbfx-mathbf0-related-in-the-context-of-machine-learning","text":"Read Linear Algebra: Theory, Intuition, Code, 2021. (pp. 230-231) .","title":"How are \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) and \\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\) related in the context of Machine Learning?"},{"location":"reighns_ml_journey/mathematics/linear_algebra/linear_algebra_interview_questions/#why-do-we-say-that-matrices-are-linear-transformations2","text":"Naive Interpretation: Let \\(\\A = (\\a_{ij}) \\in \\F^{m \\times n}\\) be an \\(m \\times n\\) matrix with entries in a field \\(\\F\\) . Let us first define the matrix-vector multiplication \\(\\A\\x = \\b\\) where \\(\\x \\in \\F^{n}\\) and \\(\\b \\in \\F^{m}\\) , and specifically, let \\(\\F^n\\) and \\(\\F^m\\) be our vector spaces \\(\\V\\) and \\(\\W\\) respectively. Define the map \\[T_{\\A}: \\F_{c}^{n} \\rightarrow F_{c}^{m}\\] \\[\\x = \\begin{bmatrix} x_{1}\\\\ x_{2}\\\\ x_{3}\\\\ \\vdots\\\\ x_{n} \\end{bmatrix} \\mapsto \\A\\x \\] Then \\(T_{\\A}\\) is a linear transformation (associated with the matrix \\(\\A\\) ) where we are mapping (sending) the vector \\(\\x\\) from \\(n\\) dimensional space to the vector \\(\\b = \\A\\x\\) in the \\(m\\) dimensional space . Important for readers: \\(T_{\\A}(\\x) = \\A\\x\\) .","title":"Why do we say that matrices are linear transformations?2"},{"location":"reighns_ml_journey/mathematics/linear_algebra/linear_algebra_interview_questions/#what-does-the-determinant-of-a-matrix-represent-geometricallychip_huyen_5123","text":"Geometrically, if we are given a 2 by 2 matrix \\(\\A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\) , then denote the columns as \\(\\v_1 = \\begin{bmatrix} a \\\\ b \\end{bmatrix}\\) and \\(\\v_2 = \\begin{bmatrix} c \\\\ d \\end{bmatrix}\\) . We can show that the \\(\\det(\\A) = ad - bc\\) is actually the area of the parallelogram spanned by the 2 vectors. This generalizes to \\(n\\) dimensional space (i.e. 3 by 3 matrix corresponds to a volume of a cuboid) https://math.stackexchange.com/questions/668/whats-an-intuitive-way-to-think-about-the-determinant https://www.khanacademy.org/math/linear-algebra/matrix-transformations/determinant-depth/v/linear-algebra-determinant-and-area-of-a-parallelogram https://www.khanacademy.org/math/linear-algebra/matrix-transformations/determinant-depth/v/linear-algebra-determinant-as-scaling-factor 3blue1brown: https://www.youtube.com/watch?v=Ip3X9LOh2dk","title":"What does the determinant of a matrix represent (geometrically)?[^chip_huyen_5.1.2.3]"},{"location":"reighns_ml_journey/mathematics/linear_algebra/linear_algebra_interview_questions/#name-some-applications-with-examples-of-a-determinant-of-a-matrix","text":"","title":"Name some applications with examples of a determinant of a matrix."},{"location":"reighns_ml_journey/mathematics/linear_algebra/linear_algebra_interview_questions/#what-is-the-direct-sum-decomposition","text":"It makes sense that a vector \\(\\v \\in V\\) can be decomposed uniquely to \\(\\u + \\w\\) from two orthogonal subspaces that complement each other. After all, imagine a 2d R2, we already know that nullspace and row space of A form R2, then it follows that if we take one vector from V, then we take one vector from nullspace, then assume nullspace not empty, then it follows that there must exists another vector from row space of A that is linearly independent from the vector in nullspace, that makes up the https://www.khanacademy.org/math/linear-algebra/alternate-bases/orthogonal-projections/v/linear-algebra-projections-onto-subspaces?modal=1","title":"What is the direct sum decomposition?"},{"location":"reighns_ml_journey/mathematics/linear_algebra/linear_algebra_interview_questions/#relationship-of-covariance-matrix-and-gram-matrixchip_huyen_5127","text":"https://stats.stackexchange.com/questions/164997/relationship-between-gram-and-covariance-matrices https://en.wikipedia.org/wiki/Consistent_and_inconsistent_equations \u21a9 Chip Huyen: ML Interviews Book, 2021. section 5.1.2, Q1 \u21a9","title":"Relationship of Covariance matrix and Gram Matrix?[^chip_huyen_5.1.2.7]"},{"location":"reighns_ml_journey/mathematics/linear_algebra/linear_algebra_matrix_determinant/","text":"\\[ \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\C}{\\mathbb{C}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\det}{\\textbf{det}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\Q}{\\mathbf{Q}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\q}{\\mathbf{q}} \\newcommand{\\e}{\\mathbf{e}} \\newcommand{\\I}{\\mathbf{I}} \\] Determinant Definition (Determinant of a 2x2 Matrix) We start off with the base case first. Given a 2 by 2 matrix \\(\\A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\) . the determinant of the \\(\\A\\) is given by: \\[ \\det(\\A) = ad - bc \\] Theorem (Rank-Reduced Matrix has Determinant of Zero) We first prove this in 2 by 2 setting, and note that it can be generalized to \\(n\\) dimensions. Consider a 2 by 2 matrix \\(\\A\\) , without loss of generality, let us represent the rank-reduced matrix such that: \\[ \\A = \\begin{bmatrix} a & b \\\\ \\lambda a & \\lambda b \\end{bmatrix} \\] where we note that the second row is \\(\\lambda\\) of the first. We can easily deduce the determinant to be: \\[ \\det(\\A) = a \\lambda b - b \\lambda a = 0 \\] This results hold for any \\(\\lambda\\) and thus is true for any rank-reduced matrix. Determinant of Triangular Matrix","title":"Linear algebra matrix determinant"},{"location":"reighns_ml_journey/mathematics/linear_algebra/linear_algebra_matrix_determinant/#determinant","text":"","title":"Determinant"},{"location":"reighns_ml_journey/mathematics/linear_algebra/linear_algebra_matrix_determinant/#definition-determinant-of-a-2x2-matrix","text":"We start off with the base case first. Given a 2 by 2 matrix \\(\\A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\) . the determinant of the \\(\\A\\) is given by: \\[ \\det(\\A) = ad - bc \\]","title":"Definition (Determinant of a 2x2 Matrix)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/linear_algebra_matrix_determinant/#theorem-rank-reduced-matrix-has-determinant-of-zero","text":"We first prove this in 2 by 2 setting, and note that it can be generalized to \\(n\\) dimensions. Consider a 2 by 2 matrix \\(\\A\\) , without loss of generality, let us represent the rank-reduced matrix such that: \\[ \\A = \\begin{bmatrix} a & b \\\\ \\lambda a & \\lambda b \\end{bmatrix} \\] where we note that the second row is \\(\\lambda\\) of the first. We can easily deduce the determinant to be: \\[ \\det(\\A) = a \\lambda b - b \\lambda a = 0 \\] This results hold for any \\(\\lambda\\) and thus is true for any rank-reduced matrix.","title":"Theorem (Rank-Reduced Matrix has Determinant of Zero)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/linear_algebra_matrix_determinant/#determinant-of-triangular-matrix","text":"","title":"Determinant of Triangular Matrix"},{"location":"reighns_ml_journey/mathematics/linear_algebra/02_vectors/02_linear_algebra_vectors_exercises/","text":"Table of Contents Exercise 1: Linear Combination Exercise 2: Dot Product and Average Exercise 3: Dot Product and Weighted Average Exercise 1: Linear Combination Given a set of weights and vectors, write a python function that outputs the linear combination of the vectors with the respective weights. The code to the solution is presented below, it is important to realize that the number of elements in the weights vector should be the same as the number of vectors. import numpy as np from typing import List , Union # as col vector v1 = np . asarray ([ 1 , 2 , 3 , 4 , 5 ]) . reshape ( - 1 , 1 ) v2 = np . asarray ([ 2 , 4 , 6 , 8 , 10 ]) . reshape ( - 1 , 1 ) v3 = np . asarray ([ 3 , 6 , 9 , 12 , 15 ]) . reshape ( - 1 , 1 ) weights = [ 10 , 20 , 30 ] def linear_combination_vectors ( weights : List [ float ], * args : np . ndarray ) -> np . ndarray : \"\"\"Computes the linear combination of vectors. Args: weights (List[float]): The set of weights corresponding to each vector. *args (np.ndarray): The set of vectors. Returns: linear_weighted_sum (np.ndarray): The linear combination of vectors. Examples: >>> v1 = np.asarray([1, 2, 3, 4, 5]).reshape(-1, 1) >>> v2 = np.asarray([2, 4, 6, 8, 10]).reshape(-1, 1) >>> v3 = np.asarray([3, 6, 9, 12, 15]).reshape(-1, 1) >>> weights = [10, 20, 30] >>> linear_combination_vectors([10, 20, 30], v1, v2, v3) \"\"\" linear_weighted_sum = np . zeros ( shape = args [ 0 ] . shape ) for weight , vec in zip ( weights , args ): linear_weighted_sum += weight * vec return linear_weighted_sum linear_combination_vectors ( weights , v1 , v2 , v3 ) array([[140.], [280.], [420.], [560.], [700.]]) Exercise 2: Dot Product and Average Develop a method to use the dot product to compute the average of a set of numbers in a vector. Since we want to compute the average of all elements in a vector \\(\\v \\in \\R^n\\) , we can first see the formula of average to be: \\( \\(\\bar{\\v} = \\frac{v_1 + v_2 + ... + v_n}{n}\\) \\) To make use of dot product, we can define \\(\\1\\) and perform \\(\\v^\\top \\cdot \\1\\) which returns the sum of all elements in \\(\\v\\) by the definition of dot product. Lastly, divide this answer by the total number of elements. def dot_product ( v1 : np . ndarray , v2 : np . ndarray ) -> float : \"\"\"Computes the dot product of two vectors. We assume both vectors are flattened, i.e. they are 1D arrays. Args: v1 (np.ndarray): The first vector. v2 (np.ndarray): The second vector. Returns: dot_product_v1_v2 (float): The dot product of two vectors. Examples: >>> v1 = np.asarray([1, 2, 3, 4, 5]) >>> v2 = np.asarray([2, 4, 6, 8, 10]) >>> dot_product(v1, v2) \"\"\" v1 , v2 = np . asarray ( v1 ) . flatten (), np . asarray ( v2 ) . flatten () dot_product_v1_v2 = 0 for element_1 , element_2 in zip ( v1 , v2 ): dot_product_v1_v2 += element_1 * element_2 # same as np.dot but does not take into the orientation of vectors assert dot_product_v1_v2 == np . dot ( v1 . T , v2 ) return dot_product_v1_v2 def average_set ( vec : Union [ np . ndarray , set ]) -> float : \"\"\"Average a set of numbers using dot product. Given a set of numbers {v1, v2, ..., vn}, the average is defined as: avg = (v1 + v2 + ... + vn) / n To use the dot product, we can convert the set to a col/row vector (array) `vec` and perform the dot product with the vector of ones to get `sum(set)`. Lastly, we divide by the number of elements in the set. Args: vec (Union[np.ndarray, set]): A set of numbers. Returns: average (float): The average of the set. \"\"\" if isinstance ( vec , set ): vec = np . asarray ( list ( vec )) . flatten () ones = np . ones ( shape = vec . shape ) total_sum = dot_product ( vec , ones ) average = total_sum / vec . shape [ 0 ] return average # as col vector v = np . asarray ([ 1 , 2 , 3 , 4 , 5 ]) v_set = { 1 , 2 , 3 , 4 , 5 } average = average_set ( v_set ) print ( f \"average of all vectors in v_set is { average } \" ) assert average == np . mean ( v ) average of all vectors in v_set is 3.0 Exercise 3: Dot Product and Weighted Average What if some numbers were more important than other numbers? Modify your answer to the previous question to devise a method to use the dot product to compute a weighted mean of a set of numbers. We assume weighted mean to be normalized such that the weights of all the vectors must sum up to 1. # as col vector v1 = np . asarray ([ 1 , 2 , 3 , 4 , 5 ]) . reshape ( - 1 , 1 ) shape_v1 = v1 . shape num_elements = shape_v1 [ 0 ] random_weights = np . random . rand ( * shape_v1 ) normalized_random_weights = random_weights / num_elements total_sum = dot_product ( v1 , normalized_random_weights ) weighted_average = total_sum / v1 . shape [ 0 ] print ( f \"weighted average is { weighted_average } \" ) weighted average is 0.4017198249010809","title":"Exercises"},{"location":"reighns_ml_journey/mathematics/linear_algebra/02_vectors/02_linear_algebra_vectors_exercises/#table-of-contents","text":"Exercise 1: Linear Combination Exercise 2: Dot Product and Average Exercise 3: Dot Product and Weighted Average","title":"Table of Contents"},{"location":"reighns_ml_journey/mathematics/linear_algebra/02_vectors/02_linear_algebra_vectors_exercises/#exercise-1-linear-combination","text":"Given a set of weights and vectors, write a python function that outputs the linear combination of the vectors with the respective weights. The code to the solution is presented below, it is important to realize that the number of elements in the weights vector should be the same as the number of vectors. import numpy as np from typing import List , Union # as col vector v1 = np . asarray ([ 1 , 2 , 3 , 4 , 5 ]) . reshape ( - 1 , 1 ) v2 = np . asarray ([ 2 , 4 , 6 , 8 , 10 ]) . reshape ( - 1 , 1 ) v3 = np . asarray ([ 3 , 6 , 9 , 12 , 15 ]) . reshape ( - 1 , 1 ) weights = [ 10 , 20 , 30 ] def linear_combination_vectors ( weights : List [ float ], * args : np . ndarray ) -> np . ndarray : \"\"\"Computes the linear combination of vectors. Args: weights (List[float]): The set of weights corresponding to each vector. *args (np.ndarray): The set of vectors. Returns: linear_weighted_sum (np.ndarray): The linear combination of vectors. Examples: >>> v1 = np.asarray([1, 2, 3, 4, 5]).reshape(-1, 1) >>> v2 = np.asarray([2, 4, 6, 8, 10]).reshape(-1, 1) >>> v3 = np.asarray([3, 6, 9, 12, 15]).reshape(-1, 1) >>> weights = [10, 20, 30] >>> linear_combination_vectors([10, 20, 30], v1, v2, v3) \"\"\" linear_weighted_sum = np . zeros ( shape = args [ 0 ] . shape ) for weight , vec in zip ( weights , args ): linear_weighted_sum += weight * vec return linear_weighted_sum linear_combination_vectors ( weights , v1 , v2 , v3 ) array([[140.], [280.], [420.], [560.], [700.]])","title":"Exercise 1: Linear Combination"},{"location":"reighns_ml_journey/mathematics/linear_algebra/02_vectors/02_linear_algebra_vectors_exercises/#exercise-2-dot-product-and-average","text":"Develop a method to use the dot product to compute the average of a set of numbers in a vector. Since we want to compute the average of all elements in a vector \\(\\v \\in \\R^n\\) , we can first see the formula of average to be: \\( \\(\\bar{\\v} = \\frac{v_1 + v_2 + ... + v_n}{n}\\) \\) To make use of dot product, we can define \\(\\1\\) and perform \\(\\v^\\top \\cdot \\1\\) which returns the sum of all elements in \\(\\v\\) by the definition of dot product. Lastly, divide this answer by the total number of elements. def dot_product ( v1 : np . ndarray , v2 : np . ndarray ) -> float : \"\"\"Computes the dot product of two vectors. We assume both vectors are flattened, i.e. they are 1D arrays. Args: v1 (np.ndarray): The first vector. v2 (np.ndarray): The second vector. Returns: dot_product_v1_v2 (float): The dot product of two vectors. Examples: >>> v1 = np.asarray([1, 2, 3, 4, 5]) >>> v2 = np.asarray([2, 4, 6, 8, 10]) >>> dot_product(v1, v2) \"\"\" v1 , v2 = np . asarray ( v1 ) . flatten (), np . asarray ( v2 ) . flatten () dot_product_v1_v2 = 0 for element_1 , element_2 in zip ( v1 , v2 ): dot_product_v1_v2 += element_1 * element_2 # same as np.dot but does not take into the orientation of vectors assert dot_product_v1_v2 == np . dot ( v1 . T , v2 ) return dot_product_v1_v2 def average_set ( vec : Union [ np . ndarray , set ]) -> float : \"\"\"Average a set of numbers using dot product. Given a set of numbers {v1, v2, ..., vn}, the average is defined as: avg = (v1 + v2 + ... + vn) / n To use the dot product, we can convert the set to a col/row vector (array) `vec` and perform the dot product with the vector of ones to get `sum(set)`. Lastly, we divide by the number of elements in the set. Args: vec (Union[np.ndarray, set]): A set of numbers. Returns: average (float): The average of the set. \"\"\" if isinstance ( vec , set ): vec = np . asarray ( list ( vec )) . flatten () ones = np . ones ( shape = vec . shape ) total_sum = dot_product ( vec , ones ) average = total_sum / vec . shape [ 0 ] return average # as col vector v = np . asarray ([ 1 , 2 , 3 , 4 , 5 ]) v_set = { 1 , 2 , 3 , 4 , 5 } average = average_set ( v_set ) print ( f \"average of all vectors in v_set is { average } \" ) assert average == np . mean ( v ) average of all vectors in v_set is 3.0","title":"Exercise 2: Dot Product and Average"},{"location":"reighns_ml_journey/mathematics/linear_algebra/02_vectors/02_linear_algebra_vectors_exercises/#exercise-3-dot-product-and-weighted-average","text":"What if some numbers were more important than other numbers? Modify your answer to the previous question to devise a method to use the dot product to compute a weighted mean of a set of numbers. We assume weighted mean to be normalized such that the weights of all the vectors must sum up to 1. # as col vector v1 = np . asarray ([ 1 , 2 , 3 , 4 , 5 ]) . reshape ( - 1 , 1 ) shape_v1 = v1 . shape num_elements = shape_v1 [ 0 ] random_weights = np . random . rand ( * shape_v1 ) normalized_random_weights = random_weights / num_elements total_sum = dot_product ( v1 , normalized_random_weights ) weighted_average = total_sum / v1 . shape [ 0 ] print ( f \"weighted average is { weighted_average } \" ) weighted average is 0.4017198249010809","title":"Exercise 3: Dot Product and Weighted Average"},{"location":"reighns_ml_journey/mathematics/linear_algebra/02_vectors/02_linear_algebra_vectors_linear_combination/","text":"\\[\\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\p}{\\mathbf{p}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\z}{\\mathbf{z}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\C}{\\mathbf{C}} \\newcommand{\\E}{\\mathbf{E}} \\newcommand{\\P}{\\mathbf{P}}\\] DO MY OWN EXAMPLES, AND USE EXAMPLES FROM WIKI EASIER ADD THE LINEAR COMB PYTHON CODE BACK TO HERE INSTEAD OF DOT PROD Precursor to Linear Combinations Building from Vectors and Scalar Multiplication Before delving into the concept of linear combinations, it's crucial to reinforce our understanding of vectors in \\(\\mathbb{R}^D\\) and the operation of scalar-vector multiplication. Recall that a vector \\(\\mathbf{v} \\in \\mathbb{R}^D\\) can be scaled by a scalar \\(\\lambda \\in \\mathbb{R}\\) , resulting in a new vector \\(\\lambda \\mathbf{v}\\) , as discussed in the previous sections. Introduction to Vector Spaces and Span The idea of a vector space is fundamental in linear algebra. A vector space over a field (such as the real numbers, \\(\\mathbb{R}\\) ) is a collection of objects (vectors) that can be added together and multiplied by scalars from that field. An essential concept within vector spaces is the span of a set of vectors. Informally, the span of a set of vectors is the set of all possible vectors that can be formed by scalar multiplication and vector addition of those vectors. Visualizing Span in 2D and 3D To visualize the concept of span, consider vectors in 2D or 3D spaces. For instance, two non-parallel vectors in \\(\\mathbb{R}^2\\) (the 2D plane) span the entire plane because any point on the plane can be reached by scaling and adding these vectors. Similarly, in \\(\\mathbb{R}^3\\) , three non-coplanar vectors span the entire 3D space. Scalar Multiplication and Span Scalar multiplication plays a pivotal role in spanning a space with vectors. For example, a single vector \\(\\mathbf{u} \\in \\mathbb{R}^2\\) can only span a line, which is all scalar multiples of \\(\\mathbf{u}\\) . However, adding another non-collinear vector \\(\\mathbf{v}\\) expands the span to cover the entire plane. Graphical representations of these vectors and their scaled versions can effectively demonstrate how they span different spaces. Toward Linear Combinations The concepts we've discussed set the stage for understanding linear combinations. A linear combination of a set of vectors involves forming new vectors by adding scaled versions of these vectors. Specifically, for vectors \\(\\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^D\\) and scalars \\(\\alpha, \\beta \\in \\mathbb{R}\\) , a linear combination is expressed as \\(\\alpha\\mathbf{u} + \\beta\\mathbf{v}\\) . This is a natural extension of what we have already explored with vector addition and scalar multiplication. In the upcoming sections, we will formalize this concept and delve into its deeper implications in linear algebra. The below work are from https://github.com/MacroAnalyst/Linear_Algebra_With_Python. Algebraic Definition (Linear Combination) Definition extracted from Wikipedia 1 . Let \\(V\\) be a vector space over the field \\(\\F\\) . As usual, we call elements of \\(V\\) vectors and call elements of \\(\\F\\) scalars. If \\(\\v_1, ... , \\v_n\\) are vectors and \\(a_1, ..., a_n\\) are scalars, then the linear combination of those vectors with those scalars as coefficients is \\[ a_1\\v_1 + a_2\\v_2 + ... + a_n\\v_n \\] Note that by definition, a linear combination involves only finitely many vectors (except as described in Generalizations below). However, the set S that the vectors are taken from (if one is mentioned) can still be infinite; each individual linear combination will only involve finitely many vectors. Also, there is no reason that n cannot be zero; in that case, we declare by convention that the result of the linear combination is the zero vector in \\(V\\) . import matplotlib.pyplot as plt import numpy as np from mpl_toolkits.mplot3d import Axes3D import sympy as sy sy . init_printing () Visualization of Linear Combination in \\(\\mathbb{R}^2\\) Consider two vectors \\(u\\) and \\(v\\) in \\(\\mathbb{R}^2\\) , they are independent of each other, i.e. not pointing to the same or opposite direction. Therefore any vector in the \\(\\mathbb{R}^2\\) can be represented by a linear combination of \\(u\\) and \\(v\\) . For instance, this is a linear combination and essentially a linear system. \\[ c_1 \\left[ \\begin{matrix} 4\\\\ 2 \\end{matrix} \\right]+ c_2 \\left[ \\begin{matrix} -2\\\\ 2 \\end{matrix} \\right] = \\left[ \\begin{matrix} 2\\\\ 10 \\end{matrix} \\right] \\] Solve the system in SymPy: A = sy . Matrix ([[ 4 , - 2 , 2 ], [ 2 , 2 , 10 ]]) A . rref () \\(\\displaystyle \\left( \\left[\\begin{matrix}1 & 0 & 2\\\\0 & 1 & 3\\end{matrix}\\right], \\ \\left( 0, \\ 1\\right)\\right)\\) The solution is \\((c_1, c_2)^T = (2, 3)^T\\) , which means the addition of \\(2\\) times of \\(\\left[ \\begin{matrix} 4\\\\ 2 \\end{matrix} \\right]\\) and \\(3\\) times of \\(\\left[ \\begin{matrix} -2\\\\ 2 \\end{matrix} \\right]\\) equals \\(\\left[ \\begin{matrix} 2\\\\ 10 \\end{matrix} \\right]\\) . Besides plotting the vector addition, we would like to plot the coordinates of basis that spanned by \\(u\\) and \\(v\\) . We will explain further in later chapter. Calculate the slope of vectors, i.e. \\(\\frac{y}{x}\\) $$ s_1 =\\frac{y}{x} = \\frac{2}{4}=.5\\ s_2 =\\frac{y}{x}= \\frac{2}{-2}=.-1 $$ The basis can be constructed as: $$ y_1 = a+.5x\\ y_2 = b-x $$ where \\(a\\) and \\(b\\) will be set as constants with regular intervals, such as \\((2.5, 5, 7.5, 10)\\) . The coordinates of basis are pink web-style grids, each line segment is a unit (like \\(1\\) in Cartesian coordinate system) in the 'new' coordinates. fig , ax = plt . subplots ( figsize = ( 8 , 8 )) vec = np . array ([[[ 0 , 0 , 4 , 2 ]], [[ 0 , 0 , - 2 , 2 ]], [[ 0 , 0 , 2 , 10 ]], [[ 0 , 0 , 8 , 4 ]], [[ 0 , 0 , - 6 , 6 ]]]) colors = [ 'b' , 'b' , 'r' , 'b' , 'b' ] for i in range ( vec . shape [ 0 ]): X , Y , U , V = zip ( * vec [ i ,:,:]) ax . quiver ( X , Y , U , V , angles = 'xy' , scale_units = 'xy' , color = colors [ i ], scale = 1 , alpha = .6 ) ax . text ( x = vec [ i , 0 , 2 ], y = vec [ i , 0 , 3 ], s = '( %.0d , %.0d )' % ( vec [ i , 0 , 2 ], vec [ i , 0 , 3 ]), fontsize = 16 ) points12 = np . array ([[ 8 , 4 ],[ 2 , 10 ]]) ax . plot ( points12 [:, 0 ], points12 [:, 1 ], c = 'b' , lw = 3.5 , alpha = 0.5 , ls = '--' ) points34 = np . array ([[ - 6 , 6 ],[ 2 , 10 ]]) ax . plot ( points34 [:, 0 ], points34 [:, 1 ], c = 'b' , lw = 3.5 , alpha = 0.5 , ls = '--' ) ax . set_xlim ([ - 10 , 10 ]) ax . set_ylim ([ 0 , 10.5 ]) ax . set_xlabel ( 'x-axis' , fontsize = 16 ) ax . set_ylabel ( 'y-axis' , fontsize = 16 ) ax . grid () ######################################Basis######################################## a = np . arange ( - 11 , 20 , 1 ) x = np . arange ( - 11 , 20 , 1 ) for i in a : y1 = i + .5 * x ax . plot ( x , y1 , ls = '--' , color = 'pink' , lw = 2 ) y2 = i - x ax . plot ( x , y2 , ls = '--' , color = 'pink' , lw = 2 ) ax . set_title ( 'Linear Combination of Two Vectors in $\\mathbf {R} ^2$' , size = 22 , x = 0.5 , y = 1.01 ) plt . show () Linear Combination Visualization in 3D We can also show that any vectors in \\(\\mathbb{R}^3\\) can be a linear combination of a standard basis in Cartesian coordinate system. Here is the function for plotting 3D linear combination from standard basis, we just feed the scalar multiplier . def linearCombo ( a , b , c ): '''This function is for visualizing linear combination of standard basis in 3D. Function syntax: linearCombo(a, b, c), where a, b, c are the scalar multiplier, also the elements of the vector. ''' fig = plt . figure ( figsize = ( 10 , 10 )) ax = fig . add_subplot ( 111 , projection = '3d' ) ######################## Standard basis and Scalar Multiplid Vectors######################### vec = np . array ([[[ 0 , 0 , 0 , 1 , 0 , 0 ]], # e1 [[ 0 , 0 , 0 , 0 , 1 , 0 ]], # e2 [[ 0 , 0 , 0 , 0 , 0 , 1 ]], # e3 [[ 0 , 0 , 0 , a , 0 , 0 ]], # a* e1 [[ 0 , 0 , 0 , 0 , b , 0 ]], # b* e2 [[ 0 , 0 , 0 , 0 , 0 , c ]], # c* e3 [[ 0 , 0 , 0 , a , b , c ]]]) # ae1 + be2 + ce3 colors = [ 'b' , 'b' , 'b' , 'r' , 'r' , 'r' , 'g' ] for i in range ( vec . shape [ 0 ]): X , Y , Z , U , V , W = zip ( * vec [ i ,:,:]) ax . quiver ( X , Y , Z , U , V , W , length = 1 , normalize = False , color = colors [ i ] , arrow_length_ratio = .08 , pivot = 'tail' , linestyles = 'solid' , linewidths = 3 , alpha = .6 ) #################################Plot Rectangle Boxes############################## dlines = np . array ([[[ a , 0 , 0 ],[ a , b , 0 ]], [[ 0 , b , 0 ],[ a , b , 0 ]], [[ 0 , 0 , c ],[ a , b , c ]], [[ 0 , 0 , c ],[ a , 0 , c ]], [[ a , 0 , c ],[ a , b , c ]], [[ 0 , 0 , c ],[ 0 , b , c ]], [[ 0 , b , c ],[ a , b , c ]], [[ a , 0 , 0 ],[ a , 0 , c ]], [[ 0 , b , 0 ],[ 0 , b , c ]], [[ a , b , 0 ],[ a , b , c ]]]) colors = [ 'k' , 'k' , 'g' , 'k' , 'k' , 'k' , 'k' , 'k' , 'k' ] for i in range ( dlines . shape [ 0 ]): ax . plot ( dlines [ i ,:, 0 ], dlines [ i ,:, 1 ], dlines [ i ,:, 2 ], lw = 3 , ls = '--' , color = 'black' , alpha = 0.5 ) #################################Annotation######################################## ax . text ( x = a , y = b , z = c , s = ' $(%0.d, %0.d, %.0d )$' % ( a , b , c ), size = 18 ) ax . text ( x = a , y = 0 , z = 0 , s = ' $%0.d e_1 = (%0.d, 0, 0)$' % ( a , a ), size = 15 ) ax . text ( x = 0 , y = b , z = 0 , s = ' $%0.d e_2 = (0, %0.d, 0)$' % ( b , b ), size = 15 ) ax . text ( x = 0 , y = 0 , z = c , s = ' $%0.d e_3 = (0, 0, %0.d)$' % ( c , c ), size = 15 ) #################################Axis Setting###################################### ax . grid () ax . set_xlim ([ 0 , a + 1 ]) ax . set_ylim ([ 0 , b + 1 ]) ax . set_zlim ([ 0 , c + 1 ]) ax . set_xlabel ( 'x-axis' , size = 18 ) ax . set_ylabel ( 'y-axis' , size = 18 ) ax . set_zlabel ( 'z-axis' , size = 18 ) ax . set_title ( 'Vector $(%0.d, %0.d, %.0d )$ Visualization' % ( a , b , c ), size = 20 ) ax . view_init ( elev = 20. , azim = 15 ) if __name__ == '__main__' : a = 7 b = 4 c = 9 linearCombo ( a , b , c ) linearCombo ( 3 , 5 , 6 ) # Try again Linear Combination of Inconsistent System Inconsistent system means no unique solution exists. It might sound weird to treat a solution of an inconsistent system as a linear combination, but it is essential a trace of line. One Free Variable Case We have seen how inconsistent systems can be solved in the earlier lectures. Now we will investigate what solution means from the perspective of linear combination. Consider a system \\[ \\left[ \\begin{matrix} 1 & 1 & 2\\\\ -2 &0 & 1\\\\ 1& 1 & 2 \\end{matrix} \\right] \\left[ \\begin{matrix} c_1\\\\c_2\\\\c_3 \\end{matrix} \\right] = \\left[ \\begin{matrix} 1\\\\-3\\\\1 \\end{matrix} \\right] \\] Solve in SymPy: A = sy . Matrix ([[ 1 , 1 , 2 , 1 ],[ - 2 , 0 , 1 , - 3 ],[ 1 , 1 , 2 , 1 ]]) A . rref () \\(\\displaystyle \\left( \\left[\\begin{matrix}1 & 0 & - \\frac{1}{2} & \\frac{3}{2}\\\\0 & 1 & \\frac{5}{2} & - \\frac{1}{2}\\\\0 & 0 & 0 & 0\\end{matrix}\\right], \\ \\left( 0, \\ 1\\right)\\right)\\) The solution is not unique due to a free variable: \\[ c_1 - \\frac{1}{2}c_3 =\\frac{3}{2}\\\\ c_2 + \\frac{5}{2}c_3 = -\\frac{1}{2}\\\\ c_3 = free \\] Let \\(c_3 = t\\) , the system can be parameterized: \\[ \\left[ \\begin{matrix} c_1\\\\c_2\\\\c_3 \\end{matrix} \\right] = \\left[ \\begin{matrix} \\frac{3}{2}+\\frac{1}{2}t\\\\ -\\frac{1}{2}-\\frac{5}{2}t\\\\ t \\end{matrix} \\right] \\] The solution is a line of infinite length, to visualize it, we set the range of \\(t\\in (-1, 1)\\) , the solution looks like: fig = plt . figure ( figsize = ( 8 , 8 )) ax = fig . add_subplot ( projection = '3d' ) t = np . linspace ( - 1 , 1 , 10 ) c1 = 3 / 2 + t / 2 c2 = - 1 / 2 - 5 / 2 * t ax . plot ( c1 , c2 , t , lw = 5 ) ax . set_xlabel ( 'x-axis' , size = 18 ) ax . set_ylabel ( 'y-axis' , size = 18 ) ax . set_zlabel ( 'z-axis' , size = 18 ) ax . set_title ( 'Solution of A Linear System with One Free Variable' , size = 18 ) plt . show () Two Free Variables Case Now consider the linear system: $$ \\left[ \\begin{matrix} 1 & -3 & -2\\ 0 &0 & 0 \\ 0& 0 & 0 \\end{matrix} \\right] \\left[ \\begin{matrix} x_1\\ x_2\\ x_3 \\end{matrix} \\right] = \\left[ \\begin{matrix} 0\\0\\0 \\end{matrix} \\right] $$ The augmented matrix is $$ \\left[ \\begin{matrix} 1 & -3 & -2 & 0\\ 0 &0 & 0 & 0\\ 0& 0 & 0 & 0 \\end{matrix} \\right] $$ We have two free variables $$ \\begin{align} x_1 &= 3x_2+2x_3\\ x_2 &= free\\ x_3 &= free \\end{align} $$ Rewrite the solution \\[ \\left[ \\begin{matrix} x_1\\\\ x_2\\\\ x_3 \\end{matrix} \\right] = \\left[ \\begin{matrix} 3x_2+2x_3\\\\ x_2\\\\ x_3 \\end{matrix} \\right] = \\left[\\begin{array}{c} 3 x_{2} \\\\ x_{2} \\\\ 0 \\end{array}\\right]+\\left[\\begin{array}{c} 2 x_{3} \\\\ 0 \\\\ x_{3} \\end{array}\\right]= x_{2}\\left[\\begin{array}{l} 3 \\\\ 1 \\\\ 0 \\end{array}\\right]+x_{3}\\left[\\begin{array}{l} 2 \\\\ 0 \\\\ 1 \\end{array}\\right] \\] The solution is a plain spanned by two vectors \\((3, 1, 0)^T\\) and \\((2, 0, 1)^T\\) . Let's draw the plane and spanning vectors. We also plot another vector \\(v = (2,2,1)\\) which is not a linear combination of \\((3, 1, 0)^T\\) and \\((2, 0, 1)^T\\) . As you pan around the view angle (in JupyterLab use %matplotlib widge ), it is apparent that \\(v\\) is not in the same plane of basis vectors. fig = plt . figure ( figsize = ( 8 , 8 )) ax = fig . add_subplot ( projection = '3d' ) x2 = np . linspace ( - 2 , 2 , 10 ) x3 = np . linspace ( - 2 , 2 , 10 ) X2 , X3 = np . meshgrid ( x2 , x3 ) X1 = 3 * X2 + 2 * X3 ax . plot_wireframe ( X1 , X2 , X3 , linewidth = 1.5 , color = 'k' , alpha = .6 ) vec = np . array ([[[ 0 , 0 , 0 , 3 , 1 , 0 ]], [[ 0 , 0 , 0 , 2 , 0 , 1 ]], [[ 0 , 0 , 0 , 5 , 1 , 1 ]], [[ 0 , 0 , 0 , 2 , 2 , 1 ]]]) colors = [ 'r' , 'b' , 'g' , 'purple' ] for i in range ( vec . shape [ 0 ]): X , Y , Z , U , V , W = zip ( * vec [ i ,:,:]) ax . quiver ( X , Y , Z , U , V , W , length = 1 , normalize = False , color = colors [ i ], arrow_length_ratio = .08 , pivot = 'tail' , linestyles = 'solid' , linewidths = 3 , alpha = .6 ) ################################Dashed Line################################ point12 = np . array ([[ 2 , 0 , 1 ],[ 5 , 1 , 1 ]]) ax . plot ( point12 [:, 0 ], point12 [:, 1 ], point12 [:, 2 ], lw = 3 , ls = '--' , color = 'black' , alpha = 0.5 ) point34 = np . array ([[ 3 , 1 , 0 ], [ 5 , 1 , 1 ]]) ax . plot ( point34 [:, 0 ], point34 [:, 1 ], point34 [:, 2 ], lw = 3 , ls = '--' , color = 'black' , alpha = 0.5 ) #################################Texts####################################### ax . text ( x = 3 , y = 1 , z = 0 , s = '$(3, 1, 0)$' , color = 'red' , size = 16 ) ax . text ( x = 2 , y = 0 , z = 1 , s = '$(2, 0, 1)$' , color = 'blue' , size = 16 ) ax . text ( x = 5 , y = 1 , z = 1 , s = '$(5, 1, 1)$' , color = 'green' , size = 16 ) ax . text ( x = 2 , y = 2 , z = 1 , s = '$v$' , color = 'purple' , size = 16 ) ax . set_xlabel ( 'x-axis' , size = 18 ) ax . set_ylabel ( 'y-axis' , size = 18 ) ax . set_zlabel ( 'z-axis' , size = 18 ) ax . view_init ( elev =- 29 , azim = 130 ) Linear Combination of Polynomial In a more general sense, a function or a polynomial can also be a linear combination of other functions or polynomials. Now consider a polynomial \\(p(x)=4 x^{3}+5 x^{2}-2 x+7\\) , determine if it is a linear combination of three polynomials below: \\[ p_{1}(x)=x^{3}+2 x^{2}-x+1 \\] \\[ p_{2}(x)=2 x^{3}+x^{2}-x+1 \\] \\[ p_{3}(x)=x^{3}-x^{2}-x-4 \\] which means that we need to figure out if the equation below holds \\[ c_{1}\\left(x^{3}+2 x^{2}-x+1\\right)+c_{2}\\left(2 x^{3}+x^{2}-x+1\\right)+c_{3}\\left(x^{3}-x^{2}-x-4\\right)=4 x^{3}+5 x^{2}-2 x+7 \\] Rearrange and collect terms $$ \\left(c_{1}+2 c_{2}+c_{3}\\right) x^{3}+\\left(2 c_{1}+c_{2}-c_{3}\\right) x^{2}+\\left(-c_{1}-c_{2}-c_{3}\\right) x+\\left(c_{1}+c_{2}-4 c_{3}\\right)=4 x^{3}+5 x^{2}-2 x+7 $$ Equate the coefficients and extract the augmented matrix $$ \\begin{aligned} &c_{1}+2 c_{2}+c_{3}=4\\ &2 c_{1}+c_{2}-c_{3}=5\\ &-c_{1}-c_{2}-c_{3}=-2\\ &c_{1}+c_{2}-4 c_{3}=7\\ &\\left[\\begin{array}{cccc} 1 & 2 & 1 & 3 \\ 2 & 1 & -1 & 5 \\ -1 & -1 & -1 & -2 \\ 1 & 1 & -4 & 7 \\end{array}\\right] \\end{aligned} $$ Before solving, we notice that the system has 4 equations, but 3 unknowns, this case is called over-determined . A = sy . Matrix ([[ 1 , 2 , 1 , 4 ],[ 2 , 1 , - 1 , 5 ],[ - 1 , - 1 , - 1 , - 2 ],[ 1 , 1 , - 4 , 7 ]]) A . rref () \\(\\displaystyle \\left( \\left[\\begin{matrix}1 & 0 & 0 & 1\\\\0 & 1 & 0 & 2\\\\0 & 0 & 1 & -1\\\\0 & 0 & 0 & 0\\end{matrix}\\right], \\ \\left( 0, \\ 1, \\ 2\\right)\\right)\\) We get the answer \\((c_1, c_2, c_3)^T = (1, 2, -1)^T\\) , plug in back to equation $$ \\left(x^{3}+2 x^{2}-x+1\\right)+2\\left(2 x^{3}+x^{2}-x+1\\right)-\\left(x^{3}-x^{2}-x-4\\right)=4 x^{3}+5 x^{2}-2 x+7 $$ Indeed we have just established a linear combination between these polynomials. https://en.wikipedia.org/wiki/Linear_combination#:~:text=In%20mathematics%2C%20a%20linear%20combination,a%20and%20b%20are%20constants). \u21a9","title":"02 linear algebra vectors linear combination"},{"location":"reighns_ml_journey/mathematics/linear_algebra/02_vectors/02_linear_algebra_vectors_linear_combination/#do-my-own-examples-and-use-examples-from-wiki-easier","text":"","title":"DO MY OWN EXAMPLES, AND USE EXAMPLES FROM WIKI EASIER"},{"location":"reighns_ml_journey/mathematics/linear_algebra/02_vectors/02_linear_algebra_vectors_linear_combination/#add-the-linear-comb-python-code-back-to-here-instead-of-dot-prod","text":"","title":"ADD THE LINEAR COMB PYTHON CODE BACK TO HERE INSTEAD OF DOT PROD"},{"location":"reighns_ml_journey/mathematics/linear_algebra/02_vectors/02_linear_algebra_vectors_linear_combination/#precursor-to-linear-combinations","text":"","title":"Precursor to Linear Combinations"},{"location":"reighns_ml_journey/mathematics/linear_algebra/02_vectors/02_linear_algebra_vectors_linear_combination/#building-from-vectors-and-scalar-multiplication","text":"Before delving into the concept of linear combinations, it's crucial to reinforce our understanding of vectors in \\(\\mathbb{R}^D\\) and the operation of scalar-vector multiplication. Recall that a vector \\(\\mathbf{v} \\in \\mathbb{R}^D\\) can be scaled by a scalar \\(\\lambda \\in \\mathbb{R}\\) , resulting in a new vector \\(\\lambda \\mathbf{v}\\) , as discussed in the previous sections.","title":"Building from Vectors and Scalar Multiplication"},{"location":"reighns_ml_journey/mathematics/linear_algebra/02_vectors/02_linear_algebra_vectors_linear_combination/#introduction-to-vector-spaces-and-span","text":"The idea of a vector space is fundamental in linear algebra. A vector space over a field (such as the real numbers, \\(\\mathbb{R}\\) ) is a collection of objects (vectors) that can be added together and multiplied by scalars from that field. An essential concept within vector spaces is the span of a set of vectors. Informally, the span of a set of vectors is the set of all possible vectors that can be formed by scalar multiplication and vector addition of those vectors.","title":"Introduction to Vector Spaces and Span"},{"location":"reighns_ml_journey/mathematics/linear_algebra/02_vectors/02_linear_algebra_vectors_linear_combination/#visualizing-span-in-2d-and-3d","text":"To visualize the concept of span, consider vectors in 2D or 3D spaces. For instance, two non-parallel vectors in \\(\\mathbb{R}^2\\) (the 2D plane) span the entire plane because any point on the plane can be reached by scaling and adding these vectors. Similarly, in \\(\\mathbb{R}^3\\) , three non-coplanar vectors span the entire 3D space.","title":"Visualizing Span in 2D and 3D"},{"location":"reighns_ml_journey/mathematics/linear_algebra/02_vectors/02_linear_algebra_vectors_linear_combination/#scalar-multiplication-and-span","text":"Scalar multiplication plays a pivotal role in spanning a space with vectors. For example, a single vector \\(\\mathbf{u} \\in \\mathbb{R}^2\\) can only span a line, which is all scalar multiples of \\(\\mathbf{u}\\) . However, adding another non-collinear vector \\(\\mathbf{v}\\) expands the span to cover the entire plane. Graphical representations of these vectors and their scaled versions can effectively demonstrate how they span different spaces.","title":"Scalar Multiplication and Span"},{"location":"reighns_ml_journey/mathematics/linear_algebra/02_vectors/02_linear_algebra_vectors_linear_combination/#toward-linear-combinations","text":"The concepts we've discussed set the stage for understanding linear combinations. A linear combination of a set of vectors involves forming new vectors by adding scaled versions of these vectors. Specifically, for vectors \\(\\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^D\\) and scalars \\(\\alpha, \\beta \\in \\mathbb{R}\\) , a linear combination is expressed as \\(\\alpha\\mathbf{u} + \\beta\\mathbf{v}\\) . This is a natural extension of what we have already explored with vector addition and scalar multiplication. In the upcoming sections, we will formalize this concept and delve into its deeper implications in linear algebra.","title":"Toward Linear Combinations"},{"location":"reighns_ml_journey/mathematics/linear_algebra/02_vectors/02_linear_algebra_vectors_linear_combination/#the-below-work-are-from-httpsgithubcommacroanalystlinear_algebra_with_python","text":"","title":"The below work are from https://github.com/MacroAnalyst/Linear_Algebra_With_Python."},{"location":"reighns_ml_journey/mathematics/linear_algebra/02_vectors/02_linear_algebra_vectors_linear_combination/#algebraic-definition-linear-combination","text":"Definition extracted from Wikipedia 1 . Let \\(V\\) be a vector space over the field \\(\\F\\) . As usual, we call elements of \\(V\\) vectors and call elements of \\(\\F\\) scalars. If \\(\\v_1, ... , \\v_n\\) are vectors and \\(a_1, ..., a_n\\) are scalars, then the linear combination of those vectors with those scalars as coefficients is \\[ a_1\\v_1 + a_2\\v_2 + ... + a_n\\v_n \\] Note that by definition, a linear combination involves only finitely many vectors (except as described in Generalizations below). However, the set S that the vectors are taken from (if one is mentioned) can still be infinite; each individual linear combination will only involve finitely many vectors. Also, there is no reason that n cannot be zero; in that case, we declare by convention that the result of the linear combination is the zero vector in \\(V\\) . import matplotlib.pyplot as plt import numpy as np from mpl_toolkits.mplot3d import Axes3D import sympy as sy sy . init_printing ()","title":"Algebraic Definition (Linear Combination)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/02_vectors/02_linear_algebra_vectors_linear_combination/#visualization-of-linear-combination-in-mathbbr2","text":"Consider two vectors \\(u\\) and \\(v\\) in \\(\\mathbb{R}^2\\) , they are independent of each other, i.e. not pointing to the same or opposite direction. Therefore any vector in the \\(\\mathbb{R}^2\\) can be represented by a linear combination of \\(u\\) and \\(v\\) . For instance, this is a linear combination and essentially a linear system. \\[ c_1 \\left[ \\begin{matrix} 4\\\\ 2 \\end{matrix} \\right]+ c_2 \\left[ \\begin{matrix} -2\\\\ 2 \\end{matrix} \\right] = \\left[ \\begin{matrix} 2\\\\ 10 \\end{matrix} \\right] \\] Solve the system in SymPy: A = sy . Matrix ([[ 4 , - 2 , 2 ], [ 2 , 2 , 10 ]]) A . rref () \\(\\displaystyle \\left( \\left[\\begin{matrix}1 & 0 & 2\\\\0 & 1 & 3\\end{matrix}\\right], \\ \\left( 0, \\ 1\\right)\\right)\\) The solution is \\((c_1, c_2)^T = (2, 3)^T\\) , which means the addition of \\(2\\) times of \\(\\left[ \\begin{matrix} 4\\\\ 2 \\end{matrix} \\right]\\) and \\(3\\) times of \\(\\left[ \\begin{matrix} -2\\\\ 2 \\end{matrix} \\right]\\) equals \\(\\left[ \\begin{matrix} 2\\\\ 10 \\end{matrix} \\right]\\) . Besides plotting the vector addition, we would like to plot the coordinates of basis that spanned by \\(u\\) and \\(v\\) . We will explain further in later chapter. Calculate the slope of vectors, i.e. \\(\\frac{y}{x}\\) $$ s_1 =\\frac{y}{x} = \\frac{2}{4}=.5\\ s_2 =\\frac{y}{x}= \\frac{2}{-2}=.-1 $$ The basis can be constructed as: $$ y_1 = a+.5x\\ y_2 = b-x $$ where \\(a\\) and \\(b\\) will be set as constants with regular intervals, such as \\((2.5, 5, 7.5, 10)\\) . The coordinates of basis are pink web-style grids, each line segment is a unit (like \\(1\\) in Cartesian coordinate system) in the 'new' coordinates. fig , ax = plt . subplots ( figsize = ( 8 , 8 )) vec = np . array ([[[ 0 , 0 , 4 , 2 ]], [[ 0 , 0 , - 2 , 2 ]], [[ 0 , 0 , 2 , 10 ]], [[ 0 , 0 , 8 , 4 ]], [[ 0 , 0 , - 6 , 6 ]]]) colors = [ 'b' , 'b' , 'r' , 'b' , 'b' ] for i in range ( vec . shape [ 0 ]): X , Y , U , V = zip ( * vec [ i ,:,:]) ax . quiver ( X , Y , U , V , angles = 'xy' , scale_units = 'xy' , color = colors [ i ], scale = 1 , alpha = .6 ) ax . text ( x = vec [ i , 0 , 2 ], y = vec [ i , 0 , 3 ], s = '( %.0d , %.0d )' % ( vec [ i , 0 , 2 ], vec [ i , 0 , 3 ]), fontsize = 16 ) points12 = np . array ([[ 8 , 4 ],[ 2 , 10 ]]) ax . plot ( points12 [:, 0 ], points12 [:, 1 ], c = 'b' , lw = 3.5 , alpha = 0.5 , ls = '--' ) points34 = np . array ([[ - 6 , 6 ],[ 2 , 10 ]]) ax . plot ( points34 [:, 0 ], points34 [:, 1 ], c = 'b' , lw = 3.5 , alpha = 0.5 , ls = '--' ) ax . set_xlim ([ - 10 , 10 ]) ax . set_ylim ([ 0 , 10.5 ]) ax . set_xlabel ( 'x-axis' , fontsize = 16 ) ax . set_ylabel ( 'y-axis' , fontsize = 16 ) ax . grid () ######################################Basis######################################## a = np . arange ( - 11 , 20 , 1 ) x = np . arange ( - 11 , 20 , 1 ) for i in a : y1 = i + .5 * x ax . plot ( x , y1 , ls = '--' , color = 'pink' , lw = 2 ) y2 = i - x ax . plot ( x , y2 , ls = '--' , color = 'pink' , lw = 2 ) ax . set_title ( 'Linear Combination of Two Vectors in $\\mathbf {R} ^2$' , size = 22 , x = 0.5 , y = 1.01 ) plt . show ()","title":" Visualization of Linear Combination in \\(\\mathbb{R}^2\\) "},{"location":"reighns_ml_journey/mathematics/linear_algebra/02_vectors/02_linear_algebra_vectors_linear_combination/#linear-combination-visualization-in-3d","text":"We can also show that any vectors in \\(\\mathbb{R}^3\\) can be a linear combination of a standard basis in Cartesian coordinate system. Here is the function for plotting 3D linear combination from standard basis, we just feed the scalar multiplier . def linearCombo ( a , b , c ): '''This function is for visualizing linear combination of standard basis in 3D. Function syntax: linearCombo(a, b, c), where a, b, c are the scalar multiplier, also the elements of the vector. ''' fig = plt . figure ( figsize = ( 10 , 10 )) ax = fig . add_subplot ( 111 , projection = '3d' ) ######################## Standard basis and Scalar Multiplid Vectors######################### vec = np . array ([[[ 0 , 0 , 0 , 1 , 0 , 0 ]], # e1 [[ 0 , 0 , 0 , 0 , 1 , 0 ]], # e2 [[ 0 , 0 , 0 , 0 , 0 , 1 ]], # e3 [[ 0 , 0 , 0 , a , 0 , 0 ]], # a* e1 [[ 0 , 0 , 0 , 0 , b , 0 ]], # b* e2 [[ 0 , 0 , 0 , 0 , 0 , c ]], # c* e3 [[ 0 , 0 , 0 , a , b , c ]]]) # ae1 + be2 + ce3 colors = [ 'b' , 'b' , 'b' , 'r' , 'r' , 'r' , 'g' ] for i in range ( vec . shape [ 0 ]): X , Y , Z , U , V , W = zip ( * vec [ i ,:,:]) ax . quiver ( X , Y , Z , U , V , W , length = 1 , normalize = False , color = colors [ i ] , arrow_length_ratio = .08 , pivot = 'tail' , linestyles = 'solid' , linewidths = 3 , alpha = .6 ) #################################Plot Rectangle Boxes############################## dlines = np . array ([[[ a , 0 , 0 ],[ a , b , 0 ]], [[ 0 , b , 0 ],[ a , b , 0 ]], [[ 0 , 0 , c ],[ a , b , c ]], [[ 0 , 0 , c ],[ a , 0 , c ]], [[ a , 0 , c ],[ a , b , c ]], [[ 0 , 0 , c ],[ 0 , b , c ]], [[ 0 , b , c ],[ a , b , c ]], [[ a , 0 , 0 ],[ a , 0 , c ]], [[ 0 , b , 0 ],[ 0 , b , c ]], [[ a , b , 0 ],[ a , b , c ]]]) colors = [ 'k' , 'k' , 'g' , 'k' , 'k' , 'k' , 'k' , 'k' , 'k' ] for i in range ( dlines . shape [ 0 ]): ax . plot ( dlines [ i ,:, 0 ], dlines [ i ,:, 1 ], dlines [ i ,:, 2 ], lw = 3 , ls = '--' , color = 'black' , alpha = 0.5 ) #################################Annotation######################################## ax . text ( x = a , y = b , z = c , s = ' $(%0.d, %0.d, %.0d )$' % ( a , b , c ), size = 18 ) ax . text ( x = a , y = 0 , z = 0 , s = ' $%0.d e_1 = (%0.d, 0, 0)$' % ( a , a ), size = 15 ) ax . text ( x = 0 , y = b , z = 0 , s = ' $%0.d e_2 = (0, %0.d, 0)$' % ( b , b ), size = 15 ) ax . text ( x = 0 , y = 0 , z = c , s = ' $%0.d e_3 = (0, 0, %0.d)$' % ( c , c ), size = 15 ) #################################Axis Setting###################################### ax . grid () ax . set_xlim ([ 0 , a + 1 ]) ax . set_ylim ([ 0 , b + 1 ]) ax . set_zlim ([ 0 , c + 1 ]) ax . set_xlabel ( 'x-axis' , size = 18 ) ax . set_ylabel ( 'y-axis' , size = 18 ) ax . set_zlabel ( 'z-axis' , size = 18 ) ax . set_title ( 'Vector $(%0.d, %0.d, %.0d )$ Visualization' % ( a , b , c ), size = 20 ) ax . view_init ( elev = 20. , azim = 15 ) if __name__ == '__main__' : a = 7 b = 4 c = 9 linearCombo ( a , b , c ) linearCombo ( 3 , 5 , 6 ) # Try again","title":" Linear Combination Visualization in 3D"},{"location":"reighns_ml_journey/mathematics/linear_algebra/02_vectors/02_linear_algebra_vectors_linear_combination/#linear-combination-of-inconsistent-system","text":"Inconsistent system means no unique solution exists. It might sound weird to treat a solution of an inconsistent system as a linear combination, but it is essential a trace of line.","title":" Linear Combination of Inconsistent System"},{"location":"reighns_ml_journey/mathematics/linear_algebra/02_vectors/02_linear_algebra_vectors_linear_combination/#one-free-variable-case","text":"We have seen how inconsistent systems can be solved in the earlier lectures. Now we will investigate what solution means from the perspective of linear combination. Consider a system \\[ \\left[ \\begin{matrix} 1 & 1 & 2\\\\ -2 &0 & 1\\\\ 1& 1 & 2 \\end{matrix} \\right] \\left[ \\begin{matrix} c_1\\\\c_2\\\\c_3 \\end{matrix} \\right] = \\left[ \\begin{matrix} 1\\\\-3\\\\1 \\end{matrix} \\right] \\] Solve in SymPy: A = sy . Matrix ([[ 1 , 1 , 2 , 1 ],[ - 2 , 0 , 1 , - 3 ],[ 1 , 1 , 2 , 1 ]]) A . rref () \\(\\displaystyle \\left( \\left[\\begin{matrix}1 & 0 & - \\frac{1}{2} & \\frac{3}{2}\\\\0 & 1 & \\frac{5}{2} & - \\frac{1}{2}\\\\0 & 0 & 0 & 0\\end{matrix}\\right], \\ \\left( 0, \\ 1\\right)\\right)\\) The solution is not unique due to a free variable: \\[ c_1 - \\frac{1}{2}c_3 =\\frac{3}{2}\\\\ c_2 + \\frac{5}{2}c_3 = -\\frac{1}{2}\\\\ c_3 = free \\] Let \\(c_3 = t\\) , the system can be parameterized: \\[ \\left[ \\begin{matrix} c_1\\\\c_2\\\\c_3 \\end{matrix} \\right] = \\left[ \\begin{matrix} \\frac{3}{2}+\\frac{1}{2}t\\\\ -\\frac{1}{2}-\\frac{5}{2}t\\\\ t \\end{matrix} \\right] \\] The solution is a line of infinite length, to visualize it, we set the range of \\(t\\in (-1, 1)\\) , the solution looks like: fig = plt . figure ( figsize = ( 8 , 8 )) ax = fig . add_subplot ( projection = '3d' ) t = np . linspace ( - 1 , 1 , 10 ) c1 = 3 / 2 + t / 2 c2 = - 1 / 2 - 5 / 2 * t ax . plot ( c1 , c2 , t , lw = 5 ) ax . set_xlabel ( 'x-axis' , size = 18 ) ax . set_ylabel ( 'y-axis' , size = 18 ) ax . set_zlabel ( 'z-axis' , size = 18 ) ax . set_title ( 'Solution of A Linear System with One Free Variable' , size = 18 ) plt . show ()","title":" One Free Variable Case"},{"location":"reighns_ml_journey/mathematics/linear_algebra/02_vectors/02_linear_algebra_vectors_linear_combination/#two-free-variables-case","text":"Now consider the linear system: $$ \\left[ \\begin{matrix} 1 & -3 & -2\\ 0 &0 & 0 \\ 0& 0 & 0 \\end{matrix} \\right] \\left[ \\begin{matrix} x_1\\ x_2\\ x_3 \\end{matrix} \\right] = \\left[ \\begin{matrix} 0\\0\\0 \\end{matrix} \\right] $$ The augmented matrix is $$ \\left[ \\begin{matrix} 1 & -3 & -2 & 0\\ 0 &0 & 0 & 0\\ 0& 0 & 0 & 0 \\end{matrix} \\right] $$ We have two free variables $$ \\begin{align} x_1 &= 3x_2+2x_3\\ x_2 &= free\\ x_3 &= free \\end{align} $$ Rewrite the solution \\[ \\left[ \\begin{matrix} x_1\\\\ x_2\\\\ x_3 \\end{matrix} \\right] = \\left[ \\begin{matrix} 3x_2+2x_3\\\\ x_2\\\\ x_3 \\end{matrix} \\right] = \\left[\\begin{array}{c} 3 x_{2} \\\\ x_{2} \\\\ 0 \\end{array}\\right]+\\left[\\begin{array}{c} 2 x_{3} \\\\ 0 \\\\ x_{3} \\end{array}\\right]= x_{2}\\left[\\begin{array}{l} 3 \\\\ 1 \\\\ 0 \\end{array}\\right]+x_{3}\\left[\\begin{array}{l} 2 \\\\ 0 \\\\ 1 \\end{array}\\right] \\] The solution is a plain spanned by two vectors \\((3, 1, 0)^T\\) and \\((2, 0, 1)^T\\) . Let's draw the plane and spanning vectors. We also plot another vector \\(v = (2,2,1)\\) which is not a linear combination of \\((3, 1, 0)^T\\) and \\((2, 0, 1)^T\\) . As you pan around the view angle (in JupyterLab use %matplotlib widge ), it is apparent that \\(v\\) is not in the same plane of basis vectors. fig = plt . figure ( figsize = ( 8 , 8 )) ax = fig . add_subplot ( projection = '3d' ) x2 = np . linspace ( - 2 , 2 , 10 ) x3 = np . linspace ( - 2 , 2 , 10 ) X2 , X3 = np . meshgrid ( x2 , x3 ) X1 = 3 * X2 + 2 * X3 ax . plot_wireframe ( X1 , X2 , X3 , linewidth = 1.5 , color = 'k' , alpha = .6 ) vec = np . array ([[[ 0 , 0 , 0 , 3 , 1 , 0 ]], [[ 0 , 0 , 0 , 2 , 0 , 1 ]], [[ 0 , 0 , 0 , 5 , 1 , 1 ]], [[ 0 , 0 , 0 , 2 , 2 , 1 ]]]) colors = [ 'r' , 'b' , 'g' , 'purple' ] for i in range ( vec . shape [ 0 ]): X , Y , Z , U , V , W = zip ( * vec [ i ,:,:]) ax . quiver ( X , Y , Z , U , V , W , length = 1 , normalize = False , color = colors [ i ], arrow_length_ratio = .08 , pivot = 'tail' , linestyles = 'solid' , linewidths = 3 , alpha = .6 ) ################################Dashed Line################################ point12 = np . array ([[ 2 , 0 , 1 ],[ 5 , 1 , 1 ]]) ax . plot ( point12 [:, 0 ], point12 [:, 1 ], point12 [:, 2 ], lw = 3 , ls = '--' , color = 'black' , alpha = 0.5 ) point34 = np . array ([[ 3 , 1 , 0 ], [ 5 , 1 , 1 ]]) ax . plot ( point34 [:, 0 ], point34 [:, 1 ], point34 [:, 2 ], lw = 3 , ls = '--' , color = 'black' , alpha = 0.5 ) #################################Texts####################################### ax . text ( x = 3 , y = 1 , z = 0 , s = '$(3, 1, 0)$' , color = 'red' , size = 16 ) ax . text ( x = 2 , y = 0 , z = 1 , s = '$(2, 0, 1)$' , color = 'blue' , size = 16 ) ax . text ( x = 5 , y = 1 , z = 1 , s = '$(5, 1, 1)$' , color = 'green' , size = 16 ) ax . text ( x = 2 , y = 2 , z = 1 , s = '$v$' , color = 'purple' , size = 16 ) ax . set_xlabel ( 'x-axis' , size = 18 ) ax . set_ylabel ( 'y-axis' , size = 18 ) ax . set_zlabel ( 'z-axis' , size = 18 ) ax . view_init ( elev =- 29 , azim = 130 )","title":" Two Free Variables Case"},{"location":"reighns_ml_journey/mathematics/linear_algebra/02_vectors/02_linear_algebra_vectors_linear_combination/#linear-combination-of-polynomial","text":"In a more general sense, a function or a polynomial can also be a linear combination of other functions or polynomials. Now consider a polynomial \\(p(x)=4 x^{3}+5 x^{2}-2 x+7\\) , determine if it is a linear combination of three polynomials below: \\[ p_{1}(x)=x^{3}+2 x^{2}-x+1 \\] \\[ p_{2}(x)=2 x^{3}+x^{2}-x+1 \\] \\[ p_{3}(x)=x^{3}-x^{2}-x-4 \\] which means that we need to figure out if the equation below holds \\[ c_{1}\\left(x^{3}+2 x^{2}-x+1\\right)+c_{2}\\left(2 x^{3}+x^{2}-x+1\\right)+c_{3}\\left(x^{3}-x^{2}-x-4\\right)=4 x^{3}+5 x^{2}-2 x+7 \\] Rearrange and collect terms $$ \\left(c_{1}+2 c_{2}+c_{3}\\right) x^{3}+\\left(2 c_{1}+c_{2}-c_{3}\\right) x^{2}+\\left(-c_{1}-c_{2}-c_{3}\\right) x+\\left(c_{1}+c_{2}-4 c_{3}\\right)=4 x^{3}+5 x^{2}-2 x+7 $$ Equate the coefficients and extract the augmented matrix $$ \\begin{aligned} &c_{1}+2 c_{2}+c_{3}=4\\ &2 c_{1}+c_{2}-c_{3}=5\\ &-c_{1}-c_{2}-c_{3}=-2\\ &c_{1}+c_{2}-4 c_{3}=7\\ &\\left[\\begin{array}{cccc} 1 & 2 & 1 & 3 \\ 2 & 1 & -1 & 5 \\ -1 & -1 & -1 & -2 \\ 1 & 1 & -4 & 7 \\end{array}\\right] \\end{aligned} $$ Before solving, we notice that the system has 4 equations, but 3 unknowns, this case is called over-determined . A = sy . Matrix ([[ 1 , 2 , 1 , 4 ],[ 2 , 1 , - 1 , 5 ],[ - 1 , - 1 , - 1 , - 2 ],[ 1 , 1 , - 4 , 7 ]]) A . rref () \\(\\displaystyle \\left( \\left[\\begin{matrix}1 & 0 & 0 & 1\\\\0 & 1 & 0 & 2\\\\0 & 0 & 1 & -1\\\\0 & 0 & 0 & 0\\end{matrix}\\right], \\ \\left( 0, \\ 1, \\ 2\\right)\\right)\\) We get the answer \\((c_1, c_2, c_3)^T = (1, 2, -1)^T\\) , plug in back to equation $$ \\left(x^{3}+2 x^{2}-x+1\\right)+2\\left(2 x^{3}+x^{2}-x+1\\right)-\\left(x^{3}-x^{2}-x-4\\right)=4 x^{3}+5 x^{2}-2 x+7 $$ Indeed we have just established a linear combination between these polynomials. https://en.wikipedia.org/wiki/Linear_combination#:~:text=In%20mathematics%2C%20a%20linear%20combination,a%20and%20b%20are%20constants). \u21a9","title":"Linear Combination of Polynomial"},{"location":"reighns_ml_journey/mathematics/linear_algebra/02_vectors/02_linear_algebra_vectors_unit_vector/","text":"\\[\\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}}\\] Table of Contents Unit Vector Geometric Interpretation of Unit Vector Algebraic Interpretation of Unit Vector Unit Vector Though not apparent now, unit vectors are important in linear algebra and have widespread applications. Geometric Interpretation of Unit Vector Consider the vector \\(\\v = [3, 4]\\) which has a norm of \\(\\|\\v_1\\| = \\sqrt{3^2 + 4^2} = 5\\) . We want to find a vector \\(\\u\\) that is in the same direction of \\(\\v\\) (assume all has starting coordinates at origin), but has norm of \\(1\\) . Geometrically, we know that we just need to divide the vector \\(\\v\\) by \\(5\\) (a.k.a the norm of the vector itself) to get a vector that has norm \\(1\\) and also lies in the same direction. But how to recover the exact vector \\(\\u\\) with its coordinates? We have learnt about Vector-Scalar Multiplication and visually, we need to multiply the vector \\(\\v\\) by \\(\\frac{1}{5}\\) to get the vector \\(\\u = [0.6, 0.8]\\) . Thus, given a vector \\(\\v\\) , we know that if we divide it by the length of itself, we can recover back a vector \\(\\u = \\frac{1}{\\|\\v\\|}\\v\\) such that \\(\\|\\u\\| = 1\\) . Fig 3.5: Unit Vectors; By Hongnan G. Algebraic Interpretation of Unit Vector Given a vector \\(\\v\\) , can we find a vector \\(\\u\\) such that: - \\(\\|\\u\\| = 1\\) - \\(\\u\\) is in the same direction as \\(\\v\\) . We need to fulfill the above two conditions, and since we know that \\(\\u\\) must be in the same direction as \\(\\v\\) , then \\(\\u = \\lambda \\v\\) by definition. Thus, our problem is reduced to finding a vector \\(\\u = \\lambda \\v\\) such that \\(\\|\\u\\| = 1\\) . And since \\(\\v\\) is known, it suffices for us to find \\(\\lambda\\) only. We also know that \\(\\|\\u\\|\\) must be \\(1\\) , as a result, let us take the norm of both sides to get \\[\\|\\u\\| = \\|\\lambda \\v\\| \\implies \\|\\u\\| = \\lambda \\|\\v\\| \\implies \\lambda = \\dfrac{\\|\\u\\|}{\\|\\v\\|}\\] Substituting back, we have \\[\\u = \\lambda \\v = \\dfrac{\\|\\u\\|}{\\|\\v\\|} \\v = \\dfrac{1}{\\|\\v\\|} \\v\\]","title":"Unit Vector"},{"location":"reighns_ml_journey/mathematics/linear_algebra/02_vectors/02_linear_algebra_vectors_unit_vector/#table-of-contents","text":"Unit Vector Geometric Interpretation of Unit Vector Algebraic Interpretation of Unit Vector","title":"Table of Contents"},{"location":"reighns_ml_journey/mathematics/linear_algebra/02_vectors/02_linear_algebra_vectors_unit_vector/#unit-vector","text":"Though not apparent now, unit vectors are important in linear algebra and have widespread applications.","title":"Unit Vector"},{"location":"reighns_ml_journey/mathematics/linear_algebra/02_vectors/02_linear_algebra_vectors_unit_vector/#geometric-interpretation-of-unit-vector","text":"Consider the vector \\(\\v = [3, 4]\\) which has a norm of \\(\\|\\v_1\\| = \\sqrt{3^2 + 4^2} = 5\\) . We want to find a vector \\(\\u\\) that is in the same direction of \\(\\v\\) (assume all has starting coordinates at origin), but has norm of \\(1\\) . Geometrically, we know that we just need to divide the vector \\(\\v\\) by \\(5\\) (a.k.a the norm of the vector itself) to get a vector that has norm \\(1\\) and also lies in the same direction. But how to recover the exact vector \\(\\u\\) with its coordinates? We have learnt about Vector-Scalar Multiplication and visually, we need to multiply the vector \\(\\v\\) by \\(\\frac{1}{5}\\) to get the vector \\(\\u = [0.6, 0.8]\\) . Thus, given a vector \\(\\v\\) , we know that if we divide it by the length of itself, we can recover back a vector \\(\\u = \\frac{1}{\\|\\v\\|}\\v\\) such that \\(\\|\\u\\| = 1\\) . Fig 3.5: Unit Vectors; By Hongnan G.","title":"Geometric Interpretation of Unit Vector"},{"location":"reighns_ml_journey/mathematics/linear_algebra/02_vectors/02_linear_algebra_vectors_unit_vector/#algebraic-interpretation-of-unit-vector","text":"Given a vector \\(\\v\\) , can we find a vector \\(\\u\\) such that: - \\(\\|\\u\\| = 1\\) - \\(\\u\\) is in the same direction as \\(\\v\\) . We need to fulfill the above two conditions, and since we know that \\(\\u\\) must be in the same direction as \\(\\v\\) , then \\(\\u = \\lambda \\v\\) by definition. Thus, our problem is reduced to finding a vector \\(\\u = \\lambda \\v\\) such that \\(\\|\\u\\| = 1\\) . And since \\(\\v\\) is known, it suffices for us to find \\(\\lambda\\) only. We also know that \\(\\|\\u\\|\\) must be \\(1\\) , as a result, let us take the norm of both sides to get \\[\\|\\u\\| = \\|\\lambda \\v\\| \\implies \\|\\u\\| = \\lambda \\|\\v\\| \\implies \\lambda = \\dfrac{\\|\\u\\|}{\\|\\v\\|}\\] Substituting back, we have \\[\\u = \\lambda \\v = \\dfrac{\\|\\u\\|}{\\|\\v\\|} \\v = \\dfrac{1}{\\|\\v\\|} \\v\\]","title":"Algebraic Interpretation of Unit Vector"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/","text":"\\[\\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\e}{\\mathbf{e}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\z}{\\mathbf{z}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\C}{\\mathbf{C}}\\] Basis Disclaimer Algebraic Definition (Basis) Minimal Generating Set Geometric Definition (Basis) Examples (Basis) Examples (Standard Basis) Visualize Standard Basis in 2D Space Visualize Non-Standard Basis in 2D Space Visualize Standard Basis in 3D Space Visualize Non-Standard Basis in 3D Space Theorem (Criterion for Basis and Unique Representation of Basis) Proof Theorem (All Basis has the same length) Dimensions Definition (Dimension) Notation (Dimension) Intuition (Dimension) Example (Dimension) Theorem (Dimension of a Subspace) Proof Basis and Dimension Theorems and Applications Theorem (Spanning Set contains Basis) Proof Theorem (Every Subspace has a Basis) Proof Theorem (Linear Independent Sets can be extended to a Basis) Proof Theorem (Equivalent Basis Definition) Proof Useful Summary Applications of Basis Efficient Space Storage Basis Disclaimer I believe learning linear algebra needs some geometrical intuition, after looking around, I finally chanced upon this GitHub repo with nice python code to plot basis vectors. We will use his code below to visualize basis vectors in this section. Please visit his repo here 1 . Algebraic Definition (Basis) A basis \\(\\B\\) of a vector space \\(V\\) over a field \\(\\F\\) 2 is a linearly independent subset of \\(V\\) that spans \\(V\\) . This means that a subset \\(\\B\\) of \\(V\\) is a basis if it satisfies the two following conditions: the linear independence property: The set of vectors \\(\\b_1, \\b_2, ..., \\b_n \\in V\\) is linearly independent if and only if the only solution to the equation \\(a_1\\mathbf{v}_1 + a_2\\mathbf{v}_2 + \\cdots + a_m\\mathbf{v}_n = \\mathbf{0}\\) is the trivial solution, the zero vector; and the spanning property: Define \\(\\B = \\{\\b_1, \\b_2, ..., \\b_m\\}\\) in \\(V\\) , and the linear combination of all the vectors \\(\\b_1, \\b_2, \\cdots, \\b_n\\) make up the main vector space \\(V\\) ; i.e. \\(\\textbf{span}(B) = V\\) . Minimal Generating Set We won't go through the formal definition, but rather a motivating example from Math Stack Exchange 3 . You can think about it as the idea from linear algebra of a basis for a space compared to a set of vectors which span the space. A basis is a set of linearly independent elements, where removing one of the elements would result in it being unable to generate every element in that space. In other words, you can think of a minimal generating set as a basis for the group, which has no redundant elements while a generating set may have redundant elements. For example, to generate \\(\\mathbb R^3\\) we have a basis \\(\\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix}, \\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix}, \\begin{bmatrix}0\\\\0\\\\1\\end{bmatrix}\\) (a minimal generating set), but this space is still generated by the set of vectors \\(\\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix}, \\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix}, \\begin{bmatrix}0\\\\0\\\\1\\end{bmatrix}, \\begin{bmatrix}0\\\\0\\\\3\\end{bmatrix}\\) (a generating set). Geometric Definition (Basis) Note Geometric understanding of basis is best understood with examples! Examples (Basis) Examples (Standard Basis) Every space \\(\\R^n\\) has a standard basis . \\[\\e = \\left\\{\\begin{bmatrix} \\color{red}1 \\\\ \\color{red}0 \\end{bmatrix}, \\begin{bmatrix} \\color{red}0 \\\\ \\color{red}{1}\\end{bmatrix} \\right\\}\\] is the standard basis for the \\(\\R^2\\) space; and for \\(\\R^3\\) : \\[\\e = \\left\\{\\begin{bmatrix} \\color{red}1 \\\\ \\color{red}0 \\\\ \\color{red}0 \\end{bmatrix}, \\begin{bmatrix} \\color{red}0 \\\\ \\color{red}{1} \\\\ \\color{red}0 \\end{bmatrix}, \\begin{bmatrix} \\color{red}0 \\\\ \\color{red}{0} \\\\ \\color{red}1 \\end{bmatrix} \\right\\}\\] and we usually denote \\[\\e_1 = \\begin{bmatrix} \\color{red}1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\quad \\e_2 = \\begin{bmatrix} \\color{red}0 \\\\ 1 \\\\ \\vdots \\\\0 \\end{bmatrix} \\quad \\e_n = \\begin{bmatrix} \\color{red}0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{bmatrix}\\] and so on. Visualize Standard Basis in 2D Space Consider a vector \\(\\v = \\begin{bmatrix}\\color{red}3 \\\\ \\color{red}6\\end{bmatrix}\\) . We can construct this vector using our standard basis \\(\\e_1, \\e_2\\) as \\(3\\e_1 + 6\\e_2\\) , as shown below. import matplotlib.pyplot as plt import numpy as np fig , ax = plt . subplots ( figsize = ( 10 , 10 )) arrows = np . array ( [ [[ 0 , 0 , 1 , 0 ]], [[ 0 , 0 , 0 , 1 ]], [[ 0 , 0 , 3 , 0 ]], [[ 0 , 0 , 0 , 6 ]], [[ 0 , 0 , 3 , 6 ]], ] ) colors = [ \"r\" , \"b\" , \"r\" , \"b\" , \"g\" ] for i in range ( arrows . shape [ 0 ]): X , Y , U , V = zip ( * arrows [ i , :, :]) ax . arrow ( X [ 0 ], Y [ 0 ], U [ 0 ], V [ 0 ], color = colors [ i ], width = 0.03 , length_includes_head = True , head_width = 0.2 , # default: 3*width head_length = 0.3 , overhang = 0.4 , ) ############################Dashed################################## line1 = np . array ([[ 3 , 0 ], [ 3 , 6 ]]) ax . plot ( line1 [:, 0 ], line1 [:, 1 ], ls = \"--\" , lw = 3 , color = \"black\" , alpha = 0.5 ) line2 = np . array ([[ 0 , 6 ], [ 3 , 6 ]]) ax . plot ( line2 [:, 0 ], line2 [:, 1 ], ls = \"--\" , lw = 3 , color = \"black\" , alpha = 0.5 ) ############################Text##################################### ax . text ( 0 , 1 , \"$e_2$\" , size = 15 ) ax . text ( 1 , 0 , \"$e_1$\" , size = 15 ) ax . text ( 0 , 6 , \"$6e_2$\" , size = 15 ) ax . text ( 3 , 0 , \"$3e_1$\" , size = 15 ) ax . text ( 3 , 6 , \"$3e_1+6e_2$\" , size = 15 ) ###########################Grid Setting############################## # Major ticks every 20, minor ticks every 5 major_ticks = np . arange ( 0 , 10 , 2 ) minor_ticks = np . arange ( 0 , 10 , 0.5 ) ax . set_xticks ( major_ticks ) ax . set_xticks ( minor_ticks , minor = True ) ax . set_yticks ( major_ticks ) ax . set_yticks ( minor_ticks , minor = True ) ax . grid ( which = \"both\" ) ax . grid ( which = \"minor\" , alpha = 0.2 ) ax . grid ( which = \"major\" , alpha = 0.5 ) ####################################################################### ax . set_xlabel ( \"x-axis\" , size = 18 ) ax . set_ylabel ( \"y-axis\" , size = 18 ) ax . axis ([ - 1 , 10 , - 1 , 10 ]) ax . grid () plt . show () Visualize Non-Standard Basis in 2D Space We can also construct the same vector using different basis. Next, consider \\(\\v_1 = \\begin{bmatrix}2 \\\\ 1 \\end{bmatrix}\\) and \\(\\v_2 = \\begin{bmatrix}-1 \\\\ 2 \\end{bmatrix}\\) . We can verify indeed this is a basis for \\(\\mathbb{R}^2\\) . But the main point is that we can have the vector constructed by \\(2.4\\v_1 + 3.6\\v_2\\) . fig , ax = plt . subplots ( figsize = ( 10 , 10 )) v1 = np . array ([ 2 , 1 ]) v2 = np . array ([ - 1 , 2 ]) v1m2 = 2.4 * v1 v2m3 = 3.6 * v2 arrows = np . array ( [ [[ 0 , 0 , v1 [ 0 ], v1 [ 1 ]]], [[ 0 , 0 , v2 [ 0 ], v2 [ 1 ]]], [[ 0 , 0 , 2.4 * v1 [ 0 ], 2.4 * v1 [ 1 ]]], [[ 0 , 0 , 3.6 * v2 [ 0 ], 3.6 * v2 [ 1 ]]], [[ 0 , 0 , ( v1m2 + v2m3 )[ 0 ], ( v1m2 + v2m3 )[ 1 ]]], ] ) colors = [ \"r\" , \"b\" , \"r\" , \"b\" , \"g\" ] for i in range ( arrows . shape [ 0 ]): X , Y , U , V = zip ( * arrows [ i , :, :]) ax . arrow ( X [ 0 ], Y [ 0 ], U [ 0 ], V [ 0 ], color = colors [ i ], width = 0.03 , length_includes_head = True , head_width = 0.2 , # default: 3*width head_length = 0.3 , overhang = 0.4 , ) # ############################ Dashed ################################## point1 = [ v2m3 [ 0 ], v2m3 [ 1 ]] point2 = [ v2m3 [ 0 ] + v1m2 [ 0 ], v2m3 [ 1 ] + v1m2 [ 1 ]] line = np . array ([ point1 , point2 ]) ax . plot ( line [:, 0 ], line [:, 1 ], ls = \"--\" , lw = 3 , color = \"black\" , alpha = 0.5 ) point1 = [ v1m2 [ 0 ], v1m2 [ 1 ]] point2 = [ v2m3 [ 0 ] + v1m2 [ 0 ], v2m3 [ 1 ] + v1m2 [ 1 ]] line = np . array ([ point1 , point2 ]) ax . plot ( line [:, 0 ], line [:, 1 ], ls = \"--\" , lw = 3 , color = \"black\" , alpha = 0.5 ) ############################Text##################################### ax . text ( 2 , 1 , \"$v_1$\" , size = 15 ) ax . text ( - 1 , 2 , \"$v_2$\" , size = 15 ) ax . text ( v1m2 [ 0 ], v1m2 [ 1 ], \"$2.4v_1$\" , size = 15 ) ax . text ( v2m3 [ 0 ], v2m3 [ 1 ], \"$3.6v_2$\" , size = 15 ) ax . text ( v1m2 [ 0 ] + v2m3 [ 0 ], v1m2 [ 1 ] + v2m3 [ 1 ], \"$2.4v_1+3.6v_2$\" , size = 15 ) ############################## Grid ############################### t = np . linspace ( - 6 , 6 ) for k in range ( - 6 , 7 ): x = 2 * k - t y = k + 2 * t ax . plot ( x , y , ls = \"--\" , color = \"red\" , alpha = 0.3 ) for k in range ( - 6 , 7 ): x = - k + 2 * t y = 2 * k + t ax . plot ( x , y , ls = \"--\" , color = \"red\" , alpha = 0.3 ) ####################################################################### ax . set_xlabel ( \"x-axis\" , size = 18 ) ax . set_ylabel ( \"y-axis\" , size = 18 ) ax . axis ([ - 6 , 6 , 0 , 10 ]) # np.linalg.norm(v1m2+v2m3) is intercept plt . show () Visualize Standard Basis in 3D Space The following shows geometrically, how standard basis in \\(\\R^3\\) constructs a vector \\(\\begin{bmatrix}2 \\\\ 3 \\\\ 4 \\end{bmatrix}\\) . import utils utils . linearCombo ( 2 , 3 , 4 ) Visualize Non-Standard Basis in 3D Space Next we show the linear combination of a non-standard basis, \\((2,1,0), (0,3,1), (0,0,3)\\) . a , b , c = 2 , 3 , 4 vec1 = np . array ([ 2 , 1 , 0 ]) vec2 = np . array ([ 0 , 3 , 1 ]) vec3 = np . array ([ 1 , 2 , 3 ]) utils . linearComboNonStd ( 2 , 3 , 4 , vec1 , vec2 , vec3 ) Theorem (Criterion for Basis and Unique Representation of Basis) Here we state another way of checking if a set \\(\\B \\subseteq V\\) is a basis of \\(V\\) . One can ignore the third condition (out of scope!). Let \\(\\B = \\{\\b_1, \\cdots,\\b_n\\}\\) and in particular \\(\\b_i \\neq \\0\\) . Let \\(\\B\\) be a finite subset of a vector space \\(V\\) over a field \\(\\F\\) . Then the following are equivalent. i) \\(\\B\\) is a basis of \\(V\\) \\(\\iff\\) ii) Unique expression condition: Every vector \\(\\v \\in V\\) can be expressed as \\[\\v = a_1\\b_1+ \\cdots +a_n\\b_n\\] for some scalars \\(a_i \\in \\F\\) and such expression of \\(\\v\\) is unique. That means whenever \\(\\v = b_1\\b_1+ \\cdots + b_n\\b_n\\) for some scalars \\(b_i \\in \\F\\) , we have \\(a_i = b_i\\) . \\(\\iff\\) iii) \\(V\\) has the following direct sum decomposition: \\[V = \\text{Span}\\{\\b_1\\} \\oplus ... \\oplus \\text{Span}\\{\\b_n\\} = \\F \\b_1 \\oplus ... \\oplus \\F\\b_n\\] Proof To prove three equivalent statements, we can simply do a round-cycle proof: \\(1 \\implies 2 \\implies 3 \\implies 1\\) will complete all \\(\\iff\\) proofs. \\((1) \\implies (2)\\) \\(B = \\{\\mathbf{v_{1}, \\cdots, v_{n}}\\}\\) finite subset of \\(V\\) . Suppose \\(\\B\\) is basis of \\(V\\) . Then pick any \\(\\v \\in V\\) and suppose we can write \\(\\v\\) in two ways. \\[\\begin{eqnarray} \\v = a_{1}\\v_1 + \\cdots a_{n}\\mathbf{v_{n}} \\\\ \\v = a_{1}^{'}\\v_1 + \\cdots a_{n}^{'}\\mathbf{v_{n}} \\end{eqnarray}\\] Equations. (1) and (2) imply \\((a_{1} - a_{1}^{'}) \\v_{1} + \\cdots (a_{n} - a_{n}^{'})\\v_{n} = 0\\) and since \\(\\B\\) is L.I set, it only has trivial solutions. Hence \\(a_{1} - a_{1}^{'} = 0\\) \\(\\forall i\\) implying \\(a_{i} = a_{i}^{'}\\) . Hence \\(\\v\\) has unique way of expressing. \\((2) \\implies (3)\\) Show first that \\(V = \\text{span}(\\{\\v_1\\}) + \\cdots \\text{span}(\\{\\mathbf{v_{n}}\\})\\) . \\(\\text{span}(\\{\\v_1\\}) + \\cdots \\text{span}(\\{\\mathbf{v_{n}}\\}) \\subseteq V\\) is obvious. We show \\(V \\subseteq \\text{span}(\\{\\v_1\\}) + \\cdots \\text{span}(\\{\\mathbf{v_{n}}\\})\\) . Let \\(\\v \\in V\\) , by hypothesis, \\[\\v = a_{1}\\v_1 + \\cdots a_{n}\\mathbf{v_{n}} \\in \\text{span}(\\{\\v_1\\})+ \\cdots + \\text{span}(\\{\\mathbf{v_{n}}\\})\\] Hence: \\[V = \\text{span}(\\{\\v_1\\})+ \\cdots + \\text{span}(\\{\\mathbf{v_{n}}\\})\\] Now we show it is direct sum. Denote \\(W_{i} = \\text{span}(\\{\\mathbf{v_{i}}\\})\\) . Suppose not, \\(\\exists x \\in \\sum\\limits_{i=1}^{k-1}W_{i} \\cap W_{k}\\) for some \\(2 \\le k \\le n\\) and \\(x \\neq 0\\) . Then $ x = \\mathbf{w_{1} + \\cdots + w_{k-1}}$ for some \\(\\mathbf{w_{i}} \\in W_{i}\\) and \\(x = -\\mathbf{w_{k}}\\) for some \\(-\\mathbf{w_{k}} \\in W_{k}\\) . But since \\(x\\) is uniquely expressed. \\begin{eqnarray} x & = & \\w_1+\\w_2+...+\\w_{k-1} \\ x & = & -\\w_k \\end{eqnarray} implying that \\[0 =\\w_1+\\w_2+...+\\w_k\\] By uniqueness \\(0 = 0 + 0 \\cdots + 0\\) , but that would means \\(\\w_1=\\w_2=...= \\w_k =0\\) , implying \\(x = 0\\) , a contradiction. \\((3) \\implies (1)\\) Want to show \\(B\\) is a basis of \\(V\\) . By hypothesis, \\begin{eqnarray} V & = & \\text{span}({\\v_{1}}) + \\cdots + \\text{span}({\\v_{n}})\\nonumber\\ & = & \\text{span}({\\v_{1}} \\cup {\\v_{2}} \\cup \\cdots \\cup {\\v_{n}}) \\nonumber \\ & = & \\text{span}({\\v_{1}, \\v_{2}, \\cdots, \\v_{n}})\\nonumber \\end{eqnarray} Now we show \\(B\\) is L.I. That is \\(a_{1}\\v_{1} + \\cdots + a_{n}\\v_{n}\\) has trivial solution. Suppose not, say \\(a_{k} \\ne 0\\) , and such that \\(\\v_{k} = b_{1}\\v_{1} + \\cdots b_{k-1}\\v_{k-1} + \\cdots + b_{n}\\v_{n}\\) where \\(b : = \\frac{-a_{i}}{a_{k}}\\) . This is contradiction as \\(\\v_{k} \\in W_{k}\\) and \\(b_{1}\\v_{1} \\cdots b_{n}\\v_{n} \\in \\sum\\limits_{i \\ne k} W_{i}\\) . Thus \\(\\v_{k} \\in \\sum\\limits_{i \\ne k}W_{i}\\cap W_{k}\\) and \\(\\v_{k} \\ne 0\\) is our assumption. This contradicts direct sum. Hence \\(\\B\\) is LI. Theorem (All Basis has the same length) This theorem states that all basis of a vector space \\(V\\) has the same length. This theorem is important to faciliate the definition of Dimension. Dimensions We have encountered the term Dimension at the start of Vector Spaces . Now, we give it a formal definition. Definition (Dimension) If a vector space \\(V\\) has a basis \\(\\B\\) with cardinality \\[|B| = n < \\infty\\] then we say that \\(V\\) is finite dimensional and define the dimension \\[\\text{dim}_{\\F}V = |B|\\] Otherwise, \\(V\\) is called infinite dimensional. Note that we implicitly assumed that all basis \\(\\B\\) has the same cardinality, which we showed as a theorem previously. Notation (Dimension) We denote the dimension of a finite vector space \\(V\\) to be \\[\\dim(V)\\] Intuition (Dimension) One may wonder why the definition of Dimension depends on the basis . Why can't we just define the dimension of a vector space \\(V\\) over a field \\(\\F^n\\) to be just the length of the element \\(\\v \\in V\\) , which is just \\(n\\) . This is a reasonable assumption, and both turns out to be equivalent. That is, the length of any element \\(\\v \\in V\\) over a field \\(\\F^n\\) is the same as the number of basis \\(\\B\\) that a vector space \\(V\\) has. We can easily think of it geometrically. Consider the 2d space \\(\\R^2\\) , we know that any element in \\(\\R^2\\) must have 2 elements and in our earlier definition, the dimension of such a space is 2. Now we understood basis, and know that we need 2 linearly independent vectors to make up the 2d space. Same logic applies to the 3d space \\(\\R^3\\) . Example (Dimension) R2 and R3 Dimensions i) Besides \\(\\{\\mathbf{0}\\}\\) and \\(\\mathbb{R}^2\\) , all subspaces of \\(\\mathbb{R}^2\\) are lines through the origin and they are of dimension \\(1\\) . (Basis of the subspaces has only one vector) ii) Besides \\(\\{\\mathbf{0}\\}\\) and \\(\\mathbb{R}^3\\) , all subspaces of \\(\\mathbb{R}^3\\) are either lines through the origin and they are of dimension \\(1\\) or planes containing the origin, which are of dimension \\(2\\) . (Basis of the subspaces has only one vector or two vectors) Theorem (Dimension of a Subspace) If \\(V\\) is a finite dimensional vector space and \\(U\\) is a subspace of \\(V\\) , then \\(\\dim(U) \\leq \\dim(V)\\) . Proof This should not even come as a surprise after the previous example. It also makes sense geometrically as a subspace \\(U\\) , is also a subset of \\(V\\) , and hence cannot \"be larger\" than its \"parent\". Prove it and check pp.45 of linear algebra done right. Basis and Dimension Theorems and Applications Theorem (Spanning Set contains Basis) A spanning set \\(S\\) (note we do not know if this is a basis or not yet) in vector space \\(V\\) necessarily contains the basis \\(\\B\\) . Proof First, intuitively we already know that the set \\(S\\) spans \\(V\\) and we acknowledge that a set \\(S\\) spanning the vector space \\(V\\) can be a linearly dependent set (refer to example (different sets can span the same vector space)). Therefore, we just need to \"remove\" the linearly dependent vectors in the set \\(S\\) so that the remaining set \\(\\B \\subseteq S\\) is both linearly independent and spans V , consequently, \\(\\B\\) is a basis of \\(V\\) . Theorem (Every Subspace has a Basis) Every finite subspace \\(V\\) has a basis \\(\\B\\) . Proof Every finite subspace can be represented by a span of a set of vectors. By Theorem (Spanning Set contains Basis) , this spanning set has a basis. Theorem (Linear Independent Sets can be extended to a Basis) Let \\(B\\) be a Linearly Independent subset of a vector space \\(V\\) over a field \\(\\F\\) . Then exactly one of the following two cases is true. i) \\(B\\) spans \\(V\\) and hence \\(B\\) is a basis of \\(V\\) . ii) Let \\(\\v \\in V \\setminus \\text{Span}(B)\\) and hence \\(\\v \\not \\in B\\) . Then \\(B \\cup \\{\\v\\}\\) is an Linearly Independent subset of \\(V\\) . iii) In particular, if \\(V\\) is of finite dimension \\(n\\) , then one can find \\(n - |B|\\) vectors \\(\\mathbf{v_{|B|+1},...,v_n}\\) in \\(V \\setminus \\text{Span}(B)\\) such that \\(B \\coprod \\{\\mathbf{v_{|B|+1},...,v_n} \\}\\) is a basis of \\(V\\) . Proof Since \\(B\\) is L.I subset of \\(V\\) , then \\(B\\) either spans \\(V\\) or do not span \\(V\\) . If it spans \\(V\\) , then \\(B\\) is a basis, which is point i). If its doesn't span \\(V\\) , take an element \\(\\w \\in V \\setminus \\text{span}(B)\\) , \\(\\w \\notin B\\) and \\(\\mathbf{w} \\notin \\text{span}(B)\\) . Then it means \\(\\mathbf{w}\\) is not a linear combination of any elements in \\(B\\) . Hence, \\(B \\cup \\{\\w\\}\\) is a L.I set as no elements in \\(B \\cup\\{\\w\\}\\) can be expressed as a LC of each other. Note if \\(B = \\{\\v_{1}, \\v_{2}, \\cdots \\v_{r}\\}, ~ B\\cup \\{\\w\\} = \\{\\v_{1}, \\cdots \\v_{r}, \\w\\}\\) . $ \\v_{i} \\ne \\w$. For point 3, this means if \\(B\\) is LI in \\(V\\) , then \\(B\\) can only have 2 cases. If Case 1 occurs, we are done as $ n - |B| = n- n = 0$, we do not need any more vectors in \\(V \\setminus \\text{span}(B)\\) to union with \\(B\\) . If Case 2 occurs, Then let \\(|B| = r\\) . then \\(\\exists \\mathbf{w_{r+1}} \\in V \\setminus \\text{span}(B)\\) s.t \\(B \\cup \\{\\mathbf{w_{r+1}}\\}\\) is L.I set. Applying the two cases to \\(B \\cup \\{\\mathbf{w_{r+1}}\\}\\) , we will have \\(B \\cup \\{\\mathbf{w_{r+1}}\\}\\) be a basis or \\(B \\cup \\{\\mathbf{w_{r+1}, w_{r+2}}\\}\\) is another L.I set. Since the dimension of \\(V\\) is \\(n\\) . Then applying this expansion inductively, \\(B \\cup \\{\\mathbf{w_{r+1}, \\cdots w_{n}}\\}\\) will eventually form a basis for \\(V\\) . If for a contradiction the case 2 continues after \\(\\mathbf{w_{n}}\\) then it contradicts the fact that a set with more than \\(n\\) vectors cannot be L.I. Theorem (Equivalent Basis Definition) Let \\(B\\) be a subset of a vector space \\(V\\) of finite dimension \\(\\text{dim}_{\\F}V = n \\geq 1\\) . Then the following are equivalent. i) \\(B\\) is a basis of \\(V\\) . ii) \\(B\\) is \\(L.I\\) and \\(|B| = n\\) . iii) \\(B\\) spans \\(V\\) and \\(|B| = n\\) . Proof We prove \\(i \\Leftrightarrow ii\\) and \\(i \\Leftrightarrow iii\\) . Now \\(i \\Rightarrow ii\\) and \\(i \\Rightarrow iii\\) are by definition. iii \\(\\Rightarrow\\) i Since \\(V = \\text{Span}(B)\\) , there exists a subset \\(B_1\\) of \\(B\\) such that \\(B_1\\) is a basis of \\(V\\) . But by our hypothesis, we have \\(|B| = n\\) . Since \\(B_1\\) is basis of \\(V\\) , we must have \\(|B_1| =\\) dimension of \\(V\\) which is \\(n\\) . And hence \\(|B_1| = |B|\\) . Since, \\(B_1 \\subseteq B\\) , and \\(|B_1| = |B|\\) , we must have \\(B_1 = B\\) as a set. Hence \\(B\\) is a basis of \\(V\\) . Alternatively, suppose \\(B\\) is not a basis of \\(V\\) . \\(B\\) is not linearly independent. Take a vector \\(\\v\\) in \\(B\\) which is a linear combination of other vectors in \\(B\\) . Then we know that \\(B - \\v\\) still spans \\(V\\) . But \\(|B-\\v| = n-1\\) by our hypothesis. And it is a contradiction since it is a subset of \\(V\\) with less than \\(n\\) vectors, hence it cannot span \\(V\\) . It is a contradiction. ii \\(\\Rightarrow\\) i Suppose that \\(B\\) is not a basis of \\(V\\) . Then \\(B\\) does not span \\(V\\) . Pick a vector \\(\\v\\) in \\(V\\) but not in Span \\((B)\\) (there exists such a vector because we say that \\(B\\) does not span \\(V\\) ) and \\(B \\cup \\{\\v\\}\\) is still a linearly independent set given that \\(B\\) is a linearly independent set. Hence it is a contradiction because \\(|B \\cup \\{\\v\\}| = n+1\\) has more vectors than even the basis set, hence it cannot be linearly independent. Useful Summary Let \\(V\\) be a vector space which has a basis \\(B\\) with \\(k\\) vectors. Then any subset \\(W\\) of \\(V\\) with more than \\(k\\) vectors is always linearly dependent. So it means if any subset \\(W\\) of \\(V\\) is linearly independent, then \\(W\\) will have less or equals to \\(k\\) vectors, in which directly translates to the dimension of \\(W\\) is less or equals to the dimension of \\(V\\) . Any subset \\(W\\) of \\(V\\) with less than \\(k\\) vectors cannot span \\(V\\) . So for any subset \\(W\\) of \\(V\\) which spans \\(V\\) , then it will have more or equals to \\(k\\) vectors. In particular, let \\(W = \\{\\w_1,...,\\w_k\\}\\) be a set of vectors in \\(V = \\mathbb{R}^n\\) . If \\(k < n\\) , then \\(S\\) cannot span \\(\\mathbb{R}^n\\) . Suppose that \\(C\\) is another basis of \\(V\\) , then \\(|B|=|C|\\) . Applications of Basis Efficient Space Storage Storing data efficiently in this era is very important. Let's give a superfluous example: Let \\(\\A\\) be a \\(3 \\times 100\\) matrix where there are 3 features and 100 samples; and if we know that the columns of the matrix has a basis \\(\\B\\) , say \\[\\B = \\left\\{\\begin{bmatrix}1 \\\\ 0 \\\\ 0\\end{bmatrix} ,\\quad \\begin{bmatrix}0 \\\\ 0 \\\\ 1 \\end{bmatrix} ,\\quad \\begin{bmatrix} 0 \\\\ 0 \\\\ 1\\end{bmatrix}\\right\\}\\] But we know that all 100 columns of \\(\\A\\) can be reconstructed using the basis vectors; i.e. for each column \\(\\a_i\\) of \\(\\A\\) , there exists constants \\(\\lambda_i\\) such that \\[\\a_i = \\lambda_1 \\B_1 + \\lambda_2 \\B_2 + \\lambda_3 \\B_3\\] As a result, we can effectively store the 3 basis vectors (9 elements and note that the 3 basis vectors can be taken from the columns of \\(\\A\\) ); the remaining 7 columns we just need \\(7 \\times 3 = 21\\) constants so that we can recover the columns using linear combination of the basis vectors. In total, we reduced the space from \\(3 \\times 100 = 300\\) elements to \\(3 \\times 3 + 7 \\times 3 = 30\\) elements, effectively 70 percent decrease. Macro Analyst Linear Algebra \u21a9 Such as the real numbers \\(\\R\\) or the complex numbers \\(\\mathbb{C}\\) . \u21a9 https://math.stackexchange.com/questions/3089880/minimal-generating-set \u21a9","title":"Basis and Dimension"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#basis","text":"","title":"Basis"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#disclaimer","text":"I believe learning linear algebra needs some geometrical intuition, after looking around, I finally chanced upon this GitHub repo with nice python code to plot basis vectors. We will use his code below to visualize basis vectors in this section. Please visit his repo here 1 .","title":"Disclaimer"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#algebraic-definition-basis","text":"A basis \\(\\B\\) of a vector space \\(V\\) over a field \\(\\F\\) 2 is a linearly independent subset of \\(V\\) that spans \\(V\\) . This means that a subset \\(\\B\\) of \\(V\\) is a basis if it satisfies the two following conditions: the linear independence property: The set of vectors \\(\\b_1, \\b_2, ..., \\b_n \\in V\\) is linearly independent if and only if the only solution to the equation \\(a_1\\mathbf{v}_1 + a_2\\mathbf{v}_2 + \\cdots + a_m\\mathbf{v}_n = \\mathbf{0}\\) is the trivial solution, the zero vector; and the spanning property: Define \\(\\B = \\{\\b_1, \\b_2, ..., \\b_m\\}\\) in \\(V\\) , and the linear combination of all the vectors \\(\\b_1, \\b_2, \\cdots, \\b_n\\) make up the main vector space \\(V\\) ; i.e. \\(\\textbf{span}(B) = V\\) .","title":"Algebraic Definition (Basis)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#minimal-generating-set","text":"We won't go through the formal definition, but rather a motivating example from Math Stack Exchange 3 . You can think about it as the idea from linear algebra of a basis for a space compared to a set of vectors which span the space. A basis is a set of linearly independent elements, where removing one of the elements would result in it being unable to generate every element in that space. In other words, you can think of a minimal generating set as a basis for the group, which has no redundant elements while a generating set may have redundant elements. For example, to generate \\(\\mathbb R^3\\) we have a basis \\(\\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix}, \\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix}, \\begin{bmatrix}0\\\\0\\\\1\\end{bmatrix}\\) (a minimal generating set), but this space is still generated by the set of vectors \\(\\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix}, \\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix}, \\begin{bmatrix}0\\\\0\\\\1\\end{bmatrix}, \\begin{bmatrix}0\\\\0\\\\3\\end{bmatrix}\\) (a generating set).","title":"Minimal Generating Set"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#geometric-definition-basis","text":"Note Geometric understanding of basis is best understood with examples!","title":"Geometric Definition (Basis)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#examples-basis","text":"","title":"Examples (Basis)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#examples-standard-basis","text":"Every space \\(\\R^n\\) has a standard basis . \\[\\e = \\left\\{\\begin{bmatrix} \\color{red}1 \\\\ \\color{red}0 \\end{bmatrix}, \\begin{bmatrix} \\color{red}0 \\\\ \\color{red}{1}\\end{bmatrix} \\right\\}\\] is the standard basis for the \\(\\R^2\\) space; and for \\(\\R^3\\) : \\[\\e = \\left\\{\\begin{bmatrix} \\color{red}1 \\\\ \\color{red}0 \\\\ \\color{red}0 \\end{bmatrix}, \\begin{bmatrix} \\color{red}0 \\\\ \\color{red}{1} \\\\ \\color{red}0 \\end{bmatrix}, \\begin{bmatrix} \\color{red}0 \\\\ \\color{red}{0} \\\\ \\color{red}1 \\end{bmatrix} \\right\\}\\] and we usually denote \\[\\e_1 = \\begin{bmatrix} \\color{red}1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\quad \\e_2 = \\begin{bmatrix} \\color{red}0 \\\\ 1 \\\\ \\vdots \\\\0 \\end{bmatrix} \\quad \\e_n = \\begin{bmatrix} \\color{red}0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{bmatrix}\\] and so on.","title":"Examples (Standard Basis)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#visualize-standard-basis-in-2d-space","text":"Consider a vector \\(\\v = \\begin{bmatrix}\\color{red}3 \\\\ \\color{red}6\\end{bmatrix}\\) . We can construct this vector using our standard basis \\(\\e_1, \\e_2\\) as \\(3\\e_1 + 6\\e_2\\) , as shown below. import matplotlib.pyplot as plt import numpy as np fig , ax = plt . subplots ( figsize = ( 10 , 10 )) arrows = np . array ( [ [[ 0 , 0 , 1 , 0 ]], [[ 0 , 0 , 0 , 1 ]], [[ 0 , 0 , 3 , 0 ]], [[ 0 , 0 , 0 , 6 ]], [[ 0 , 0 , 3 , 6 ]], ] ) colors = [ \"r\" , \"b\" , \"r\" , \"b\" , \"g\" ] for i in range ( arrows . shape [ 0 ]): X , Y , U , V = zip ( * arrows [ i , :, :]) ax . arrow ( X [ 0 ], Y [ 0 ], U [ 0 ], V [ 0 ], color = colors [ i ], width = 0.03 , length_includes_head = True , head_width = 0.2 , # default: 3*width head_length = 0.3 , overhang = 0.4 , ) ############################Dashed################################## line1 = np . array ([[ 3 , 0 ], [ 3 , 6 ]]) ax . plot ( line1 [:, 0 ], line1 [:, 1 ], ls = \"--\" , lw = 3 , color = \"black\" , alpha = 0.5 ) line2 = np . array ([[ 0 , 6 ], [ 3 , 6 ]]) ax . plot ( line2 [:, 0 ], line2 [:, 1 ], ls = \"--\" , lw = 3 , color = \"black\" , alpha = 0.5 ) ############################Text##################################### ax . text ( 0 , 1 , \"$e_2$\" , size = 15 ) ax . text ( 1 , 0 , \"$e_1$\" , size = 15 ) ax . text ( 0 , 6 , \"$6e_2$\" , size = 15 ) ax . text ( 3 , 0 , \"$3e_1$\" , size = 15 ) ax . text ( 3 , 6 , \"$3e_1+6e_2$\" , size = 15 ) ###########################Grid Setting############################## # Major ticks every 20, minor ticks every 5 major_ticks = np . arange ( 0 , 10 , 2 ) minor_ticks = np . arange ( 0 , 10 , 0.5 ) ax . set_xticks ( major_ticks ) ax . set_xticks ( minor_ticks , minor = True ) ax . set_yticks ( major_ticks ) ax . set_yticks ( minor_ticks , minor = True ) ax . grid ( which = \"both\" ) ax . grid ( which = \"minor\" , alpha = 0.2 ) ax . grid ( which = \"major\" , alpha = 0.5 ) ####################################################################### ax . set_xlabel ( \"x-axis\" , size = 18 ) ax . set_ylabel ( \"y-axis\" , size = 18 ) ax . axis ([ - 1 , 10 , - 1 , 10 ]) ax . grid () plt . show ()","title":"Visualize Standard Basis in 2D Space"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#visualize-non-standard-basis-in-2d-space","text":"We can also construct the same vector using different basis. Next, consider \\(\\v_1 = \\begin{bmatrix}2 \\\\ 1 \\end{bmatrix}\\) and \\(\\v_2 = \\begin{bmatrix}-1 \\\\ 2 \\end{bmatrix}\\) . We can verify indeed this is a basis for \\(\\mathbb{R}^2\\) . But the main point is that we can have the vector constructed by \\(2.4\\v_1 + 3.6\\v_2\\) . fig , ax = plt . subplots ( figsize = ( 10 , 10 )) v1 = np . array ([ 2 , 1 ]) v2 = np . array ([ - 1 , 2 ]) v1m2 = 2.4 * v1 v2m3 = 3.6 * v2 arrows = np . array ( [ [[ 0 , 0 , v1 [ 0 ], v1 [ 1 ]]], [[ 0 , 0 , v2 [ 0 ], v2 [ 1 ]]], [[ 0 , 0 , 2.4 * v1 [ 0 ], 2.4 * v1 [ 1 ]]], [[ 0 , 0 , 3.6 * v2 [ 0 ], 3.6 * v2 [ 1 ]]], [[ 0 , 0 , ( v1m2 + v2m3 )[ 0 ], ( v1m2 + v2m3 )[ 1 ]]], ] ) colors = [ \"r\" , \"b\" , \"r\" , \"b\" , \"g\" ] for i in range ( arrows . shape [ 0 ]): X , Y , U , V = zip ( * arrows [ i , :, :]) ax . arrow ( X [ 0 ], Y [ 0 ], U [ 0 ], V [ 0 ], color = colors [ i ], width = 0.03 , length_includes_head = True , head_width = 0.2 , # default: 3*width head_length = 0.3 , overhang = 0.4 , ) # ############################ Dashed ################################## point1 = [ v2m3 [ 0 ], v2m3 [ 1 ]] point2 = [ v2m3 [ 0 ] + v1m2 [ 0 ], v2m3 [ 1 ] + v1m2 [ 1 ]] line = np . array ([ point1 , point2 ]) ax . plot ( line [:, 0 ], line [:, 1 ], ls = \"--\" , lw = 3 , color = \"black\" , alpha = 0.5 ) point1 = [ v1m2 [ 0 ], v1m2 [ 1 ]] point2 = [ v2m3 [ 0 ] + v1m2 [ 0 ], v2m3 [ 1 ] + v1m2 [ 1 ]] line = np . array ([ point1 , point2 ]) ax . plot ( line [:, 0 ], line [:, 1 ], ls = \"--\" , lw = 3 , color = \"black\" , alpha = 0.5 ) ############################Text##################################### ax . text ( 2 , 1 , \"$v_1$\" , size = 15 ) ax . text ( - 1 , 2 , \"$v_2$\" , size = 15 ) ax . text ( v1m2 [ 0 ], v1m2 [ 1 ], \"$2.4v_1$\" , size = 15 ) ax . text ( v2m3 [ 0 ], v2m3 [ 1 ], \"$3.6v_2$\" , size = 15 ) ax . text ( v1m2 [ 0 ] + v2m3 [ 0 ], v1m2 [ 1 ] + v2m3 [ 1 ], \"$2.4v_1+3.6v_2$\" , size = 15 ) ############################## Grid ############################### t = np . linspace ( - 6 , 6 ) for k in range ( - 6 , 7 ): x = 2 * k - t y = k + 2 * t ax . plot ( x , y , ls = \"--\" , color = \"red\" , alpha = 0.3 ) for k in range ( - 6 , 7 ): x = - k + 2 * t y = 2 * k + t ax . plot ( x , y , ls = \"--\" , color = \"red\" , alpha = 0.3 ) ####################################################################### ax . set_xlabel ( \"x-axis\" , size = 18 ) ax . set_ylabel ( \"y-axis\" , size = 18 ) ax . axis ([ - 6 , 6 , 0 , 10 ]) # np.linalg.norm(v1m2+v2m3) is intercept plt . show ()","title":"Visualize Non-Standard Basis in 2D Space"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#visualize-standard-basis-in-3d-space","text":"The following shows geometrically, how standard basis in \\(\\R^3\\) constructs a vector \\(\\begin{bmatrix}2 \\\\ 3 \\\\ 4 \\end{bmatrix}\\) . import utils utils . linearCombo ( 2 , 3 , 4 )","title":"Visualize Standard Basis in 3D Space"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#visualize-non-standard-basis-in-3d-space","text":"Next we show the linear combination of a non-standard basis, \\((2,1,0), (0,3,1), (0,0,3)\\) . a , b , c = 2 , 3 , 4 vec1 = np . array ([ 2 , 1 , 0 ]) vec2 = np . array ([ 0 , 3 , 1 ]) vec3 = np . array ([ 1 , 2 , 3 ]) utils . linearComboNonStd ( 2 , 3 , 4 , vec1 , vec2 , vec3 )","title":"Visualize Non-Standard Basis in 3D Space"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#theorem-criterion-for-basis-and-unique-representation-of-basis","text":"Here we state another way of checking if a set \\(\\B \\subseteq V\\) is a basis of \\(V\\) . One can ignore the third condition (out of scope!). Let \\(\\B = \\{\\b_1, \\cdots,\\b_n\\}\\) and in particular \\(\\b_i \\neq \\0\\) . Let \\(\\B\\) be a finite subset of a vector space \\(V\\) over a field \\(\\F\\) . Then the following are equivalent. i) \\(\\B\\) is a basis of \\(V\\) \\(\\iff\\) ii) Unique expression condition: Every vector \\(\\v \\in V\\) can be expressed as \\[\\v = a_1\\b_1+ \\cdots +a_n\\b_n\\] for some scalars \\(a_i \\in \\F\\) and such expression of \\(\\v\\) is unique. That means whenever \\(\\v = b_1\\b_1+ \\cdots + b_n\\b_n\\) for some scalars \\(b_i \\in \\F\\) , we have \\(a_i = b_i\\) . \\(\\iff\\) iii) \\(V\\) has the following direct sum decomposition: \\[V = \\text{Span}\\{\\b_1\\} \\oplus ... \\oplus \\text{Span}\\{\\b_n\\} = \\F \\b_1 \\oplus ... \\oplus \\F\\b_n\\]","title":"Theorem (Criterion for Basis and Unique Representation of Basis)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#proof","text":"To prove three equivalent statements, we can simply do a round-cycle proof: \\(1 \\implies 2 \\implies 3 \\implies 1\\) will complete all \\(\\iff\\) proofs. \\((1) \\implies (2)\\) \\(B = \\{\\mathbf{v_{1}, \\cdots, v_{n}}\\}\\) finite subset of \\(V\\) . Suppose \\(\\B\\) is basis of \\(V\\) . Then pick any \\(\\v \\in V\\) and suppose we can write \\(\\v\\) in two ways. \\[\\begin{eqnarray} \\v = a_{1}\\v_1 + \\cdots a_{n}\\mathbf{v_{n}} \\\\ \\v = a_{1}^{'}\\v_1 + \\cdots a_{n}^{'}\\mathbf{v_{n}} \\end{eqnarray}\\] Equations. (1) and (2) imply \\((a_{1} - a_{1}^{'}) \\v_{1} + \\cdots (a_{n} - a_{n}^{'})\\v_{n} = 0\\) and since \\(\\B\\) is L.I set, it only has trivial solutions. Hence \\(a_{1} - a_{1}^{'} = 0\\) \\(\\forall i\\) implying \\(a_{i} = a_{i}^{'}\\) . Hence \\(\\v\\) has unique way of expressing. \\((2) \\implies (3)\\) Show first that \\(V = \\text{span}(\\{\\v_1\\}) + \\cdots \\text{span}(\\{\\mathbf{v_{n}}\\})\\) . \\(\\text{span}(\\{\\v_1\\}) + \\cdots \\text{span}(\\{\\mathbf{v_{n}}\\}) \\subseteq V\\) is obvious. We show \\(V \\subseteq \\text{span}(\\{\\v_1\\}) + \\cdots \\text{span}(\\{\\mathbf{v_{n}}\\})\\) . Let \\(\\v \\in V\\) , by hypothesis, \\[\\v = a_{1}\\v_1 + \\cdots a_{n}\\mathbf{v_{n}} \\in \\text{span}(\\{\\v_1\\})+ \\cdots + \\text{span}(\\{\\mathbf{v_{n}}\\})\\] Hence: \\[V = \\text{span}(\\{\\v_1\\})+ \\cdots + \\text{span}(\\{\\mathbf{v_{n}}\\})\\] Now we show it is direct sum. Denote \\(W_{i} = \\text{span}(\\{\\mathbf{v_{i}}\\})\\) . Suppose not, \\(\\exists x \\in \\sum\\limits_{i=1}^{k-1}W_{i} \\cap W_{k}\\) for some \\(2 \\le k \\le n\\) and \\(x \\neq 0\\) . Then $ x = \\mathbf{w_{1} + \\cdots + w_{k-1}}$ for some \\(\\mathbf{w_{i}} \\in W_{i}\\) and \\(x = -\\mathbf{w_{k}}\\) for some \\(-\\mathbf{w_{k}} \\in W_{k}\\) . But since \\(x\\) is uniquely expressed. \\begin{eqnarray} x & = & \\w_1+\\w_2+...+\\w_{k-1} \\ x & = & -\\w_k \\end{eqnarray} implying that \\[0 =\\w_1+\\w_2+...+\\w_k\\] By uniqueness \\(0 = 0 + 0 \\cdots + 0\\) , but that would means \\(\\w_1=\\w_2=...= \\w_k =0\\) , implying \\(x = 0\\) , a contradiction. \\((3) \\implies (1)\\) Want to show \\(B\\) is a basis of \\(V\\) . By hypothesis, \\begin{eqnarray} V & = & \\text{span}({\\v_{1}}) + \\cdots + \\text{span}({\\v_{n}})\\nonumber\\ & = & \\text{span}({\\v_{1}} \\cup {\\v_{2}} \\cup \\cdots \\cup {\\v_{n}}) \\nonumber \\ & = & \\text{span}({\\v_{1}, \\v_{2}, \\cdots, \\v_{n}})\\nonumber \\end{eqnarray} Now we show \\(B\\) is L.I. That is \\(a_{1}\\v_{1} + \\cdots + a_{n}\\v_{n}\\) has trivial solution. Suppose not, say \\(a_{k} \\ne 0\\) , and such that \\(\\v_{k} = b_{1}\\v_{1} + \\cdots b_{k-1}\\v_{k-1} + \\cdots + b_{n}\\v_{n}\\) where \\(b : = \\frac{-a_{i}}{a_{k}}\\) . This is contradiction as \\(\\v_{k} \\in W_{k}\\) and \\(b_{1}\\v_{1} \\cdots b_{n}\\v_{n} \\in \\sum\\limits_{i \\ne k} W_{i}\\) . Thus \\(\\v_{k} \\in \\sum\\limits_{i \\ne k}W_{i}\\cap W_{k}\\) and \\(\\v_{k} \\ne 0\\) is our assumption. This contradicts direct sum. Hence \\(\\B\\) is LI.","title":"Proof"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#theorem-all-basis-has-the-same-length","text":"This theorem states that all basis of a vector space \\(V\\) has the same length. This theorem is important to faciliate the definition of Dimension.","title":"Theorem (All Basis has the same length)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#dimensions","text":"We have encountered the term Dimension at the start of Vector Spaces . Now, we give it a formal definition.","title":"Dimensions"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#definition-dimension","text":"If a vector space \\(V\\) has a basis \\(\\B\\) with cardinality \\[|B| = n < \\infty\\] then we say that \\(V\\) is finite dimensional and define the dimension \\[\\text{dim}_{\\F}V = |B|\\] Otherwise, \\(V\\) is called infinite dimensional. Note that we implicitly assumed that all basis \\(\\B\\) has the same cardinality, which we showed as a theorem previously.","title":"Definition (Dimension)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#notation-dimension","text":"We denote the dimension of a finite vector space \\(V\\) to be \\[\\dim(V)\\]","title":"Notation (Dimension)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#intuition-dimension","text":"One may wonder why the definition of Dimension depends on the basis . Why can't we just define the dimension of a vector space \\(V\\) over a field \\(\\F^n\\) to be just the length of the element \\(\\v \\in V\\) , which is just \\(n\\) . This is a reasonable assumption, and both turns out to be equivalent. That is, the length of any element \\(\\v \\in V\\) over a field \\(\\F^n\\) is the same as the number of basis \\(\\B\\) that a vector space \\(V\\) has. We can easily think of it geometrically. Consider the 2d space \\(\\R^2\\) , we know that any element in \\(\\R^2\\) must have 2 elements and in our earlier definition, the dimension of such a space is 2. Now we understood basis, and know that we need 2 linearly independent vectors to make up the 2d space. Same logic applies to the 3d space \\(\\R^3\\) .","title":"Intuition (Dimension)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#example-dimension","text":"R2 and R3 Dimensions i) Besides \\(\\{\\mathbf{0}\\}\\) and \\(\\mathbb{R}^2\\) , all subspaces of \\(\\mathbb{R}^2\\) are lines through the origin and they are of dimension \\(1\\) . (Basis of the subspaces has only one vector) ii) Besides \\(\\{\\mathbf{0}\\}\\) and \\(\\mathbb{R}^3\\) , all subspaces of \\(\\mathbb{R}^3\\) are either lines through the origin and they are of dimension \\(1\\) or planes containing the origin, which are of dimension \\(2\\) . (Basis of the subspaces has only one vector or two vectors)","title":"Example (Dimension)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#theorem-dimension-of-a-subspace","text":"If \\(V\\) is a finite dimensional vector space and \\(U\\) is a subspace of \\(V\\) , then \\(\\dim(U) \\leq \\dim(V)\\) .","title":"Theorem (Dimension of a Subspace)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#proof_1","text":"This should not even come as a surprise after the previous example. It also makes sense geometrically as a subspace \\(U\\) , is also a subset of \\(V\\) , and hence cannot \"be larger\" than its \"parent\". Prove it and check pp.45 of linear algebra done right.","title":"Proof"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#basis-and-dimension-theorems-and-applications","text":"","title":"Basis and Dimension Theorems and Applications"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#theorem-spanning-set-contains-basis","text":"A spanning set \\(S\\) (note we do not know if this is a basis or not yet) in vector space \\(V\\) necessarily contains the basis \\(\\B\\) .","title":"Theorem (Spanning Set contains Basis)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#proof_2","text":"First, intuitively we already know that the set \\(S\\) spans \\(V\\) and we acknowledge that a set \\(S\\) spanning the vector space \\(V\\) can be a linearly dependent set (refer to example (different sets can span the same vector space)). Therefore, we just need to \"remove\" the linearly dependent vectors in the set \\(S\\) so that the remaining set \\(\\B \\subseteq S\\) is both linearly independent and spans V , consequently, \\(\\B\\) is a basis of \\(V\\) .","title":"Proof"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#theorem-every-subspace-has-a-basis","text":"Every finite subspace \\(V\\) has a basis \\(\\B\\) .","title":"Theorem (Every Subspace has a Basis)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#proof_3","text":"Every finite subspace can be represented by a span of a set of vectors. By Theorem (Spanning Set contains Basis) , this spanning set has a basis.","title":"Proof"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#theorem-linear-independent-sets-can-be-extended-to-a-basis","text":"Let \\(B\\) be a Linearly Independent subset of a vector space \\(V\\) over a field \\(\\F\\) . Then exactly one of the following two cases is true. i) \\(B\\) spans \\(V\\) and hence \\(B\\) is a basis of \\(V\\) . ii) Let \\(\\v \\in V \\setminus \\text{Span}(B)\\) and hence \\(\\v \\not \\in B\\) . Then \\(B \\cup \\{\\v\\}\\) is an Linearly Independent subset of \\(V\\) . iii) In particular, if \\(V\\) is of finite dimension \\(n\\) , then one can find \\(n - |B|\\) vectors \\(\\mathbf{v_{|B|+1},...,v_n}\\) in \\(V \\setminus \\text{Span}(B)\\) such that \\(B \\coprod \\{\\mathbf{v_{|B|+1},...,v_n} \\}\\) is a basis of \\(V\\) .","title":"Theorem (Linear Independent Sets can be extended to a Basis)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#proof_4","text":"Since \\(B\\) is L.I subset of \\(V\\) , then \\(B\\) either spans \\(V\\) or do not span \\(V\\) . If it spans \\(V\\) , then \\(B\\) is a basis, which is point i). If its doesn't span \\(V\\) , take an element \\(\\w \\in V \\setminus \\text{span}(B)\\) , \\(\\w \\notin B\\) and \\(\\mathbf{w} \\notin \\text{span}(B)\\) . Then it means \\(\\mathbf{w}\\) is not a linear combination of any elements in \\(B\\) . Hence, \\(B \\cup \\{\\w\\}\\) is a L.I set as no elements in \\(B \\cup\\{\\w\\}\\) can be expressed as a LC of each other. Note if \\(B = \\{\\v_{1}, \\v_{2}, \\cdots \\v_{r}\\}, ~ B\\cup \\{\\w\\} = \\{\\v_{1}, \\cdots \\v_{r}, \\w\\}\\) . $ \\v_{i} \\ne \\w$. For point 3, this means if \\(B\\) is LI in \\(V\\) , then \\(B\\) can only have 2 cases. If Case 1 occurs, we are done as $ n - |B| = n- n = 0$, we do not need any more vectors in \\(V \\setminus \\text{span}(B)\\) to union with \\(B\\) . If Case 2 occurs, Then let \\(|B| = r\\) . then \\(\\exists \\mathbf{w_{r+1}} \\in V \\setminus \\text{span}(B)\\) s.t \\(B \\cup \\{\\mathbf{w_{r+1}}\\}\\) is L.I set. Applying the two cases to \\(B \\cup \\{\\mathbf{w_{r+1}}\\}\\) , we will have \\(B \\cup \\{\\mathbf{w_{r+1}}\\}\\) be a basis or \\(B \\cup \\{\\mathbf{w_{r+1}, w_{r+2}}\\}\\) is another L.I set. Since the dimension of \\(V\\) is \\(n\\) . Then applying this expansion inductively, \\(B \\cup \\{\\mathbf{w_{r+1}, \\cdots w_{n}}\\}\\) will eventually form a basis for \\(V\\) . If for a contradiction the case 2 continues after \\(\\mathbf{w_{n}}\\) then it contradicts the fact that a set with more than \\(n\\) vectors cannot be L.I.","title":"Proof"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#theorem-equivalent-basis-definition","text":"Let \\(B\\) be a subset of a vector space \\(V\\) of finite dimension \\(\\text{dim}_{\\F}V = n \\geq 1\\) . Then the following are equivalent. i) \\(B\\) is a basis of \\(V\\) . ii) \\(B\\) is \\(L.I\\) and \\(|B| = n\\) . iii) \\(B\\) spans \\(V\\) and \\(|B| = n\\) .","title":"Theorem (Equivalent Basis Definition)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#proof_5","text":"We prove \\(i \\Leftrightarrow ii\\) and \\(i \\Leftrightarrow iii\\) . Now \\(i \\Rightarrow ii\\) and \\(i \\Rightarrow iii\\) are by definition. iii \\(\\Rightarrow\\) i Since \\(V = \\text{Span}(B)\\) , there exists a subset \\(B_1\\) of \\(B\\) such that \\(B_1\\) is a basis of \\(V\\) . But by our hypothesis, we have \\(|B| = n\\) . Since \\(B_1\\) is basis of \\(V\\) , we must have \\(|B_1| =\\) dimension of \\(V\\) which is \\(n\\) . And hence \\(|B_1| = |B|\\) . Since, \\(B_1 \\subseteq B\\) , and \\(|B_1| = |B|\\) , we must have \\(B_1 = B\\) as a set. Hence \\(B\\) is a basis of \\(V\\) . Alternatively, suppose \\(B\\) is not a basis of \\(V\\) . \\(B\\) is not linearly independent. Take a vector \\(\\v\\) in \\(B\\) which is a linear combination of other vectors in \\(B\\) . Then we know that \\(B - \\v\\) still spans \\(V\\) . But \\(|B-\\v| = n-1\\) by our hypothesis. And it is a contradiction since it is a subset of \\(V\\) with less than \\(n\\) vectors, hence it cannot span \\(V\\) . It is a contradiction. ii \\(\\Rightarrow\\) i Suppose that \\(B\\) is not a basis of \\(V\\) . Then \\(B\\) does not span \\(V\\) . Pick a vector \\(\\v\\) in \\(V\\) but not in Span \\((B)\\) (there exists such a vector because we say that \\(B\\) does not span \\(V\\) ) and \\(B \\cup \\{\\v\\}\\) is still a linearly independent set given that \\(B\\) is a linearly independent set. Hence it is a contradiction because \\(|B \\cup \\{\\v\\}| = n+1\\) has more vectors than even the basis set, hence it cannot be linearly independent.","title":"Proof"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#useful-summary","text":"Let \\(V\\) be a vector space which has a basis \\(B\\) with \\(k\\) vectors. Then any subset \\(W\\) of \\(V\\) with more than \\(k\\) vectors is always linearly dependent. So it means if any subset \\(W\\) of \\(V\\) is linearly independent, then \\(W\\) will have less or equals to \\(k\\) vectors, in which directly translates to the dimension of \\(W\\) is less or equals to the dimension of \\(V\\) . Any subset \\(W\\) of \\(V\\) with less than \\(k\\) vectors cannot span \\(V\\) . So for any subset \\(W\\) of \\(V\\) which spans \\(V\\) , then it will have more or equals to \\(k\\) vectors. In particular, let \\(W = \\{\\w_1,...,\\w_k\\}\\) be a set of vectors in \\(V = \\mathbb{R}^n\\) . If \\(k < n\\) , then \\(S\\) cannot span \\(\\mathbb{R}^n\\) . Suppose that \\(C\\) is another basis of \\(V\\) , then \\(|B|=|C|\\) .","title":"Useful Summary"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#applications-of-basis","text":"","title":"Applications of Basis"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_basis_dimension/#efficient-space-storage","text":"Storing data efficiently in this era is very important. Let's give a superfluous example: Let \\(\\A\\) be a \\(3 \\times 100\\) matrix where there are 3 features and 100 samples; and if we know that the columns of the matrix has a basis \\(\\B\\) , say \\[\\B = \\left\\{\\begin{bmatrix}1 \\\\ 0 \\\\ 0\\end{bmatrix} ,\\quad \\begin{bmatrix}0 \\\\ 0 \\\\ 1 \\end{bmatrix} ,\\quad \\begin{bmatrix} 0 \\\\ 0 \\\\ 1\\end{bmatrix}\\right\\}\\] But we know that all 100 columns of \\(\\A\\) can be reconstructed using the basis vectors; i.e. for each column \\(\\a_i\\) of \\(\\A\\) , there exists constants \\(\\lambda_i\\) such that \\[\\a_i = \\lambda_1 \\B_1 + \\lambda_2 \\B_2 + \\lambda_3 \\B_3\\] As a result, we can effectively store the 3 basis vectors (9 elements and note that the 3 basis vectors can be taken from the columns of \\(\\A\\) ); the remaining 7 columns we just need \\(7 \\times 3 = 21\\) constants so that we can recover the columns using linear combination of the basis vectors. In total, we reduced the space from \\(3 \\times 100 = 300\\) elements to \\(3 \\times 3 + 7 \\times 3 = 30\\) elements, effectively 70 percent decrease. Macro Analyst Linear Algebra \u21a9 Such as the real numbers \\(\\R\\) or the complex numbers \\(\\mathbb{C}\\) . \u21a9 https://math.stackexchange.com/questions/3089880/minimal-generating-set \u21a9","title":"Efficient Space Storage"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_linear_independence/","text":"\\[\\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\z}{\\mathbf{z}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\C}{\\mathbf{C}}\\] Linear Independence Algebraic Definition (Linear Dependence) Example (Linear Dependence in \\(\\mathbb R^2\\) ) Example (Linear Independence in \\(\\mathbb R^3\\) ) Algebraic Definition (Linear Independence) Equivalent Algebraic Definition (Linear Independence) Theorem (Linear Combination Implies Linear Dependence) Proof Theorem (The zero vector and linear independence) Proof Theorem (Union of linearly independent vectors) Intuition (Linear (In)Dependence) Theorem (Uniqueness of Representation) Theorem (The Span of Linearly Dependent Vectors is Identical V2) Theorem (Inheritance of Linear Independence) Theorem (Length of Linear Independent Set \\(\\leq\\) Length of Spanning Set) Example (Usage of Theorem) Corollary (Length of Linear Indepedent Set and Spanning Set) Proof Example Geometric Definition (Linear Independence) How to determine if a set is Linearly Independent (Non-Matrix Algorithmic Way) Linkedin Summary Linear Independence References Linear Independence Algebraic Definition (Linear Dependence) Definition Let \\(V\\) be a vector space over a field \\(\\F\\) . The set of vectors \\(\\v_1, \\v_2, ..., \\v_m \\in V\\) is linearly dependent if and only if one of the vectors \\(\\v_i\\) , where \\(i \\in [1, m]\\) , is the zero vector or that at least one of \\(\\v_i\\) is a linear combination of the rest of the vectors (i.e. \\(\\v_i = \\sum_{j \\neq i}\\alpha_j\\v_j\\) ). More formally 1 , a sequence of vectors \\(\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_m\\) from a vector space \\(V\\) is said to be linearly dependent , if there exist scalars \\(a_1, a_2, \\dots, a_m\\) , not all zero, such that \\(a_1\\mathbf{v}_1 + a_2\\mathbf{v}_2 + \\cdots + a_m\\mathbf{v}_m = \\mathbf{0}\\) , where \\(\\mathbf{0}\\) denotes the zero vector. This implies that at least one of the scalars is nonzero, say \\(a_1\\neq 0\\) , and the above equation can be written as \\[\\mathbf{v}_1 = \\frac{-a_2}{a_1}\\mathbf{v}_2 + \\cdots + \\frac{-a_m}{a_1} \\mathbf{v}_m, a_i \\in \\F, a_1 \\neq 0\\] Example (Linear Dependence in \\(\\mathbb R^2\\) ) Example Consider column vectors \\[\\v_1 = \\begin{bmatrix}1 \\\\ 2 \\end{bmatrix},\\quad \\v_2 = \\begin{bmatrix}3 \\\\ 6 \\end{bmatrix},\\quad \\v_3 = \\begin{bmatrix}-6 \\\\ -12 \\end{bmatrix}\\] in the ambient \\(\\R^2\\) space. They are linearly dependent because the vector \\[\\v_2 = \\begin{bmatrix}3 \\\\ 6 \\end{bmatrix} = 3\\v_1 + 0\\v_3\\] and note that we are using \\(\\v_2\\) as an example here, you can also say that \\(\\v_1\\) is a linear combination of the other 2 vectors; but so long there is at least one such vector in the set, then this set is linearly dependent . Note I put a coefficient \\(0\\) in front of \\(\\v_3\\) to indicate that the scalars are not all zero but can have some zeros. Fig; Linear Dependent Vectors; By Hongnan G. import matplotlib.pyplot as plt import numpy as np # Courtesy of https://github.com/MacroAnalyst/Linear_Algebra_With_Python fig , ax = plt . subplots ( figsize = ( 8 , 8 )) #######################Arrows####################### arrows = np . array ([[[ 0 , 0 , 1 , 2 ]], [[ 0 , 0 , 3 , 6 ]], [[ 0 , 0 , - 6 , - 12 ]]]) colors = [ \"r\" , \"b\" , \"g\" ] for i in range ( arrows . shape [ 0 ]): X , Y , U , V = zip ( * arrows [ i , :, :]) ax . arrow ( X [ 0 ], Y [ 0 ], U [ 0 ], V [ 0 ], color = colors [ i ], width = 0.18 , length_includes_head = True , head_width = 0.3 , # default: 3*width head_length = 0.6 , overhang = 0.4 , zorder =- i , ) ax . scatter ( 0 , 0 , ec = \"red\" , fc = \"black\" , zorder = 5 ) ax . text ( 1.5 , 2 , \"$(1, 2)$\" ) ax . text ( 3.5 , 6 , \"$(3, 6)$\" ) ax . text ( - 9 , - 12 , \"$(-9, -12)$\" ) ax . grid ( True ) ax . set_title ( \"Linear Dependence Visualization ((1,2), (3, 6), (-9, -12))\" ) # axis([xmin, xmax, ymin, ymax]) ax . axis ([ - 12 , 10 , - 20 , 10 ]) ax . set_xlabel ( \"x-axis\" , size = 18 ) ax . set_ylabel ( \"y-axis\" , size = 18 ) plt . savefig ( \"linear_dependence.svg\" , format = \"svg\" , dpi = 600 ) plt . show () Example (Linear Independence in \\(\\mathbb R^3\\) ) Courtesy of MacroAnalyst's Linear Algebra with Python . Example Next, we visualize linear independence in \\(\\mathbb{R}^3\\) with vectors \\((1,-2,1)^T\\) , \\((2,1,2)^T\\) , \\((-1,2,3)^T\\) . Pan around the image (either by setting ax.view_init or using JupyterLab widget), we can see that the green vector is not in the plane spanned by red and blue vector, thus they are linearly independent. # %matplotlib notebook, use this only when you are in Jupyter Notebook, it doesn't work in Jupyterlab import matplotlib.pyplot as plt import numpy as np # %matplotlib notebook, use this only when you are in Jupyter Notebook, it doesn't work in Jupyterlab fig = plt . figure ( figsize = ( 10 , 10 )) ax = fig . add_subplot ( projection = '3d' ) s = np . linspace ( - 1 , 1 , 10 ) t = np . linspace ( - 1 , 1 , 10 ) S , T = np . meshgrid ( s , t ) X = S + 2 * T Y = - 2 * S + T Z = S + 2 * T ax . plot_wireframe ( X , Y , Z , linewidth = 1.5 , color = 'k' , alpha = .6 ) vec = np . array ([[[ 0 , 0 , 0 , 1 , - 2 , 1 ]], [[ 0 , 0 , 0 , 2 , 1 , 2 ]], [[ 0 , 0 , 0 , - 1 , 2 , 3 ]]]) colors = [ 'r' , 'b' , 'g' ] for i in range ( vec . shape [ 0 ]): X , Y , Z , U , V , W = zip ( * vec [ i ,:,:]) ax . quiver ( X , Y , Z , U , V , W , length = 1 , normalize = False , color = colors [ i ], arrow_length_ratio = .08 , pivot = 'tail' , linestyles = 'solid' , linewidths = 3 , alpha = .6 ) ax . set_title ( 'Linear Independence Visualization' ) ax . set_xlabel ( 'x-axis' , size = 18 ) ax . set_ylabel ( 'y-axis' , size = 18 ) ax . set_zlabel ( 'z-axis' , size = 18 ) ax . view_init ( elev = 60. , azim = 0 ) plt . show () Algebraic Definition (Linear Independence) Definition Let \\(V\\) be a vector space over a field \\(\\F\\) . The set of vectors \\(\\v_1, \\v_2, ..., \\v_m \\in V\\) is linearly independent if and only if the only solution to the equation \\(a_1\\mathbf{v}_1 + a_2\\mathbf{v}_2 + \\cdots + a_m\\mathbf{v}_m = \\mathbf{0}\\) is the trivial solution, the zero vector \\(\\0\\) . Equivalent Algebraic Definition (Linear Independence) Equivalent Linear Independence Definition Let \\(V\\) be a vector space over a field \\(\\F\\) . Let the set of vectors \\(S = \\v_1, \\v_2, ..., \\v_m \\in V\\) ; Then \\(S\\) is linearly independent if and only if i) the only solution to the equation \\(a_1\\mathbf{v}_1 + a_2\\mathbf{v}_2 + \\cdots + a_m\\mathbf{v}_m = \\mathbf{0}\\) is the trivial solution, the zero vector \\(\\0\\) ; iif ii) \\(S\\) is called a linearly dependent set and \\(\\mathbf{\\v_1,\\v_2,...,\\v_m}\\) are said to be linearly dependent iff \\(a_1\\mathbf{v}_1 + a_2\\mathbf{v}_2 + \\cdots + a_m\\mathbf{v}_m = \\mathbf{0}\\) has non - trivial solutions. That is there exists real numbers \\(a_1,a_2,...,a_m\\) not all of them are zero, such that \\(a_1\\v_1+a_2\\v_2+...+a_m\\v_m=0\\) ; iif iii) The set \\(S\\) is a linearly independent set if every non empty finite subset of \\(S\\) is linearly independent. The set \\(S\\) is a linearly dependent set if at least one non empty finite subset of \\(S\\) is linearly dependent. Theorem (Linear Combination Implies Linear Dependence) This is a very useful, and often better understood theorem! Theorem i) Let \\(S = \\{\\mathbf{v_1,v_2,...,v_m}\\}\\) be a finite subset of a vector space \\(V\\) over a field \\(\\F\\) where \\(m \\geq 2\\) , then \\(S\\) is linearly dependent if and only if at least one vector \\(\\mathbf{v_i} \\in S\\) can be written as a linear combination of other vectors in \\(S\\) . This means that there are scalars \\(a_1,a_2,...a_{i-1},a_{i+1},...,a_m \\in \\F\\) (scalars vanishing allowed) such that \\[\\mathbf{v_i} = a_1\\v_1+...+a_{i-1}\\mathbf{v_{i-1}}+a_{i+1}\\mathbf{v_{i+1}}+...+a_m\\v_m\\] ii) It follows that the contrapositive of this theorem: \\(S\\) is linearly independent if and only if no vector in \\(S\\) can be written as a linear combination of other vectors in \\(S\\) . Proof Proof We will just prove part i). \\(\\implies\\) \\(S = \\{\\mathbf{v_{1}, \\cdots , v_{m}}\\}\\) is finite \\(\\implies\\) \\(|S| = n < \\infty\\) , \\(S \\subseteq V\\) . If S is LD, then \\(a_{1}\\v_1 + a_{2}\\v_2 + \\cdots + a_{m}\\mathbf{v_{m}} = 0\\) has non trivial solutions. This means not all \\(a_{i} = 0\\) . Suppose \\(a_{k} \\ne 0\\) , thus one can write in this way: \\[ \\begin{eqnarray} a_{k}\\mathbf{v_m} & = & -a_{1}\\v_1 - a_{2}\\v_2 \\cdots -a_{m-1}\\mathbf{v_{m-1}} - \\cdots -a_{m}\\mathbf{v_{m}} \\nonumber \\\\ \\mathbf{v_m} & = & \\frac{-a_{1}}{a_{k}}\\v_1 - \\cdots - \\frac{a_{m-1}}{a_{k}}\\mathbf{v_{m-1}} - \\frac{a_{m}}{a_{k}}\\mathbf{v_{m}} \\nonumber \\\\ & = & b_{1}\\v_1 + \\cdots + b_{m-1}\\mathbf{v_{m-1}} + \\cdots + b_{m}\\mathbf{v_{m}} \\nonumber \\end{eqnarray} \\] where \\(b_{i} = \\frac{-a_{1}}{a_{k}}\\) . \\(\\Leftarrow\\) If \\(\\exists \\mathbf{v_m} = a_{1}\\v_1 + \\cdots + a_{m-1}\\mathbf{v_{m-1}} + \\cdots a_{m}\\mathbf{v_{m}}\\) , Then \\(a_{1}\\v_1 + \\cdots + a_{m-1}\\mathbf{v_{m-1}} + \\cdots a_{m}\\mathbf{v_{m}} = 0\\) will have non-trivial solution. Note \\((-1)\\mathbf{v_m} = a_{1}\\v_1 - \\cdots - a_{m-1}\\mathbf{v_{m-1}} - \\cdots - a_{m}\\mathbf{v_{m}}\\) . Then set \\(a_{k} = -1\\) , we have \\[a_{1}\\v_1 + \\cdots + (-1)\\mathbf{v_{k}} + \\cdots a_{m}\\mathbf{v_{m}} = a_1\\v_1+ \\cdots -a_{1}\\v_1 - a_2\\v_2 \\cdots - a_{m}\\mathbf{v_{m}} + \\cdots a_{m}\\mathbf{v_{m}} = 0\\] Hence, \\((a_{1}, \\cdots a_{m-1},-1, a_{k+1}, \\cdots a_{m})\\) is non trivial solution. Thus \\(\\v_1, \\cdots \\mathbf{v_{m}}\\) is LD. Theorem (The zero vector and linear independence) Theorem (The zero vector and linear independence) Note that \\(\\{\\mathbf{0}\\}\\) is a linearly dependent set, which means that if \\(\\mathbf{0} \\in S\\) , then \\(S\\) is a linearly dependent set. Proof Proof Write \\(c \\cdot \\mathbf{0} = 0\\) . Then one can easily see this equation has solutions other than \\(c = 0\\) . Since by killing power of zero, we can have the variable \\(c\\) to be any number. Hence by definition, \\(\\{\\mathbf{0}\\}\\) is a linearly dependent set. If \\(\\mathbf{0} \\in S\\) , then \\(\\mathbf{0}\\) can be expressed as a linear combination of any other vectors in the set by setting coefficients to be \\(0\\) . Hence it is considered a redundant vector and therefore causing \\(S\\) to be a linearly dependent set. Theorem (Union of linearly independent vectors) Theorem (Union of linearly independent vectors) Let \\(\\mathbf{v_1, v_2,...,v_k}\\) be LI vectors in \\(\\mathbb{R}^n\\) . If \\(\\mathbf{v_{k+1}}\\) is a vector in \\(\\mathbb{R}^n\\) and it is not a linearly combination of \\(\\mathbf{v_1,v_2,...,v_k}\\) , then \\(\\mathbf{v_1,...,v_k,v_{k+1}}\\) are linearly independent. Intuition (Linear (In)Dependence) Consider a xy cartesian plane, we have the x axis, where we can visualize as walking left or right (east or west), and the y axis, walking forward and backwards (north or south). We claim that the set of vectors \\[ S = \\left\\{\\begin{bmatrix} \\color{red}1 \\\\ \\color{red}0 \\end{bmatrix}, \\begin{bmatrix} \\color{red}0 \\\\ \\color{red}{1} \\end{bmatrix} \\right\\} \\] is linearly independent, as neither vector can be represented by the other in any way. But if we add one more vector \\[ S_1 = \\left\\{\\begin{bmatrix} \\color{red}1 \\\\ \\color{red}0 \\end{bmatrix}, \\begin{bmatrix} \\color{red}0 \\\\ \\color{red}{1} \\end{bmatrix}, \\begin{bmatrix} \\color{red}1 \\\\ \\color{red}{1} \\end{bmatrix} \\right\\}\\] then we say this set is linearly dependent because the vector \\([1, 1]\\) can be linearly combined by the other two, and is thus redundant. But why the weird definition? Why does having a non-trivial solution to the equation means linear dependence ? A good intuition from the post in mathstackexchange 2 : Intuition Your basic intuition is quite right --- a set of vectors are linearly independent if they don't affect each other (which, as it stands, is a somewhat ambiguous statement, hence the formal definition). Perhaps it would be helpful to start with just two vectors, say the standard vectors $$ \\begin{pmatrix} 1\\ 0 \\end{pmatrix} \\hspace{20pt}\\text{and}\\hspace{20pt} \\begin{pmatrix} 0\\ 1 \\end{pmatrix} $$ which are linearly independent in \\(\\mathbb{R}^{2}\\) (two-dimensional coordinate space). Natural intuition would be to think that these two are linearly independent because there is no scalar that you can multiply one by to get the other. That is, for \\(\\alpha \\in \\mathbb{R}\\) , the equation $$ \\alpha\\begin{pmatrix} 1\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0\\ 1 \\end{pmatrix} $$ has no solutions. This is a good rule for any set of two vectors, and is in fact equivalent to the formal definition for if there were scalars \\(\\alpha_{1}, \\alpha_{2} \\in \\mathbb{R}\\) such that $$ \\alpha_{1}\\begin{pmatrix} 1\\ 0 \\end{pmatrix} + \\alpha_{2}\\begin{pmatrix} 0\\ 1 \\end{pmatrix} = 0 $$ (which is just the formal definition), then we could rearrange it to get $$ -\\frac{\\alpha_{1}}{\\alpha_{2}}\\begin{pmatrix} 1\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0\\ 1 \\end{pmatrix}. $$ This is clearly false! The formal definition for an arbitrary number of vectors is an extension of this idea. We pick \\(0\\) to be the right hand side for convenience, since every vector space must have the zero vector (by definition), so that if some vectors affect each other in some way, they must be able to be combined some way to give you the zero vector. In fact, if a set of vectors are linearly independent, then the only vector that they agree on is the zero vector, and it can only be obtained by multiplying each vector by zero (and summing them up). Here I chime in a bit: consider \\((0, 1), (1, 0), (1, 1)\\) in \\(\\R^2\\) , then we know \\((0, 1) + (1, 0) = (1, 1)\\) where \\((1, 1)\\) is a linear combination of the former two vectors; and since \\((0, 1) + (1, 0) = (1, 1)\\) , then \\(((0, 1) + (1, 0)) - (1, 1) = (0, 0)\\) , the zero vector, this must be true for every vector that can be expressed as a linear combination of the rest. If they can be combined in some way to give you the zero vector, then you can just rearrange them (like above) and find one of the vectors as a linear combination of the others, which would clearly imply that it is not independent of the others. To make this more clear, let \\(\\{v_{1}, v_{2}, \\ldots, v_{n}\\}\\) be a set of vectors, and suppose there exists non-zero scalars \\(\\alpha_{i} \\in \\mathbb{R}\\) such that $$ \\alpha_{1}v_{1} + \\alpha_{2}v_{2} + \\ldots + \\alpha_{n}v_{n} = 0. $$ Then you can just rearrange this to get, for example, the equation $$ \\frac{-\\alpha_{1}}{\\alpha_{n}}v_{1} + \\frac{-\\alpha_{2}}{\\alpha_{n}}v_{2} + \\ldots + \\frac{-\\alpha_{n-1}}{\\alpha_{n}}v_{n-1} = v_{n}. $$ Then \\(v_{n}\\) depends on the other vectors, so the set is not linearly independent ! Thus, the only way that you can get a set of linearly independent vectors to all give you the zero vector, is that all must be multiplied by zero, else you can rearrange the equation to show that one of the vectors depends on the others, which would be a contradiction. This is why the definition is the way it is. As a note for your third point, the definition wants \\(\\alpha_{i} = 0\\) for all \\(i = 1, \\ldots, n\\) . That is, every coefficient has to be zero, not just one of them. Machine Learning Though a bit too early, but we often represent the variable/feature space of the dataset using the Design Matrix and each column (or row) represents the encoded information of the variables. If there are many redundant vectors inside the Design Matrix, this means that we are having additional information that we do not need (which may hurt performance). PCA can also help to reduce the dimensionality too. Theorem (Uniqueness of Representation) Theorem (Uniqueness of Representation) From the above, we can deduce something less apparent from linear independence, that is, for the same \\(\\v \\in V\\) , and the same set of vectors \\(\\v_1, \\v_2, ..., \\v_m \\in V\\) , there is only one unique representation of \\(\\v \\in V\\) using the set of vectors. For example, if we consider a simple example where \\(\\v = a_1 \\v_1 + a_2 \\v_2\\) and \\(\\v = b_1 \\v_1 + b_2 \\v_2\\) , where \\(a_1, a_2, b_1, b_2\\) are assumed to be distinct scalars in \\(\\F\\) , then subtracting both results in \\[\\0 = (a_1-b_1)\\v_1 + (a_2-b_2)\\v_2 \\implies a_i-b_i = 0\\] because we note that the equation can only have the trivial solution \\(\\0\\) . This results in a contradiction as we stated that \\(a_i, b_i\\) are distinct. So conclusion, if a set of vectors are linearly independent in \\(V\\) , then any vectors in \\(\\v \\in V\\) can only have a unique representation in \\(V\\) . Using the example in Intuition (Linear (In)Dependence) , we know that \\(S\\) is a linearly independent set, and imagine we take a vector \\((1, 1)\\) , can we actually find another way to represent \\((1, 1)\\) using this exact set of vectors? In other words, using \\((1, 0), (0, 1)\\) as the \"base vectors\", we must move 1 unit to the right and 1 unit up to reach \\((1, 1)\\) , can we move in any other way to get to \\((1, 1)\\) ? The answer is no. Theorem (The Span of Linearly Dependent Vectors is Identical V2) Branching from the Theorem (Linear Dependence Span equals each other) in the Chapter Span and taken from Sheldon Axler: Linear Algebra Done Right, pp. 35 : Theorem (The Span of Linearly Dependent Vectors is Identical V2) Let \\(\\v_1,...,\\v_m\\) be linearly dependent vectors in a vector space \\(V\\) . Then there exists \\(j \\in \\{1,2,\\cdots,m\\}\\) such that the following holds: \\(\\v_j \\in \\textbf{span}(\\v_1,\\cdots,\\v_{j-1})\\) ; if the \\(j\\) -th term is removed from the linearly dependent set, then $\\textbf{span}(\\v_1, \\cdots, \\v_{j-1}, \\v_{j+1}, \\cdots \\v_m) = \\textbf{span}(\\v_1, \\cdots, \\v_{j-1},\\v_j, \\v_{j+1}, \\cdots \\v_m) $. Theorem (Inheritance of Linear Independence) Theorem (Inheritance of Linear Independence) Let \\(S_1 \\subseteq S_2\\) , if the smaller set \\(S_1\\) is linearly dependent then so is the larger set \\(S_2\\) . \\(S_2\\) is linearly dependent follows from part iii) of Equivalent Definition of Linear independence . Now the contrapositive of this is if the larger set \\(S_2\\) is linearly independent then so is \\(S_1\\) being linearly independent. Theorem (Length of Linear Independent Set \\(\\leq\\) Length of Spanning Set) Theorem Taken from Sheldon Axler: Linear Algebra Done Right, pp. 35: In a finite-dimensional vector space, the length of every linearly independent set of vectors is less than or equal to the length of every spanning set of vectors. Example (Usage of Theorem) Example 1: Usage of Theorem to show a set of vectors cannot be linear independent Show that the set of vectors \\(\\{(1,2,3), (4,5,8), (9,6,7), (-3,2,8)\\}\\) is not linearly independent in \\(\\R^3\\) . Solution: We know that the set of vectors (basis) \\(\\{(1,0,0), (0,1,0), (0,0,1)\\}\\) spans \\(\\R^3\\) . And hence, if the set of vectors in question is linearly indepedent, then its cardinality cannot exceed 3. Example 2: Usage of Theorem to show a set of vectors cannot span the vector space Show that the set of vectors \\(\\{(1,2,3,5), (4, 5, 8, 3), (9, 6, 7, 1)\\}\\) does not span \\(\\R^4\\) . Solution: We know that the set of vectors (basis) \\(\\{(1,0,0,0), (0,1,0,0), (0,0,1,0), (0,0,0,1)\\}\\) is linearly independent in \\(\\R^4\\) . From theorem, we know the length (cardinality) of the set of linearly independent vectors is 4, and in the question, the set of vectors has only cardinality 3, and hence, it cannot span \\(\\R^4\\) . Corollary (Length of Linear Indepedent Set and Spanning Set) Corollary As a corollary of the Theorem (Inheritance of Linear Independence) above: Let \\(S = \\{\\mathbf{u_1,u_2, \\cdots, u_k}\\} \\subseteq \\mathbb{R}^{n}\\) . If \\(k >n\\) , then \\(S\\) is linearly dependent. Proof Proof Let \\(\\mathbf{u_i} = (a_{{i1}},a_{{i2}},...,a_{{in}})\\) for \\(i=1,2,...,k\\) . Then we write \\[c_1\\u_1+c_2\\u_2+...+c_k\\u_k=0\\] and check if it has the trivial solution only or has non trivial solutions. \\(\\begin{cases} a_{11}c_1+a_{21}c_2+...+a_{k1}c_k = 0\\\\ a_{12}c_1+a_{22}c_2+...+a_{k2}c_k = 0\\\\ ~~~~~~~~~~~\\vdots\\\\ a_{1n}c_1+a_{2n}c_2+...+a_{kn}c_k = 0 \\end{cases}\\) This system has \\(k\\) unknowns and \\(n\\) equations. By our previous results, since \\(k > n\\) , the system must have non trivial solution. Hence \\(S\\) is linearly dependent. Example Example In \\(\\mathbb{R}^{2}\\) , a set of three or more vectors must be linearly dependent; In \\(\\mathbb{R}^{3}\\) , a set of four or more vectors must be linearly dependent. Fig; Note that the first figure shows a set of dependent vectors; the second figure shows a set of independent vectors; and the third figure shows a set of dependent vectors; By Hongnan G. The above image illustrates that in \\(\\R^2\\) space, if a set of vectors has cardinality more than the dimension of its ambient space (read: 2), then it must be linearly dependent . Intuition In the example above, our ambient space is \\(\\R^2\\) , the author Mike provided a good intuition here on why a set of vectors whose cardinality is more than its ambient space cannot be linearly independent. Assume a contradiction, that they are linearly independent, then these 3 vectors span a 3d-subspace, which cannot happen if all 3 vectors are in 2d-subspace. To visualize it, imagine drawing 3 vectors on a 2d-plane (a piece of paper), then can you ever construct a cube (3d-space) from these 3 vectors? No. Geometric Definition (Linear Independence) We go through the Geometric Definition after the previous theorem. From Mike X Cohen: Linear Algebra: Theory, Intuition, Code, 2021. (pp. 90) , a set of vectors independent if the subspace dimensions spanned by the set of vectors is equal to the number of vectors in the set. Recall that up till now, the subspace dimension simply means the \"number of axes\" in the subspace. This can be understood with the following example: For example, a set with one vector spans a line (assuming it is not the zeros vector) and is always an independent set (1 vector, 1 dimension); a linearly independent set of two vectors spans a plane (2 vectors, 2 dimensions); an independent set with three vectors spans a 3D space (3 vectors, 3 dimensions). For more info, read Mike X Cohen: Linear Algebra: Theory, Intuition, Code, 2021. (pp. 90-92) . How to determine if a set is Linearly Independent (Non-Matrix Algorithmic Way) pp. 95-96 Mike's book. Linkedin Summary Today marks day 9 of round 1, continuing my journey in #linearalgebra, following the book \"Linear Algebra, Theory, Intuition and Code\", I went through the definition and some examples of what constitutes a linear in(dependent) set . In short, for a vector space V over a field F^n, a subset S = {v_1, v_2, ..., v_m} of V is a linearly independent set if a_1v_1 + a_2v_2 + ... + a_m_vm = 0 has only the trivial solution. This definition might seem a bit weird at first sight, we can break down and see what it really means. We can understand this definition better from its contraposition: if subset S of V is linearly dependent, then it must have at least one non-trivial solution; consider this solution to be (a_1, a_2, ..., a_m) = (1, 2, 0, ..., 0), this implies that v_1 = -2v_2. Though not a proof, we now know that if at least one vector can be represented by a linear combination of the other vectors in the set, then this set is linearly dependent . So another way to word the linear independence definition is that no vector in S can be represented as a linear combination of the other vectors in S. An immediate consequence of this definition is that an linearly independent set must have an unique representation in the vector space spanned by this set. Suppose not (a contradiction), then there exists a_i, b_i's such that a_1v_1 + ... a_mv_m = 0 and b_1v_1 + ... + b_mv_m = 0, where a_i and b_i's are not all the same. This implies (a_1-b_1)v_1 + ... + (a_m-b_m)v_m = 0 has a non-trivial solution since there exists at least one pair of a_i != b_i. Geometrically, one can see the below image, 3 vectors in a 2d-plane is necessarily linearly dependent, you can also verify that to get (10, 10) can be made up by (0, 10) + (10, 0), or 1*(10, 10), which has two representations. Linear Independence References https://www.machinelearningmindset.com/linear-independence-of-vectors/ https://math.stackexchange.com/questions/2779918/intuition-for-formal-definition-of-linear-independence https://medium.com/swlh/how-to-understand-linear-independence-linear-algebra-8bab1d918509 Linear Independence Wikipedia \u21a9 https://math.stackexchange.com/questions/2779918/intuition-for-formal-definition-of-linear-independence \u21a9","title":"Linear Independence"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_linear_independence/#linear-independence","text":"","title":"Linear Independence"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_linear_independence/#algebraic-definition-linear-dependence","text":"Definition Let \\(V\\) be a vector space over a field \\(\\F\\) . The set of vectors \\(\\v_1, \\v_2, ..., \\v_m \\in V\\) is linearly dependent if and only if one of the vectors \\(\\v_i\\) , where \\(i \\in [1, m]\\) , is the zero vector or that at least one of \\(\\v_i\\) is a linear combination of the rest of the vectors (i.e. \\(\\v_i = \\sum_{j \\neq i}\\alpha_j\\v_j\\) ). More formally 1 , a sequence of vectors \\(\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_m\\) from a vector space \\(V\\) is said to be linearly dependent , if there exist scalars \\(a_1, a_2, \\dots, a_m\\) , not all zero, such that \\(a_1\\mathbf{v}_1 + a_2\\mathbf{v}_2 + \\cdots + a_m\\mathbf{v}_m = \\mathbf{0}\\) , where \\(\\mathbf{0}\\) denotes the zero vector. This implies that at least one of the scalars is nonzero, say \\(a_1\\neq 0\\) , and the above equation can be written as \\[\\mathbf{v}_1 = \\frac{-a_2}{a_1}\\mathbf{v}_2 + \\cdots + \\frac{-a_m}{a_1} \\mathbf{v}_m, a_i \\in \\F, a_1 \\neq 0\\]","title":"Algebraic Definition (Linear Dependence)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_linear_independence/#example-linear-dependence-in-mathbb-r2","text":"Example Consider column vectors \\[\\v_1 = \\begin{bmatrix}1 \\\\ 2 \\end{bmatrix},\\quad \\v_2 = \\begin{bmatrix}3 \\\\ 6 \\end{bmatrix},\\quad \\v_3 = \\begin{bmatrix}-6 \\\\ -12 \\end{bmatrix}\\] in the ambient \\(\\R^2\\) space. They are linearly dependent because the vector \\[\\v_2 = \\begin{bmatrix}3 \\\\ 6 \\end{bmatrix} = 3\\v_1 + 0\\v_3\\] and note that we are using \\(\\v_2\\) as an example here, you can also say that \\(\\v_1\\) is a linear combination of the other 2 vectors; but so long there is at least one such vector in the set, then this set is linearly dependent . Note I put a coefficient \\(0\\) in front of \\(\\v_3\\) to indicate that the scalars are not all zero but can have some zeros. Fig; Linear Dependent Vectors; By Hongnan G. import matplotlib.pyplot as plt import numpy as np # Courtesy of https://github.com/MacroAnalyst/Linear_Algebra_With_Python fig , ax = plt . subplots ( figsize = ( 8 , 8 )) #######################Arrows####################### arrows = np . array ([[[ 0 , 0 , 1 , 2 ]], [[ 0 , 0 , 3 , 6 ]], [[ 0 , 0 , - 6 , - 12 ]]]) colors = [ \"r\" , \"b\" , \"g\" ] for i in range ( arrows . shape [ 0 ]): X , Y , U , V = zip ( * arrows [ i , :, :]) ax . arrow ( X [ 0 ], Y [ 0 ], U [ 0 ], V [ 0 ], color = colors [ i ], width = 0.18 , length_includes_head = True , head_width = 0.3 , # default: 3*width head_length = 0.6 , overhang = 0.4 , zorder =- i , ) ax . scatter ( 0 , 0 , ec = \"red\" , fc = \"black\" , zorder = 5 ) ax . text ( 1.5 , 2 , \"$(1, 2)$\" ) ax . text ( 3.5 , 6 , \"$(3, 6)$\" ) ax . text ( - 9 , - 12 , \"$(-9, -12)$\" ) ax . grid ( True ) ax . set_title ( \"Linear Dependence Visualization ((1,2), (3, 6), (-9, -12))\" ) # axis([xmin, xmax, ymin, ymax]) ax . axis ([ - 12 , 10 , - 20 , 10 ]) ax . set_xlabel ( \"x-axis\" , size = 18 ) ax . set_ylabel ( \"y-axis\" , size = 18 ) plt . savefig ( \"linear_dependence.svg\" , format = \"svg\" , dpi = 600 ) plt . show ()","title":"Example (Linear Dependence in \\(\\mathbb R^2\\))"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_linear_independence/#example-linear-independence-in-mathbb-r3","text":"Courtesy of MacroAnalyst's Linear Algebra with Python . Example Next, we visualize linear independence in \\(\\mathbb{R}^3\\) with vectors \\((1,-2,1)^T\\) , \\((2,1,2)^T\\) , \\((-1,2,3)^T\\) . Pan around the image (either by setting ax.view_init or using JupyterLab widget), we can see that the green vector is not in the plane spanned by red and blue vector, thus they are linearly independent. # %matplotlib notebook, use this only when you are in Jupyter Notebook, it doesn't work in Jupyterlab import matplotlib.pyplot as plt import numpy as np # %matplotlib notebook, use this only when you are in Jupyter Notebook, it doesn't work in Jupyterlab fig = plt . figure ( figsize = ( 10 , 10 )) ax = fig . add_subplot ( projection = '3d' ) s = np . linspace ( - 1 , 1 , 10 ) t = np . linspace ( - 1 , 1 , 10 ) S , T = np . meshgrid ( s , t ) X = S + 2 * T Y = - 2 * S + T Z = S + 2 * T ax . plot_wireframe ( X , Y , Z , linewidth = 1.5 , color = 'k' , alpha = .6 ) vec = np . array ([[[ 0 , 0 , 0 , 1 , - 2 , 1 ]], [[ 0 , 0 , 0 , 2 , 1 , 2 ]], [[ 0 , 0 , 0 , - 1 , 2 , 3 ]]]) colors = [ 'r' , 'b' , 'g' ] for i in range ( vec . shape [ 0 ]): X , Y , Z , U , V , W = zip ( * vec [ i ,:,:]) ax . quiver ( X , Y , Z , U , V , W , length = 1 , normalize = False , color = colors [ i ], arrow_length_ratio = .08 , pivot = 'tail' , linestyles = 'solid' , linewidths = 3 , alpha = .6 ) ax . set_title ( 'Linear Independence Visualization' ) ax . set_xlabel ( 'x-axis' , size = 18 ) ax . set_ylabel ( 'y-axis' , size = 18 ) ax . set_zlabel ( 'z-axis' , size = 18 ) ax . view_init ( elev = 60. , azim = 0 ) plt . show ()","title":"Example (Linear Independence in \\(\\mathbb R^3\\))"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_linear_independence/#algebraic-definition-linear-independence","text":"Definition Let \\(V\\) be a vector space over a field \\(\\F\\) . The set of vectors \\(\\v_1, \\v_2, ..., \\v_m \\in V\\) is linearly independent if and only if the only solution to the equation \\(a_1\\mathbf{v}_1 + a_2\\mathbf{v}_2 + \\cdots + a_m\\mathbf{v}_m = \\mathbf{0}\\) is the trivial solution, the zero vector \\(\\0\\) .","title":"Algebraic Definition (Linear Independence)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_linear_independence/#equivalent-algebraic-definition-linear-independence","text":"Equivalent Linear Independence Definition Let \\(V\\) be a vector space over a field \\(\\F\\) . Let the set of vectors \\(S = \\v_1, \\v_2, ..., \\v_m \\in V\\) ; Then \\(S\\) is linearly independent if and only if i) the only solution to the equation \\(a_1\\mathbf{v}_1 + a_2\\mathbf{v}_2 + \\cdots + a_m\\mathbf{v}_m = \\mathbf{0}\\) is the trivial solution, the zero vector \\(\\0\\) ; iif ii) \\(S\\) is called a linearly dependent set and \\(\\mathbf{\\v_1,\\v_2,...,\\v_m}\\) are said to be linearly dependent iff \\(a_1\\mathbf{v}_1 + a_2\\mathbf{v}_2 + \\cdots + a_m\\mathbf{v}_m = \\mathbf{0}\\) has non - trivial solutions. That is there exists real numbers \\(a_1,a_2,...,a_m\\) not all of them are zero, such that \\(a_1\\v_1+a_2\\v_2+...+a_m\\v_m=0\\) ; iif iii) The set \\(S\\) is a linearly independent set if every non empty finite subset of \\(S\\) is linearly independent. The set \\(S\\) is a linearly dependent set if at least one non empty finite subset of \\(S\\) is linearly dependent.","title":"Equivalent Algebraic Definition (Linear Independence)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_linear_independence/#theorem-linear-combination-implies-linear-dependence","text":"This is a very useful, and often better understood theorem! Theorem i) Let \\(S = \\{\\mathbf{v_1,v_2,...,v_m}\\}\\) be a finite subset of a vector space \\(V\\) over a field \\(\\F\\) where \\(m \\geq 2\\) , then \\(S\\) is linearly dependent if and only if at least one vector \\(\\mathbf{v_i} \\in S\\) can be written as a linear combination of other vectors in \\(S\\) . This means that there are scalars \\(a_1,a_2,...a_{i-1},a_{i+1},...,a_m \\in \\F\\) (scalars vanishing allowed) such that \\[\\mathbf{v_i} = a_1\\v_1+...+a_{i-1}\\mathbf{v_{i-1}}+a_{i+1}\\mathbf{v_{i+1}}+...+a_m\\v_m\\] ii) It follows that the contrapositive of this theorem: \\(S\\) is linearly independent if and only if no vector in \\(S\\) can be written as a linear combination of other vectors in \\(S\\) .","title":"Theorem (Linear Combination Implies Linear Dependence)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_linear_independence/#proof","text":"Proof We will just prove part i). \\(\\implies\\) \\(S = \\{\\mathbf{v_{1}, \\cdots , v_{m}}\\}\\) is finite \\(\\implies\\) \\(|S| = n < \\infty\\) , \\(S \\subseteq V\\) . If S is LD, then \\(a_{1}\\v_1 + a_{2}\\v_2 + \\cdots + a_{m}\\mathbf{v_{m}} = 0\\) has non trivial solutions. This means not all \\(a_{i} = 0\\) . Suppose \\(a_{k} \\ne 0\\) , thus one can write in this way: \\[ \\begin{eqnarray} a_{k}\\mathbf{v_m} & = & -a_{1}\\v_1 - a_{2}\\v_2 \\cdots -a_{m-1}\\mathbf{v_{m-1}} - \\cdots -a_{m}\\mathbf{v_{m}} \\nonumber \\\\ \\mathbf{v_m} & = & \\frac{-a_{1}}{a_{k}}\\v_1 - \\cdots - \\frac{a_{m-1}}{a_{k}}\\mathbf{v_{m-1}} - \\frac{a_{m}}{a_{k}}\\mathbf{v_{m}} \\nonumber \\\\ & = & b_{1}\\v_1 + \\cdots + b_{m-1}\\mathbf{v_{m-1}} + \\cdots + b_{m}\\mathbf{v_{m}} \\nonumber \\end{eqnarray} \\] where \\(b_{i} = \\frac{-a_{1}}{a_{k}}\\) . \\(\\Leftarrow\\) If \\(\\exists \\mathbf{v_m} = a_{1}\\v_1 + \\cdots + a_{m-1}\\mathbf{v_{m-1}} + \\cdots a_{m}\\mathbf{v_{m}}\\) , Then \\(a_{1}\\v_1 + \\cdots + a_{m-1}\\mathbf{v_{m-1}} + \\cdots a_{m}\\mathbf{v_{m}} = 0\\) will have non-trivial solution. Note \\((-1)\\mathbf{v_m} = a_{1}\\v_1 - \\cdots - a_{m-1}\\mathbf{v_{m-1}} - \\cdots - a_{m}\\mathbf{v_{m}}\\) . Then set \\(a_{k} = -1\\) , we have \\[a_{1}\\v_1 + \\cdots + (-1)\\mathbf{v_{k}} + \\cdots a_{m}\\mathbf{v_{m}} = a_1\\v_1+ \\cdots -a_{1}\\v_1 - a_2\\v_2 \\cdots - a_{m}\\mathbf{v_{m}} + \\cdots a_{m}\\mathbf{v_{m}} = 0\\] Hence, \\((a_{1}, \\cdots a_{m-1},-1, a_{k+1}, \\cdots a_{m})\\) is non trivial solution. Thus \\(\\v_1, \\cdots \\mathbf{v_{m}}\\) is LD.","title":"Proof"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_linear_independence/#theorem-the-zero-vector-and-linear-independence","text":"Theorem (The zero vector and linear independence) Note that \\(\\{\\mathbf{0}\\}\\) is a linearly dependent set, which means that if \\(\\mathbf{0} \\in S\\) , then \\(S\\) is a linearly dependent set.","title":"Theorem (The zero vector and linear independence)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_linear_independence/#proof_1","text":"Proof Write \\(c \\cdot \\mathbf{0} = 0\\) . Then one can easily see this equation has solutions other than \\(c = 0\\) . Since by killing power of zero, we can have the variable \\(c\\) to be any number. Hence by definition, \\(\\{\\mathbf{0}\\}\\) is a linearly dependent set. If \\(\\mathbf{0} \\in S\\) , then \\(\\mathbf{0}\\) can be expressed as a linear combination of any other vectors in the set by setting coefficients to be \\(0\\) . Hence it is considered a redundant vector and therefore causing \\(S\\) to be a linearly dependent set.","title":"Proof"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_linear_independence/#theorem-union-of-linearly-independent-vectors","text":"Theorem (Union of linearly independent vectors) Let \\(\\mathbf{v_1, v_2,...,v_k}\\) be LI vectors in \\(\\mathbb{R}^n\\) . If \\(\\mathbf{v_{k+1}}\\) is a vector in \\(\\mathbb{R}^n\\) and it is not a linearly combination of \\(\\mathbf{v_1,v_2,...,v_k}\\) , then \\(\\mathbf{v_1,...,v_k,v_{k+1}}\\) are linearly independent.","title":"Theorem (Union of linearly independent vectors)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_linear_independence/#intuition-linear-independence","text":"Consider a xy cartesian plane, we have the x axis, where we can visualize as walking left or right (east or west), and the y axis, walking forward and backwards (north or south). We claim that the set of vectors \\[ S = \\left\\{\\begin{bmatrix} \\color{red}1 \\\\ \\color{red}0 \\end{bmatrix}, \\begin{bmatrix} \\color{red}0 \\\\ \\color{red}{1} \\end{bmatrix} \\right\\} \\] is linearly independent, as neither vector can be represented by the other in any way. But if we add one more vector \\[ S_1 = \\left\\{\\begin{bmatrix} \\color{red}1 \\\\ \\color{red}0 \\end{bmatrix}, \\begin{bmatrix} \\color{red}0 \\\\ \\color{red}{1} \\end{bmatrix}, \\begin{bmatrix} \\color{red}1 \\\\ \\color{red}{1} \\end{bmatrix} \\right\\}\\] then we say this set is linearly dependent because the vector \\([1, 1]\\) can be linearly combined by the other two, and is thus redundant. But why the weird definition? Why does having a non-trivial solution to the equation means linear dependence ? A good intuition from the post in mathstackexchange 2 : Intuition Your basic intuition is quite right --- a set of vectors are linearly independent if they don't affect each other (which, as it stands, is a somewhat ambiguous statement, hence the formal definition). Perhaps it would be helpful to start with just two vectors, say the standard vectors $$ \\begin{pmatrix} 1\\ 0 \\end{pmatrix} \\hspace{20pt}\\text{and}\\hspace{20pt} \\begin{pmatrix} 0\\ 1 \\end{pmatrix} $$ which are linearly independent in \\(\\mathbb{R}^{2}\\) (two-dimensional coordinate space). Natural intuition would be to think that these two are linearly independent because there is no scalar that you can multiply one by to get the other. That is, for \\(\\alpha \\in \\mathbb{R}\\) , the equation $$ \\alpha\\begin{pmatrix} 1\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0\\ 1 \\end{pmatrix} $$ has no solutions. This is a good rule for any set of two vectors, and is in fact equivalent to the formal definition for if there were scalars \\(\\alpha_{1}, \\alpha_{2} \\in \\mathbb{R}\\) such that $$ \\alpha_{1}\\begin{pmatrix} 1\\ 0 \\end{pmatrix} + \\alpha_{2}\\begin{pmatrix} 0\\ 1 \\end{pmatrix} = 0 $$ (which is just the formal definition), then we could rearrange it to get $$ -\\frac{\\alpha_{1}}{\\alpha_{2}}\\begin{pmatrix} 1\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0\\ 1 \\end{pmatrix}. $$ This is clearly false! The formal definition for an arbitrary number of vectors is an extension of this idea. We pick \\(0\\) to be the right hand side for convenience, since every vector space must have the zero vector (by definition), so that if some vectors affect each other in some way, they must be able to be combined some way to give you the zero vector. In fact, if a set of vectors are linearly independent, then the only vector that they agree on is the zero vector, and it can only be obtained by multiplying each vector by zero (and summing them up). Here I chime in a bit: consider \\((0, 1), (1, 0), (1, 1)\\) in \\(\\R^2\\) , then we know \\((0, 1) + (1, 0) = (1, 1)\\) where \\((1, 1)\\) is a linear combination of the former two vectors; and since \\((0, 1) + (1, 0) = (1, 1)\\) , then \\(((0, 1) + (1, 0)) - (1, 1) = (0, 0)\\) , the zero vector, this must be true for every vector that can be expressed as a linear combination of the rest. If they can be combined in some way to give you the zero vector, then you can just rearrange them (like above) and find one of the vectors as a linear combination of the others, which would clearly imply that it is not independent of the others. To make this more clear, let \\(\\{v_{1}, v_{2}, \\ldots, v_{n}\\}\\) be a set of vectors, and suppose there exists non-zero scalars \\(\\alpha_{i} \\in \\mathbb{R}\\) such that $$ \\alpha_{1}v_{1} + \\alpha_{2}v_{2} + \\ldots + \\alpha_{n}v_{n} = 0. $$ Then you can just rearrange this to get, for example, the equation $$ \\frac{-\\alpha_{1}}{\\alpha_{n}}v_{1} + \\frac{-\\alpha_{2}}{\\alpha_{n}}v_{2} + \\ldots + \\frac{-\\alpha_{n-1}}{\\alpha_{n}}v_{n-1} = v_{n}. $$ Then \\(v_{n}\\) depends on the other vectors, so the set is not linearly independent ! Thus, the only way that you can get a set of linearly independent vectors to all give you the zero vector, is that all must be multiplied by zero, else you can rearrange the equation to show that one of the vectors depends on the others, which would be a contradiction. This is why the definition is the way it is. As a note for your third point, the definition wants \\(\\alpha_{i} = 0\\) for all \\(i = 1, \\ldots, n\\) . That is, every coefficient has to be zero, not just one of them. Machine Learning Though a bit too early, but we often represent the variable/feature space of the dataset using the Design Matrix and each column (or row) represents the encoded information of the variables. If there are many redundant vectors inside the Design Matrix, this means that we are having additional information that we do not need (which may hurt performance). PCA can also help to reduce the dimensionality too.","title":"Intuition (Linear (In)Dependence)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_linear_independence/#theorem-uniqueness-of-representation","text":"Theorem (Uniqueness of Representation) From the above, we can deduce something less apparent from linear independence, that is, for the same \\(\\v \\in V\\) , and the same set of vectors \\(\\v_1, \\v_2, ..., \\v_m \\in V\\) , there is only one unique representation of \\(\\v \\in V\\) using the set of vectors. For example, if we consider a simple example where \\(\\v = a_1 \\v_1 + a_2 \\v_2\\) and \\(\\v = b_1 \\v_1 + b_2 \\v_2\\) , where \\(a_1, a_2, b_1, b_2\\) are assumed to be distinct scalars in \\(\\F\\) , then subtracting both results in \\[\\0 = (a_1-b_1)\\v_1 + (a_2-b_2)\\v_2 \\implies a_i-b_i = 0\\] because we note that the equation can only have the trivial solution \\(\\0\\) . This results in a contradiction as we stated that \\(a_i, b_i\\) are distinct. So conclusion, if a set of vectors are linearly independent in \\(V\\) , then any vectors in \\(\\v \\in V\\) can only have a unique representation in \\(V\\) . Using the example in Intuition (Linear (In)Dependence) , we know that \\(S\\) is a linearly independent set, and imagine we take a vector \\((1, 1)\\) , can we actually find another way to represent \\((1, 1)\\) using this exact set of vectors? In other words, using \\((1, 0), (0, 1)\\) as the \"base vectors\", we must move 1 unit to the right and 1 unit up to reach \\((1, 1)\\) , can we move in any other way to get to \\((1, 1)\\) ? The answer is no.","title":"Theorem (Uniqueness of Representation)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_linear_independence/#theorem-the-span-of-linearly-dependent-vectors-is-identical-v2","text":"Branching from the Theorem (Linear Dependence Span equals each other) in the Chapter Span and taken from Sheldon Axler: Linear Algebra Done Right, pp. 35 : Theorem (The Span of Linearly Dependent Vectors is Identical V2) Let \\(\\v_1,...,\\v_m\\) be linearly dependent vectors in a vector space \\(V\\) . Then there exists \\(j \\in \\{1,2,\\cdots,m\\}\\) such that the following holds: \\(\\v_j \\in \\textbf{span}(\\v_1,\\cdots,\\v_{j-1})\\) ; if the \\(j\\) -th term is removed from the linearly dependent set, then $\\textbf{span}(\\v_1, \\cdots, \\v_{j-1}, \\v_{j+1}, \\cdots \\v_m) = \\textbf{span}(\\v_1, \\cdots, \\v_{j-1},\\v_j, \\v_{j+1}, \\cdots \\v_m) $.","title":"Theorem (The Span of Linearly Dependent Vectors is Identical V2)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_linear_independence/#theorem-inheritance-of-linear-independence","text":"Theorem (Inheritance of Linear Independence) Let \\(S_1 \\subseteq S_2\\) , if the smaller set \\(S_1\\) is linearly dependent then so is the larger set \\(S_2\\) . \\(S_2\\) is linearly dependent follows from part iii) of Equivalent Definition of Linear independence . Now the contrapositive of this is if the larger set \\(S_2\\) is linearly independent then so is \\(S_1\\) being linearly independent.","title":"Theorem (Inheritance of Linear Independence)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_linear_independence/#theorem-length-of-linear-independent-set-leq-length-of-spanning-set","text":"Theorem Taken from Sheldon Axler: Linear Algebra Done Right, pp. 35: In a finite-dimensional vector space, the length of every linearly independent set of vectors is less than or equal to the length of every spanning set of vectors.","title":"Theorem (Length of Linear Independent Set \\(\\leq\\) Length of Spanning Set)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_linear_independence/#example-usage-of-theorem","text":"Example 1: Usage of Theorem to show a set of vectors cannot be linear independent Show that the set of vectors \\(\\{(1,2,3), (4,5,8), (9,6,7), (-3,2,8)\\}\\) is not linearly independent in \\(\\R^3\\) . Solution: We know that the set of vectors (basis) \\(\\{(1,0,0), (0,1,0), (0,0,1)\\}\\) spans \\(\\R^3\\) . And hence, if the set of vectors in question is linearly indepedent, then its cardinality cannot exceed 3. Example 2: Usage of Theorem to show a set of vectors cannot span the vector space Show that the set of vectors \\(\\{(1,2,3,5), (4, 5, 8, 3), (9, 6, 7, 1)\\}\\) does not span \\(\\R^4\\) . Solution: We know that the set of vectors (basis) \\(\\{(1,0,0,0), (0,1,0,0), (0,0,1,0), (0,0,0,1)\\}\\) is linearly independent in \\(\\R^4\\) . From theorem, we know the length (cardinality) of the set of linearly independent vectors is 4, and in the question, the set of vectors has only cardinality 3, and hence, it cannot span \\(\\R^4\\) .","title":"Example (Usage of Theorem)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_linear_independence/#corollary-length-of-linear-indepedent-set-and-spanning-set","text":"Corollary As a corollary of the Theorem (Inheritance of Linear Independence) above: Let \\(S = \\{\\mathbf{u_1,u_2, \\cdots, u_k}\\} \\subseteq \\mathbb{R}^{n}\\) . If \\(k >n\\) , then \\(S\\) is linearly dependent.","title":"Corollary (Length of Linear Indepedent Set and Spanning Set)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_linear_independence/#proof_2","text":"Proof Let \\(\\mathbf{u_i} = (a_{{i1}},a_{{i2}},...,a_{{in}})\\) for \\(i=1,2,...,k\\) . Then we write \\[c_1\\u_1+c_2\\u_2+...+c_k\\u_k=0\\] and check if it has the trivial solution only or has non trivial solutions. \\(\\begin{cases} a_{11}c_1+a_{21}c_2+...+a_{k1}c_k = 0\\\\ a_{12}c_1+a_{22}c_2+...+a_{k2}c_k = 0\\\\ ~~~~~~~~~~~\\vdots\\\\ a_{1n}c_1+a_{2n}c_2+...+a_{kn}c_k = 0 \\end{cases}\\) This system has \\(k\\) unknowns and \\(n\\) equations. By our previous results, since \\(k > n\\) , the system must have non trivial solution. Hence \\(S\\) is linearly dependent.","title":"Proof"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_linear_independence/#example","text":"Example In \\(\\mathbb{R}^{2}\\) , a set of three or more vectors must be linearly dependent; In \\(\\mathbb{R}^{3}\\) , a set of four or more vectors must be linearly dependent. Fig; Note that the first figure shows a set of dependent vectors; the second figure shows a set of independent vectors; and the third figure shows a set of dependent vectors; By Hongnan G. The above image illustrates that in \\(\\R^2\\) space, if a set of vectors has cardinality more than the dimension of its ambient space (read: 2), then it must be linearly dependent . Intuition In the example above, our ambient space is \\(\\R^2\\) , the author Mike provided a good intuition here on why a set of vectors whose cardinality is more than its ambient space cannot be linearly independent. Assume a contradiction, that they are linearly independent, then these 3 vectors span a 3d-subspace, which cannot happen if all 3 vectors are in 2d-subspace. To visualize it, imagine drawing 3 vectors on a 2d-plane (a piece of paper), then can you ever construct a cube (3d-space) from these 3 vectors? No.","title":"Example"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_linear_independence/#geometric-definition-linear-independence","text":"We go through the Geometric Definition after the previous theorem. From Mike X Cohen: Linear Algebra: Theory, Intuition, Code, 2021. (pp. 90) , a set of vectors independent if the subspace dimensions spanned by the set of vectors is equal to the number of vectors in the set. Recall that up till now, the subspace dimension simply means the \"number of axes\" in the subspace. This can be understood with the following example: For example, a set with one vector spans a line (assuming it is not the zeros vector) and is always an independent set (1 vector, 1 dimension); a linearly independent set of two vectors spans a plane (2 vectors, 2 dimensions); an independent set with three vectors spans a 3D space (3 vectors, 3 dimensions). For more info, read Mike X Cohen: Linear Algebra: Theory, Intuition, Code, 2021. (pp. 90-92) .","title":"Geometric Definition (Linear Independence)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_linear_independence/#how-to-determine-if-a-set-is-linearly-independent-non-matrix-algorithmic-way","text":"pp. 95-96 Mike's book.","title":"How to determine if a set is Linearly Independent (Non-Matrix Algorithmic Way)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_linear_independence/#linkedin-summary","text":"Today marks day 9 of round 1, continuing my journey in #linearalgebra, following the book \"Linear Algebra, Theory, Intuition and Code\", I went through the definition and some examples of what constitutes a linear in(dependent) set . In short, for a vector space V over a field F^n, a subset S = {v_1, v_2, ..., v_m} of V is a linearly independent set if a_1v_1 + a_2v_2 + ... + a_m_vm = 0 has only the trivial solution. This definition might seem a bit weird at first sight, we can break down and see what it really means. We can understand this definition better from its contraposition: if subset S of V is linearly dependent, then it must have at least one non-trivial solution; consider this solution to be (a_1, a_2, ..., a_m) = (1, 2, 0, ..., 0), this implies that v_1 = -2v_2. Though not a proof, we now know that if at least one vector can be represented by a linear combination of the other vectors in the set, then this set is linearly dependent . So another way to word the linear independence definition is that no vector in S can be represented as a linear combination of the other vectors in S. An immediate consequence of this definition is that an linearly independent set must have an unique representation in the vector space spanned by this set. Suppose not (a contradiction), then there exists a_i, b_i's such that a_1v_1 + ... a_mv_m = 0 and b_1v_1 + ... + b_mv_m = 0, where a_i and b_i's are not all the same. This implies (a_1-b_1)v_1 + ... + (a_m-b_m)v_m = 0 has a non-trivial solution since there exists at least one pair of a_i != b_i. Geometrically, one can see the below image, 3 vectors in a 2d-plane is necessarily linearly dependent, you can also verify that to get (10, 10) can be made up by (0, 10) + (10, 0), or 1*(10, 10), which has two representations.","title":"Linkedin Summary"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_linear_independence/#linear-independence-references","text":"https://www.machinelearningmindset.com/linear-independence-of-vectors/ https://math.stackexchange.com/questions/2779918/intuition-for-formal-definition-of-linear-independence https://medium.com/swlh/how-to-understand-linear-independence-linear-algebra-8bab1d918509 Linear Independence Wikipedia \u21a9 https://math.stackexchange.com/questions/2779918/intuition-for-formal-definition-of-linear-independence \u21a9","title":"Linear Independence References"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_span/","text":"\\[\\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\z}{\\mathbf{z}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\C}{\\mathbf{C}}\\] Span Algebraic Definition (Span) Geometrical Definition (Span) Spanned By Span Membership (Examples) Theorem (Span is a subspace) Theorem (Span is a subspace and is the smallest subspace of V) Example (Different Sets can span the same Vector Space) Proof Theorem (Linear Dependence Span equals each other) Examples (Geometrical Interpretation of Span) Python (Two Linearly Independent Vectors in \\(\\mathbb R^3\\) Spans a Plane) Python (Two Linearly Dependent Vectors in \\(\\mathbb R^3\\) Spans a Line) Python (Three Linearly Inependent Vectors in \\(\\mathbb R^3\\) Spans the whole Ambient Space) Span Algebraic Definition (Span) Algebraic Definition (Span) Let \\(S \\subseteq V\\) be a non-empty subset of the vector space \\(V\\) . To be more explicit, define \\(S = \\{\\v_1, \\v_2, ..., \\v_m\\}\\) to be a subset in \\(V\\) , then the linear combination of all the vectors \\(\\v_1, \\v_2, ..., \\v_m\\) is called the span of \\(\\v_1, \\v_2, ..., \\v_m\\) , which we denote as follows: \\[\\text{span}(S) = \\text{span}(\\v_1, \\v_2,...,\\v_m) := \\{\\lambda_1\\v_1 + \\lambda_2\\v_2 + ... + \\lambda_m\\v_m : \\lambda_i \\in \\F, \\v_i \\in S\\}\\] Geometrical Definition (Span) Span is a really similar concept as subspace, and they are easy to confuse. A subspace is the region of ambient space that can be reached by any linear combination of a set of vectors. And then those vectors span that subspace. You can think about the difference using grammar: a subspace is a noun and span is a verb. A set of vectors spans, and the result of their spanning is a subspace. For example, the subspace defined as all of \\(\\R^2\\) can be created by the span of the vectors \\([0, 1]\\) and \\([1, 0]\\) . Another good example is the vector \\([0,1]\\) spans a 1D subspace that is embedded inside \\(\\R^2\\) (not \\(\\R^1\\) since the vector has 2 elements). Then the vector \\([1, 2]\\) also spans 1D subspace but it is a difference 1D subspace from that spanned by \\([0,1]\\) . - Mike X Cohen: Linear Algebra: Theory, Intuition, Code, 2021. (pp. 86) Spanned By If \\(W\\) is a vector subspace of \\(V\\) and \\(W = \\text{Span}(S)\\) , we say that \\(S\\) is a spanning set (or generating set) of \\(W\\) and \\(W\\) is spanned or generated by \\(S\\) . Important The intuition is that if a vector space \\(W\\) is spanned by a set of vectors \\(S\\) , then this means for every vector \\(\\w_i \\in W\\) , there exists scalars \\(\\lambda_i \\in \\F\\) , such that \\(\\w_i = \\lambda_1 \\w_1 + \\lambda_2 \\w_2 + \\cdots + \\lambda_m \\w_m\\) . In laymen terms, this set of vectors \\(S\\) can build up every single element in \\(W\\) . Span Membership (Examples) Example The author mentioned a commonly used terms in linear algebra, which is to check if one vector \\(\\v\\) is \"in the span\" of a set of vectors. This is illustrated clearly with the example provided below: \\[ \\v= \\begin{bmatrix} \\color{red}6 \\\\ \\color{red}3 \\\\ \\color{red}{0} \\end{bmatrix} ,\\quad \\w= \\begin{bmatrix} \\color{red}2 \\\\ \\color{red}3 \\\\ \\color{red}{-5} \\end{bmatrix} \\] and the set \\[ S = \\left\\{\\begin{bmatrix} \\color{red}2 \\\\ \\color{red}4 \\\\ \\color{red}{0} \\end{bmatrix}, \\begin{bmatrix} \\color{red}4 \\\\ \\color{red}{-1}\\\\ \\color{red}{0} \\end{bmatrix} \\right\\}\\] Then the question is whether \\(\\v\\) and \\(\\w\\) are in the span of \\(S\\) . In simple words, can the linear combination of the vectors \\(\\begin{bmatrix} \\color{red}2 \\\\ \\color{red}4 \\\\ \\color{red}{0} \\end{bmatrix}\\) and \\(\\begin{bmatrix} \\color{red}4 \\\\ \\color{red}{-1}\\\\ \\color{red}{0} \\end{bmatrix}\\) form \\(\\v\\) or \\(\\w\\) ? Well the answer for \\(\\v\\) is easy (purposely made easy), we just add them up and get \\(\\v\\) , and in this case, we say \\(\\v\\) is in the \\(\\text{span}(S)\\) . But for \\(\\w\\) , we can quickly conclude that \\(\\w \\not \\in \\text{span}(S)\\) because the third component of the span of \\(S\\) is still \\(0\\) , and thus no linear combination can ever form to \\(\\w\\) 's third component, \\(-5\\) . Theorem (Span is a subspace) Theorem (Span is a subspace) Let \\(V\\) be a vector space over a field \\(\\F\\) . Let \\(S \\subseteq V\\) be a subset of \\(V\\) . Then Span \\((S)\\) of \\(V\\) is a vector subspace of \\(V\\) . An immediate consequence of this definition is that span(S) must be a subspace of V. Why? Recall that to be a subspace, closure under vector addition and vector-scalar multiplication must be satisfied; this follows because the definition of span(S) is the linear combination of all vectors in S, and hence any vector addition or vector-scalar multiplication is a member of the linear combination. Note We will prove this together in the next theorem. Theorem (Span is a subspace and is the smallest subspace of V) Theorem (Span is a subspace and is the smallest subspace of V) Let \\(V\\) be a vector space over a field \\(\\F\\) . Let \\(S \\subseteq V\\) be a subset of \\(V\\) . Then \\(\\text{span}(S)\\) of \\(V\\) is a vector subspace of \\(V\\) . In particular, \\(\\text{span}(S)\\) is the smallest vector subspace of \\(V\\) containing the set \\(S\\) (or containing all the vectors in \\(S\\) ). Example (Different Sets can span the same Vector Space) Different Sets can span the same Vector Space Before we go through the proof, it is important to understand some possible confusions. One might be confused over the notion of smallest . We can motivate it with an example: \\[ S = \\left\\{\\begin{bmatrix} \\color{red}1 \\\\ \\color{red}0 \\end{bmatrix}, \\begin{bmatrix} \\color{red}0 \\\\ \\color{red}{1} \\end{bmatrix} \\right\\}\\] spans the whole \\(\\R^2\\) space, although we have yet to learn any algorithmic way of checking whether a set spans a particular vector space, this is an obvious enough example (hopefully one can see that the linear combination of these two vectors can reach the whole ambient space \\(\\R^2\\) ). This is because the linear combination of vectors in \\(S\\) can form any vectors in the 2 dimensional space . Then consider \\[ S_1 = \\left\\{\\begin{bmatrix} \\color{red}1 \\\\ \\color{red}0 \\end{bmatrix}, \\begin{bmatrix} \\color{red}0 \\\\ \\color{red}{1} \\end{bmatrix}, \\begin{bmatrix} \\color{red}1 \\\\ \\color{red}{1} \\end{bmatrix} \\right\\}\\] which also spans the whole \\(\\R^2\\) space as well. Both \\(S\\) and \\(S_1\\) are subsets of \\(\\R^2\\) and are subspaces themselves (by inspection). However, the cardinality or the len of \\(S\\) is 2 and \\(S_1\\) is 3. Since both sets span the \\(\\R^2\\) space, why do we say that the span is the smallest subspace of \\(V\\) when \\(S_1\\) has more \"elements\"? The confusion is that the theorem states that the span of a set of vectors in \\(V\\) is the smallest subspace of \\(V\\) which contains this set of vectors , not span of ANY set of vectors in \\(V\\) is the smallest subspace of \\(V\\) . Therefore, \\(S\\) is the smallest subspace of \\(V\\) which contains the two vectors \\(\\left\\{\\begin{bmatrix} \\color{red}1 \\\\ \\color{red}0 \\end{bmatrix}, \\begin{bmatrix} \\color{red}0 \\\\ \\color{red}{1} \\end{bmatrix} \\right\\}\\) and \\(S_1\\) is the smallest subspace of \\(V\\) which contains the three vectors \\(\\left\\{\\begin{bmatrix} \\color{red}1 \\\\ \\color{red}0 \\end{bmatrix}, \\begin{bmatrix} \\color{red}0 \\\\ \\color{red}{1} \\end{bmatrix}, \\begin{bmatrix} \\color{red}1 \\\\ \\color{red}{1} \\end{bmatrix} \\right\\}\\) . An extract from StackExchange : Written in words that theorem states, that any subspace \\(W\\) , that exists and contains \\(v_1,\u2026,v_n\\) , also contains \\(\\text{span}(v_1,\u2026,v_k)\\) . Hence \\(W\\) is larger (or equal) to \\(\\text{span}(v_1,\u2026,v_k)\\) , since it contains \\(\\text{span}(v_1,\u2026,v_k)\\) and could contain some more elements. And since any other subspace \\(W\\) is larger (or equal) it follows that \\(\\text{span}(v_1,\u2026,v_k)\\) the smallest subspace. That is like saying: Any number in \\(\u2115\u222a\\{0\\}\\) is larger (or equal) to \\(0\\) , hence \\(0\\) is the smallest number in \\(\u2115\u222a\\{0\\}\\) . Or even closer to the original problem: Any subset \\(M\u2282(\u2115\u222a\\{0\\})\\) with \\(0\u2208M\\) is larger or equal to \\(\\{0\\}\\) , hence \\(\\{0\\}\\) has to be the smallest subset of \\(\u2115\u222a\\{0\\}\\) that contains \\(0\\) . Proof Proof We first show that \\(\\text{span}(S)\\) is a vector subspace of \\(V\\) . And then we show that if \\(\\text{span}(S)\\) is a vector subspace of \\(V\\) containing \\(S\\) , then if \\(T\\) is another vector subspace of \\(V\\) containing \\(S\\) , we must have \\(\\text{span}(S) \\subseteq T\\) . \\(\\text{span}(S)\\) is a subspace : It suffices to check for the conditions S2 and S3 of subspaces . To show S2, we need to show that for any \\(\\v, \\w \\in \\text{span}(S)\\) , \\(\\v + \\w \\in \\text{span}(S)\\) . We also note that for both \\(\\v\\) and \\(\\w\\) , they can be expressed as linear combination of the set of vectors in \\(S\\) , (i.e. \\(\\v = a_1 \\v_1 + a_2 \\v2 + ... + a_m \\v_m\\) , \\(\\w = \\b_1 \\v_1 + \\b_2 \\v_2 + ... + \\b_m \\v_m\\) for some \\(a_i, b_i \\in \\F\\) ) Then adding them up we yield: \\[\\begin{align} \\v + \\w &= (a_1 \\v_1 + a_2 \\v2 + ... + a_m \\v_m) + (\\b_1 \\v_1 + \\b_2 \\v_2 + ... + \\b_m \\v_m) \\label{eq1}\\tag{1} \\\\ &= (a_1 + b_1)\\v_1 + (a_2 + b_2)\\v_2 + ... + (a_m + b_m)\\v_m \\label{eq2}\\tag{2} \\end{align}\\] Equation (\\ref{eq2}) shows us that it is still a linear combination of the set of vectors in \\(S\\) , and hence also in \\(\\text{span}(S)\\) , which is closed under addition. For S3, the same logic applies (can you show it?). \\(\\text{span}(S)\\) is the smallest subspace that contains \\(S\\) : Let \\(T\\) be any subspace of \\(V\\) that contains the set \\(S\\) . In order to show \\(\\text{span}(S) \\subseteq T\\) , we will use a proving method by showing that for all elements \\(\\v \\in \\text{span}(S)\\) , \\(\\v\\) is in \\(T\\) as well. We start off by picking any element \\(\\v \\in \\text{span}(S)\\) , (note that \\(\\v\\) may not be in \\(S\\) , but it definitely must be in \\(\\text{span}(S)\\) ! Do not get confused here!), for this \\(\\v\\) , it can be expressed as the linear combination of the vectors of \\(S\\) , \\(\\v =\\lambda_1\\v_1 + \\lambda_2\\v_2 + ... + \\lambda_m\\v_m\\) , then \\(\\v \\in T\\) as well since \\(T\\) contains the set \\(S\\) , and also is a subspace , thus closed under scalar multiplication and vector addition. Theorem (Linear Dependence Span equals each other) Theorem (Linear Dependence Span equals each other) Let \\(\\u_1,...,\\u_k\\) be vectors in \\(\\mathbb{R}^n\\) . If \\(\\u_k\\) is a linear combination of \\(\\mathbf{u_1,...,u_{k-1}}\\) , then \\[\\text{Span}\\{\\mathbf{u_1,...,u_{k-1}}\\} = \\text{Span}\\{\\mathbf{u_1,...,u_{k-1},u_k}\\}\\] This should be an obvious fact by using the same example in the Theorem (Span is a subspace and is the smallest subspace of \\(V\\) ) . Examples (Geometrical Interpretation of Span) Geometrical Interpretation of Span The \\(xz\\) -plane in \\(\\R^3\\) can be parameterized by the equations \\[x = t_1, \\;\\;\\; y = 0, \\;\\;\\; z = t_2.\\] As a subspace, the \\(xz\\) -plane is spanned by the vectors (1, 0, 0) and (0, 0, 1). Every vector in the \\(xz\\) -plane can be written as a linear combination of these two: \\[(t_1, 0, t_2) = t_1(1,0,0) + t_2(0,0,1)\\] Geometrically, this corresponds to the fact that every point on the \\(xz\\) -plane can be reached from the origin by first moving some distance in the direction of (1, 0, 0) and then moving some distance in the direction of (0, 0, 1). Python (Two Linearly Independent Vectors in \\(\\mathbb R^3\\) Spans a Plane) The below three examples are referenced from Macro Analyst's notes here 1 . The example below assumes the ambient space \\(\\R^3\\) and assumes that the term 2 linearly independent vectors just simply mean that these two vectors are not multiple of each other. We know that in earlier chapter that two linearly indepedent vectors in \\(\\R^3\\) space forms a subspace that is a plane. In fact, we can also say that two linearly independent vectors span a plane in \\(\\R^3\\) . We can verify below visually using python, with courtesy of MacroAnalyst's Linear Algebra with Python . Firstly, let's say we have two vectors: \\((3, 9, 2)\\) , \\((1,7,5)\\) which are linearly independent . The author made use of matrix multiplication in his code to demonstrate span. For more general span, a basic fact of matrix multiplication can assist us in demonstrating: \\[ AB = A[b_1\\ b_2\\ b_3,...,b_p]=[Ab_1\\ Ab_2\\ Ab_3,...,Ab_p] \\] where \\(A\\) is the spanning set of vectors, \\(b_k\\) is vector of weights for linear combination. We can generate a random matrix \\(B\\) to form various linear combinations to visually verify if they are all contained in the spanned plane. We define $$ A=\\left[\\begin{array}{rr} 3 & 1 \\ 9 & 7 \\ 2 & 5 \\end{array}\\right]\\qquad b_i\\sim N(\\mathbb{0}, 1) $$ In other words, the columns of \\(\\A\\) are the set of vectors \\[ S = \\left\\{\\begin{bmatrix} \\color{red}3 \\\\ \\color{red}9 \\\\ \\color{red}2 \\end{bmatrix}, \\begin{bmatrix} \\color{red}1 \\\\ \\color{red}7 \\\\ \\color{red}5 \\end{bmatrix} \\right\\}\\] and we want to generate say, 300 random linear combinations of this set of vectors and show visually that all these linear combinations are IN THE PLANE SPANNED BY THE SET OF VECTORS . Technically, there are infinite number of linear combinations, but we just show 300 for good illustration. Note that these 300 random linear combination are stored in the matrix \\(\\B\\) . import numpy as np import matplotlib.pyplot as plt from typing import List , Union , Tuple # The plane spanned by {[1,0,1], [0,1,1]} A = np . array ([[ 1 , 0 , 1 ], [ 0 , 1 , 1 ]]) . T B = 10 * np . random . randn ( 2 , 3000 ) # i = 300, i.e. 300 random weight vectors vecs = A @ B s = np . linspace ( - 10 , 10 , 10 ) t = np . linspace ( - 10 , 10 , 10 ) S , T = np . meshgrid ( s , t ) X = S Y = T Z = S + T fig , ax = plt . subplots ( figsize = ( 10 , 10 )) ax = fig . add_subplot ( projection = \"3d\" ) ax . plot_wireframe ( X , Y , Z , linewidth = 1.5 , color = \"k\" , alpha = 0.6 ) ax . scatter ( 0 , 0 , 0 , s = 200 , ec = \"red\" , fc = \"black\" ) colors = np . random . rand ( vecs . shape [ 1 ], 3 ) for i in range ( vecs . shape [ 1 ]): vec = np . array ([[ 0 , 0 , 0 , vecs [ 0 , i ], vecs [ 1 , i ], vecs [ 2 , i ]]]) X , Y , Z , U , V , W = zip ( * vec ) ax . quiver ( X , Y , Z , U , V , W , length = 1 , color = colors [ i ], normalize = False , arrow_length_ratio = 0.07 , pivot = \"tail\" , linestyles = \"solid\" , linewidths = 2 , alpha = 0.9 , ) ax . view_init ( elev =- 156 , azim =- 56 ) fig . savefig ( \"span_plane.svg\" , format = \"svg\" , dpi = 600 ) plt . show () import numpy as np import matplotlib.pyplot as plt from typing import List , Union , Tuple # The plane spanned by {[3,9,2], [1,7,5]} A = np . array ([[ 3 , 9 , 2 ], [ 1 , 7 , 5 ]]) . T B = 10 * np . random . randn ( 2 , 3000 ) # i = 300, i.e. 300 random weight vectors vecs = A @ B s = np . linspace ( - 10 , 10 , 10 ) t = np . linspace ( - 10 , 10 , 10 ) S , T = np . meshgrid ( s , t ) X = 3 * S + T Y = 9 * S + 7 * T Z = 2 * S + 5 * T fig , ax = plt . subplots ( figsize = ( 10 , 10 )) ax = fig . add_subplot ( projection = \"3d\" ) ax . plot_wireframe ( X , Y , Z , linewidth = 1.5 , color = \"k\" , alpha = 0.6 ) ax . scatter ( 0 , 0 , 0 , s = 200 , ec = \"red\" , fc = \"black\" ) colors = np . random . rand ( vecs . shape [ 1 ], 3 ) for i in range ( vecs . shape [ 1 ]): vec = np . array ([[ 0 , 0 , 0 , vecs [ 0 , i ], vecs [ 1 , i ], vecs [ 2 , i ]]]) X , Y , Z , U , V , W = zip ( * vec ) ax . quiver ( X , Y , Z , U , V , W , length = 1 , color = colors [ i ], normalize = False , arrow_length_ratio = 0.07 , pivot = \"tail\" , linestyles = \"solid\" , linewidths = 2 , alpha = 0.9 , ) ax . view_init ( elev =- 156 , azim =- 56 ) plt . show () Python (Two Linearly Dependent Vectors in \\(\\mathbb R^3\\) Spans a Line) What if the two vectors are not linear independent? Then the following example will convince you that two linearly dependent vectors span a line in \\(\\R^3\\) space. Note that this is equivalent to \"one vector\" in \\(\\R^3\\) spans a line. # Although spanned by 2 vectors, but actually reduces to 1 vector. # So it is a line. A = np . array ([[ 3 , 9 , 2 ], [ 6 , 18 , 4 ]]) . T B = 10 * np . random . randn ( 2 , 3000 ) # i = 300, i.e. 300 random weight vectors vecs = A @ B s = np . linspace ( - 10 , 10 , 10 ) t = np . linspace ( - 10 , 10 , 10 ) S , T = np . meshgrid ( s , t ) X = 3 * S + 6 * T Y = 9 * S + 18 * T Z = 2 * S + 4 * T fig , ax = plt . subplots ( figsize = ( 10 , 10 )) ax = fig . add_subplot ( projection = \"3d\" ) ax . plot_wireframe ( X , Y , Z , linewidth = 1.5 , color = \"k\" , alpha = 0.6 ) ax . scatter ( 0 , 0 , 0 , s = 200 , ec = \"red\" , fc = \"black\" ) colors = np . random . rand ( vecs . shape [ 1 ], 3 ) for i in range ( vecs . shape [ 1 ]): vec = np . array ([[ 0 , 0 , 0 , vecs [ 0 , i ], vecs [ 1 , i ], vecs [ 2 , i ]]]) X , Y , Z , U , V , W = zip ( * vec ) ax . quiver ( X , Y , Z , U , V , W , length = 1 , color = colors [ i ], normalize = False , arrow_length_ratio = 0.07 , pivot = \"tail\" , linestyles = \"solid\" , linewidths = 2 , alpha = 0.9 , ) ax . view_init ( elev =- 156 , azim =- 56 ) fig . savefig ( \"span_line.svg\" , format = \"svg\" , dpi = 600 ) plt . show () Python (Three Linearly Inependent Vectors in \\(\\mathbb R^3\\) Spans the whole Ambient Space) Reproduce the code above, but we have three vectors: \\((1,0,1)\\) , \\((1,1,0)\\) , \\((0,1,1)\\) . Again we create a random coefficent matrix to form different linear combinations. A = np . array ([[ 1 , 0 , 1 ], [ 1 , 1 , 0 ], [ 0 , 1 , 1 ]]) . T B = 5 * np . random . randn ( 3 , 300 ) vecs = A @ B s = np . linspace ( - 10 , 10 , 10 ) t = np . linspace ( - 10 , 10 , 10 ) S , T = np . meshgrid ( s , t ) X = S + T Y = T Z = S fig , ax = plt . subplots ( figsize = ( 10 , 10 )) ax = fig . add_subplot ( projection = \"3d\" ) ax . plot_wireframe ( X , Y , Z , linewidth = 1.5 , color = \"k\" , alpha = 0.6 ) ax . scatter ( 0 , 0 , 0 , s = 200 , ec = \"red\" , fc = \"black\" ) colors = np . random . rand ( vecs . shape [ 1 ], 3 ) for i in range ( vecs . shape [ 1 ]): vec = np . array ([[ 0 , 0 , 0 , vecs [ 0 , i ], vecs [ 1 , i ], vecs [ 2 , i ]]]) X , Y , Z , U , V , W = zip ( * vec ) ax . quiver ( X , Y , Z , U , V , W , length = 1 , color = colors [ i ], normalize = False , arrow_length_ratio = 0.07 , pivot = \"tail\" , linestyles = \"solid\" , linewidths = 2 , alpha = 0.9 , ) ax . view_init ( elev = 21 , azim =- 110 ) fig . savefig ( \"span_ambient_space.svg\" , format = \"svg\" , dpi = 600 ) plt . show () MacroAnalyst's Linear Algebra with Python . \u21a9","title":"Vector Span"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_span/#span","text":"","title":"Span"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_span/#algebraic-definition-span","text":"Algebraic Definition (Span) Let \\(S \\subseteq V\\) be a non-empty subset of the vector space \\(V\\) . To be more explicit, define \\(S = \\{\\v_1, \\v_2, ..., \\v_m\\}\\) to be a subset in \\(V\\) , then the linear combination of all the vectors \\(\\v_1, \\v_2, ..., \\v_m\\) is called the span of \\(\\v_1, \\v_2, ..., \\v_m\\) , which we denote as follows: \\[\\text{span}(S) = \\text{span}(\\v_1, \\v_2,...,\\v_m) := \\{\\lambda_1\\v_1 + \\lambda_2\\v_2 + ... + \\lambda_m\\v_m : \\lambda_i \\in \\F, \\v_i \\in S\\}\\]","title":"Algebraic Definition (Span)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_span/#geometrical-definition-span","text":"Span is a really similar concept as subspace, and they are easy to confuse. A subspace is the region of ambient space that can be reached by any linear combination of a set of vectors. And then those vectors span that subspace. You can think about the difference using grammar: a subspace is a noun and span is a verb. A set of vectors spans, and the result of their spanning is a subspace. For example, the subspace defined as all of \\(\\R^2\\) can be created by the span of the vectors \\([0, 1]\\) and \\([1, 0]\\) . Another good example is the vector \\([0,1]\\) spans a 1D subspace that is embedded inside \\(\\R^2\\) (not \\(\\R^1\\) since the vector has 2 elements). Then the vector \\([1, 2]\\) also spans 1D subspace but it is a difference 1D subspace from that spanned by \\([0,1]\\) . - Mike X Cohen: Linear Algebra: Theory, Intuition, Code, 2021. (pp. 86)","title":"Geometrical Definition (Span)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_span/#spanned-by","text":"If \\(W\\) is a vector subspace of \\(V\\) and \\(W = \\text{Span}(S)\\) , we say that \\(S\\) is a spanning set (or generating set) of \\(W\\) and \\(W\\) is spanned or generated by \\(S\\) . Important The intuition is that if a vector space \\(W\\) is spanned by a set of vectors \\(S\\) , then this means for every vector \\(\\w_i \\in W\\) , there exists scalars \\(\\lambda_i \\in \\F\\) , such that \\(\\w_i = \\lambda_1 \\w_1 + \\lambda_2 \\w_2 + \\cdots + \\lambda_m \\w_m\\) . In laymen terms, this set of vectors \\(S\\) can build up every single element in \\(W\\) .","title":"Spanned By"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_span/#span-membership-examples","text":"Example The author mentioned a commonly used terms in linear algebra, which is to check if one vector \\(\\v\\) is \"in the span\" of a set of vectors. This is illustrated clearly with the example provided below: \\[ \\v= \\begin{bmatrix} \\color{red}6 \\\\ \\color{red}3 \\\\ \\color{red}{0} \\end{bmatrix} ,\\quad \\w= \\begin{bmatrix} \\color{red}2 \\\\ \\color{red}3 \\\\ \\color{red}{-5} \\end{bmatrix} \\] and the set \\[ S = \\left\\{\\begin{bmatrix} \\color{red}2 \\\\ \\color{red}4 \\\\ \\color{red}{0} \\end{bmatrix}, \\begin{bmatrix} \\color{red}4 \\\\ \\color{red}{-1}\\\\ \\color{red}{0} \\end{bmatrix} \\right\\}\\] Then the question is whether \\(\\v\\) and \\(\\w\\) are in the span of \\(S\\) . In simple words, can the linear combination of the vectors \\(\\begin{bmatrix} \\color{red}2 \\\\ \\color{red}4 \\\\ \\color{red}{0} \\end{bmatrix}\\) and \\(\\begin{bmatrix} \\color{red}4 \\\\ \\color{red}{-1}\\\\ \\color{red}{0} \\end{bmatrix}\\) form \\(\\v\\) or \\(\\w\\) ? Well the answer for \\(\\v\\) is easy (purposely made easy), we just add them up and get \\(\\v\\) , and in this case, we say \\(\\v\\) is in the \\(\\text{span}(S)\\) . But for \\(\\w\\) , we can quickly conclude that \\(\\w \\not \\in \\text{span}(S)\\) because the third component of the span of \\(S\\) is still \\(0\\) , and thus no linear combination can ever form to \\(\\w\\) 's third component, \\(-5\\) .","title":"Span Membership (Examples)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_span/#theorem-span-is-a-subspace","text":"Theorem (Span is a subspace) Let \\(V\\) be a vector space over a field \\(\\F\\) . Let \\(S \\subseteq V\\) be a subset of \\(V\\) . Then Span \\((S)\\) of \\(V\\) is a vector subspace of \\(V\\) . An immediate consequence of this definition is that span(S) must be a subspace of V. Why? Recall that to be a subspace, closure under vector addition and vector-scalar multiplication must be satisfied; this follows because the definition of span(S) is the linear combination of all vectors in S, and hence any vector addition or vector-scalar multiplication is a member of the linear combination. Note We will prove this together in the next theorem.","title":"Theorem (Span is a subspace)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_span/#theorem-span-is-a-subspace-and-is-the-smallest-subspace-of-v","text":"Theorem (Span is a subspace and is the smallest subspace of V) Let \\(V\\) be a vector space over a field \\(\\F\\) . Let \\(S \\subseteq V\\) be a subset of \\(V\\) . Then \\(\\text{span}(S)\\) of \\(V\\) is a vector subspace of \\(V\\) . In particular, \\(\\text{span}(S)\\) is the smallest vector subspace of \\(V\\) containing the set \\(S\\) (or containing all the vectors in \\(S\\) ).","title":"Theorem (Span is a subspace and is the smallest subspace of V)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_span/#example-different-sets-can-span-the-same-vector-space","text":"Different Sets can span the same Vector Space Before we go through the proof, it is important to understand some possible confusions. One might be confused over the notion of smallest . We can motivate it with an example: \\[ S = \\left\\{\\begin{bmatrix} \\color{red}1 \\\\ \\color{red}0 \\end{bmatrix}, \\begin{bmatrix} \\color{red}0 \\\\ \\color{red}{1} \\end{bmatrix} \\right\\}\\] spans the whole \\(\\R^2\\) space, although we have yet to learn any algorithmic way of checking whether a set spans a particular vector space, this is an obvious enough example (hopefully one can see that the linear combination of these two vectors can reach the whole ambient space \\(\\R^2\\) ). This is because the linear combination of vectors in \\(S\\) can form any vectors in the 2 dimensional space . Then consider \\[ S_1 = \\left\\{\\begin{bmatrix} \\color{red}1 \\\\ \\color{red}0 \\end{bmatrix}, \\begin{bmatrix} \\color{red}0 \\\\ \\color{red}{1} \\end{bmatrix}, \\begin{bmatrix} \\color{red}1 \\\\ \\color{red}{1} \\end{bmatrix} \\right\\}\\] which also spans the whole \\(\\R^2\\) space as well. Both \\(S\\) and \\(S_1\\) are subsets of \\(\\R^2\\) and are subspaces themselves (by inspection). However, the cardinality or the len of \\(S\\) is 2 and \\(S_1\\) is 3. Since both sets span the \\(\\R^2\\) space, why do we say that the span is the smallest subspace of \\(V\\) when \\(S_1\\) has more \"elements\"? The confusion is that the theorem states that the span of a set of vectors in \\(V\\) is the smallest subspace of \\(V\\) which contains this set of vectors , not span of ANY set of vectors in \\(V\\) is the smallest subspace of \\(V\\) . Therefore, \\(S\\) is the smallest subspace of \\(V\\) which contains the two vectors \\(\\left\\{\\begin{bmatrix} \\color{red}1 \\\\ \\color{red}0 \\end{bmatrix}, \\begin{bmatrix} \\color{red}0 \\\\ \\color{red}{1} \\end{bmatrix} \\right\\}\\) and \\(S_1\\) is the smallest subspace of \\(V\\) which contains the three vectors \\(\\left\\{\\begin{bmatrix} \\color{red}1 \\\\ \\color{red}0 \\end{bmatrix}, \\begin{bmatrix} \\color{red}0 \\\\ \\color{red}{1} \\end{bmatrix}, \\begin{bmatrix} \\color{red}1 \\\\ \\color{red}{1} \\end{bmatrix} \\right\\}\\) . An extract from StackExchange : Written in words that theorem states, that any subspace \\(W\\) , that exists and contains \\(v_1,\u2026,v_n\\) , also contains \\(\\text{span}(v_1,\u2026,v_k)\\) . Hence \\(W\\) is larger (or equal) to \\(\\text{span}(v_1,\u2026,v_k)\\) , since it contains \\(\\text{span}(v_1,\u2026,v_k)\\) and could contain some more elements. And since any other subspace \\(W\\) is larger (or equal) it follows that \\(\\text{span}(v_1,\u2026,v_k)\\) the smallest subspace. That is like saying: Any number in \\(\u2115\u222a\\{0\\}\\) is larger (or equal) to \\(0\\) , hence \\(0\\) is the smallest number in \\(\u2115\u222a\\{0\\}\\) . Or even closer to the original problem: Any subset \\(M\u2282(\u2115\u222a\\{0\\})\\) with \\(0\u2208M\\) is larger or equal to \\(\\{0\\}\\) , hence \\(\\{0\\}\\) has to be the smallest subset of \\(\u2115\u222a\\{0\\}\\) that contains \\(0\\) .","title":"Example (Different Sets can span the same Vector Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_span/#proof","text":"Proof We first show that \\(\\text{span}(S)\\) is a vector subspace of \\(V\\) . And then we show that if \\(\\text{span}(S)\\) is a vector subspace of \\(V\\) containing \\(S\\) , then if \\(T\\) is another vector subspace of \\(V\\) containing \\(S\\) , we must have \\(\\text{span}(S) \\subseteq T\\) . \\(\\text{span}(S)\\) is a subspace : It suffices to check for the conditions S2 and S3 of subspaces . To show S2, we need to show that for any \\(\\v, \\w \\in \\text{span}(S)\\) , \\(\\v + \\w \\in \\text{span}(S)\\) . We also note that for both \\(\\v\\) and \\(\\w\\) , they can be expressed as linear combination of the set of vectors in \\(S\\) , (i.e. \\(\\v = a_1 \\v_1 + a_2 \\v2 + ... + a_m \\v_m\\) , \\(\\w = \\b_1 \\v_1 + \\b_2 \\v_2 + ... + \\b_m \\v_m\\) for some \\(a_i, b_i \\in \\F\\) ) Then adding them up we yield: \\[\\begin{align} \\v + \\w &= (a_1 \\v_1 + a_2 \\v2 + ... + a_m \\v_m) + (\\b_1 \\v_1 + \\b_2 \\v_2 + ... + \\b_m \\v_m) \\label{eq1}\\tag{1} \\\\ &= (a_1 + b_1)\\v_1 + (a_2 + b_2)\\v_2 + ... + (a_m + b_m)\\v_m \\label{eq2}\\tag{2} \\end{align}\\] Equation (\\ref{eq2}) shows us that it is still a linear combination of the set of vectors in \\(S\\) , and hence also in \\(\\text{span}(S)\\) , which is closed under addition. For S3, the same logic applies (can you show it?). \\(\\text{span}(S)\\) is the smallest subspace that contains \\(S\\) : Let \\(T\\) be any subspace of \\(V\\) that contains the set \\(S\\) . In order to show \\(\\text{span}(S) \\subseteq T\\) , we will use a proving method by showing that for all elements \\(\\v \\in \\text{span}(S)\\) , \\(\\v\\) is in \\(T\\) as well. We start off by picking any element \\(\\v \\in \\text{span}(S)\\) , (note that \\(\\v\\) may not be in \\(S\\) , but it definitely must be in \\(\\text{span}(S)\\) ! Do not get confused here!), for this \\(\\v\\) , it can be expressed as the linear combination of the vectors of \\(S\\) , \\(\\v =\\lambda_1\\v_1 + \\lambda_2\\v_2 + ... + \\lambda_m\\v_m\\) , then \\(\\v \\in T\\) as well since \\(T\\) contains the set \\(S\\) , and also is a subspace , thus closed under scalar multiplication and vector addition.","title":"Proof"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_span/#theorem-linear-dependence-span-equals-each-other","text":"Theorem (Linear Dependence Span equals each other) Let \\(\\u_1,...,\\u_k\\) be vectors in \\(\\mathbb{R}^n\\) . If \\(\\u_k\\) is a linear combination of \\(\\mathbf{u_1,...,u_{k-1}}\\) , then \\[\\text{Span}\\{\\mathbf{u_1,...,u_{k-1}}\\} = \\text{Span}\\{\\mathbf{u_1,...,u_{k-1},u_k}\\}\\] This should be an obvious fact by using the same example in the Theorem (Span is a subspace and is the smallest subspace of \\(V\\) ) .","title":"Theorem (Linear Dependence Span equals each other)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_span/#examples","text":"","title":"Examples"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_span/#geometrical-interpretation-of-span","text":"Geometrical Interpretation of Span The \\(xz\\) -plane in \\(\\R^3\\) can be parameterized by the equations \\[x = t_1, \\;\\;\\; y = 0, \\;\\;\\; z = t_2.\\] As a subspace, the \\(xz\\) -plane is spanned by the vectors (1, 0, 0) and (0, 0, 1). Every vector in the \\(xz\\) -plane can be written as a linear combination of these two: \\[(t_1, 0, t_2) = t_1(1,0,0) + t_2(0,0,1)\\] Geometrically, this corresponds to the fact that every point on the \\(xz\\) -plane can be reached from the origin by first moving some distance in the direction of (1, 0, 0) and then moving some distance in the direction of (0, 0, 1).","title":"(Geometrical Interpretation of Span)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_span/#python-two-linearly-independent-vectors-in-mathbb-r3-spans-a-plane","text":"The below three examples are referenced from Macro Analyst's notes here 1 . The example below assumes the ambient space \\(\\R^3\\) and assumes that the term 2 linearly independent vectors just simply mean that these two vectors are not multiple of each other. We know that in earlier chapter that two linearly indepedent vectors in \\(\\R^3\\) space forms a subspace that is a plane. In fact, we can also say that two linearly independent vectors span a plane in \\(\\R^3\\) . We can verify below visually using python, with courtesy of MacroAnalyst's Linear Algebra with Python . Firstly, let's say we have two vectors: \\((3, 9, 2)\\) , \\((1,7,5)\\) which are linearly independent . The author made use of matrix multiplication in his code to demonstrate span. For more general span, a basic fact of matrix multiplication can assist us in demonstrating: \\[ AB = A[b_1\\ b_2\\ b_3,...,b_p]=[Ab_1\\ Ab_2\\ Ab_3,...,Ab_p] \\] where \\(A\\) is the spanning set of vectors, \\(b_k\\) is vector of weights for linear combination. We can generate a random matrix \\(B\\) to form various linear combinations to visually verify if they are all contained in the spanned plane. We define $$ A=\\left[\\begin{array}{rr} 3 & 1 \\ 9 & 7 \\ 2 & 5 \\end{array}\\right]\\qquad b_i\\sim N(\\mathbb{0}, 1) $$ In other words, the columns of \\(\\A\\) are the set of vectors \\[ S = \\left\\{\\begin{bmatrix} \\color{red}3 \\\\ \\color{red}9 \\\\ \\color{red}2 \\end{bmatrix}, \\begin{bmatrix} \\color{red}1 \\\\ \\color{red}7 \\\\ \\color{red}5 \\end{bmatrix} \\right\\}\\] and we want to generate say, 300 random linear combinations of this set of vectors and show visually that all these linear combinations are IN THE PLANE SPANNED BY THE SET OF VECTORS . Technically, there are infinite number of linear combinations, but we just show 300 for good illustration. Note that these 300 random linear combination are stored in the matrix \\(\\B\\) . import numpy as np import matplotlib.pyplot as plt from typing import List , Union , Tuple # The plane spanned by {[1,0,1], [0,1,1]} A = np . array ([[ 1 , 0 , 1 ], [ 0 , 1 , 1 ]]) . T B = 10 * np . random . randn ( 2 , 3000 ) # i = 300, i.e. 300 random weight vectors vecs = A @ B s = np . linspace ( - 10 , 10 , 10 ) t = np . linspace ( - 10 , 10 , 10 ) S , T = np . meshgrid ( s , t ) X = S Y = T Z = S + T fig , ax = plt . subplots ( figsize = ( 10 , 10 )) ax = fig . add_subplot ( projection = \"3d\" ) ax . plot_wireframe ( X , Y , Z , linewidth = 1.5 , color = \"k\" , alpha = 0.6 ) ax . scatter ( 0 , 0 , 0 , s = 200 , ec = \"red\" , fc = \"black\" ) colors = np . random . rand ( vecs . shape [ 1 ], 3 ) for i in range ( vecs . shape [ 1 ]): vec = np . array ([[ 0 , 0 , 0 , vecs [ 0 , i ], vecs [ 1 , i ], vecs [ 2 , i ]]]) X , Y , Z , U , V , W = zip ( * vec ) ax . quiver ( X , Y , Z , U , V , W , length = 1 , color = colors [ i ], normalize = False , arrow_length_ratio = 0.07 , pivot = \"tail\" , linestyles = \"solid\" , linewidths = 2 , alpha = 0.9 , ) ax . view_init ( elev =- 156 , azim =- 56 ) fig . savefig ( \"span_plane.svg\" , format = \"svg\" , dpi = 600 ) plt . show () import numpy as np import matplotlib.pyplot as plt from typing import List , Union , Tuple # The plane spanned by {[3,9,2], [1,7,5]} A = np . array ([[ 3 , 9 , 2 ], [ 1 , 7 , 5 ]]) . T B = 10 * np . random . randn ( 2 , 3000 ) # i = 300, i.e. 300 random weight vectors vecs = A @ B s = np . linspace ( - 10 , 10 , 10 ) t = np . linspace ( - 10 , 10 , 10 ) S , T = np . meshgrid ( s , t ) X = 3 * S + T Y = 9 * S + 7 * T Z = 2 * S + 5 * T fig , ax = plt . subplots ( figsize = ( 10 , 10 )) ax = fig . add_subplot ( projection = \"3d\" ) ax . plot_wireframe ( X , Y , Z , linewidth = 1.5 , color = \"k\" , alpha = 0.6 ) ax . scatter ( 0 , 0 , 0 , s = 200 , ec = \"red\" , fc = \"black\" ) colors = np . random . rand ( vecs . shape [ 1 ], 3 ) for i in range ( vecs . shape [ 1 ]): vec = np . array ([[ 0 , 0 , 0 , vecs [ 0 , i ], vecs [ 1 , i ], vecs [ 2 , i ]]]) X , Y , Z , U , V , W = zip ( * vec ) ax . quiver ( X , Y , Z , U , V , W , length = 1 , color = colors [ i ], normalize = False , arrow_length_ratio = 0.07 , pivot = \"tail\" , linestyles = \"solid\" , linewidths = 2 , alpha = 0.9 , ) ax . view_init ( elev =- 156 , azim =- 56 ) plt . show ()","title":"Python (Two Linearly Independent Vectors in \\(\\mathbb R^3\\) Spans a Plane)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_span/#python-two-linearly-dependent-vectors-in-mathbb-r3-spans-a-line","text":"What if the two vectors are not linear independent? Then the following example will convince you that two linearly dependent vectors span a line in \\(\\R^3\\) space. Note that this is equivalent to \"one vector\" in \\(\\R^3\\) spans a line. # Although spanned by 2 vectors, but actually reduces to 1 vector. # So it is a line. A = np . array ([[ 3 , 9 , 2 ], [ 6 , 18 , 4 ]]) . T B = 10 * np . random . randn ( 2 , 3000 ) # i = 300, i.e. 300 random weight vectors vecs = A @ B s = np . linspace ( - 10 , 10 , 10 ) t = np . linspace ( - 10 , 10 , 10 ) S , T = np . meshgrid ( s , t ) X = 3 * S + 6 * T Y = 9 * S + 18 * T Z = 2 * S + 4 * T fig , ax = plt . subplots ( figsize = ( 10 , 10 )) ax = fig . add_subplot ( projection = \"3d\" ) ax . plot_wireframe ( X , Y , Z , linewidth = 1.5 , color = \"k\" , alpha = 0.6 ) ax . scatter ( 0 , 0 , 0 , s = 200 , ec = \"red\" , fc = \"black\" ) colors = np . random . rand ( vecs . shape [ 1 ], 3 ) for i in range ( vecs . shape [ 1 ]): vec = np . array ([[ 0 , 0 , 0 , vecs [ 0 , i ], vecs [ 1 , i ], vecs [ 2 , i ]]]) X , Y , Z , U , V , W = zip ( * vec ) ax . quiver ( X , Y , Z , U , V , W , length = 1 , color = colors [ i ], normalize = False , arrow_length_ratio = 0.07 , pivot = \"tail\" , linestyles = \"solid\" , linewidths = 2 , alpha = 0.9 , ) ax . view_init ( elev =- 156 , azim =- 56 ) fig . savefig ( \"span_line.svg\" , format = \"svg\" , dpi = 600 ) plt . show ()","title":"Python (Two Linearly Dependent Vectors in \\(\\mathbb R^3\\) Spans a Line)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_span/#python-three-linearly-inependent-vectors-in-mathbb-r3-spans-the-whole-ambient-space","text":"Reproduce the code above, but we have three vectors: \\((1,0,1)\\) , \\((1,1,0)\\) , \\((0,1,1)\\) . Again we create a random coefficent matrix to form different linear combinations. A = np . array ([[ 1 , 0 , 1 ], [ 1 , 1 , 0 ], [ 0 , 1 , 1 ]]) . T B = 5 * np . random . randn ( 3 , 300 ) vecs = A @ B s = np . linspace ( - 10 , 10 , 10 ) t = np . linspace ( - 10 , 10 , 10 ) S , T = np . meshgrid ( s , t ) X = S + T Y = T Z = S fig , ax = plt . subplots ( figsize = ( 10 , 10 )) ax = fig . add_subplot ( projection = \"3d\" ) ax . plot_wireframe ( X , Y , Z , linewidth = 1.5 , color = \"k\" , alpha = 0.6 ) ax . scatter ( 0 , 0 , 0 , s = 200 , ec = \"red\" , fc = \"black\" ) colors = np . random . rand ( vecs . shape [ 1 ], 3 ) for i in range ( vecs . shape [ 1 ]): vec = np . array ([[ 0 , 0 , 0 , vecs [ 0 , i ], vecs [ 1 , i ], vecs [ 2 , i ]]]) X , Y , Z , U , V , W = zip ( * vec ) ax . quiver ( X , Y , Z , U , V , W , length = 1 , color = colors [ i ], normalize = False , arrow_length_ratio = 0.07 , pivot = \"tail\" , linestyles = \"solid\" , linewidths = 2 , alpha = 0.9 , ) ax . view_init ( elev = 21 , azim =- 110 ) fig . savefig ( \"span_ambient_space.svg\" , format = \"svg\" , dpi = 600 ) plt . show () MacroAnalyst's Linear Algebra with Python . \u21a9","title":"Python (Three Linearly Inependent Vectors in \\(\\mathbb R^3\\) Spans the whole Ambient Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_subspaces/","text":"\\[\\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\z}{\\mathbf{z}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\C}{\\mathbf{C}}\\] Vector Spaces Notations Dimensions Algebraic Definition (Dimension) Geometric Definition (Dimension) Vector Space Definition (Vector Space) Vector Space over a Field Example of Vector Spaces over a Field Subspaces Geometric Definition (Subspaces) Algebraic Definition (Subspaces) Example and Intuition of Subspaces Theorem (The Subspace Criterion) Proof Linear Combination (Definition revisited) Theorem (Closure Linear Combination implies Subspace) Vector subspaces of \\(\\mathbb{R}^2\\) Examples of Non-Subspace Visualization of Subspaces in \\(\\mathbb{R}^2\\) Vector subspaces of \\(\\mathbb{R}^3\\) Visualization of Subspaces in \\(\\mathbb{R}^3\\) Theorem (Intersection of Subspaces is a Subspace) Proof Union of Two Subspaces may not be a Subspace Ambient Space Vector Spaces Notations For this section, we will treat our field \\(\\F\\) to be the real space \\(\\R\\) ; and when we later mention vector space or vector subspace \\(V\\) , we actually mean the vector space/subspace over the field \\(\\R\\) . Dimensions Normally, dimensions are formally defined after the introduction of basis. We will give an informal treatment of dimension here to better understand the idea when mentioning this word. Algebraic Definition (Dimension) Algebraic Definition Consider a vector \\(\\v \\in \\R^n\\) , then we can represent \\[ \\v= \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix} \\in \\R^n \\] and we can naively define the dimension of the vector to be the number of elements: \\[\\text{dim}(\\v) = |\\v| = n\\] Geometric Definition (Dimension) Geometric Definition The dimensionality of a vector is the number of coordinate axes in which that vector exists. A 2D vector is a line in a 2D space (think of the typical Cartesian XY plane); a 3D vector is a line in a 3D space. Note that both a 2D vector and a 3D vector are both lines, but they exist in a different number of dimensions. - Mike X Cohen: Linear Algebra: Theory, Intuition, Code, 2021. (pp. 76) Vector Space Definition (Vector Space) Definition (Vector Space) A vector space is a set \\(V\\) consisting of the following: a) a field \\(\\F\\) , where the elements in \\(\\F\\) are called scalars . b) a non empty set \\(V\\) , where the elements in \\(V\\) are called vectors (be less pedantic about the word vectors as we can treat say polynomial ring as a vector space and we can also treat a field \\(\\F\\) over itself as a vector space as well. So the word vectors here are not only the usual ones we see in \"vectors\") c) an operation of vector addition \\(\\u + \\v\\) between every pair of vectors \\(\\u, \\v \\in V\\) and d) an operation of scalar multiplication \\(\\lambda\\v\\) between every \\(\\lambda \\in \\F\\) and every vector \\(\\v \\in V\\) . Furthermore, all elements in \\(V\\) must satisfy the closure properties and the following axioms: \\(\\textbf{V(0)}: \\textbf{Closure (Well Defined)}\\) : For all \\(\\mathbf{u,v} \\in V, \\mathbf{u+v} \\in V\\) ; For all \\(\\lambda \\in \\F\\) and \\(\\v \\in V\\) , then \\(\\lambda \\v \\in V\\) ; Consequently, all linear combinations of vectors in \\(V\\) is in \\(V\\) . \\(\\textbf{V(1)}: \\textbf{Existence of the Zero Vector (Additive Identity)}\\) : There exists a vector \\(\\mathbf{0} \\in V\\) , called the zero vector, such that \\(\\mathbf{v + 0 = u = 0+v}\\) for all \\(\\v \\in V\\) . We should know it is unique. \\(\\textbf{V(2)}: \\textbf{Commutative Law for Vector Addition}\\) : For all \\(\\mathbf{u,v} \\in V\\) , \\(\\mathbf{u+v = v+u}\\) . \\(\\textbf{V(3)}: \\textbf{Associative Law for Vector Addition}\\) : For all \\(\\mathbf{u,v,w} \\in V\\) , we have \\(\\mathbf{u + (v+w)} = \\mathbf{(u+v)+w}\\) . \\(\\textbf{V(4)}: \\textbf{Existence of Additive Inverse}\\) : For every vector \\(\\v \\in V\\) , there exists a vector \\(-\\v \\in V\\) such that \\(\\mathbf{v+(-v) = 0 = (-v)+v}\\) . \\(\\textbf{V(5)}: \\textbf{Existence of the Multiplicative Identity}\\) : There exists \\(\\mathbf{1}\\in V\\) such that \\(\\mathbf{1} \\times \\v = \\v \\times \\mathbf{1} = \\v\\) for all \\(\\v \\in \\F\\) . \\(\\textbf{V(6)}: \\textbf{Associative Law for Vector-Scalar Multiplication}\\) : For all \\(a,b \\in \\F\\) and \\(\\v \\in V\\) , we have \\(a(b\\v) = (ab)\\v\\) \\(\\textbf{V(7)}: \\textbf{Distributive Law}\\) : \\(a(\\mathbf{u+v}) = a\\u + a\\v ,~~~\\forall a \\in \\F, \\mathbf{u,v} \\in V\\) \\((a+b)\\v = a\\v + b\\v,~~~\\forall a,b \\in \\F, \\v \\in V\\) . Vector Space over a Field Warning Note carefully that a vector space \\(V\\) is dependent on the field \\(\\F\\) since scalar multiplication draws \\(\\lambda\\) from \\(\\F\\) . Thus, we don't usually talk only about vector space \\(V\\) alone, instead we use the notion of Vector Space over a Field \\(\\F\\) . Example of Vector Spaces over a Field Real Vector Space We can imagine elements \\(\\v \\in V\\) as vectors/lines/points. We can take the field \\(\\F\\) to be \\(\\R\\) . Then a vector space \\(V\\) over \\(\\R\\) is called the real vector space . Subspaces Geometric Definition (Subspaces) For a better treatment of understanding subspaces geometrically, do read page 79-80 of the book Linear Algebra: Theory, Intuition, Code . Algebraic Definition (Subspaces) Algebraic Definition (Subspaces) Let \\(V\\) be a vector space over a field \\(\\F\\) . A non empty subset \\(W \\subseteq V\\) of a vector space \\(V\\) is called a subspace of \\(V\\) if \\(W\\) is itself a vector space using the same vector addition and scalar multiplication as in \\(V\\) . Consequence Let \\(W \\subset V\\) and \\(\\w \\in W\\) , we must show that \\(W\\) obeys all conditions from the definition of a vector space . For any \\(\\w \\in W\\) , we must have \\(w \\in V\\) by definition of a subset; this means that \\(V2,V3,V5,V6,V7\\) are automatically true since \\(\\w \\in W \\subset V\\) . We just need to check if it satisfies \\(V0,V1,V6\\) , namely, the two closure properties, the existence of zero vector, and the existence of the additive inverse for vector addition. We take V2 as an example, take any two elements \\(\\w_1, \\w_2 \\in W\\) , then \\(\\w_1, \\w_2 \\in V\\) , and hence obeys V2. But we cannot say for sure about V1, as there is no guarantee the subset \\(W\\) will contain the zero vector! Example and Intuition of Subspaces Example and Intuition of Subspaces Let us start with an informal treatment of a subspace . First, we say that a subspace \\(W\\) of \\(V\\) must fulfill two conditions: \\(W\\) is a subset of \\(V\\) and; \\(W\\) is a vector space itself over the field \\(\\F\\) . The classic example of a \\(3\\) -dimensional real vector space is the Euclidean space \\(\\mathbb R^3\\) where \\(\\R^3 = \\{(x,y,z)~|~x,y,z \\in \\R\\}\\) , which is the 3-dimensional space, or to be more formal, it is the vector space \\(V\\) over \\(\\R\\) , a collection of all 3-tuple vectors. Now, consider the set \\(L = \\{\\lambda(1,2,3) ~|~ \\lambda \\in \\R\\}\\) , we check that the set \\(L\\) informally satisfies our conditions to be a subspace of \\(\\R^3\\) . I emphasized the word set because of the idea of subset , that is, a vector subspace first must be a subset of the vector space . Our \\(L\\) is a subset of \\(\\R^3\\) by definition, in fact, it is the set of all lines of the form \\(\\lambda(1,2,3)\\) . Only being a subset is not sufficient to be called a subspace , we have fulfilled the idea of a sub set, but not the space part. It suffices to check if our \\(L\\) is a valid vector space over \\(\\R\\) . And to check that we can use back the definition of vector space to do so. The main point is that a single vector, \\((1,2,3)\\) scaled by any \\(\\lambda \\in \\R\\) makes up a subspace . It also happens to be the 1D-subspace which is just a line. It is also important to mention that the line must pass through the origin to be considered a subspace . Next, consider the set \\(P = \\{\\lambda_1(1,0,0) + \\lambda_2(0,0,1) ~|~ \\lambda_i \\in \\R\\}\\) . Geometrically, we have known from earlier that these two vectors form a 2D-plane and we will show later that it is the 2D-subspace, a plane. Theorem (The Subspace Criterion) From the previous section, we know that we just need to check V0, V1 and V4 to be true. This reduces our conditions for \\(W \\subset V\\) to be a subspace to be as follows: Theorem (The Subspace Criterion) Let \\(V\\) be a vector space over a field \\(\\F\\) . A subset \\(W\\) of \\(V\\) is a subspace of \\(V\\) if and only if S1) \\(\\textbf{(Containing the Zero Vector)}\\) : \\(\\0 \\in W\\) ; S2) \\(\\textbf{(Closure under Vector Addition)}\\) : \\(\\forall \\mathbf{u,w} \\in W, \\u+\\w \\in W\\) S3) \\(\\textbf{(Closure under Scalar Multiplication)}\\) : \\(\\forall \\lambda \\in \\F\\) and \\(\\w \\in W\\) , we have \\(\\lambda\\w \\in W\\) . Proof Proof \\(\\Rightarrow\\) If \\(W\\) is also a subspace , then by the definition of subspace, \\(W\\) is also a vector space, and hence \\(S1,S2,S3\\) are satisfied. \\(\\Leftarrow\\) We should be clear that for other conditions V2, V3, V5, V6 and V7 do not warrant checking since these axioms are automatically satisfied for \\(W\\) since it holds for the larger subset \\(V\\) . We need only check the following: Zero Vector: If S1 is satisfied, then the corresponding V1 is satisfied. Closure Properties: S2 and S3 both implies V0. Additive Inverse for Vector Addition : Let \\(\\w \\in W\\) , take \\(-1 \\in \\F\\) , by S3 (closure of scalar multiplication), we must have \\(-1(\\w) \\in W\\) .$ This implies V4. One thing that you notice is that by the closure of addition \\(\\mathbf{w+(-w)} = \\mathbf{0} \\in W\\) or by the closure of scalar multiplication \\(0 \\cdot \\w = \\0 \\in W\\) (note \\(0 \\in \\F\\) ). Consequently, this implies that S2 and S3 are sufficient to imply S1 . You can trace the proof by ignoring the S1. Linear Combination (Definition revisited) Definition Let \\(V\\) be a vector space over a field \\(\\F\\) . A vector \\(\\v\\) is a linear combination of vectors \\(\\mathbf{v_1,v_2,...,v_m} \\in V\\) if \\( \\(\\v = \\lambda_1\\v_1+...+\\lambda_m\\mathbf{v_m}\\) \\) for some scalars \\(\\lambda_i \\in \\F\\) . Theorem (Closure Linear Combination implies Subspace) Theorem (Closure Linear Combination implies Subspace) Let \\(V\\) be a vector space over a field \\(\\F\\) and \\(W\\subseteq V\\) a non empty subset. Then \\(W\\) is a subspace of \\(V\\) if and only if \\(W\\) is closed under linear combination, (i.e \\(\\forall \\lambda_i \\in \\F, \\forall \\mathbf{w_i} \\in W \\Rightarrow \\lambda_1\\w_1+ \\lambda_2\\w_2 \\in W\\) ). This should not come as a surprise as this is just a rebranding of Conditions to be a Subspace Theorem (Equivalent Subspace Definition) . Vector subspaces of \\(\\mathbb{R}^2\\) There are three types of vector subspaces of the vector space \\(\\mathbb{R}^2\\) . i) The origin \\(\\mathbf{0} = \\{0,0\\}\\) ii) The line through the origin : \\(L = \\{\\lambda\\v~|~\\lambda \\in \\mathbb{R}, \\v \\in \\R^2\\}\\) iii) \\(\\mathbb{R}^2\\) Examples of Non-Subspace Non-Subspace Consider the set \\(L = \\{\\lambda (0, -3) ~|~ \\lambda \\in \\R\\}\\) , why is it not a subspace of \\(\\R^2\\) ? Though one can answer that since this line does not pass through the origin, and thus does not contain the zero vector, hence not a subspace, we can take a step back and think of it geometrically: We can check whether the line is closed under addition and vector-scalar multiplication. Geometrically, it seems that any two vectors inside the set will still be in the set since adding or subtracting vectors inside is just \"line + line | line - line\", which still lies on the same line. It is easy to gloss over that if you take \\(\\v\\) and \\(-\\v\\) from the set \\(L\\) , then the sum is the zero vector, which is not inside \\(L\\) . This can be further verified by vector-scalar, we can just take \\(0 \\in \\R\\) , and claim that \\(0\\v\\) for all \\(\\v \\in L\\) is not in \\(L\\) ! Visualization of Subspaces in \\(\\mathbb{R}^2\\) Courtesy of MacroAnalyst's Linear Algebra with Python . Notice that the line \\(y = -3 + \\frac{2}{3}x\\) does not pass through the origin \\((0, 0)\\) , and hence not a subspace. Note this example (line) is the set \\(L\\) in the previous example. import matplotlib.pyplot as plt import numpy as np fig , ax = plt . subplots ( figsize = ( 10 , 10 )) ####################### Arrows ####################### x = np . arange ( - 10 , 11 , 1 ) y = 4 / 6 * x ax . plot ( x , y , lw = 4 , color = \"blue\" , label = r \"$y = \\frac {2}{3} x$, subspace of $\\mathbf {R} ^2$\" , ) y = - 3 + 4 / 6 * x ax . plot ( x , y , lw = 4 , color = \"red\" , label = r \"$y = -3+\\frac {2}{3} x$, not a subspace of $\\mathbf {R} ^2$\" , ) ax . grid ( True ) ax . set_title ( \"Visualization of Subspace in $R^2$ \" , size = 18 ) ax . scatter ( 0 , 0 , s = 100 , fc = \"black\" , ec = \"red\" ) ax . text ( - 2 , 0 , \"$(0,0)$\" , size = 18 ) ax . legend ( fontsize = 16 ) ax . axis ([ - 10 , 10 , - 10 , 10 ]) ax . set_xlabel ( \"x-axis\" , size = 18 ) ax . set_ylabel ( \"y-axis\" , size = 18 ) plt . show () Vector subspaces of \\(\\mathbb{R}^3\\) There are four types of vector subspaces of the vector space \\(\\mathbb{R}^3\\) . i) The origin \\(\\mathbf{0} = \\{0,0,0\\}\\) ii) The line \\(L\\) passing through the origin: \\(L = \\{\\lambda \\v~|~\\lambda \\in \\mathbb{R}, \\v \\in \\R^3\\}\\) iii) A plane \\(P\\) passing through the origin: \\(P = \\{(x,y,z) \\in \\mathbb{R}^3~|~ ax + by + cz = 0\\}\\) iv) \\(\\mathbb{R}^3\\) Visualization of Subspaces in \\(\\mathbb{R}^3\\) Courtesy of MacroAnalyst's Linear Algebra with Python . We have not learnt of the term span , but for now when we say a span of two vectors \\(\\u\\) and \\(\\v\\) in \\(\\R^3\\) , it means these two vectors are not multiple of each other. Consider a span of two vectors \\(u = (1,-2,1)^T\\) and \\(v=(2,1,2)^T\\) . The span of \\((u,v)\\) is a subspace of \\(\\R^3\\) , where \\(s\\) and \\(t\\) are the scalars of the vectors. We also plot a plane which is not a subspace by adding \\(2\\) to the third equation: \\(z = s + 2t + 2\\) . import matplotlib.pyplot as plt import numpy as np # %matplotlib # %matplotlib notebook, use this only if you are in Jupyter Notebook fig = plt . figure ( figsize = ( 8 , 8 )) ax = fig . add_subplot ( 111 , projection = \"3d\" ) s = np . linspace ( - 1 , 1 , 10 ) t = np . linspace ( - 1 , 1 , 10 ) S , T = np . meshgrid ( s , t ) X = S + 2 * T Y = - 2 * S + T Z = S + 2 * T ax . plot_surface ( X , Y , Z , alpha = 0.9 , cmap = plt . cm . coolwarm ) Z2 = S + 2 * T + 2 # this is not a subspace anymore ax . plot_surface ( X , Y , Z2 , alpha = 0.6 , cmap = plt . cm . jet ) ax . scatter ( 0 , 0 , 0 , s = 200 , color = \"red\" ) ax . text ( 0 , 0 , 0 , \"$(0,0,0)$\" , size = 18 , zorder = 5 ) ax . set_title ( \"Visualization of Subspace of $\\mathbb {R} ^3$\" , x = 0.5 , y = 1.1 , size = 20 ) ax . set_xlabel ( \"x-axis\" , size = 18 ) ax . set_ylabel ( \"y-axis\" , size = 18 ) ax . set_zlabel ( \"z-axis\" , size = 18 ) ax . view_init ( elev =- 29 , azim = 132 ) plt . show () Theorem (Intersection of Subspaces is a Subspace) Theorem (Intersection of Subspaces is a Subspace) Let \\(V\\) be a vector space over a field \\(\\F\\) and let \\(W_\\alpha \\subseteq V\\) ( \\(\\alpha \\in I\\) ) be vector subspaces of \\(V\\) , then the intersection \\( \\(\\cap_{\\alpha \\in I}W_\\alpha\\) \\) is again a vector subspace of \\(V\\) . Proof Proof Before we start, the intuition is simple, call \\(C = A \\cap B\\) , and to think intuitively that \\(C\\) is itself a subspace, we just take an element \\(\\mathbf{c_1}, \\mathbf{c_2} \\in C\\) and \\(\\lambda \\in \\F\\) , and ask whether these vectors obey the closure property. For a start, for any two vectors in \\(C\\) , they are closed under addition because both vectors are elements of \\(A\\) and (or) \\(B\\) . Let \\(W_1\\) and \\(W_2\\) be subspaces of a vector space \\(V\\) , then it suffices for us to just prove \\(W_1 \\cap W_2\\) is a subspace of \\(V\\) as we can use induction to prove for \\(n\\) intersections. We denote the subset \\(W_1 \\cap W_2\\) to be \\(U\\) . Closed under Vector Addition: To show that \\(U\\) has closure properties, we take any element \\(\\mathbf{u_1}, \\mathbf{u_2} \\in U\\) , any \\(\\lambda \\in \\mathbb{F}\\) and show that \\(\\mathbf{u_1} + \\mathbf{u_2} \\in U\\) and \\(\\lambda \\mathbf{u} \\in U\\) . Since \\(\\mathbf{u_1}, \\mathbf{u_2} \\in U\\) , then \\(\\mathbf{u_1}, \\mathbf{u_2} \\in W_1\\) and \\(\\mathbf{u_1}, \\mathbf{u_2} \\in W_2\\) respectively. This means that \\(\\mathbf{u_1} + \\mathbf{u_2} \\in W_1\\) and \\(\\mathbf{u_1} + \\mathbf{u_2} \\in W_2\\) since both \\(W_1\\) and \\(W_2\\) are closed under addition. Consequently, another way of saying \\(\\mathbf{u_1} + \\mathbf{u_2} \\in W_1\\) and \\(\\mathbf{u_1} + \\mathbf{u_2} \\in W_2\\) is \\(\\mathbf{u_1} + \\mathbf{u_2} \\in W_1 \\cap W_2 = U\\) , which implies that \\(U\\) is closed under vector addition. Closed under Vector-Scalar Multiplication: The same line of logic can be applied for closure under vector-scalar multiplication. We have shown that \\(W_1 \\cap W_2\\) is a subspace. Union of Two Subspaces may not be a Subspace The union of two subspaces may not be a subspace. The reason why this can happen is that all vector spaces, and hence subspaces too, must be closed under addition (and scalar multiplication). The union of two subspaces takes all the elements already in those spaces, and nothing more. In the union of subspaces \\(W_1\\) and \\(W_2\\) , there are new combinations of vectors we can add together that we couldn't before, like \\(\\w_1 + \\w_2\\) where \\(\\w_1 \\in W_1\\) and \\(\\w_2 \\in W_2\\) . For example, take \\(W_1\\) to be the \\(x\\) -axis and \\(W_2\\) the \\(y\\) -axis, both subspaces of \\(\\mathbb{R}^2\\) . Their union includes both \\((3,0)\\) and \\((0,5)\\) , whose sum, \\((3,5)\\) , is not in the union. Hence, the union is not a vector space. Ambient Space The term ambient space is used frequently in the book. We can understand it with an example: Example If you are dealing with a vector subspace \\(U\\) as a subset of \\(\\mathbb R^2\\) , the ambient space is \\(\\mathbb R^2\\) . So basically, the ambient space is the \"parent space\" of the subspace.","title":"Vector Space and Subspaces"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_subspaces/#vector-spaces","text":"","title":"Vector Spaces"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_subspaces/#notations","text":"For this section, we will treat our field \\(\\F\\) to be the real space \\(\\R\\) ; and when we later mention vector space or vector subspace \\(V\\) , we actually mean the vector space/subspace over the field \\(\\R\\) .","title":"Notations"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_subspaces/#dimensions","text":"Normally, dimensions are formally defined after the introduction of basis. We will give an informal treatment of dimension here to better understand the idea when mentioning this word.","title":"Dimensions"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_subspaces/#algebraic-definition-dimension","text":"Algebraic Definition Consider a vector \\(\\v \\in \\R^n\\) , then we can represent \\[ \\v= \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix} \\in \\R^n \\] and we can naively define the dimension of the vector to be the number of elements: \\[\\text{dim}(\\v) = |\\v| = n\\]","title":"Algebraic Definition (Dimension)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_subspaces/#geometric-definition-dimension","text":"Geometric Definition The dimensionality of a vector is the number of coordinate axes in which that vector exists. A 2D vector is a line in a 2D space (think of the typical Cartesian XY plane); a 3D vector is a line in a 3D space. Note that both a 2D vector and a 3D vector are both lines, but they exist in a different number of dimensions. - Mike X Cohen: Linear Algebra: Theory, Intuition, Code, 2021. (pp. 76)","title":"Geometric Definition (Dimension)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_subspaces/#vector-space","text":"","title":"Vector Space"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_subspaces/#definition-vector-space","text":"Definition (Vector Space) A vector space is a set \\(V\\) consisting of the following: a) a field \\(\\F\\) , where the elements in \\(\\F\\) are called scalars . b) a non empty set \\(V\\) , where the elements in \\(V\\) are called vectors (be less pedantic about the word vectors as we can treat say polynomial ring as a vector space and we can also treat a field \\(\\F\\) over itself as a vector space as well. So the word vectors here are not only the usual ones we see in \"vectors\") c) an operation of vector addition \\(\\u + \\v\\) between every pair of vectors \\(\\u, \\v \\in V\\) and d) an operation of scalar multiplication \\(\\lambda\\v\\) between every \\(\\lambda \\in \\F\\) and every vector \\(\\v \\in V\\) . Furthermore, all elements in \\(V\\) must satisfy the closure properties and the following axioms: \\(\\textbf{V(0)}: \\textbf{Closure (Well Defined)}\\) : For all \\(\\mathbf{u,v} \\in V, \\mathbf{u+v} \\in V\\) ; For all \\(\\lambda \\in \\F\\) and \\(\\v \\in V\\) , then \\(\\lambda \\v \\in V\\) ; Consequently, all linear combinations of vectors in \\(V\\) is in \\(V\\) . \\(\\textbf{V(1)}: \\textbf{Existence of the Zero Vector (Additive Identity)}\\) : There exists a vector \\(\\mathbf{0} \\in V\\) , called the zero vector, such that \\(\\mathbf{v + 0 = u = 0+v}\\) for all \\(\\v \\in V\\) . We should know it is unique. \\(\\textbf{V(2)}: \\textbf{Commutative Law for Vector Addition}\\) : For all \\(\\mathbf{u,v} \\in V\\) , \\(\\mathbf{u+v = v+u}\\) . \\(\\textbf{V(3)}: \\textbf{Associative Law for Vector Addition}\\) : For all \\(\\mathbf{u,v,w} \\in V\\) , we have \\(\\mathbf{u + (v+w)} = \\mathbf{(u+v)+w}\\) . \\(\\textbf{V(4)}: \\textbf{Existence of Additive Inverse}\\) : For every vector \\(\\v \\in V\\) , there exists a vector \\(-\\v \\in V\\) such that \\(\\mathbf{v+(-v) = 0 = (-v)+v}\\) . \\(\\textbf{V(5)}: \\textbf{Existence of the Multiplicative Identity}\\) : There exists \\(\\mathbf{1}\\in V\\) such that \\(\\mathbf{1} \\times \\v = \\v \\times \\mathbf{1} = \\v\\) for all \\(\\v \\in \\F\\) . \\(\\textbf{V(6)}: \\textbf{Associative Law for Vector-Scalar Multiplication}\\) : For all \\(a,b \\in \\F\\) and \\(\\v \\in V\\) , we have \\(a(b\\v) = (ab)\\v\\) \\(\\textbf{V(7)}: \\textbf{Distributive Law}\\) : \\(a(\\mathbf{u+v}) = a\\u + a\\v ,~~~\\forall a \\in \\F, \\mathbf{u,v} \\in V\\) \\((a+b)\\v = a\\v + b\\v,~~~\\forall a,b \\in \\F, \\v \\in V\\) .","title":"Definition (Vector Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_subspaces/#vector-space-over-a-field","text":"Warning Note carefully that a vector space \\(V\\) is dependent on the field \\(\\F\\) since scalar multiplication draws \\(\\lambda\\) from \\(\\F\\) . Thus, we don't usually talk only about vector space \\(V\\) alone, instead we use the notion of Vector Space over a Field \\(\\F\\) .","title":"Vector Space over a Field"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_subspaces/#example-of-vector-spaces-over-a-field","text":"Real Vector Space We can imagine elements \\(\\v \\in V\\) as vectors/lines/points. We can take the field \\(\\F\\) to be \\(\\R\\) . Then a vector space \\(V\\) over \\(\\R\\) is called the real vector space .","title":"Example of Vector Spaces over a Field"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_subspaces/#subspaces","text":"","title":"Subspaces"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_subspaces/#geometric-definition-subspaces","text":"For a better treatment of understanding subspaces geometrically, do read page 79-80 of the book Linear Algebra: Theory, Intuition, Code .","title":"Geometric Definition (Subspaces)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_subspaces/#algebraic-definition-subspaces","text":"Algebraic Definition (Subspaces) Let \\(V\\) be a vector space over a field \\(\\F\\) . A non empty subset \\(W \\subseteq V\\) of a vector space \\(V\\) is called a subspace of \\(V\\) if \\(W\\) is itself a vector space using the same vector addition and scalar multiplication as in \\(V\\) . Consequence Let \\(W \\subset V\\) and \\(\\w \\in W\\) , we must show that \\(W\\) obeys all conditions from the definition of a vector space . For any \\(\\w \\in W\\) , we must have \\(w \\in V\\) by definition of a subset; this means that \\(V2,V3,V5,V6,V7\\) are automatically true since \\(\\w \\in W \\subset V\\) . We just need to check if it satisfies \\(V0,V1,V6\\) , namely, the two closure properties, the existence of zero vector, and the existence of the additive inverse for vector addition. We take V2 as an example, take any two elements \\(\\w_1, \\w_2 \\in W\\) , then \\(\\w_1, \\w_2 \\in V\\) , and hence obeys V2. But we cannot say for sure about V1, as there is no guarantee the subset \\(W\\) will contain the zero vector!","title":"Algebraic Definition (Subspaces)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_subspaces/#example-and-intuition-of-subspaces","text":"Example and Intuition of Subspaces Let us start with an informal treatment of a subspace . First, we say that a subspace \\(W\\) of \\(V\\) must fulfill two conditions: \\(W\\) is a subset of \\(V\\) and; \\(W\\) is a vector space itself over the field \\(\\F\\) . The classic example of a \\(3\\) -dimensional real vector space is the Euclidean space \\(\\mathbb R^3\\) where \\(\\R^3 = \\{(x,y,z)~|~x,y,z \\in \\R\\}\\) , which is the 3-dimensional space, or to be more formal, it is the vector space \\(V\\) over \\(\\R\\) , a collection of all 3-tuple vectors. Now, consider the set \\(L = \\{\\lambda(1,2,3) ~|~ \\lambda \\in \\R\\}\\) , we check that the set \\(L\\) informally satisfies our conditions to be a subspace of \\(\\R^3\\) . I emphasized the word set because of the idea of subset , that is, a vector subspace first must be a subset of the vector space . Our \\(L\\) is a subset of \\(\\R^3\\) by definition, in fact, it is the set of all lines of the form \\(\\lambda(1,2,3)\\) . Only being a subset is not sufficient to be called a subspace , we have fulfilled the idea of a sub set, but not the space part. It suffices to check if our \\(L\\) is a valid vector space over \\(\\R\\) . And to check that we can use back the definition of vector space to do so. The main point is that a single vector, \\((1,2,3)\\) scaled by any \\(\\lambda \\in \\R\\) makes up a subspace . It also happens to be the 1D-subspace which is just a line. It is also important to mention that the line must pass through the origin to be considered a subspace . Next, consider the set \\(P = \\{\\lambda_1(1,0,0) + \\lambda_2(0,0,1) ~|~ \\lambda_i \\in \\R\\}\\) . Geometrically, we have known from earlier that these two vectors form a 2D-plane and we will show later that it is the 2D-subspace, a plane.","title":"Example and Intuition of Subspaces"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_subspaces/#theorem-the-subspace-criterion","text":"From the previous section, we know that we just need to check V0, V1 and V4 to be true. This reduces our conditions for \\(W \\subset V\\) to be a subspace to be as follows: Theorem (The Subspace Criterion) Let \\(V\\) be a vector space over a field \\(\\F\\) . A subset \\(W\\) of \\(V\\) is a subspace of \\(V\\) if and only if S1) \\(\\textbf{(Containing the Zero Vector)}\\) : \\(\\0 \\in W\\) ; S2) \\(\\textbf{(Closure under Vector Addition)}\\) : \\(\\forall \\mathbf{u,w} \\in W, \\u+\\w \\in W\\) S3) \\(\\textbf{(Closure under Scalar Multiplication)}\\) : \\(\\forall \\lambda \\in \\F\\) and \\(\\w \\in W\\) , we have \\(\\lambda\\w \\in W\\) .","title":"Theorem (The Subspace Criterion)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_subspaces/#proof","text":"Proof \\(\\Rightarrow\\) If \\(W\\) is also a subspace , then by the definition of subspace, \\(W\\) is also a vector space, and hence \\(S1,S2,S3\\) are satisfied. \\(\\Leftarrow\\) We should be clear that for other conditions V2, V3, V5, V6 and V7 do not warrant checking since these axioms are automatically satisfied for \\(W\\) since it holds for the larger subset \\(V\\) . We need only check the following: Zero Vector: If S1 is satisfied, then the corresponding V1 is satisfied. Closure Properties: S2 and S3 both implies V0. Additive Inverse for Vector Addition : Let \\(\\w \\in W\\) , take \\(-1 \\in \\F\\) , by S3 (closure of scalar multiplication), we must have \\(-1(\\w) \\in W\\) .$ This implies V4. One thing that you notice is that by the closure of addition \\(\\mathbf{w+(-w)} = \\mathbf{0} \\in W\\) or by the closure of scalar multiplication \\(0 \\cdot \\w = \\0 \\in W\\) (note \\(0 \\in \\F\\) ). Consequently, this implies that S2 and S3 are sufficient to imply S1 . You can trace the proof by ignoring the S1.","title":"Proof"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_subspaces/#linear-combination-definition-revisited","text":"Definition Let \\(V\\) be a vector space over a field \\(\\F\\) . A vector \\(\\v\\) is a linear combination of vectors \\(\\mathbf{v_1,v_2,...,v_m} \\in V\\) if \\( \\(\\v = \\lambda_1\\v_1+...+\\lambda_m\\mathbf{v_m}\\) \\) for some scalars \\(\\lambda_i \\in \\F\\) .","title":"Linear Combination (Definition revisited)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_subspaces/#theorem-closure-linear-combination-implies-subspace","text":"Theorem (Closure Linear Combination implies Subspace) Let \\(V\\) be a vector space over a field \\(\\F\\) and \\(W\\subseteq V\\) a non empty subset. Then \\(W\\) is a subspace of \\(V\\) if and only if \\(W\\) is closed under linear combination, (i.e \\(\\forall \\lambda_i \\in \\F, \\forall \\mathbf{w_i} \\in W \\Rightarrow \\lambda_1\\w_1+ \\lambda_2\\w_2 \\in W\\) ). This should not come as a surprise as this is just a rebranding of Conditions to be a Subspace Theorem (Equivalent Subspace Definition) .","title":"Theorem (Closure Linear Combination implies Subspace)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_subspaces/#vector-subspaces-of-mathbbr2","text":"There are three types of vector subspaces of the vector space \\(\\mathbb{R}^2\\) . i) The origin \\(\\mathbf{0} = \\{0,0\\}\\) ii) The line through the origin : \\(L = \\{\\lambda\\v~|~\\lambda \\in \\mathbb{R}, \\v \\in \\R^2\\}\\) iii) \\(\\mathbb{R}^2\\)","title":"Vector subspaces of \\(\\mathbb{R}^2\\)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_subspaces/#examples-of-non-subspace","text":"Non-Subspace Consider the set \\(L = \\{\\lambda (0, -3) ~|~ \\lambda \\in \\R\\}\\) , why is it not a subspace of \\(\\R^2\\) ? Though one can answer that since this line does not pass through the origin, and thus does not contain the zero vector, hence not a subspace, we can take a step back and think of it geometrically: We can check whether the line is closed under addition and vector-scalar multiplication. Geometrically, it seems that any two vectors inside the set will still be in the set since adding or subtracting vectors inside is just \"line + line | line - line\", which still lies on the same line. It is easy to gloss over that if you take \\(\\v\\) and \\(-\\v\\) from the set \\(L\\) , then the sum is the zero vector, which is not inside \\(L\\) . This can be further verified by vector-scalar, we can just take \\(0 \\in \\R\\) , and claim that \\(0\\v\\) for all \\(\\v \\in L\\) is not in \\(L\\) !","title":"Examples of Non-Subspace"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_subspaces/#visualization-of-subspaces-in-mathbbr2","text":"Courtesy of MacroAnalyst's Linear Algebra with Python . Notice that the line \\(y = -3 + \\frac{2}{3}x\\) does not pass through the origin \\((0, 0)\\) , and hence not a subspace. Note this example (line) is the set \\(L\\) in the previous example. import matplotlib.pyplot as plt import numpy as np fig , ax = plt . subplots ( figsize = ( 10 , 10 )) ####################### Arrows ####################### x = np . arange ( - 10 , 11 , 1 ) y = 4 / 6 * x ax . plot ( x , y , lw = 4 , color = \"blue\" , label = r \"$y = \\frac {2}{3} x$, subspace of $\\mathbf {R} ^2$\" , ) y = - 3 + 4 / 6 * x ax . plot ( x , y , lw = 4 , color = \"red\" , label = r \"$y = -3+\\frac {2}{3} x$, not a subspace of $\\mathbf {R} ^2$\" , ) ax . grid ( True ) ax . set_title ( \"Visualization of Subspace in $R^2$ \" , size = 18 ) ax . scatter ( 0 , 0 , s = 100 , fc = \"black\" , ec = \"red\" ) ax . text ( - 2 , 0 , \"$(0,0)$\" , size = 18 ) ax . legend ( fontsize = 16 ) ax . axis ([ - 10 , 10 , - 10 , 10 ]) ax . set_xlabel ( \"x-axis\" , size = 18 ) ax . set_ylabel ( \"y-axis\" , size = 18 ) plt . show ()","title":"Visualization of Subspaces in \\(\\mathbb{R}^2\\)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_subspaces/#vector-subspaces-of-mathbbr3","text":"There are four types of vector subspaces of the vector space \\(\\mathbb{R}^3\\) . i) The origin \\(\\mathbf{0} = \\{0,0,0\\}\\) ii) The line \\(L\\) passing through the origin: \\(L = \\{\\lambda \\v~|~\\lambda \\in \\mathbb{R}, \\v \\in \\R^3\\}\\) iii) A plane \\(P\\) passing through the origin: \\(P = \\{(x,y,z) \\in \\mathbb{R}^3~|~ ax + by + cz = 0\\}\\) iv) \\(\\mathbb{R}^3\\)","title":"Vector subspaces of \\(\\mathbb{R}^3\\)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_subspaces/#visualization-of-subspaces-in-mathbbr3","text":"Courtesy of MacroAnalyst's Linear Algebra with Python . We have not learnt of the term span , but for now when we say a span of two vectors \\(\\u\\) and \\(\\v\\) in \\(\\R^3\\) , it means these two vectors are not multiple of each other. Consider a span of two vectors \\(u = (1,-2,1)^T\\) and \\(v=(2,1,2)^T\\) . The span of \\((u,v)\\) is a subspace of \\(\\R^3\\) , where \\(s\\) and \\(t\\) are the scalars of the vectors. We also plot a plane which is not a subspace by adding \\(2\\) to the third equation: \\(z = s + 2t + 2\\) . import matplotlib.pyplot as plt import numpy as np # %matplotlib # %matplotlib notebook, use this only if you are in Jupyter Notebook fig = plt . figure ( figsize = ( 8 , 8 )) ax = fig . add_subplot ( 111 , projection = \"3d\" ) s = np . linspace ( - 1 , 1 , 10 ) t = np . linspace ( - 1 , 1 , 10 ) S , T = np . meshgrid ( s , t ) X = S + 2 * T Y = - 2 * S + T Z = S + 2 * T ax . plot_surface ( X , Y , Z , alpha = 0.9 , cmap = plt . cm . coolwarm ) Z2 = S + 2 * T + 2 # this is not a subspace anymore ax . plot_surface ( X , Y , Z2 , alpha = 0.6 , cmap = plt . cm . jet ) ax . scatter ( 0 , 0 , 0 , s = 200 , color = \"red\" ) ax . text ( 0 , 0 , 0 , \"$(0,0,0)$\" , size = 18 , zorder = 5 ) ax . set_title ( \"Visualization of Subspace of $\\mathbb {R} ^3$\" , x = 0.5 , y = 1.1 , size = 20 ) ax . set_xlabel ( \"x-axis\" , size = 18 ) ax . set_ylabel ( \"y-axis\" , size = 18 ) ax . set_zlabel ( \"z-axis\" , size = 18 ) ax . view_init ( elev =- 29 , azim = 132 ) plt . show ()","title":"Visualization of Subspaces in \\(\\mathbb{R}^3\\)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_subspaces/#theorem-intersection-of-subspaces-is-a-subspace","text":"Theorem (Intersection of Subspaces is a Subspace) Let \\(V\\) be a vector space over a field \\(\\F\\) and let \\(W_\\alpha \\subseteq V\\) ( \\(\\alpha \\in I\\) ) be vector subspaces of \\(V\\) , then the intersection \\( \\(\\cap_{\\alpha \\in I}W_\\alpha\\) \\) is again a vector subspace of \\(V\\) .","title":"Theorem (Intersection of Subspaces is a Subspace)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_subspaces/#proof_1","text":"Proof Before we start, the intuition is simple, call \\(C = A \\cap B\\) , and to think intuitively that \\(C\\) is itself a subspace, we just take an element \\(\\mathbf{c_1}, \\mathbf{c_2} \\in C\\) and \\(\\lambda \\in \\F\\) , and ask whether these vectors obey the closure property. For a start, for any two vectors in \\(C\\) , they are closed under addition because both vectors are elements of \\(A\\) and (or) \\(B\\) . Let \\(W_1\\) and \\(W_2\\) be subspaces of a vector space \\(V\\) , then it suffices for us to just prove \\(W_1 \\cap W_2\\) is a subspace of \\(V\\) as we can use induction to prove for \\(n\\) intersections. We denote the subset \\(W_1 \\cap W_2\\) to be \\(U\\) . Closed under Vector Addition: To show that \\(U\\) has closure properties, we take any element \\(\\mathbf{u_1}, \\mathbf{u_2} \\in U\\) , any \\(\\lambda \\in \\mathbb{F}\\) and show that \\(\\mathbf{u_1} + \\mathbf{u_2} \\in U\\) and \\(\\lambda \\mathbf{u} \\in U\\) . Since \\(\\mathbf{u_1}, \\mathbf{u_2} \\in U\\) , then \\(\\mathbf{u_1}, \\mathbf{u_2} \\in W_1\\) and \\(\\mathbf{u_1}, \\mathbf{u_2} \\in W_2\\) respectively. This means that \\(\\mathbf{u_1} + \\mathbf{u_2} \\in W_1\\) and \\(\\mathbf{u_1} + \\mathbf{u_2} \\in W_2\\) since both \\(W_1\\) and \\(W_2\\) are closed under addition. Consequently, another way of saying \\(\\mathbf{u_1} + \\mathbf{u_2} \\in W_1\\) and \\(\\mathbf{u_1} + \\mathbf{u_2} \\in W_2\\) is \\(\\mathbf{u_1} + \\mathbf{u_2} \\in W_1 \\cap W_2 = U\\) , which implies that \\(U\\) is closed under vector addition. Closed under Vector-Scalar Multiplication: The same line of logic can be applied for closure under vector-scalar multiplication. We have shown that \\(W_1 \\cap W_2\\) is a subspace.","title":"Proof"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_subspaces/#union-of-two-subspaces-may-not-be-a-subspace","text":"The union of two subspaces may not be a subspace. The reason why this can happen is that all vector spaces, and hence subspaces too, must be closed under addition (and scalar multiplication). The union of two subspaces takes all the elements already in those spaces, and nothing more. In the union of subspaces \\(W_1\\) and \\(W_2\\) , there are new combinations of vectors we can add together that we couldn't before, like \\(\\w_1 + \\w_2\\) where \\(\\w_1 \\in W_1\\) and \\(\\w_2 \\in W_2\\) . For example, take \\(W_1\\) to be the \\(x\\) -axis and \\(W_2\\) the \\(y\\) -axis, both subspaces of \\(\\mathbb{R}^2\\) . Their union includes both \\((3,0)\\) and \\((0,5)\\) , whose sum, \\((3,5)\\) , is not in the union. Hence, the union is not a vector space.","title":"Union of Two Subspaces may not be a Subspace"},{"location":"reighns_ml_journey/mathematics/linear_algebra/03_vector_spaces/03_linear_algebra_vector_spaces_vector_subspaces/#ambient-space","text":"The term ambient space is used frequently in the book. We can understand it with an example: Example If you are dealing with a vector subspace \\(U\\) as a subset of \\(\\mathbb R^2\\) , the ambient space is \\(\\mathbb R^2\\) . So basically, the ambient space is the \"parent space\" of the subspace.","title":"Ambient Space"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix/","text":"\\[ \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\C}{\\mathbb{C}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\Q}{\\mathbf{Q}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\q}{\\mathbf{q}} \\newcommand{\\e}{\\mathbf{e}} \\newcommand{\\I}{\\mathbf{I}} \\] Definition (Matrix) A matrix is a rectangular array of numbers (or other mathematical objects), called the entries of the matrix. Matrices are subject to standard operations such as addition and multiplication . Most commonly, a matrix over a field \\(\\F\\) is a rectangular array of elements of \\(\\F\\) . A real matrix and a complex matrix are matrices whose entries are respectively real numbers or complex numbers . More general types of entries are discussed below . For instance, this is a real matrix: \\[\\mathbf{A} = \\begin{bmatrix} -1.3 & 0.6 \\\\ 20.4 & 5.5 \\\\ 9.7 & -6.2 \\end{bmatrix}\\] The numbers, symbols, or expressions in the matrix are called its entries or its elements . The horizontal and vertical lines of entries in a matrix are called rows and columns , respectively. Notation (Matrix) Matrices are commonly written in box brackets or parentheses : \\[\\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\end{bmatrix} = \\begin{pmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\end{pmatrix}=\\left(a_{ij}\\right) \\in \\mathbb{R}^{m \\times n}\\] The specifics of symbolic matrix notation vary widely, with some prevailing trends. Matrices are usually symbolized using upper-case letters (such as A in the examples above). If we say a matrix \\(\\A \\in \\F^{m \\times n}\\) , then this means: \\(\\A\\) has \\(m\\) rows and \\(n\\) columns. \\(\\A\\) is a matrix with elements drawn from the field \\(\\F\\) . Furthermore, we read the \\((i, j)\\) -th entry of the matrix \\(\\A\\) as \\(\\A_{i, j}\\) . Definition (Dimension and Size of a Matrix) As detailed in Notation (Matrix) , we usually denote the size of a matrix as \\(\\A \\in \\F^{m \\times n}\\) .","title":"Matrix"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix/#definition-matrix","text":"A matrix is a rectangular array of numbers (or other mathematical objects), called the entries of the matrix. Matrices are subject to standard operations such as addition and multiplication . Most commonly, a matrix over a field \\(\\F\\) is a rectangular array of elements of \\(\\F\\) . A real matrix and a complex matrix are matrices whose entries are respectively real numbers or complex numbers . More general types of entries are discussed below . For instance, this is a real matrix: \\[\\mathbf{A} = \\begin{bmatrix} -1.3 & 0.6 \\\\ 20.4 & 5.5 \\\\ 9.7 & -6.2 \\end{bmatrix}\\] The numbers, symbols, or expressions in the matrix are called its entries or its elements . The horizontal and vertical lines of entries in a matrix are called rows and columns , respectively.","title":"Definition (Matrix)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix/#notation-matrix","text":"Matrices are commonly written in box brackets or parentheses : \\[\\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\end{bmatrix} = \\begin{pmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\end{pmatrix}=\\left(a_{ij}\\right) \\in \\mathbb{R}^{m \\times n}\\] The specifics of symbolic matrix notation vary widely, with some prevailing trends. Matrices are usually symbolized using upper-case letters (such as A in the examples above). If we say a matrix \\(\\A \\in \\F^{m \\times n}\\) , then this means: \\(\\A\\) has \\(m\\) rows and \\(n\\) columns. \\(\\A\\) is a matrix with elements drawn from the field \\(\\F\\) . Furthermore, we read the \\((i, j)\\) -th entry of the matrix \\(\\A\\) as \\(\\A_{i, j}\\) .","title":"Notation (Matrix)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix/#definition-dimension-and-size-of-a-matrix","text":"As detailed in Notation (Matrix) , we usually denote the size of a matrix as \\(\\A \\in \\F^{m \\times n}\\) .","title":"Definition (Dimension and Size of a Matrix)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_matrix_types/","text":"\\[ \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\C}{\\mathbb{C}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\Q}{\\mathbf{Q}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\q}{\\mathbf{q}} \\newcommand{\\e}{\\mathbf{e}} \\newcommand{\\I}{\\mathbf{I}} \\] import numpy as np import sympy as sy sy . init_printing () Matrix Types Rectangular Matrices A matrix \\(\\A \\in \\R^{m \\times n}\\) is a rectangular matrix with \\(m\\) rows and \\(n\\) columns Square Matrices A square matrix \\(\\A \\in \\R^{n \\times n}\\) is a special case of a rectangular matrix, where it has equal number of rows and columns. Main Diagonal The main diagonal of a square matrix \\(\\A\\) is the entries \\(\\A_{i, i}\\) . Diagonal and Triangular Matrix Diagonal and Triangular Matrix can appear in various matrix decompositions, and of course, eigendecomposition. Upper Triangular Matrix If all entries of A below the main diagonal are zero, A is called an upper triangular matrix . \\[ \\begin{bmatrix} a_{11} & a_{12} & a_{13} \\\\ 0 & a_{22} & a_{23} \\\\ 0 & 0 & a_{33} \\\\ \\end{bmatrix} \\] Lower Triangular Matrix If all entries of A above the main diagonal are zero, A is called an lower triangular matrix . \\[ \\begin{bmatrix} a_{11} & 0 & 0 \\\\ a_{21} & a_{22} & 0 \\\\ a_{31} & a_{32} & a_{33} \\\\ \\end{bmatrix} \\] Diagonal Matrix If all entries outside the main diagonal are zero, A is called a diagonal matrix . \\[ \\begin{bmatrix} a_{11} & 0 & 0 \\\\ 0 & a_{22} & 0 \\\\ 0 & 0 & a_{33} \\\\ \\end{bmatrix} \\] Identity Matrices (Multiplicative Identity) The identity matrix \\(\\I\\) of size n is the n -by- n matrix in which all the elements on the main diagonal are equal to 1 and all other elements are equal to 0. Note that the Identity Matrix , as its name suggest, is the identity element which maps any matrices to \\(\\1\\) . See our section on Fields for a refresher on the Multiplicative Identity . For example, \\[\\mathbf{I}_1 = \\begin{bmatrix} 1 \\end{bmatrix}, \\ \\mathbf{I}_2 = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}, \\ \\ldots , \\ \\mathbf{I}_n = \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\ 0 & 1 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 1 \\end{bmatrix}\\] It is a square matrix of order n , and also a special kind of diagonal matrix . It is called an identity matrix because multiplication with it leaves a matrix unchanged, that is, for any matrix \\(\\A \\in \\R^{m \\times n}\\) and any vector \\(\\v\\) of the appropriate size, then we have: \\[ \\A\\I_n = \\I_m \\A = \\A \\] and \\[\\v\\I = \\v\\] Zero Matrices (Additive Identity) A matrix \\(\\A \\in \\R^{m \\times n}\\) is the zero matrix if all entries are \\(0\\) . Note that this is the additive identity and fulfills the property of additive identity. Symmetric and Skew-Symmetric Matrices Symmetric Matrix A quare matrix A that is equal to its transpose, that is: \\[ \\A = \\A^\\top \\] Visually, a symmetric matrix is a \"mirrored\" matrix where the top and bottom of its diagonals are \"flipped/mirrored\". Note that only square matrix can be symmetric. symmetric_matrix = sy . Matrix ([[ 1 , 3.14 , 2.71 ], [ 3.14 , 7 , 2 ], [ 2.71 , 2 , 0 ]]) symmetric_matrix \\(\\displaystyle \\left[\\begin{matrix}1 & 3.14 & 2.71\\\\3.14 & 7 & 2\\\\2.71 & 2 & 0\\end{matrix}\\right]\\) symmetric_matrix . T == symmetric_matrix True The author noted that symmetric matrices is very useful and has many attractive properties. He also mentioned that creating symmetric matrices from non-symmetric matrices are central to many statistics and machine learning applications, such as least-squares and eigendecomposition. For example, you might have heard of a \"covariance matrix\"; this is simply a symmetric matrix created from a rectangular data matrix. - Mike X Cohen: Linear Algebra: Theory, Intuition, Code, 2021. (pp. 114) Skew-Symmetric If instead, \\(\\A\\) is equal to the negative of its transpose, that is: \\[\\A = -\\A^\\top\\] then \\(\\A\\) is a skew-symmetric matrix. skewed_symmetric_matrix = sy . Matrix ([[ 0 , - 3.14 , - 2.71 ], [ 3.14 , 0 , 2 ], [ 2.71 , - 2 , 0 ]]) skewed_symmetric_matrix \\(\\displaystyle \\left[\\begin{matrix}0 & -3.14 & -2.71\\\\3.14 & 0 & 2\\\\2.71 & -2 & 0\\end{matrix}\\right]\\) skewed_symmetric_matrix . T == - 1 * skewed_symmetric_matrix True Note to the readers that the diagonal of skewed-symmetric matrices are always zero, because by definition, the diagonal entry must fulfil: \\[\\A_{i, i} = -\\A_{i, i}\\] Dense and Sparse Matrix Dense Matrix A dense matrix is a matrix with most or all of its elements/entries being non-zero. Sparse Matrix A sparse matrix is a matrix with most or all of its elements/entries being zero. Sparse matrix is an important topic in modern numerical analysis because it is an efficient matrix. Orthogonal Matrix An orthogonal matrix is a square matrix \\(\\Q\\) with real entries whose columns and rows are orthogonal unit vectors (that is, orthonormal vectors). This means: All columns of the matrix are pairwise orthogonal, which means for any \\(\\q_i, \\q_j\\) of the columns \\(\\Q\\) , we have \\(\\q_i \\cdot \\q_j = \\0\\) Each column of the matrix has unit length and are therefore unit vectors: \\(\\Vert \\q_i \\Vert = \\1\\) . From the above, we can deduce a notation: $$ \\Q_i \\cdot \\Q_j = \\begin{cases} 1\\hspace{0.5cm} \\text{if } i=j \\ 0\\hspace{0.5cm} \\text{if } i \\neq j \\end{cases} $$ To make the notation/expression more compact, we can write that a matrix \\(\\Q\\) is orthogonal if: \\[ \\Q^\\top \\Q = \\I \\] Equivalently, we have its transpose is equal to its inverse : \\( \\(\\mathbf{A}^\\mathrm{T}=\\mathbf{A}^{-1}, \\,\\) \\) which entails \\( \\(\\mathbf{A}^\\mathrm{T} \\mathbf{A} = \\mathbf{A} \\mathbf{A}^\\mathrm{T} = \\mathbf{I}_n,\\) \\) where I is the identity matrix of size n . Importance of A transpose A This matrix is so important that it must be mentioned here even if we have not learnt some concepts. Here are the properties one should remember, with reference from Mike X Cohen: Linear Algebra: Theory, Intuition, Code, 2021. (pp. 117) : \\(\\A^\\top\\A\\) is a square matrix, even if \\(\\A\\) is not square. Positive semidefinite- ness \\(\\A^\\top\\A\\) has real and positive eigenvalues The trace is positive (the trace is the sum of eigenvalues) The determinant is positive (the determinant is the product of the eigenvalues) The diagonal entries are all positive \\(\\A^\\top\\A\\) has Orthogonal eigenvectors . Diagonalizable as \\(Q\\Lambda Q^T\\) \\(\\A^\\top\\A\\) is full rank if \\(\\A\\) is full column rank. Rank of \\(A^TA\\) is the same as rank of \\(A\\) . \\(\\text{ker}(A^TA)=\\text{ker}(A)\\) \\(\\A^\\top\\A\\) is symmetric, even if \\(\\A\\) is not symmetric. \\(\\A^\\top\\A\\) is invertible if \\(\\A\\) is full column rank. \\(\\A^\\top\\A\\) and \\(\\A\\) have the same row space. \\(\\A^\\top\\A\\) is often called a covariance matrix 1 . https://math.stackexchange.com/questions/1896628/properties-of-the-a-transpose-a-matrix \u21a9","title":"Basic Matrix Types"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_matrix_types/#matrix-types","text":"","title":"Matrix Types"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_matrix_types/#rectangular-matrices","text":"A matrix \\(\\A \\in \\R^{m \\times n}\\) is a rectangular matrix with \\(m\\) rows and \\(n\\) columns","title":"Rectangular Matrices"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_matrix_types/#square-matrices","text":"A square matrix \\(\\A \\in \\R^{n \\times n}\\) is a special case of a rectangular matrix, where it has equal number of rows and columns.","title":"Square Matrices"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_matrix_types/#main-diagonal","text":"The main diagonal of a square matrix \\(\\A\\) is the entries \\(\\A_{i, i}\\) .","title":"Main Diagonal"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_matrix_types/#diagonal-and-triangular-matrix","text":"Diagonal and Triangular Matrix can appear in various matrix decompositions, and of course, eigendecomposition.","title":"Diagonal and Triangular Matrix"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_matrix_types/#upper-triangular-matrix","text":"If all entries of A below the main diagonal are zero, A is called an upper triangular matrix . \\[ \\begin{bmatrix} a_{11} & a_{12} & a_{13} \\\\ 0 & a_{22} & a_{23} \\\\ 0 & 0 & a_{33} \\\\ \\end{bmatrix} \\]","title":"Upper Triangular Matrix"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_matrix_types/#lower-triangular-matrix","text":"If all entries of A above the main diagonal are zero, A is called an lower triangular matrix . \\[ \\begin{bmatrix} a_{11} & 0 & 0 \\\\ a_{21} & a_{22} & 0 \\\\ a_{31} & a_{32} & a_{33} \\\\ \\end{bmatrix} \\]","title":"Lower Triangular Matrix"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_matrix_types/#diagonal-matrix","text":"If all entries outside the main diagonal are zero, A is called a diagonal matrix . \\[ \\begin{bmatrix} a_{11} & 0 & 0 \\\\ 0 & a_{22} & 0 \\\\ 0 & 0 & a_{33} \\\\ \\end{bmatrix} \\]","title":"Diagonal Matrix"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_matrix_types/#identity-matrices-multiplicative-identity","text":"The identity matrix \\(\\I\\) of size n is the n -by- n matrix in which all the elements on the main diagonal are equal to 1 and all other elements are equal to 0. Note that the Identity Matrix , as its name suggest, is the identity element which maps any matrices to \\(\\1\\) . See our section on Fields for a refresher on the Multiplicative Identity . For example, \\[\\mathbf{I}_1 = \\begin{bmatrix} 1 \\end{bmatrix}, \\ \\mathbf{I}_2 = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}, \\ \\ldots , \\ \\mathbf{I}_n = \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\ 0 & 1 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 1 \\end{bmatrix}\\] It is a square matrix of order n , and also a special kind of diagonal matrix . It is called an identity matrix because multiplication with it leaves a matrix unchanged, that is, for any matrix \\(\\A \\in \\R^{m \\times n}\\) and any vector \\(\\v\\) of the appropriate size, then we have: \\[ \\A\\I_n = \\I_m \\A = \\A \\] and \\[\\v\\I = \\v\\]","title":"Identity Matrices (Multiplicative Identity)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_matrix_types/#zero-matrices-additive-identity","text":"A matrix \\(\\A \\in \\R^{m \\times n}\\) is the zero matrix if all entries are \\(0\\) . Note that this is the additive identity and fulfills the property of additive identity.","title":"Zero Matrices (Additive Identity)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_matrix_types/#symmetric-and-skew-symmetric-matrices","text":"","title":"Symmetric and Skew-Symmetric Matrices"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_matrix_types/#symmetric-matrix","text":"A quare matrix A that is equal to its transpose, that is: \\[ \\A = \\A^\\top \\] Visually, a symmetric matrix is a \"mirrored\" matrix where the top and bottom of its diagonals are \"flipped/mirrored\". Note that only square matrix can be symmetric. symmetric_matrix = sy . Matrix ([[ 1 , 3.14 , 2.71 ], [ 3.14 , 7 , 2 ], [ 2.71 , 2 , 0 ]]) symmetric_matrix \\(\\displaystyle \\left[\\begin{matrix}1 & 3.14 & 2.71\\\\3.14 & 7 & 2\\\\2.71 & 2 & 0\\end{matrix}\\right]\\) symmetric_matrix . T == symmetric_matrix True The author noted that symmetric matrices is very useful and has many attractive properties. He also mentioned that creating symmetric matrices from non-symmetric matrices are central to many statistics and machine learning applications, such as least-squares and eigendecomposition. For example, you might have heard of a \"covariance matrix\"; this is simply a symmetric matrix created from a rectangular data matrix. - Mike X Cohen: Linear Algebra: Theory, Intuition, Code, 2021. (pp. 114)","title":"Symmetric Matrix"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_matrix_types/#skew-symmetric","text":"If instead, \\(\\A\\) is equal to the negative of its transpose, that is: \\[\\A = -\\A^\\top\\] then \\(\\A\\) is a skew-symmetric matrix. skewed_symmetric_matrix = sy . Matrix ([[ 0 , - 3.14 , - 2.71 ], [ 3.14 , 0 , 2 ], [ 2.71 , - 2 , 0 ]]) skewed_symmetric_matrix \\(\\displaystyle \\left[\\begin{matrix}0 & -3.14 & -2.71\\\\3.14 & 0 & 2\\\\2.71 & -2 & 0\\end{matrix}\\right]\\) skewed_symmetric_matrix . T == - 1 * skewed_symmetric_matrix True Note to the readers that the diagonal of skewed-symmetric matrices are always zero, because by definition, the diagonal entry must fulfil: \\[\\A_{i, i} = -\\A_{i, i}\\]","title":"Skew-Symmetric"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_matrix_types/#dense-and-sparse-matrix","text":"","title":"Dense and Sparse Matrix"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_matrix_types/#dense-matrix","text":"A dense matrix is a matrix with most or all of its elements/entries being non-zero.","title":"Dense Matrix"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_matrix_types/#sparse-matrix","text":"A sparse matrix is a matrix with most or all of its elements/entries being zero. Sparse matrix is an important topic in modern numerical analysis because it is an efficient matrix.","title":"Sparse Matrix"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_matrix_types/#orthogonal-matrix","text":"An orthogonal matrix is a square matrix \\(\\Q\\) with real entries whose columns and rows are orthogonal unit vectors (that is, orthonormal vectors). This means: All columns of the matrix are pairwise orthogonal, which means for any \\(\\q_i, \\q_j\\) of the columns \\(\\Q\\) , we have \\(\\q_i \\cdot \\q_j = \\0\\) Each column of the matrix has unit length and are therefore unit vectors: \\(\\Vert \\q_i \\Vert = \\1\\) . From the above, we can deduce a notation: $$ \\Q_i \\cdot \\Q_j = \\begin{cases} 1\\hspace{0.5cm} \\text{if } i=j \\ 0\\hspace{0.5cm} \\text{if } i \\neq j \\end{cases} $$ To make the notation/expression more compact, we can write that a matrix \\(\\Q\\) is orthogonal if: \\[ \\Q^\\top \\Q = \\I \\] Equivalently, we have its transpose is equal to its inverse : \\( \\(\\mathbf{A}^\\mathrm{T}=\\mathbf{A}^{-1}, \\,\\) \\) which entails \\( \\(\\mathbf{A}^\\mathrm{T} \\mathbf{A} = \\mathbf{A} \\mathbf{A}^\\mathrm{T} = \\mathbf{I}_n,\\) \\) where I is the identity matrix of size n .","title":"Orthogonal Matrix"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_matrix_types/#importance-of-a-transpose-a","text":"This matrix is so important that it must be mentioned here even if we have not learnt some concepts. Here are the properties one should remember, with reference from Mike X Cohen: Linear Algebra: Theory, Intuition, Code, 2021. (pp. 117) : \\(\\A^\\top\\A\\) is a square matrix, even if \\(\\A\\) is not square. Positive semidefinite- ness \\(\\A^\\top\\A\\) has real and positive eigenvalues The trace is positive (the trace is the sum of eigenvalues) The determinant is positive (the determinant is the product of the eigenvalues) The diagonal entries are all positive \\(\\A^\\top\\A\\) has Orthogonal eigenvectors . Diagonalizable as \\(Q\\Lambda Q^T\\) \\(\\A^\\top\\A\\) is full rank if \\(\\A\\) is full column rank. Rank of \\(A^TA\\) is the same as rank of \\(A\\) . \\(\\text{ker}(A^TA)=\\text{ker}(A)\\) \\(\\A^\\top\\A\\) is symmetric, even if \\(\\A\\) is not symmetric. \\(\\A^\\top\\A\\) is invertible if \\(\\A\\) is full column rank. \\(\\A^\\top\\A\\) and \\(\\A\\) have the same row space. \\(\\A^\\top\\A\\) is often called a covariance matrix 1 . https://math.stackexchange.com/questions/1896628/properties-of-the-a-transpose-a-matrix \u21a9","title":"Importance of A transpose A"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_operations/","text":"\\[ \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\C}{\\mathbb{C}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\Q}{\\mathbf{Q}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\q}{\\mathbf{q}} \\newcommand{\\e}{\\mathbf{e}} \\newcommand{\\I}{\\mathbf{I}} \\] Matrix Operations Matrix Addition and Subraction The sum of two matrices of the same size \\(m \\times n\\) , \\(\\A\\) and \\(\\B\\) are calculated elementwise: \\[ (\\A \\pm \\B)_{i, j} = \\A_{i, j} \\pm \\B_{i, j} \\] where \\(1 \\leq i \\leq m , \\quad 1 \\leq j \\leq n\\) . Scalar-Matrix Multiplication For any scalar \\(\\lambda \\in \\F\\) , the Matrix-Scalar Multiplication \\(\\lambda \\A\\) is given by: \\[ (\\lambda \\A)_{i, j} = \\lambda \\cdot \\A_{i, j} \\] Commutative of Scalar-Matrix Multiplication Not surprisingly, the operation is commutative such that, given any scalar \\(\\lambda\\) , and any sequence of matrices \\(\\A, \\B\\) , we have: \\[ \\lambda \\A\\B = \\A\\lambda\\B = \\A\\B\\lambda \\] Matrix Operations Fulfill Field Properties In fact, matrix operations fulfill the properties of field properties. That is, for any matrix \\(\\A\\) and \\(\\B\\) of the same shape and size, we have 1 : \\(\\A+ \\B= \\B+ \\A\\) \\((\\A+\\B)+ C=\\A+(\\B+C)\\) \\(c(\\A+\\B)=c\\A+c\\B\\) \\((c+d)\\A=c\\A+c{D}\\) \\(c(d\\A)=(cd)\\A\\) \\(\\A+=\\A\\) , where \\({0}\\) is the zero matrix For any \\(\\A\\) , there exists a \\(-\\A\\) , such that $ \\A+(- \\A)=0$. Although we have not learn matrix multiplication , their properties are: $ \\A({\\B\\mathbf{C}})=({\\A\\B}) \\mathbf{C}$ \\(\\mathbf{C}({\\A\\B})=(\\mathbf{C}\\A)\\B=\\A(\\mathbf{C}\\B)\\) \\(\\A(\\B+ \\mathbf{C})={\\A\\B}+{\\A\\mathbf{C}}\\) \\((\\B+\\mathbf{C})\\A={\\B\\A}+{\\mathbf{C}\\A}\\) Matrix Tranpose Given a matrix \\(\\A \\in \\R^{m \\times n}\\) , the transpose of \\(\\A\\) is denoted \\(\\A^\\top\\) and formed by mapping the rows of \\(\\A\\) to columns and columns of \\(\\A\\) to rows, as illustrated: \\[ (\\A^\\top)_{i, j} = \\A_{j, i} \\] Theorem (A Matrixs transpose is itself) Prove that the transpose of a matrix \\(\\mathbf{A}\\) 's transpose is \\(\\mathbf{A}\\) : \\((\\mathbf{A}^\\top)^\\top = \\mathbf{A}\\) . Proof Consider a matrix \\(A_{n \\times k}\\) as follows, $$\\mathbf{A}=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1k} \\ a_{21} & a_{22} & \\cdots & a_{2k} \\ \\vdots & \\vdots & \\ddots & \\vdots \\ a_{n1} & a_{n2} & \\cdots & a_{nk} \\ \\end{bmatrix} $$ By definition of Transpose , all \\((i,j)\\) entries of \\(A\\) is mapped to \\((j,i)\\) , for example, \\(a_{1,2}\\) becomes \\(a_{2,1}\\) when transposed. Performing a transpose once more will then map all \\((j,i)\\) entries back to \\((i,j)\\) . \\(A\\) is unchanged and thus \\((\\mathbf{A}^\\top)^\\top = \\mathbf{A}\\) . Q.E.D Theorem (Sum of Transpose is Transpose of Sum) Given two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) , show that the sum of transposes is equal to the transpose of a sum: \\(\\mathbf{A}^\\top + \\mathbf{B}^\\top = (\\mathbf{A} + \\mathbf{B})^\\top\\) . Proof Say that we have two matrices \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times k}\\) and \\(\\mathbf{B} \\in \\mathbb{R}^{k \\times m}\\) : \\[\\mathbf{A}=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1k} \\\\ a_{21} & a_{22} & \\cdots & a_{2k} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{n1} & a_{n2} & \\cdots & a_{nk} \\\\ \\end{bmatrix},\\quad \\mathbf{B}=\\begin{bmatrix} b_{11} & b_{12} & \\cdots & b_{1m} \\\\ b_{21} & b_{22} & \\cdots & b_{2m} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ b_{k1} & b_{k2} & \\cdots & b_{km} \\\\ \\end{bmatrix},\\quad \\mathbf{A+B}=\\begin{bmatrix} a_{11}+b_{11} & a_{12}+b_{12} & \\cdots & a_{1m}+b_{1m} \\\\ a_{21}+b_{21} & a_{22}+b_{22} & \\cdots & a_{2m}+b_{2m} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{k1}+b_{k1} & a_{k2}+b_{k2} & \\cdots & a_{km}+b_{km} \\\\ \\end{bmatrix}.\\] Then we can prove it by simply computing the LHS and RHS respectively. Without loss of generality, we pick any pair of point \\(a_{i,j} \\in \\mathbf{A}, b_{i,j} \\in \\mathbf{B}\\) and this pair of point corresponds to \\(a_{i,j}+b_{i,j} \\in \\mathbf{A}+\\mathbf{B}\\) . Note in particular that \\(a_{i,j}+b_{i,j} = (a+b)_{i,j}\\) . Then the transpose of the point \\(a_{i,j}\\) and \\(b_{i,j}\\) is \\(a_{j,i}\\) and \\(b_{j,i}\\) , which sums to \\(a_{j,i}+b_{j,i} = (a+b)_{j,i}\\) , which is the transpose of the point \\(a_{i,j}+b_{i,j} = (a+b)_{i,j}\\) . Q.E.D Shifting a Matrix When we say we shift a matrix, we really mean the following: Given a square matrix \\(\\A \\in \\R^{n \\times n}\\) , then shifting a matrix by a constant \\(\\lambda\\) is the following operation: \\[ \\widetilde{\\A} = \\A + \\lambda \\I_n \\quad \\A \\in \\R^{n \\times n}, \\lambda \\in \\R \\] Example and Motivation The author Mike gave us an example with some motivation behind, with reference to Linear Algebra: Theory, Intuition, Code, 2021. (pp. 127) , we consider the matrix: \\[ \\widetilde{A} = \\A + 0.1 \\cdot \\I_3 = \\begin{bmatrix}1 & 3 & 0 \\\\ 1 & 3 & 0 \\\\ 2 & 2 & 7 \\end{bmatrix} + 0.1 \\cdot \\I_3 = \\begin{bmatrix}1.1 & 3 & 0 \\\\ 1 & 3.1 & 0 \\\\ 2 & 2 & 7.1 \\end{bmatrix} \\] Then we observed: Diagonal Elements will be affected by shifting, but nothing else. This is obvious as off-diagonal elements of the scaled identity matrix are all zero entries. Note that row 1 and 2 of \\(\\A\\) are identical, and thus linearly dependent, but just by shifting a little, we will have distinct rows in \\(\\widetilde{A}\\) . In practice, we choose \\(\\lambda\\) to be small so that the shifted matrix is similar to the original matrix \\(\\A\\) , while still satisfying some constraints. Applications in Machine Learning The well known regularization technique is shifting a matrix in disguise. One can read it more here 2 . Diagonal We can extract the diagonal of a matrix into a vector: \\[ \\v = \\text{diag}(\\A) \\quad \\A \\in \\R^{m, n}, v_i = \\A_{i, i}, i = \\{1, 2, ..., \\min{(m, n)}\\} \\] Applications in Machine Learning The diagonal elements of a matrix can be extracted and placed into a vector. This is used, for example, in statistics: the diagonal elements of a covariance matrix contain the variance of each variable. - Mike X Cohen: Linear Algebra: Theory, Intuition, Code, 2021. (pp. 129) Trace The trace of a matrix \\(\\A \\in \\R^{m \\times n}\\) is: \\[ \\text{tr}(\\A) = \\sum_{i=1}^{m}a_{i, i} \\] Applications in Machine Learning The trace operation has two applications in machine learning: It is used to compute the Frobenius norm of a matrix (a measure of the magnitude of a matrix) and it is used to measure the \"distance\" between two matrices. - Mike X Cohen: Linear Algebra: Theory, Intuition, Code, 2021. (pp. 129) https://github.com/MacroAnalyst/Linear_Algebra_With_Python \u21a9 https://en.wikipedia.org/wiki/Tikhonov_regularization \u21a9","title":"Basic Matrix Operations"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_operations/#matrix-operations","text":"","title":"Matrix Operations"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_operations/#matrix-addition-and-subraction","text":"The sum of two matrices of the same size \\(m \\times n\\) , \\(\\A\\) and \\(\\B\\) are calculated elementwise: \\[ (\\A \\pm \\B)_{i, j} = \\A_{i, j} \\pm \\B_{i, j} \\] where \\(1 \\leq i \\leq m , \\quad 1 \\leq j \\leq n\\) .","title":"Matrix Addition and Subraction"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_operations/#scalar-matrix-multiplication","text":"For any scalar \\(\\lambda \\in \\F\\) , the Matrix-Scalar Multiplication \\(\\lambda \\A\\) is given by: \\[ (\\lambda \\A)_{i, j} = \\lambda \\cdot \\A_{i, j} \\]","title":"Scalar-Matrix Multiplication"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_operations/#commutative-of-scalar-matrix-multiplication","text":"Not surprisingly, the operation is commutative such that, given any scalar \\(\\lambda\\) , and any sequence of matrices \\(\\A, \\B\\) , we have: \\[ \\lambda \\A\\B = \\A\\lambda\\B = \\A\\B\\lambda \\]","title":"Commutative of Scalar-Matrix Multiplication"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_operations/#matrix-operations-fulfill-field-properties","text":"In fact, matrix operations fulfill the properties of field properties. That is, for any matrix \\(\\A\\) and \\(\\B\\) of the same shape and size, we have 1 : \\(\\A+ \\B= \\B+ \\A\\) \\((\\A+\\B)+ C=\\A+(\\B+C)\\) \\(c(\\A+\\B)=c\\A+c\\B\\) \\((c+d)\\A=c\\A+c{D}\\) \\(c(d\\A)=(cd)\\A\\) \\(\\A+=\\A\\) , where \\({0}\\) is the zero matrix For any \\(\\A\\) , there exists a \\(-\\A\\) , such that $ \\A+(- \\A)=0$. Although we have not learn matrix multiplication , their properties are: $ \\A({\\B\\mathbf{C}})=({\\A\\B}) \\mathbf{C}$ \\(\\mathbf{C}({\\A\\B})=(\\mathbf{C}\\A)\\B=\\A(\\mathbf{C}\\B)\\) \\(\\A(\\B+ \\mathbf{C})={\\A\\B}+{\\A\\mathbf{C}}\\) \\((\\B+\\mathbf{C})\\A={\\B\\A}+{\\mathbf{C}\\A}\\)","title":"Matrix Operations Fulfill Field Properties"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_operations/#matrix-tranpose","text":"Given a matrix \\(\\A \\in \\R^{m \\times n}\\) , the transpose of \\(\\A\\) is denoted \\(\\A^\\top\\) and formed by mapping the rows of \\(\\A\\) to columns and columns of \\(\\A\\) to rows, as illustrated: \\[ (\\A^\\top)_{i, j} = \\A_{j, i} \\]","title":"Matrix Tranpose"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_operations/#theorem-a-matrixs-transpose-is-itself","text":"Prove that the transpose of a matrix \\(\\mathbf{A}\\) 's transpose is \\(\\mathbf{A}\\) : \\((\\mathbf{A}^\\top)^\\top = \\mathbf{A}\\) .","title":"Theorem (A Matrixs transpose is itself)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_operations/#proof","text":"Consider a matrix \\(A_{n \\times k}\\) as follows, $$\\mathbf{A}=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1k} \\ a_{21} & a_{22} & \\cdots & a_{2k} \\ \\vdots & \\vdots & \\ddots & \\vdots \\ a_{n1} & a_{n2} & \\cdots & a_{nk} \\ \\end{bmatrix} $$ By definition of Transpose , all \\((i,j)\\) entries of \\(A\\) is mapped to \\((j,i)\\) , for example, \\(a_{1,2}\\) becomes \\(a_{2,1}\\) when transposed. Performing a transpose once more will then map all \\((j,i)\\) entries back to \\((i,j)\\) . \\(A\\) is unchanged and thus \\((\\mathbf{A}^\\top)^\\top = \\mathbf{A}\\) . Q.E.D","title":"Proof"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_operations/#theorem-sum-of-transpose-is-transpose-of-sum","text":"Given two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) , show that the sum of transposes is equal to the transpose of a sum: \\(\\mathbf{A}^\\top + \\mathbf{B}^\\top = (\\mathbf{A} + \\mathbf{B})^\\top\\) .","title":"Theorem (Sum of Transpose is Transpose of Sum)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_operations/#proof_1","text":"Say that we have two matrices \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times k}\\) and \\(\\mathbf{B} \\in \\mathbb{R}^{k \\times m}\\) : \\[\\mathbf{A}=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1k} \\\\ a_{21} & a_{22} & \\cdots & a_{2k} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{n1} & a_{n2} & \\cdots & a_{nk} \\\\ \\end{bmatrix},\\quad \\mathbf{B}=\\begin{bmatrix} b_{11} & b_{12} & \\cdots & b_{1m} \\\\ b_{21} & b_{22} & \\cdots & b_{2m} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ b_{k1} & b_{k2} & \\cdots & b_{km} \\\\ \\end{bmatrix},\\quad \\mathbf{A+B}=\\begin{bmatrix} a_{11}+b_{11} & a_{12}+b_{12} & \\cdots & a_{1m}+b_{1m} \\\\ a_{21}+b_{21} & a_{22}+b_{22} & \\cdots & a_{2m}+b_{2m} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{k1}+b_{k1} & a_{k2}+b_{k2} & \\cdots & a_{km}+b_{km} \\\\ \\end{bmatrix}.\\] Then we can prove it by simply computing the LHS and RHS respectively. Without loss of generality, we pick any pair of point \\(a_{i,j} \\in \\mathbf{A}, b_{i,j} \\in \\mathbf{B}\\) and this pair of point corresponds to \\(a_{i,j}+b_{i,j} \\in \\mathbf{A}+\\mathbf{B}\\) . Note in particular that \\(a_{i,j}+b_{i,j} = (a+b)_{i,j}\\) . Then the transpose of the point \\(a_{i,j}\\) and \\(b_{i,j}\\) is \\(a_{j,i}\\) and \\(b_{j,i}\\) , which sums to \\(a_{j,i}+b_{j,i} = (a+b)_{j,i}\\) , which is the transpose of the point \\(a_{i,j}+b_{i,j} = (a+b)_{i,j}\\) . Q.E.D","title":"Proof"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_operations/#shifting-a-matrix","text":"When we say we shift a matrix, we really mean the following: Given a square matrix \\(\\A \\in \\R^{n \\times n}\\) , then shifting a matrix by a constant \\(\\lambda\\) is the following operation: \\[ \\widetilde{\\A} = \\A + \\lambda \\I_n \\quad \\A \\in \\R^{n \\times n}, \\lambda \\in \\R \\]","title":"Shifting a Matrix"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_operations/#example-and-motivation","text":"The author Mike gave us an example with some motivation behind, with reference to Linear Algebra: Theory, Intuition, Code, 2021. (pp. 127) , we consider the matrix: \\[ \\widetilde{A} = \\A + 0.1 \\cdot \\I_3 = \\begin{bmatrix}1 & 3 & 0 \\\\ 1 & 3 & 0 \\\\ 2 & 2 & 7 \\end{bmatrix} + 0.1 \\cdot \\I_3 = \\begin{bmatrix}1.1 & 3 & 0 \\\\ 1 & 3.1 & 0 \\\\ 2 & 2 & 7.1 \\end{bmatrix} \\] Then we observed: Diagonal Elements will be affected by shifting, but nothing else. This is obvious as off-diagonal elements of the scaled identity matrix are all zero entries. Note that row 1 and 2 of \\(\\A\\) are identical, and thus linearly dependent, but just by shifting a little, we will have distinct rows in \\(\\widetilde{A}\\) . In practice, we choose \\(\\lambda\\) to be small so that the shifted matrix is similar to the original matrix \\(\\A\\) , while still satisfying some constraints.","title":"Example and Motivation"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_operations/#applications-in-machine-learning","text":"The well known regularization technique is shifting a matrix in disguise. One can read it more here 2 .","title":"Applications in Machine Learning"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_operations/#diagonal","text":"We can extract the diagonal of a matrix into a vector: \\[ \\v = \\text{diag}(\\A) \\quad \\A \\in \\R^{m, n}, v_i = \\A_{i, i}, i = \\{1, 2, ..., \\min{(m, n)}\\} \\]","title":"Diagonal"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_operations/#applications-in-machine-learning_1","text":"The diagonal elements of a matrix can be extracted and placed into a vector. This is used, for example, in statistics: the diagonal elements of a covariance matrix contain the variance of each variable. - Mike X Cohen: Linear Algebra: Theory, Intuition, Code, 2021. (pp. 129)","title":"Applications in Machine Learning"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_operations/#trace","text":"The trace of a matrix \\(\\A \\in \\R^{m \\times n}\\) is: \\[ \\text{tr}(\\A) = \\sum_{i=1}^{m}a_{i, i} \\]","title":"Trace"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_basic_operations/#applications-in-machine-learning_2","text":"The trace operation has two applications in machine learning: It is used to compute the Frobenius norm of a matrix (a measure of the magnitude of a matrix) and it is used to measure the \"distance\" between two matrices. - Mike X Cohen: Linear Algebra: Theory, Intuition, Code, 2021. (pp. 129) https://github.com/MacroAnalyst/Linear_Algebra_With_Python \u21a9 https://en.wikipedia.org/wiki/Tikhonov_regularization \u21a9","title":"Applications in Machine Learning"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_multiplication/","text":"\\[\\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\z}{\\mathbf{z}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\C}{\\mathbf{C}}\\] Table of Contents Matrix Multiplication Validity of Matrix Multiplication Basic Formula for Matrix Multiplication Matrix-Vector Multiplication Right Multiplication (Linear Combination of Columns) Example (Right Multiplication) Example (Linear Transformation) Definition (Right Multiplication) Linear Equations (Right Multiplication) Column Space (Right Multiplication) Left Multiplication (Linear Combination of Rows) Example (Left Multiplication) The Four Interpretations of Matrix Multiplication Element Wise Matrix Multiplication Significance (Element Wise Matrix Multiplication) Outer Product Wise Matrix Multiplication Matrix Multiplication using Right Multiplication (Columns) Significance Matrix Multiplication using Right Multiplication (Rows) Significance Matrix Multiplication Properties Symmetric Matrices Matrix Multiplication (Naive) in Python References Learning Objectives Matrix Multiplication Matrix Multiplication Validity of Matrix Multiplication If you have dabbled in deep learning before, then the dreaded error shape mismatch is omnipresent . This is because matrix multiplication is only defined for matrices of a certain shape. If \\(\\A\\) is an \\(m \\times n\\) matrix and \\(\\B\\) is an \\(n \\times p\\) matrix, then \\(\\A\\B\\) is well defined because the columns of \\(\\A\\) is equals to the rows of \\(\\B\\) . If this is not true, then the \"shape is mismatched\". Consequently, if the matrix multiplication is well defined, then \\(\\C = \\A\\B\\) has shape (size) of \\(m \\times p\\) . Basic Formula for Matrix Multiplication Let us go through the most basic formula for matrix multiplication. Quoting from Wikipedia 1 : Let \\[\\A=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix},\\quad\\mathbf{B}=\\begin{bmatrix} b_{11} & b_{12} & \\cdots & b_{1p} \\\\ b_{21} & b_{22} & \\cdots & b_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ b_{n1} & b_{n2} & \\cdots & b_{np} \\\\ \\end{bmatrix}\\] then the matrix product \\(\\C = \\A\\B\\) is defined to be the \\(m \\times p\\) matrix: \\[\\mathbf{C}=\\A\\B=\\begin{bmatrix} c_{11} & c_{12} & \\cdots & c_{1p} \\\\ c_{21} & c_{22} & \\cdots & c_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ c_{m1} & c_{m2} & \\cdots & c_{mp} \\\\ \\end{bmatrix}\\] where \\(c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} +\\cdots + a_{in}b_{nj}= \\sum_{k=1}^n a_{ik}b_{kj}\\) for \\(i = 1, \\cdots , m\\) and \\(j = 1, \\cdots , p\\) . That is, the entry \\(c_{i,j}\\) of the product is obtained by multiplying term-by-term the entries of the \\(i\\) -th row of \\(\\A\\) and the \\(j\\) -th column of \\(\\B\\) , and summing these \\(n\\) products. In other words, \\(c_{i,j}\\) is the dot product of the \\(i\\) -th row of \\(\\A\\) and the \\(j\\) -th column of \\(\\B\\) . Therefore, \\(\\C = \\A\\B\\) can also be written as \\[\\mathbf{C}=\\A\\B=\\begin{bmatrix} a_{11}b_{11} +\\cdots + a_{1n}b_{n1} & a_{11}b_{12} +\\cdots + a_{1n}b_{n2} & \\cdots & a_{11}b_{1p} +\\cdots + a_{1n}b_{np} \\\\ a_{21}b_{11} +\\cdots + a_{2n}b_{n1} & a_{21}b_{12} +\\cdots + a_{2n}b_{n2} & \\cdots & a_{21}b_{1p} +\\cdots + a_{2n}b_{np} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1}b_{11} +\\cdots + a_{mn}b_{n1} & a_{m1}b_{12} +\\cdots + a_{mn}b_{n2} & \\cdots & a_{m1}b_{1p} +\\cdots + a_{mn}b_{np} \\\\ \\end{bmatrix} \\] Thus the product \\(\\A\\B\\) is defined if and only if the number of columns in \\(\\A\\) equals the number of rows in \\(\\B\\) . Matrix-Vector Multiplication Before we go into Matrix-Matrix Multiplication, it is important to understand how Matrix-Vector Multiplication. Take a mental note on the usage of linear combination here. We will be referencing heavily from the article written by Eli Bendersky 2 . Right Multiplication (Linear Combination of Columns) Example (Right Multiplication) We motivate this with an example. Given a 3 by 3 matrix \\(A = \\begin{bmatrix} x_1 & y_1 & z_1 \\\\ x_2 & y_2 & z_2 \\\\ x_3 & y_3 & z_3 \\\\ \\end{bmatrix}\\) and \\(\\x = \\begin{bmatrix} a \\\\ b \\\\ c \\\\ \\end{bmatrix}\\) then \\[\\A\\x = \\begin{bmatrix} x_1 & y_1 & z_1 \\\\ x_2 & y_2 & z_2 \\\\ x_3 & y_3 & z_3 \\\\ \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ c \\\\ \\end{bmatrix} = \\begin{bmatrix} ax_1+by_1+cz_1 \\\\ ax_2+by_2+cz_2 \\\\ ax_3+by_3+cz_3 \\\\ \\end{bmatrix}\\] But notice that the above can also be written as: \\[\\A\\x = \\begin{bmatrix} x_1 & y_1 & z_1 \\\\ x_2 & y_2 & z_2 \\\\ x_3 & y_3 & z_3 \\\\ \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ c \\\\ \\end{bmatrix} = \\begin{bmatrix} ax_1+by_1+cz_1 \\\\ ax_2+by_2+cz_2 \\\\ ax_3+by_3+cz_3 \\\\ \\end{bmatrix} = \\color{red}{a}\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\end{bmatrix} + \\color{green}{b}\\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ \\end{bmatrix} + \\color{blue}{c}\\begin{bmatrix} z_1 \\\\ z_2 \\\\ z_3 \\\\ \\end{bmatrix}\\] This above expression is called: The matrix \\(\\A\\) acts on the vector \\(\\x\\) and the output is a linear combination of the columns of the matrix \\(\\A\\) . Example (Linear Transformation) We won't be going through the formal definition yet, but one could directly see as a consequence of the previous example. Given a 3 by 3 matrix \\(A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 2 & 4 & 6 \\\\ 3 & 6 & 9 \\\\ \\end{bmatrix}\\) and \\(\\x = \\begin{bmatrix} 1 \\\\ 2 \\\\ 4 \\\\ \\end{bmatrix}\\) then the geometric meaning of \\(\\A\\x\\) can be defined by a series of \"linear transformations\" categorized by scaling the first column of \\(\\A\\) by 1, then add the result to 2 times of the second column of \\(\\A\\) , and add the result to 4 times of the third column of \\(\\A\\) . Definition (Right Multiplication) Given a \\(m \\times n\\) matrix \\(\\A=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix}\\) and a column vector \\(\\x=\\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{n}\\end{bmatrix}\\) then \\[\\A\\x = \\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix}\\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{n}\\end{bmatrix} = \\color{red}{x_1}\\begin{bmatrix} a_{11} \\\\ a_{12} \\\\ \\vdots \\\\ a_{m1} \\\\ \\end{bmatrix} + \\color{green}{x_2}\\begin{bmatrix} a_{12} \\\\ a_{22} \\\\ \\vdots \\\\ a_{m2} \\\\ \\end{bmatrix} + \\cdots + \\color{blue}{x_n}\\begin{bmatrix} a_{1n} \\\\ a_{2n} \\\\ \\vdots \\\\ a_{mn} \\\\ \\end{bmatrix}\\] If the formula looks daunting, just remember that \\(\\A\\x\\) gives nothing but the linear combination of the columns of \\(\\A\\) with values of \\(\\x\\) as coefficients. Linear Equations (Right Multiplication) This is a very important realization that one must have, I will mention it right here first and will repeat it throughout. Given a 3 by 3 matrix \\(A = \\begin{bmatrix} x_1 & y_1 & z_1 \\\\ x_2 & y_2 & z_2 \\\\ x_3 & y_3 & z_3 \\\\ \\end{bmatrix}\\) and \\(\\x = \\begin{bmatrix} a \\\\ b \\\\ c \\\\ \\end{bmatrix}\\) then \\[\\A\\x = \\begin{bmatrix} x_1 & y_1 & z_1 \\\\ x_2 & y_2 & z_2 \\\\ x_3 & y_3 & z_3 \\\\ \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ c \\\\ \\end{bmatrix} = \\begin{bmatrix} ax_1+by_1+cz_1 \\\\ ax_2+by_2+cz_2 \\\\ ax_3+by_3+cz_3 \\\\ \\end{bmatrix} = \\color{red}{a}\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\end{bmatrix} + \\color{green}{b}\\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ \\end{bmatrix} + \\color{blue}{c}\\begin{bmatrix} z_1 \\\\ z_2 \\\\ z_3 \\\\ \\end{bmatrix}\\] In the examples above, \\(\\A\\) and \\(\\x\\) are known and we want to find the unknown \\(\\b\\) which is the product of \\(\\A\\x\\) . Now, if we know \\(\\A\\) and \\(\\b\\) and wish to find \\(\\x\\) that solves the linear equation/system \\(\\A\\x = \\b\\) instead, what can we understand from the above? Since we know \\(\\b = \\A\\x\\) is a linear combination of the columns of \\(\\A\\) with values of \\(\\x\\) as coefficients. Then if we want to solve for \\(\\x\\) , we ask ourselves what combination of the columns of \\(\\A = \\begin{bmatrix} \\x & \\y & \\z \\end{bmatrix}\\) gives rise to the vector \\(\\b\\) ? If we can find the combination \\(a, b, c\\) , we can recover \\(\\x = \\begin{bmatrix} a \\\\ b \\\\ c \\\\ \\end{bmatrix}\\) Column Space (Right Multiplication) A column space of a matrix \\(\\A\\) is just the set of linear combination of the columns of \\(\\A\\) , and is a subspace. We will go through it in more details, but for now, I want to introduce this idea first. As a consequence of the example prior, one should realize two things: \\(\\A\\x = \\b\\) may not always have a solution \\(\\x\\) . If \\(\\A\\x = \\b\\) has a solution \\(\\x\\) , then the product \\(\\b\\) must be a linear combination of the columns of \\(\\A\\) . This has important consequences later on. For now, just know that if \\(\\A\\x = \\b\\) has a solution, then \\(\\b\\) resides in the column space of \\(\\A\\) . Left Multiplication (Linear Combination of Rows) This part is also necessary to better understand matrix multiplication later. Example (Left Multiplication) We motivate this with an example. Given a 3 by 3 matrix \\(A = \\begin{bmatrix} x_1 & y_1 & z_1 \\\\ x_2 & y_2 & z_2 \\\\ x_3 & y_3 & z_3 \\\\ \\end{bmatrix}\\) and \\(\\x = \\begin{bmatrix} a & b & c \\end{bmatrix}\\) then \\[\\x\\A = \\begin{bmatrix} a & b & c \\end{bmatrix} \\begin{bmatrix} x_1 & y_1 & z_1 \\\\ x_2 & y_2 & z_2 \\\\ x_3 & y_3 & z_3 \\\\ \\end{bmatrix} = \\begin{bmatrix} ax_1+bx_2+cx_3 & ay_1+by_2+cy_3 & az_1+bz_2+cz_3 \\end{bmatrix}\\] But notice that the above can also be written as: \\[\\x\\A = \\begin{bmatrix} x_1 & y_1 & z_1 \\\\ x_2 & y_2 & z_2 \\\\ x_3 & y_3 & z_3 \\\\ \\end{bmatrix} = \\begin{bmatrix} ax_1+bx_2+cx_3 & ay_1+by_2+cy_3 & az_1+bz_2+cz_3 \\end{bmatrix} = \\color{red}{a}\\begin{bmatrix} x_1 & y_1 & z_1 \\end{bmatrix} + \\color{green}{b}\\begin{bmatrix} x_2 & y_2 & z_2 \\end{bmatrix} + \\color{blue}{c}\\begin{bmatrix} x_3 & y_3 & z_3 \\end{bmatrix}\\] Notice that now \\(\\x\\A\\) is just a linear combination of the rows of \\(\\A\\) . The Four Interpretations of Matrix Multiplication Element Wise Matrix Multiplication We have mentioned in the previous section. Here we repeat again for the sake of modularity. Let \\[\\A=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix},\\quad\\mathbf{B}=\\begin{bmatrix} b_{11} & b_{12} & \\cdots & b_{1p} \\\\ b_{21} & b_{22} & \\cdots & b_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ b_{n1} & b_{n2} & \\cdots & b_{np} \\\\ \\end{bmatrix}\\] then the matrix product \\(\\C = \\A\\B\\) is defined to be the \\(m \\times p\\) matrix: \\[\\mathbf{C}=\\A\\B=\\begin{bmatrix} c_{11} & c_{12} & \\cdots & c_{1p} \\\\ c_{21} & c_{22} & \\cdots & c_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ c_{m1} & c_{m2} & \\cdots & c_{mp} \\\\ \\end{bmatrix}\\] where \\(c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} +\\cdots + a_{in}b_{nj}= \\sum_{k=1}^n a_{ik}b_{kj}\\) for \\(i = 1, \\cdots , m\\) and \\(j = 1, \\cdots , p\\) . That is, the entry \\(c_{i,j}\\) of the product is obtained by multiplying term-by-term the entries of the \\(i\\) -th row of \\(\\A\\) and the \\(j\\) -th column of \\(\\B\\) , and summing these \\(n\\) products. In other words, \\(c_{i,j}\\) is the dot product of the \\(i\\) -th row of \\(\\A\\) and the \\(j\\) -th column of \\(\\B\\) . Therefore, \\(\\C = \\A\\B\\) can also be written as \\[\\mathbf{C}=\\A\\B=\\begin{bmatrix} a_{11}b_{11} +\\cdots + a_{1n}b_{n1} & a_{11}b_{12} +\\cdots + a_{1n}b_{n2} & \\cdots & a_{11}b_{1p} +\\cdots + a_{1n}b_{np} \\\\ a_{21}b_{11} +\\cdots + a_{2n}b_{n1} & a_{21}b_{12} +\\cdots + a_{2n}b_{n2} & \\cdots & a_{21}b_{1p} +\\cdots + a_{2n}b_{np} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1}b_{11} +\\cdots + a_{mn}b_{n1} & a_{m1}b_{12} +\\cdots + a_{mn}b_{n2} & \\cdots & a_{m1}b_{1p} +\\cdots + a_{mn}b_{np} \\\\ \\end{bmatrix} \\] Thus the product \\(\\A\\B\\) is defined if and only if the number of columns in \\(\\A\\) equals the number of rows in \\(\\B\\) . Significance (Element Wise Matrix Multiplication) From Mike X Cohen: Linear Algebra: Theory, Intuition, Code, 2021. (pp. 144) , he mentioned the following: The diagonal of \\(\\C\\) contains dot products between row \\(i\\) and column \\(j\\) of \\(\\A\\) and \\(\\B\\) respectively, this is relevant in data covariance matrices. The lower triangle of \\(\\C\\) contains dot products between row \\(i\\) in \\(\\A\\) and column \\(j\\) in \\(\\B\\) where \\(i > j\\) . The upper triangle of \\(\\C\\) contains dot products between row \\(i\\) in \\(\\A\\) and column \\(j\\) in \\(\\B\\) where \\(i < j\\) . Both are important in matrix decompositions, such as QR decomposition and generalized eigendecomposition . Outer Product Wise Matrix Multiplication To simplify notation, we denote: \\( \\(\\A = \\begin{bmatrix} \\a_1 & \\a_2 \\cdots & \\a_n \\end{bmatrix}, \\quad \\B = \\begin{bmatrix} \\b_1^\\top \\\\ \\b_2^\\top \\\\ \\cdots \\\\ \\b_n^\\top \\end{bmatrix}\\) \\) as shorthand where \\(\\a_i\\) is the \\(i\\) -th column of \\(\\A\\) and \\(\\b_j^\\top\\) is the \\(j\\) -th row of \\(\\B\\) . Then \\[\\A\\B = \\a_1\\b_1^\\top + \\a_2\\b_2^\\top + \\cdots + \\a_n\\b_n^\\top\\] Notice that in each of the \\(\\a_i\\b_i^\\top\\) , the columns form a dependent set (the same can be said of the rows). However, the sum of these singular matrices\u2014the product matrix\u2014has columns that form a linearly independent set. Each of these matrices is rank-1 (to be defined later). This forms the basis for the Singular Value Decomposition. - Mike X Cohen: Linear Algebra: Theory, Intuition, Code, 2021. (pp. 145) Matrix Multiplication using Right Multiplication (Columns) Using back the notation in the Element Wise Matrix Multiplication , we can define \\[\\A\\B = \\A \\begin{bmatrix} \\b_1 & \\b_2 & \\cdots & \\b_p \\end{bmatrix} = \\begin{bmatrix} \\A\\b_1 & \\A\\b_2 & \\cdots & \\A\\b_p \\end{bmatrix}\\] where \\(\\b_i\\) is the column \\(i\\) of the matrix \\(\\B\\) . This means that each column of the matrix \\(\\C = \\A\\B\\) is defined by \\(\\A\\b_i\\) , and recall in the section \"Matrix-Vector Right Multiplication\", \\(\\A\\b_i\\) means a linear combination of the columns of \\(\\A\\) with weight coefficients in \\(\\b_i\\) . They say a picture is worth a thousand words. The below images are taken from Eli Bendersky's website here . Matrix Multiplication, Column Perspective; Courtesy of Eli Bendersky Significance The column perspective of matrix multiplication is useful in statistics, when the columns of the left matrix contain a set of regressors (a simplified model of the data), and the right matrix contains coefficients. The coefficients encode the importance of each regressor, and the goal of statistical model-fitting is to find the best coefficients such that the weighted combination of regressors matches the data. - Mike X Cohen: Linear Algebra: Theory, Intuition, Code, 2021. (pp. 146) Matrix Multiplication using Left Multiplication (Rows) Using back the notation in the Element Wise Matrix Multiplication , we can define \\[\\A\\B = \\begin{bmatrix}\\a_1 \\\\ \\a_2 \\\\ \\vdots \\\\ \\a_m \\end{bmatrix}\\B = \\begin{bmatrix}\\a_1\\B \\\\ \\a_2\\B \\\\ \\vdots \\\\ \\a_m\\B \\end{bmatrix}\\] where \\(\\a_i\\) is the row \\(i\\) of the matrix \\(\\A\\) . This means that each row of the matrix \\(\\C = \\A\\B\\) is defined by \\(\\a_i\\B\\) , and recall in the section \"Matrix-Vector Left Multiplication\", \\(\\a_i\\B\\) means a linear combination of the row of \\(\\B\\) with weight coefficients in \\(\\a_i\\) . This becomes apparent when we come to the chapter of Row-Echolon Form . They say a picture is worth a thousand words. The below images are taken from Eli Bendersky's website here . Matrix Multiplication, Row Perspective; Courtesy of Eli Bendersky Significance The row perspective is useful, for example in principal components analysis, where the rows of the right matrix contain data (observations in rows and features in columns) and the rows of the left matrix contain weights for combining the features. Then the weighted sum of data creates the principal component scores. - Mike X Cohen: Linear Algebra: Theory, Intuition, Code, 2021. (pp. 147) Matrix Multiplication Properties Read Wikipedia . To fill in when free as it is relatively light. Symmetric Matrices Matrix Multiplication (Naive) in Python For a Divide-and-conquer method, see here 3 . import os import random import numpy as np import torch def seed_all ( seed : int = 19921930 ) -> None : \"\"\"Seed all random number generators.\"\"\" print ( f \"Using Seed Number { seed } \" ) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) # set PYTHONHASHSEED env var at fixed value torch . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) torch . cuda . manual_seed ( seed ) # pytorch (both CPU and CUDA) np . random . seed ( seed ) # for numpy pseudo-random generator # set fixed value for python built-in pseudo-random generator random . seed ( seed ) torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = False torch . backends . cudnn . enabled = False seed_all () Using Seed Number 19921930 import torch import numpy as np from typing import Tuple , List A = np . random . randint ( 0 , 10 , size = ( 3 , 2 )) B = np . random . randint ( 0 , 10 , size = ( 2 , 2 )) Pseudo Code Let us run through the matrix multiplication between two simple matrices \\(\\A\\) and \\(\\B\\) . We define their product \\(\\C = \\A\\B\\) to be a matrix of shape \\((3, 2)\\) . Take first row of \\(\\A\\) and first column of \\(\\B\\) , compute the dot product of both and assign the value to the first entry of \\(\\C\\) (i.e. \\(\\C_{1, 1}\\) ). Take first row of \\(\\A\\) and second column of \\(\\B\\) , compute the dot product of both and assign the value to the first entry of \\(\\C\\) (i.e. \\(\\C_{1, 2}\\) ). Take second row of \\(\\A\\) and first column of \\(\\B\\) , compute the dot product of both and assign the value to the first entry of \\(\\C\\) (i.e. \\(\\C_{2, 1}\\) ). Take second row of \\(\\A\\) and second column of \\(\\B\\) , compute the dot product of both and assign the value to the first entry of \\(\\C\\) (i.e. \\(\\C_{2, 2}\\) ). Take third row of \\(\\A\\) and first column of \\(\\B\\) , compute the dot product of both and assign the value to the first entry of \\(\\C\\) (i.e. \\(\\C_{3, 1}\\) ). Take third row of \\(\\A\\) and second column of \\(\\B\\) , compute the dot product of both and assign the value to the first entry of \\(\\C\\) (i.e. \\(\\C_{3, 2}\\) ). We have seen the above in laymen english. We can easily convert the above to pseudo-code, but first we need to recognize that the above method is the \"Element-wise\" method, where the outer loop is rows of \\(\\A\\) , and the inner loop is columns of \\(\\B\\) . Input: matrices \\(\\A\\) and \\(\\B\\) of size \\((m, n)\\) and \\((n, p)\\) respectively. Initialize matrix \\(\\C\\) to be the product of \\(\\A\\B\\) with the correct shape. For row i from 1 to m: For col j from 1 to p: compute dot product of row i of A and col j of B: dot assign the dot product to entry \\(C_{i, j}\\) return \\(\\C\\) We have removed a layer of abstraction above, and that is the computation of the dot product, in which we have coded up previously. If there is no abstraction, then: Input: matrices \\(\\A\\) and \\(\\B\\) of size \\((m, n)\\) and \\((n, p)\\) respectively. Initialize matrix \\(\\C\\) to be the product of \\(\\A\\B\\) with the correct shape. For row i from 1 to m: Initialize summation = 0 For col j from 1 to p: for k from 1 to n: summation += dot product of row i of A and col j of B assign the dot product summation to entry \\(C_{i, j}\\) return \\(\\C\\) Python Code def linear_combination_vectors ( weights : List [ float ], * args : np . ndarray ) -> np . ndarray : \"\"\"Computes the linear combination of vectors. Args: weights (List[float]): The set of weights corresponding to each vector. Returns: linear_weighted_sum (np.ndarray): The linear combination of vectors. Examples: >>> v1 = np.asarray([1, 2, 3, 4, 5]).reshape(-1, 1) >>> v2 = np.asarray([2, 4, 6, 8, 10]).reshape(-1, 1) >>> v3 = np.asarray([3, 6, 9, 12, 15]).reshape(-1, 1) >>> weights = [10, 20, 30] >>> linear_combination_vectors([10, 20, 30], v1, v2, v3) \"\"\" linear_weighted_sum = np . zeros ( shape = args [ 0 ] . shape ) for weight , vec in zip ( weights , args ): linear_weighted_sum += weight * vec return linear_weighted_sum def dot_product ( v1 : np . ndarray , v2 : np . ndarray ) -> float : \"\"\"Computes the dot product of two vectors. We assume both vectors are flattened, i.e. they are 1D arrays. Args: v1 (np.ndarray): The first vector. v2 (np.ndarray): The second vector. Returns: dot_product_v1_v2 (float): The dot product of two vectors. Examples: >>> v1 = np.asarray([1, 2, 3, 4, 5]) >>> v2 = np.asarray([2, 4, 6, 8, 10]) >>> dot_product(v1, v2) \"\"\" v1 , v2 = np . asarray ( v1 ) . flatten (), np . asarray ( v2 ) . flatten () dot_product_v1_v2 = 0 for element_1 , element_2 in zip ( v1 , v2 ): dot_product_v1_v2 += element_1 * element_2 # same as np.dot but does not take into the orientation of vectors assert dot_product_v1_v2 == np . dot ( v1 . T , v2 ) return dot_product_v1_v2 def get_matmul_shape ( A : np . ndarray , B : np . ndarray ) -> Tuple [ int , int , int ]: \"\"\"Check if the shape of the matrices A and B are compatible for matrix multiplication. If A and B are of size (m, n) and (n, p), respectively, then the shape of the resulting matrix is (m, p). Args: A (np.ndarray): The first matrix. B (np.ndarray): The second matrix. Raises: ValueError: Raises a ValueError if the shape of the matrices A and B are not compatible for matrix multiplication. Returns: (Tuple[int, int, int]): (m, n, p) where (m, n) is the shape of A and (n, p) is the shape of B. \"\"\" if A . shape [ 1 ] != B . shape [ 0 ]: raise ValueError ( f \"The number of columns of A must be equal to the number of rows of B, but got { A . shape [ 1 ] } and { B . shape [ 0 ] } respectively.\" ) return ( A . shape [ 0 ], A . shape [ 1 ], B . shape [ 1 ]) def np_matmul_naive ( A : np . ndarray , B : np . ndarray ) -> np . ndarray : \"\"\"Computes the matrix multiplication of two matrices. Args: A (np.ndarray): The first matrix. B (np.ndarray): The second matrix. Returns: matmul (np.ndarray): The matrix multiplication of two matrices. \"\"\" num_rows_A , common_index , num_cols_B = check_matmul_shape ( A , B ) matmul = np . zeros ( shape = ( num_rows_A , num_cols_B )) # 1st loop: loops through first matrix A for i in range ( num_rows_A ): summation = 0 # 2nd loop: loops through second matrix B for j in range ( num_cols_B ): # 3rd loop: computes dot prod for k in range ( common_index ): summation += A [ i , k ] * B [ k , j ] matmul [ i , j ] = summation return matmul def np_matmul_element_wise ( A : np . ndarray , B : np . ndarray ) -> np . ndarray : \"\"\"Computes the matrix multiplication of two matrices using element wise method. Args: A (np.ndarray): The first matrix. B (np.ndarray): The second matrix. Returns: matmul (np.ndarray): The matrix multiplication of two matrices. \"\"\" num_rows_A , _ , num_cols_B = check_matmul_shape ( A , B ) matmul = np . zeros ( shape = ( num_rows_A , num_cols_B )) # 1st loop: loops through first matrix A for row_i in range ( num_rows_A ): # 2nd loop: loops through second matrix B for col_j in range ( num_cols_B ): # computes dot product of row i with column j of B and # assign the result to the element of the matrix matmul at row i and column j. matmul [ row_i , col_j ] = dot_product ( A [ row_i , :], B [:, col_j ]) return matmul def np_matmul_column_wise ( A : np . ndarray , B : np . ndarray ) -> np . ndarray : \"\"\"Computes the matrix multiplication of two matrices using column wise method. Recall the section on Matrix Multiplication using Right Multiplication. Column i of C is represented by: Ab_i Args: A (np.ndarray): The first matrix. B (np.ndarray): The second matrix. Returns: matmul (np.ndarray): The matrix multiplication of two matrices. \"\"\" num_rows_A , _ , num_cols_B = check_matmul_shape ( A , B ) matmul = np . zeros ( shape = ( num_rows_A , num_cols_B )) # we just need to populate the columns of C for col_i in range ( matmul . shape [ 1 ]): # b_i col_i_B = B [:, col_i ] # Ab_i linear_comb_A_on_col_i_B = linear_combination_vectors ( col_i_B , * A . T ) # C_i = Ab_i matmul [:, col_i ] = linear_comb_A_on_col_i_B return matmul # assert the function output is similar to numpy's A @ B np . allclose ( np_matmul_element_wise ( A , B ), np_matmul_column_wise ( A , B ), A @ B ) True Time Complexity The time complexity can be easily found in the pseudo-code. Input: matrices \\(\\A\\) and \\(\\B\\) of size \\((m, n)\\) and \\((n, p)\\) respectively. Initialize matrix \\(\\C\\) to be the product of \\(\\A\\B\\) with the correct shape. For row i from 1 to m: Initialize summation = 0 For col j from 1 to p: for k from 1 to n: summation += dot product of row i of A and col j of B assign the dot product summation to entry \\(C_{i, j}\\) return \\(\\C\\) In rough terms, the first outer loop has \\(m\\) operations, the second loop has \\(p\\) operations and last loop has \\(n\\) operations; we further assume each operation is \\(\\mathcal{O}(1)\\) and thus we have a total of \\(m \\times n \\times p\\) loops (operations), so the time complexity is \\( \\(\\mathcal{O}(mnp) \\approx \\mathcal{O}(n^3)\\) \\) if \\(m \\approx n \\approx p\\) . References https://eli.thegreenplace.net/2015/visualizing-matrix-multiplication-as-a-linear-combination/ https://math.stackexchange.com/questions/192835/fastest-and-intuitive-ways-to-look-at-matrix-multiplication https://math.stackexchange.com/questions/24456/matrix-multiplication-interpreting-and-understanding-the-process https://en.wikipedia.org/wiki/Matrix_multiplication \u21a9 https://eli.thegreenplace.net/2015/visualizing-matrix-multiplication-as-a-linear-combination/ \u21a9 solvay_strassen_algorithm \u21a9","title":"Matrix Multiplication"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_multiplication/#table-of-contents","text":"Matrix Multiplication Validity of Matrix Multiplication Basic Formula for Matrix Multiplication Matrix-Vector Multiplication Right Multiplication (Linear Combination of Columns) Example (Right Multiplication) Example (Linear Transformation) Definition (Right Multiplication) Linear Equations (Right Multiplication) Column Space (Right Multiplication) Left Multiplication (Linear Combination of Rows) Example (Left Multiplication) The Four Interpretations of Matrix Multiplication Element Wise Matrix Multiplication Significance (Element Wise Matrix Multiplication) Outer Product Wise Matrix Multiplication Matrix Multiplication using Right Multiplication (Columns) Significance Matrix Multiplication using Right Multiplication (Rows) Significance Matrix Multiplication Properties Symmetric Matrices Matrix Multiplication (Naive) in Python References Learning Objectives Matrix Multiplication","title":"Table of Contents"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_multiplication/#matrix-multiplication","text":"","title":"Matrix Multiplication"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_multiplication/#validity-of-matrix-multiplication","text":"If you have dabbled in deep learning before, then the dreaded error shape mismatch is omnipresent . This is because matrix multiplication is only defined for matrices of a certain shape. If \\(\\A\\) is an \\(m \\times n\\) matrix and \\(\\B\\) is an \\(n \\times p\\) matrix, then \\(\\A\\B\\) is well defined because the columns of \\(\\A\\) is equals to the rows of \\(\\B\\) . If this is not true, then the \"shape is mismatched\". Consequently, if the matrix multiplication is well defined, then \\(\\C = \\A\\B\\) has shape (size) of \\(m \\times p\\) .","title":"Validity of Matrix Multiplication"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_multiplication/#basic-formula-for-matrix-multiplication","text":"Let us go through the most basic formula for matrix multiplication. Quoting from Wikipedia 1 : Let \\[\\A=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix},\\quad\\mathbf{B}=\\begin{bmatrix} b_{11} & b_{12} & \\cdots & b_{1p} \\\\ b_{21} & b_{22} & \\cdots & b_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ b_{n1} & b_{n2} & \\cdots & b_{np} \\\\ \\end{bmatrix}\\] then the matrix product \\(\\C = \\A\\B\\) is defined to be the \\(m \\times p\\) matrix: \\[\\mathbf{C}=\\A\\B=\\begin{bmatrix} c_{11} & c_{12} & \\cdots & c_{1p} \\\\ c_{21} & c_{22} & \\cdots & c_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ c_{m1} & c_{m2} & \\cdots & c_{mp} \\\\ \\end{bmatrix}\\] where \\(c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} +\\cdots + a_{in}b_{nj}= \\sum_{k=1}^n a_{ik}b_{kj}\\) for \\(i = 1, \\cdots , m\\) and \\(j = 1, \\cdots , p\\) . That is, the entry \\(c_{i,j}\\) of the product is obtained by multiplying term-by-term the entries of the \\(i\\) -th row of \\(\\A\\) and the \\(j\\) -th column of \\(\\B\\) , and summing these \\(n\\) products. In other words, \\(c_{i,j}\\) is the dot product of the \\(i\\) -th row of \\(\\A\\) and the \\(j\\) -th column of \\(\\B\\) . Therefore, \\(\\C = \\A\\B\\) can also be written as \\[\\mathbf{C}=\\A\\B=\\begin{bmatrix} a_{11}b_{11} +\\cdots + a_{1n}b_{n1} & a_{11}b_{12} +\\cdots + a_{1n}b_{n2} & \\cdots & a_{11}b_{1p} +\\cdots + a_{1n}b_{np} \\\\ a_{21}b_{11} +\\cdots + a_{2n}b_{n1} & a_{21}b_{12} +\\cdots + a_{2n}b_{n2} & \\cdots & a_{21}b_{1p} +\\cdots + a_{2n}b_{np} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1}b_{11} +\\cdots + a_{mn}b_{n1} & a_{m1}b_{12} +\\cdots + a_{mn}b_{n2} & \\cdots & a_{m1}b_{1p} +\\cdots + a_{mn}b_{np} \\\\ \\end{bmatrix} \\] Thus the product \\(\\A\\B\\) is defined if and only if the number of columns in \\(\\A\\) equals the number of rows in \\(\\B\\) .","title":"Basic Formula for Matrix Multiplication"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_multiplication/#matrix-vector-multiplication","text":"Before we go into Matrix-Matrix Multiplication, it is important to understand how Matrix-Vector Multiplication. Take a mental note on the usage of linear combination here. We will be referencing heavily from the article written by Eli Bendersky 2 .","title":"Matrix-Vector Multiplication"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_multiplication/#right-multiplication-linear-combination-of-columns","text":"","title":"Right Multiplication (Linear Combination of Columns)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_multiplication/#example-right-multiplication","text":"We motivate this with an example. Given a 3 by 3 matrix \\(A = \\begin{bmatrix} x_1 & y_1 & z_1 \\\\ x_2 & y_2 & z_2 \\\\ x_3 & y_3 & z_3 \\\\ \\end{bmatrix}\\) and \\(\\x = \\begin{bmatrix} a \\\\ b \\\\ c \\\\ \\end{bmatrix}\\) then \\[\\A\\x = \\begin{bmatrix} x_1 & y_1 & z_1 \\\\ x_2 & y_2 & z_2 \\\\ x_3 & y_3 & z_3 \\\\ \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ c \\\\ \\end{bmatrix} = \\begin{bmatrix} ax_1+by_1+cz_1 \\\\ ax_2+by_2+cz_2 \\\\ ax_3+by_3+cz_3 \\\\ \\end{bmatrix}\\] But notice that the above can also be written as: \\[\\A\\x = \\begin{bmatrix} x_1 & y_1 & z_1 \\\\ x_2 & y_2 & z_2 \\\\ x_3 & y_3 & z_3 \\\\ \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ c \\\\ \\end{bmatrix} = \\begin{bmatrix} ax_1+by_1+cz_1 \\\\ ax_2+by_2+cz_2 \\\\ ax_3+by_3+cz_3 \\\\ \\end{bmatrix} = \\color{red}{a}\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\end{bmatrix} + \\color{green}{b}\\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ \\end{bmatrix} + \\color{blue}{c}\\begin{bmatrix} z_1 \\\\ z_2 \\\\ z_3 \\\\ \\end{bmatrix}\\] This above expression is called: The matrix \\(\\A\\) acts on the vector \\(\\x\\) and the output is a linear combination of the columns of the matrix \\(\\A\\) .","title":"Example (Right Multiplication)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_multiplication/#example-linear-transformation","text":"We won't be going through the formal definition yet, but one could directly see as a consequence of the previous example. Given a 3 by 3 matrix \\(A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 2 & 4 & 6 \\\\ 3 & 6 & 9 \\\\ \\end{bmatrix}\\) and \\(\\x = \\begin{bmatrix} 1 \\\\ 2 \\\\ 4 \\\\ \\end{bmatrix}\\) then the geometric meaning of \\(\\A\\x\\) can be defined by a series of \"linear transformations\" categorized by scaling the first column of \\(\\A\\) by 1, then add the result to 2 times of the second column of \\(\\A\\) , and add the result to 4 times of the third column of \\(\\A\\) .","title":"Example (Linear Transformation)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_multiplication/#definition-right-multiplication","text":"Given a \\(m \\times n\\) matrix \\(\\A=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix}\\) and a column vector \\(\\x=\\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{n}\\end{bmatrix}\\) then \\[\\A\\x = \\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix}\\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{n}\\end{bmatrix} = \\color{red}{x_1}\\begin{bmatrix} a_{11} \\\\ a_{12} \\\\ \\vdots \\\\ a_{m1} \\\\ \\end{bmatrix} + \\color{green}{x_2}\\begin{bmatrix} a_{12} \\\\ a_{22} \\\\ \\vdots \\\\ a_{m2} \\\\ \\end{bmatrix} + \\cdots + \\color{blue}{x_n}\\begin{bmatrix} a_{1n} \\\\ a_{2n} \\\\ \\vdots \\\\ a_{mn} \\\\ \\end{bmatrix}\\] If the formula looks daunting, just remember that \\(\\A\\x\\) gives nothing but the linear combination of the columns of \\(\\A\\) with values of \\(\\x\\) as coefficients.","title":"Definition (Right Multiplication)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_multiplication/#linear-equations-right-multiplication","text":"This is a very important realization that one must have, I will mention it right here first and will repeat it throughout. Given a 3 by 3 matrix \\(A = \\begin{bmatrix} x_1 & y_1 & z_1 \\\\ x_2 & y_2 & z_2 \\\\ x_3 & y_3 & z_3 \\\\ \\end{bmatrix}\\) and \\(\\x = \\begin{bmatrix} a \\\\ b \\\\ c \\\\ \\end{bmatrix}\\) then \\[\\A\\x = \\begin{bmatrix} x_1 & y_1 & z_1 \\\\ x_2 & y_2 & z_2 \\\\ x_3 & y_3 & z_3 \\\\ \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ c \\\\ \\end{bmatrix} = \\begin{bmatrix} ax_1+by_1+cz_1 \\\\ ax_2+by_2+cz_2 \\\\ ax_3+by_3+cz_3 \\\\ \\end{bmatrix} = \\color{red}{a}\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\end{bmatrix} + \\color{green}{b}\\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ \\end{bmatrix} + \\color{blue}{c}\\begin{bmatrix} z_1 \\\\ z_2 \\\\ z_3 \\\\ \\end{bmatrix}\\] In the examples above, \\(\\A\\) and \\(\\x\\) are known and we want to find the unknown \\(\\b\\) which is the product of \\(\\A\\x\\) . Now, if we know \\(\\A\\) and \\(\\b\\) and wish to find \\(\\x\\) that solves the linear equation/system \\(\\A\\x = \\b\\) instead, what can we understand from the above? Since we know \\(\\b = \\A\\x\\) is a linear combination of the columns of \\(\\A\\) with values of \\(\\x\\) as coefficients. Then if we want to solve for \\(\\x\\) , we ask ourselves what combination of the columns of \\(\\A = \\begin{bmatrix} \\x & \\y & \\z \\end{bmatrix}\\) gives rise to the vector \\(\\b\\) ? If we can find the combination \\(a, b, c\\) , we can recover \\(\\x = \\begin{bmatrix} a \\\\ b \\\\ c \\\\ \\end{bmatrix}\\)","title":"Linear Equations (Right Multiplication)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_multiplication/#column-space-right-multiplication","text":"A column space of a matrix \\(\\A\\) is just the set of linear combination of the columns of \\(\\A\\) , and is a subspace. We will go through it in more details, but for now, I want to introduce this idea first. As a consequence of the example prior, one should realize two things: \\(\\A\\x = \\b\\) may not always have a solution \\(\\x\\) . If \\(\\A\\x = \\b\\) has a solution \\(\\x\\) , then the product \\(\\b\\) must be a linear combination of the columns of \\(\\A\\) . This has important consequences later on. For now, just know that if \\(\\A\\x = \\b\\) has a solution, then \\(\\b\\) resides in the column space of \\(\\A\\) .","title":"Column Space (Right Multiplication)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_multiplication/#left-multiplication-linear-combination-of-rows","text":"This part is also necessary to better understand matrix multiplication later.","title":"Left Multiplication (Linear Combination of Rows)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_multiplication/#example-left-multiplication","text":"We motivate this with an example. Given a 3 by 3 matrix \\(A = \\begin{bmatrix} x_1 & y_1 & z_1 \\\\ x_2 & y_2 & z_2 \\\\ x_3 & y_3 & z_3 \\\\ \\end{bmatrix}\\) and \\(\\x = \\begin{bmatrix} a & b & c \\end{bmatrix}\\) then \\[\\x\\A = \\begin{bmatrix} a & b & c \\end{bmatrix} \\begin{bmatrix} x_1 & y_1 & z_1 \\\\ x_2 & y_2 & z_2 \\\\ x_3 & y_3 & z_3 \\\\ \\end{bmatrix} = \\begin{bmatrix} ax_1+bx_2+cx_3 & ay_1+by_2+cy_3 & az_1+bz_2+cz_3 \\end{bmatrix}\\] But notice that the above can also be written as: \\[\\x\\A = \\begin{bmatrix} x_1 & y_1 & z_1 \\\\ x_2 & y_2 & z_2 \\\\ x_3 & y_3 & z_3 \\\\ \\end{bmatrix} = \\begin{bmatrix} ax_1+bx_2+cx_3 & ay_1+by_2+cy_3 & az_1+bz_2+cz_3 \\end{bmatrix} = \\color{red}{a}\\begin{bmatrix} x_1 & y_1 & z_1 \\end{bmatrix} + \\color{green}{b}\\begin{bmatrix} x_2 & y_2 & z_2 \\end{bmatrix} + \\color{blue}{c}\\begin{bmatrix} x_3 & y_3 & z_3 \\end{bmatrix}\\] Notice that now \\(\\x\\A\\) is just a linear combination of the rows of \\(\\A\\) .","title":"Example (Left Multiplication)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_multiplication/#the-four-interpretations-of-matrix-multiplication","text":"","title":"The Four Interpretations of Matrix Multiplication"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_multiplication/#element-wise-matrix-multiplication","text":"We have mentioned in the previous section. Here we repeat again for the sake of modularity. Let \\[\\A=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix},\\quad\\mathbf{B}=\\begin{bmatrix} b_{11} & b_{12} & \\cdots & b_{1p} \\\\ b_{21} & b_{22} & \\cdots & b_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ b_{n1} & b_{n2} & \\cdots & b_{np} \\\\ \\end{bmatrix}\\] then the matrix product \\(\\C = \\A\\B\\) is defined to be the \\(m \\times p\\) matrix: \\[\\mathbf{C}=\\A\\B=\\begin{bmatrix} c_{11} & c_{12} & \\cdots & c_{1p} \\\\ c_{21} & c_{22} & \\cdots & c_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ c_{m1} & c_{m2} & \\cdots & c_{mp} \\\\ \\end{bmatrix}\\] where \\(c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} +\\cdots + a_{in}b_{nj}= \\sum_{k=1}^n a_{ik}b_{kj}\\) for \\(i = 1, \\cdots , m\\) and \\(j = 1, \\cdots , p\\) . That is, the entry \\(c_{i,j}\\) of the product is obtained by multiplying term-by-term the entries of the \\(i\\) -th row of \\(\\A\\) and the \\(j\\) -th column of \\(\\B\\) , and summing these \\(n\\) products. In other words, \\(c_{i,j}\\) is the dot product of the \\(i\\) -th row of \\(\\A\\) and the \\(j\\) -th column of \\(\\B\\) . Therefore, \\(\\C = \\A\\B\\) can also be written as \\[\\mathbf{C}=\\A\\B=\\begin{bmatrix} a_{11}b_{11} +\\cdots + a_{1n}b_{n1} & a_{11}b_{12} +\\cdots + a_{1n}b_{n2} & \\cdots & a_{11}b_{1p} +\\cdots + a_{1n}b_{np} \\\\ a_{21}b_{11} +\\cdots + a_{2n}b_{n1} & a_{21}b_{12} +\\cdots + a_{2n}b_{n2} & \\cdots & a_{21}b_{1p} +\\cdots + a_{2n}b_{np} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1}b_{11} +\\cdots + a_{mn}b_{n1} & a_{m1}b_{12} +\\cdots + a_{mn}b_{n2} & \\cdots & a_{m1}b_{1p} +\\cdots + a_{mn}b_{np} \\\\ \\end{bmatrix} \\] Thus the product \\(\\A\\B\\) is defined if and only if the number of columns in \\(\\A\\) equals the number of rows in \\(\\B\\) .","title":"Element Wise Matrix Multiplication"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_multiplication/#significance-element-wise-matrix-multiplication","text":"From Mike X Cohen: Linear Algebra: Theory, Intuition, Code, 2021. (pp. 144) , he mentioned the following: The diagonal of \\(\\C\\) contains dot products between row \\(i\\) and column \\(j\\) of \\(\\A\\) and \\(\\B\\) respectively, this is relevant in data covariance matrices. The lower triangle of \\(\\C\\) contains dot products between row \\(i\\) in \\(\\A\\) and column \\(j\\) in \\(\\B\\) where \\(i > j\\) . The upper triangle of \\(\\C\\) contains dot products between row \\(i\\) in \\(\\A\\) and column \\(j\\) in \\(\\B\\) where \\(i < j\\) . Both are important in matrix decompositions, such as QR decomposition and generalized eigendecomposition .","title":"Significance (Element Wise Matrix Multiplication)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_multiplication/#outer-product-wise-matrix-multiplication","text":"To simplify notation, we denote: \\( \\(\\A = \\begin{bmatrix} \\a_1 & \\a_2 \\cdots & \\a_n \\end{bmatrix}, \\quad \\B = \\begin{bmatrix} \\b_1^\\top \\\\ \\b_2^\\top \\\\ \\cdots \\\\ \\b_n^\\top \\end{bmatrix}\\) \\) as shorthand where \\(\\a_i\\) is the \\(i\\) -th column of \\(\\A\\) and \\(\\b_j^\\top\\) is the \\(j\\) -th row of \\(\\B\\) . Then \\[\\A\\B = \\a_1\\b_1^\\top + \\a_2\\b_2^\\top + \\cdots + \\a_n\\b_n^\\top\\] Notice that in each of the \\(\\a_i\\b_i^\\top\\) , the columns form a dependent set (the same can be said of the rows). However, the sum of these singular matrices\u2014the product matrix\u2014has columns that form a linearly independent set. Each of these matrices is rank-1 (to be defined later). This forms the basis for the Singular Value Decomposition. - Mike X Cohen: Linear Algebra: Theory, Intuition, Code, 2021. (pp. 145)","title":"Outer Product Wise Matrix Multiplication"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_multiplication/#matrix-multiplication-using-right-multiplication-columns","text":"Using back the notation in the Element Wise Matrix Multiplication , we can define \\[\\A\\B = \\A \\begin{bmatrix} \\b_1 & \\b_2 & \\cdots & \\b_p \\end{bmatrix} = \\begin{bmatrix} \\A\\b_1 & \\A\\b_2 & \\cdots & \\A\\b_p \\end{bmatrix}\\] where \\(\\b_i\\) is the column \\(i\\) of the matrix \\(\\B\\) . This means that each column of the matrix \\(\\C = \\A\\B\\) is defined by \\(\\A\\b_i\\) , and recall in the section \"Matrix-Vector Right Multiplication\", \\(\\A\\b_i\\) means a linear combination of the columns of \\(\\A\\) with weight coefficients in \\(\\b_i\\) . They say a picture is worth a thousand words. The below images are taken from Eli Bendersky's website here . Matrix Multiplication, Column Perspective; Courtesy of Eli Bendersky","title":"Matrix Multiplication using Right Multiplication (Columns)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_multiplication/#significance","text":"The column perspective of matrix multiplication is useful in statistics, when the columns of the left matrix contain a set of regressors (a simplified model of the data), and the right matrix contains coefficients. The coefficients encode the importance of each regressor, and the goal of statistical model-fitting is to find the best coefficients such that the weighted combination of regressors matches the data. - Mike X Cohen: Linear Algebra: Theory, Intuition, Code, 2021. (pp. 146)","title":"Significance"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_multiplication/#matrix-multiplication-using-left-multiplication-rows","text":"Using back the notation in the Element Wise Matrix Multiplication , we can define \\[\\A\\B = \\begin{bmatrix}\\a_1 \\\\ \\a_2 \\\\ \\vdots \\\\ \\a_m \\end{bmatrix}\\B = \\begin{bmatrix}\\a_1\\B \\\\ \\a_2\\B \\\\ \\vdots \\\\ \\a_m\\B \\end{bmatrix}\\] where \\(\\a_i\\) is the row \\(i\\) of the matrix \\(\\A\\) . This means that each row of the matrix \\(\\C = \\A\\B\\) is defined by \\(\\a_i\\B\\) , and recall in the section \"Matrix-Vector Left Multiplication\", \\(\\a_i\\B\\) means a linear combination of the row of \\(\\B\\) with weight coefficients in \\(\\a_i\\) . This becomes apparent when we come to the chapter of Row-Echolon Form . They say a picture is worth a thousand words. The below images are taken from Eli Bendersky's website here . Matrix Multiplication, Row Perspective; Courtesy of Eli Bendersky","title":"Matrix Multiplication using Left Multiplication (Rows)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_multiplication/#significance_1","text":"The row perspective is useful, for example in principal components analysis, where the rows of the right matrix contain data (observations in rows and features in columns) and the rows of the left matrix contain weights for combining the features. Then the weighted sum of data creates the principal component scores. - Mike X Cohen: Linear Algebra: Theory, Intuition, Code, 2021. (pp. 147)","title":"Significance"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_multiplication/#matrix-multiplication-properties","text":"Read Wikipedia . To fill in when free as it is relatively light.","title":"Matrix Multiplication Properties"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_multiplication/#symmetric-matrices","text":"","title":"Symmetric Matrices"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_multiplication/#matrix-multiplication-naive-in-python","text":"For a Divide-and-conquer method, see here 3 . import os import random import numpy as np import torch def seed_all ( seed : int = 19921930 ) -> None : \"\"\"Seed all random number generators.\"\"\" print ( f \"Using Seed Number { seed } \" ) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) # set PYTHONHASHSEED env var at fixed value torch . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) torch . cuda . manual_seed ( seed ) # pytorch (both CPU and CUDA) np . random . seed ( seed ) # for numpy pseudo-random generator # set fixed value for python built-in pseudo-random generator random . seed ( seed ) torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = False torch . backends . cudnn . enabled = False seed_all () Using Seed Number 19921930 import torch import numpy as np from typing import Tuple , List A = np . random . randint ( 0 , 10 , size = ( 3 , 2 )) B = np . random . randint ( 0 , 10 , size = ( 2 , 2 ))","title":"Matrix Multiplication (Naive) in Python"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_multiplication/#pseudo-code","text":"Let us run through the matrix multiplication between two simple matrices \\(\\A\\) and \\(\\B\\) . We define their product \\(\\C = \\A\\B\\) to be a matrix of shape \\((3, 2)\\) . Take first row of \\(\\A\\) and first column of \\(\\B\\) , compute the dot product of both and assign the value to the first entry of \\(\\C\\) (i.e. \\(\\C_{1, 1}\\) ). Take first row of \\(\\A\\) and second column of \\(\\B\\) , compute the dot product of both and assign the value to the first entry of \\(\\C\\) (i.e. \\(\\C_{1, 2}\\) ). Take second row of \\(\\A\\) and first column of \\(\\B\\) , compute the dot product of both and assign the value to the first entry of \\(\\C\\) (i.e. \\(\\C_{2, 1}\\) ). Take second row of \\(\\A\\) and second column of \\(\\B\\) , compute the dot product of both and assign the value to the first entry of \\(\\C\\) (i.e. \\(\\C_{2, 2}\\) ). Take third row of \\(\\A\\) and first column of \\(\\B\\) , compute the dot product of both and assign the value to the first entry of \\(\\C\\) (i.e. \\(\\C_{3, 1}\\) ). Take third row of \\(\\A\\) and second column of \\(\\B\\) , compute the dot product of both and assign the value to the first entry of \\(\\C\\) (i.e. \\(\\C_{3, 2}\\) ). We have seen the above in laymen english. We can easily convert the above to pseudo-code, but first we need to recognize that the above method is the \"Element-wise\" method, where the outer loop is rows of \\(\\A\\) , and the inner loop is columns of \\(\\B\\) . Input: matrices \\(\\A\\) and \\(\\B\\) of size \\((m, n)\\) and \\((n, p)\\) respectively. Initialize matrix \\(\\C\\) to be the product of \\(\\A\\B\\) with the correct shape. For row i from 1 to m: For col j from 1 to p: compute dot product of row i of A and col j of B: dot assign the dot product to entry \\(C_{i, j}\\) return \\(\\C\\) We have removed a layer of abstraction above, and that is the computation of the dot product, in which we have coded up previously. If there is no abstraction, then: Input: matrices \\(\\A\\) and \\(\\B\\) of size \\((m, n)\\) and \\((n, p)\\) respectively. Initialize matrix \\(\\C\\) to be the product of \\(\\A\\B\\) with the correct shape. For row i from 1 to m: Initialize summation = 0 For col j from 1 to p: for k from 1 to n: summation += dot product of row i of A and col j of B assign the dot product summation to entry \\(C_{i, j}\\) return \\(\\C\\)","title":"Pseudo Code"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_multiplication/#python-code","text":"def linear_combination_vectors ( weights : List [ float ], * args : np . ndarray ) -> np . ndarray : \"\"\"Computes the linear combination of vectors. Args: weights (List[float]): The set of weights corresponding to each vector. Returns: linear_weighted_sum (np.ndarray): The linear combination of vectors. Examples: >>> v1 = np.asarray([1, 2, 3, 4, 5]).reshape(-1, 1) >>> v2 = np.asarray([2, 4, 6, 8, 10]).reshape(-1, 1) >>> v3 = np.asarray([3, 6, 9, 12, 15]).reshape(-1, 1) >>> weights = [10, 20, 30] >>> linear_combination_vectors([10, 20, 30], v1, v2, v3) \"\"\" linear_weighted_sum = np . zeros ( shape = args [ 0 ] . shape ) for weight , vec in zip ( weights , args ): linear_weighted_sum += weight * vec return linear_weighted_sum def dot_product ( v1 : np . ndarray , v2 : np . ndarray ) -> float : \"\"\"Computes the dot product of two vectors. We assume both vectors are flattened, i.e. they are 1D arrays. Args: v1 (np.ndarray): The first vector. v2 (np.ndarray): The second vector. Returns: dot_product_v1_v2 (float): The dot product of two vectors. Examples: >>> v1 = np.asarray([1, 2, 3, 4, 5]) >>> v2 = np.asarray([2, 4, 6, 8, 10]) >>> dot_product(v1, v2) \"\"\" v1 , v2 = np . asarray ( v1 ) . flatten (), np . asarray ( v2 ) . flatten () dot_product_v1_v2 = 0 for element_1 , element_2 in zip ( v1 , v2 ): dot_product_v1_v2 += element_1 * element_2 # same as np.dot but does not take into the orientation of vectors assert dot_product_v1_v2 == np . dot ( v1 . T , v2 ) return dot_product_v1_v2 def get_matmul_shape ( A : np . ndarray , B : np . ndarray ) -> Tuple [ int , int , int ]: \"\"\"Check if the shape of the matrices A and B are compatible for matrix multiplication. If A and B are of size (m, n) and (n, p), respectively, then the shape of the resulting matrix is (m, p). Args: A (np.ndarray): The first matrix. B (np.ndarray): The second matrix. Raises: ValueError: Raises a ValueError if the shape of the matrices A and B are not compatible for matrix multiplication. Returns: (Tuple[int, int, int]): (m, n, p) where (m, n) is the shape of A and (n, p) is the shape of B. \"\"\" if A . shape [ 1 ] != B . shape [ 0 ]: raise ValueError ( f \"The number of columns of A must be equal to the number of rows of B, but got { A . shape [ 1 ] } and { B . shape [ 0 ] } respectively.\" ) return ( A . shape [ 0 ], A . shape [ 1 ], B . shape [ 1 ]) def np_matmul_naive ( A : np . ndarray , B : np . ndarray ) -> np . ndarray : \"\"\"Computes the matrix multiplication of two matrices. Args: A (np.ndarray): The first matrix. B (np.ndarray): The second matrix. Returns: matmul (np.ndarray): The matrix multiplication of two matrices. \"\"\" num_rows_A , common_index , num_cols_B = check_matmul_shape ( A , B ) matmul = np . zeros ( shape = ( num_rows_A , num_cols_B )) # 1st loop: loops through first matrix A for i in range ( num_rows_A ): summation = 0 # 2nd loop: loops through second matrix B for j in range ( num_cols_B ): # 3rd loop: computes dot prod for k in range ( common_index ): summation += A [ i , k ] * B [ k , j ] matmul [ i , j ] = summation return matmul def np_matmul_element_wise ( A : np . ndarray , B : np . ndarray ) -> np . ndarray : \"\"\"Computes the matrix multiplication of two matrices using element wise method. Args: A (np.ndarray): The first matrix. B (np.ndarray): The second matrix. Returns: matmul (np.ndarray): The matrix multiplication of two matrices. \"\"\" num_rows_A , _ , num_cols_B = check_matmul_shape ( A , B ) matmul = np . zeros ( shape = ( num_rows_A , num_cols_B )) # 1st loop: loops through first matrix A for row_i in range ( num_rows_A ): # 2nd loop: loops through second matrix B for col_j in range ( num_cols_B ): # computes dot product of row i with column j of B and # assign the result to the element of the matrix matmul at row i and column j. matmul [ row_i , col_j ] = dot_product ( A [ row_i , :], B [:, col_j ]) return matmul def np_matmul_column_wise ( A : np . ndarray , B : np . ndarray ) -> np . ndarray : \"\"\"Computes the matrix multiplication of two matrices using column wise method. Recall the section on Matrix Multiplication using Right Multiplication. Column i of C is represented by: Ab_i Args: A (np.ndarray): The first matrix. B (np.ndarray): The second matrix. Returns: matmul (np.ndarray): The matrix multiplication of two matrices. \"\"\" num_rows_A , _ , num_cols_B = check_matmul_shape ( A , B ) matmul = np . zeros ( shape = ( num_rows_A , num_cols_B )) # we just need to populate the columns of C for col_i in range ( matmul . shape [ 1 ]): # b_i col_i_B = B [:, col_i ] # Ab_i linear_comb_A_on_col_i_B = linear_combination_vectors ( col_i_B , * A . T ) # C_i = Ab_i matmul [:, col_i ] = linear_comb_A_on_col_i_B return matmul # assert the function output is similar to numpy's A @ B np . allclose ( np_matmul_element_wise ( A , B ), np_matmul_column_wise ( A , B ), A @ B ) True","title":"Python Code"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_multiplication/#time-complexity","text":"The time complexity can be easily found in the pseudo-code. Input: matrices \\(\\A\\) and \\(\\B\\) of size \\((m, n)\\) and \\((n, p)\\) respectively. Initialize matrix \\(\\C\\) to be the product of \\(\\A\\B\\) with the correct shape. For row i from 1 to m: Initialize summation = 0 For col j from 1 to p: for k from 1 to n: summation += dot product of row i of A and col j of B assign the dot product summation to entry \\(C_{i, j}\\) return \\(\\C\\) In rough terms, the first outer loop has \\(m\\) operations, the second loop has \\(p\\) operations and last loop has \\(n\\) operations; we further assume each operation is \\(\\mathcal{O}(1)\\) and thus we have a total of \\(m \\times n \\times p\\) loops (operations), so the time complexity is \\( \\(\\mathcal{O}(mnp) \\approx \\mathcal{O}(n^3)\\) \\) if \\(m \\approx n \\approx p\\) .","title":"Time Complexity"},{"location":"reighns_ml_journey/mathematics/linear_algebra/04_matrices/04_linear_algebra_matrix_multiplication/#references","text":"https://eli.thegreenplace.net/2015/visualizing-matrix-multiplication-as-a-linear-combination/ https://math.stackexchange.com/questions/192835/fastest-and-intuitive-ways-to-look-at-matrix-multiplication https://math.stackexchange.com/questions/24456/matrix-multiplication-interpreting-and-understanding-the-process https://en.wikipedia.org/wiki/Matrix_multiplication \u21a9 https://eli.thegreenplace.net/2015/visualizing-matrix-multiplication-as-a-linear-combination/ \u21a9 solvay_strassen_algorithm \u21a9","title":"References"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/","text":"\\[ \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\r}{\\mathbf{r}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\z}{\\mathbf{z}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\C}{\\mathbf{C}} \\newcommand{\\rank}{\\textbf{rank}} \\] Preamble Both Professor Mike and Gilbert started off with this statement: The entirety and gist of the matrix space can be summarized as answering two key questions: Given a matrix \\(\\A \\in \\F^{m \\times n}\\) and vector \\(\\b \\in \\F^{m}\\) and \\(\\0 \\in \\F^{m}\\) , then can we find a vector \\(\\x \\in \\F^{n}\\) such that \\( \\(\\A\\x = \\b\\) \\) \\( \\(\\A\\x = \\0\\) \\) From the matrix-multiplication section, we can easily re-interpret the question as: \\(\\A\\x=\\b\\) is equivalent to asking can we find linear combination of columns of \\(\\A\\) such that the sum is \\(\\b\\) ? \\(\\A\\x=\\0\\) is equivalent to asking can we find linear combination of columns of \\(\\A\\) such that the sum is \\(\\0\\) assuming \\(\\x \\neq \\0\\) . Column Space Motivation We often frame our Machine Learning problems into a linear system of equations of the form \\(\\A\\x=\\b\\) . Not every linear system of equation is easily solvable, and has a solution \\(\\x\\) exists if and only if \\(\\b\\) belongs to the column space of \\(\\A\\) . Why so? Recall in the section linear algebra and matrix multiplication the following (if you forgot go read up): A column space of a matrix \\(\\A\\) is just the set of linear combination of the columns of \\(\\A\\) , and is a subspace. We will go through it in more details, but for now, I want to introduce this idea first. As a consequence of the example prior, one should realize three things: \\(\\A\\x\\) is a combination of the columns of the matrix \\(\\A\\) . \\(\\A\\x = \\b\\) may not always have a solution \\(\\x\\) . If \\(\\A\\x = \\b\\) has a solution \\(\\x\\) , then the product \\(\\b\\) must be a linear combination of the columns of \\(\\A\\) . This has important consequences later on. For now, just know that if \\(\\A\\x = \\b\\) has a solution, then \\(\\b\\) resides in the column space of \\(\\A\\) . Algebraic Definition (Column Space) Let \\(\\F\\) be a field of scalars and let \\(\\A\\) a \\(m \\times n\\) matrix, with column vectors \\(\\a_1, \\a_2, \\cdots, \\a_n\\) . Note that a linear combination of these vectors is any vector of the form \\[c_1 \\mathbf{a}_1 + c_2 \\mathbf{a}_2 + \\cdots + c_n \\mathbf{a}_n\\] where \\(c_i\\) are scalars; Then the set of all possible linear combinations of \\(\\a_1, \\a_2, \\cdots, \\a_n\\) is called the column space of \\(\\A\\) ; In other words, the column space of \\(\\A\\) is the linear span of the vectors \\(\\a_1, \\a_2, \\cdots, \\a_n\\) . And as noted in the right multiplication method in the section matrix-vector multiplication , any linear combination of the column vectors of a matrix \\(\\A\\) can be written as the product of \\(\\A\\) with a column vector where the column vectors hold the coefficients of the linear combination: \\[ \\begin{array} {rcl} \\A \\begin{bmatrix} c_1 \\\\ \\vdots \\\\ c_n \\end{bmatrix} & = & \\begin{bmatrix} a_{11} & \\cdots & a_{1n} \\\\ \\vdots & \\ddots & \\vdots \\\\ a_{m1} & \\cdots & a_{mn} \\end{bmatrix} \\begin{bmatrix} c_1 \\\\ \\vdots \\\\ c_n \\end{bmatrix} = \\begin{bmatrix} c_1 a_{11} + \\cdots + c_{n} a_{1n} \\\\ \\vdots \\\\ c_{1} a_{m1} + \\cdots + c_{n} a_{mn} \\end{bmatrix} = c_1 \\begin{bmatrix} a_{11} \\\\ \\vdots \\\\ a_{m1} \\end{bmatrix} + \\cdots + c_n \\begin{bmatrix} a_{1n} \\\\ \\vdots \\\\ a_{mn} \\end{bmatrix} \\\\ & = & c_1 \\mathbf{v}_1 + \\cdots + c_n \\mathbf{v}_n \\end{array} \\] Therefore, the column space of \\(\\A\\) consists of all possible products \\(\\A\\x\\) for \\(\\x \\in \\F^n\\) . This is the same as the image or range of the corresponding matrix transformation in which we will learn more on in the linear transformation chapter. Notation (Column Space) The column space of a matrix is denoted as \\(C(\\A)\\) , which is the space spanned by all columns of the matrix \\(\\A\\) . Note that the column space \\(C(\\A)\\) resides in the \\(\\F^{m}\\) space. Example (Column Space) If \\(A = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 2 & 0 \\end{bmatrix}\\) , then the column vectors are \\(\\a_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 2 \\end{bmatrix} \\quad \\a_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0\\end{bmatrix}\\) . A linear combination of \\(\\a_1, \\a_2\\) is any vector of the form \\[c_1 \\begin{bmatrix} 1 \\\\ 0 \\\\ 2 \\end{bmatrix} + c_2 \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ 2c_1 \\end{bmatrix}\\] The set of all such vectors is the column space of \\(\\A\\) and in this case, the column space is precisely the set of vectors \\((x, y, z) \\in \\R^3\\) satisfying the equation \\(z=2x\\) using Cartesian coordinates, this set is a plane through the origin in three-dimensional space. Column Space and Basis Note that the columns of the matrix \\(\\A\\) spans the column space, but may not form a basis since there is no guarantee that the columns are linearly independent. However, recall the theorem that elementary row elimination/reduction does not affect the dependency of the column vectors , therefore, we still can use row reduction to find a basis for the column space. Dimension and Rank The dimension of the column space is called the rank of the matrix. Thus, if the rank of the matrix \\(\\A\\) is \\(r = n\\) , then the column space spans all of \\(\\F^{n}\\) and is a basis of \\(\\F^{n}\\) , else it is a \\(r\\) -dimensional subspace embedded in \\(\\F^{n}\\) . Theorem (Column Space of \\(\\mathbf{A}\\) and \\(\\mathbf{A}\\mathbf{A}^\\top\\) ) \\(\\mathbf{A}\\) and \\(\\mathbf{A}\\mathbf{A}^\\top\\) have the same column space. Proof (Column Space of \\(\\mathbf{A}\\) and \\(\\mathbf{A}\\mathbf{A}^\\top\\) ) We first write \\(\\B = \\A\\A^\\top\\) and use the method that if \\(M \\subseteq N\\) and \\(N \\subseteq M\\) , then \\(N=M\\) to prove this. We first show that \\(C(\\B) \\subseteq C(\\A)\\) . The column space \\(C(\\A) = \\text{span}(\\a_1, \\a_2, ..., \\a_n) \\in \\F^{m}\\) and \\(C(\\B) = \\text{span}(\\b_1, \\b_2, ..., \\b_m)\\) . We note that by the right matrix multiplication , each column of \\(\\B = \\A\\A^\\top\\) is a linear combination of the columns of the left matrix \\(\\A\\) . That is to say, we can represent each column in \\(\\B\\) as \\(\\b_i = \\lambda_1 \\a_1 + \\lambda_2 \\a_2 + \\cdots + \\lambda_n \\a_n\\) . Consequently, by definition, any column from \\(\\B\\) is an element of \\(\\text{span}(\\a_1, ..., \\a_n)\\) , and hence in the column space \\(C(\\A)\\) . Thus, for any element in \\(C(\\B)\\) , this element must be in \\(C(\\A)\\) , and thus \\(C(\\B) \\subseteq C(\\A)\\) . We then show that \\(C(\\A) \\subseteq C(\\B)\\) . Take any element from \\(C(\\A)\\) , and recall that \\(C(\\B) = \\text{span}(\\b_1, \\b_2, ..., \\b_m) = c_1 \\b_1 + c_2 \\b_2 + \\cdots + c_m \\b_m = c_1 (\\lambda_1 \\a_1 + \\lambda_2 \\a_2 + \\cdots + \\lambda_m \\a_m) + c_2 (\\lambda_1 \\a_1 + \\lambda_2 \\a_2 + \\cdots + \\lambda_m \\a_m) + ... + c_2 (\\lambda_1 \\a_1 + \\lambda_2 \\a_2 + \\cdots + \\lambda_m \\a_m) = d_1\\a_1 + d_2\\a_2 + ... + d_m \\a_m\\) for some \\(d_i \\in \\F\\) . Then any element taken from the column space of \\(\\A\\) is a linear combination of \\(\\a_1, ..., \\a_m\\) ... What can we tell? We cannot tell anything since if \\(n > m\\) , then it may be the case that an element from \\(C(\\A)\\) may not cover. The \"Augment-Rank\" Algorithm to determine membership of Column Space Given a set of vectors \\(S = \\{\\v_1, \\v_2, ..., \\v_n\\}\\) , and a vector \\(\\w\\) , deduce if \\(\\w \\in \\textbf{span}(S)\\) . Assume \\(\\v_i \\in \\R^{m}\\) and \\(\\w \\in \\R^{m}\\) . Construct a matrix \\(\\mathbf{S} = \\begin{bmatrix} \\v_1 & \\v_2 & \\cdots & \\v_m \\end{bmatrix}_{m \\times n}\\) where \\(\\mathbf{S} \\in \\R^{m \\times n}\\) . Compute the rank of \\(\\mathbf{S}\\) and call it \\(s_1\\) . Horizontally concatenate \\(\\mathbf{S}\\) and \\(\\w\\) to get \\(\\mathbf{S}_w = \\mathbf{S} \\cup \\w\\) . Compute rank of \\(\\mathbf{S}_w\\) to be \\(s_2\\) . Then: If \\(s_1 = s_2\\) , this means that column space of \\(\\mathbf{S}\\) equals to the column space of \\(\\mathbf{S}_w\\) , and thus \\(\\w\\) did not alter the column space of \\(\\mathbf{S}\\) , which means \\(\\w\\) must be part of the column space of \\(\\mathbf{S}\\) , and thus in the span of \\(S\\) . If \\(s_2 > s_1\\) , then \\(\\w \\not\\in S\\) because if it is, the column space of \\(\\mathbf{S}_w\\) should not change. We can easily use this algorithm to check if a vector \\(\\w\\) is in the column space of a matrix \\(\\A\\) by setting \\(S\\) to be the set that contains all the columns of \\(\\A\\) . Geometric Intuition Mike mentioned on Linear Algebra: Theory, Intuition, Code, 2021. (pp. 211) that we can think of the above cases geometrically. In the first case where \\(s_1 = s_2\\) , the rank is the same, this means the vector \\(\\w\\) is in the column space of \\(\\A\\) . This makes sense because when we add the new vector to the set, and yet the rank (dimension) did not change, this coincides with the idea of vector \\(\\w\\) sitting somewhere in the column space of \\(\\A\\) , and hence no new geometric directions are obatined by including this vector . In the second case where \\(s_2 > s_1\\) however, if v is outside the column space, then it points off in some other geometric dimension that is not spanned by the column space; hence, B has one extra geometric dimension not contained in A, and thus the rank is one higher. In the image below, we denote \\(C(\\A)\\) as the 1d-subspace in red, then vector \\(\\v\\) is apparently lying in the 1d-subspace, and hence a member of \\(C(\\A)\\) , then \\(\\w\\) , is slightly pointing to a different direction, and hence not in the column space of \\(\\A\\) . Fig; Column Space of A; By Hongnan G. Corollary If \\(\\A\\) is now a full ranked square matrix \\(m \\times m\\) , which means rank of \\(\\A\\) is \\(m\\) , then any vector \\(\\w \\in \\R^{m}\\) will be in the column space of \\(\\A\\) since the column space of \\(\\A\\) is actually a basis of the ambient subspace \\(\\F^{m}\\) . Theorem (Row Operations preserves Column Space) Row operations will not change the dependence of the columns of a matrix. Visualizing Column Space Courtesy of Macro Analyst's linear algebra with python . import matplotlib.pyplot as plt import numpy as np from mpl_toolkits.mplot3d import Axes3D import scipy as sp import scipy.linalg import sympy as sy sy . init_printing () Consider two matrix with \\[\\A = \\begin{bmatrix}1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{bmatrix} ,\\quad \\B = \\begin{bmatrix}3 & -1 \\\\ 2 & 4 \\\\ -1 & 1 \\end{bmatrix}\\] Then the column space \\(C(\\A)\\) is a 2d-plane given by \\[\\text{plane}(C(\\A)) = \\0 + s\\left[\\matrix{1\\cr 0\\cr 0}\\right] + t\\left[\\matrix{0\\cr 1\\cr 0}\\right]\\] and the column space \\(C(\\B)\\) is a 2d-plane given by \\[\\text{plane}(C(\\B)) = \\0 + s\\left[\\matrix{3\\cr 2\\cr -1}\\right] + t\\left[\\matrix{-1\\cr 4\\cr 1}\\right]\\] fig = plt . figure ( figsize = ( 10 , 10 )) ax = fig . add_subplot ( projection = '3d' ) s = np . linspace ( - 2 , 2 , 20 ) t = np . linspace ( - 2 , 2 , 20 ) S , T = np . meshgrid ( s , t ) X = 3 * S - T Y = 2 * S + 4 * T Z = - S + T ax . plot_wireframe ( X , Y , Z , linewidth = .5 , color = 'r' ) s = np . linspace ( - 10 , 10 , 20 ) t = np . linspace ( - 10 , 10 , 20 ) S , T = np . meshgrid ( s , t ) X = S Y = T Z = np . zeros ( S . shape ) ax . plot_wireframe ( X , Y , Z , linewidth = .5 , color = 'k' ) ax . view_init ( elev = 14 , azim = 58 ) Row Space Algebraic Definition (Row Space) Let \\(\\F\\) be a field of scalars and let \\(\\A\\) a \\(m \\times n\\) matrix, with row vectors \\(\\a_1, \\a_2, \\cdots, \\a_m\\) . Note that a linear combination of these vectors is any vector of the form \\[c_1 \\mathbf{a}_1 + c_2 \\mathbf{a}_2 + \\cdots + c_m \\mathbf{a}_m\\] where \\(c_i\\) are scalars; Then the set of all possible linear combinations of \\(\\a_1, \\a_2, \\cdots, \\a_m\\) is called the row space of \\(\\A\\) ; In other words, the column space of \\(\\A\\) is the linear span of the vectors \\(\\a_1, \\a_2, \\cdots, \\a_m\\) . Note in particular that the row space \\(R(\\A)\\) resides in the \\(\\F^{n}\\) space. Notation (Row Space) The row space of a matrix is denoted as \\(R(\\A)\\) , which is the space spanned by all rows of the matrix \\(\\A\\) . Row Space and Basis Note that elementary row operations do not affect the row space, consequently, row reduction can be used to find a basis for the row space. Dimension and Rank The dimension of the row space is called the rank of the matrix. Thus, if the rank of the matrix \\(\\A\\) is \\(r = m\\) , then the column space spans all of \\(\\F^{m}\\) and is a basis of \\(\\F^{m}\\) , else it is a \\(r\\) -dimensional subspace embedded in \\(\\F^{m}\\) . Theorem (Row space of \\(\\mathbf{A}\\) is the Column space of \\(\\mathbf{A}^\\top\\) ) Given a matrix \\(\\A \\in \\F^{m \\times n}\\) , the rows of \\(\\A\\) is the columns of \\(\\A^\\top\\) , and hence the theorem follows. Theorem (Row Space of \\(\\mathbf{A}\\) and \\(\\mathbf{A}\\mathbf{A}^\\top\\) ) \\(\\mathbf{A}\\) and \\(\\mathbf{A}\\mathbf{A}^\\top\\) have the same row space. Applications in Machine Learning The author Mike mentioned in Linear Algebra: Theory, Intuition, Code, 2021. (pp. 214) that \\(R(\\A) = R(\\A^\\top\\A)\\) is an example of dimensionality reduction where both matrices have the same row space, but \\(\\A^\\top\\A\\) might be a much smaller matrix and hence computationally more efficient. (Right) Null Space (Kernel) Motivation To fill in. Algebraic Definition (Null Space) Let \\(\\F\\) be a field of scalars and let \\(\\A\\) a \\(m \\times n\\) matrix, with column vectors \\(\\a_1, \\a_2, \\cdots, \\a_n\\) . Note that a linear combination of these vectors is any vector of the form \\[c_1 \\mathbf{a}_1 + c_2 \\mathbf{a}_2 + \\cdots + c_n \\mathbf{a}_n\\] where \\(c_i\\) are scalars; Then the nullspace of \\(\\A\\) is the set: \\[ N(\\A) = \\{\\v \\in \\F^{n} ~|~ \\A\\v = \\0\\} \\] Note that the trivial solution \\(\\0\\) is always a solution to the nullspace. Note that if there exists a non-trivial \\(\\v\\) such that \\(\\A\\v = \\0\\) , then for any \\(\\lambda \\in F\\) , we also have \\(\\A(\\lambda\\v) = \\0\\) . Note in particular that the column space \\(N(\\A)\\) resides in the \\(\\F^{n}\\) space. Notation (Null Space) The null space of a matrix is denoted as \\(N(\\A)\\) . What does Null Space tell you? Now one important consequence is that, if there exists a non-trivial solution to the matrix \\(\\A\\x = \\0\\) , then what does it mean? Recall the right multiplication of columns for matrix-vector multiplication : It means that the linear combination of the columns of \\(\\A\\) can form the zero vector \\(\\0\\) . In particular, this linear combination is NON-TRIVIAL! Then the matrix \\(\\A\\) is formed by columns that are linearly dependent, and hence not full rank! We can formalize it as a theorem below. Theorem (Full Ranked Matrix has an Empty Nullspace) A matrix \\(\\A \\in \\F^{m \\times n}\\) is full rank if and only if the nullspace \\(N(\\A)\\) has only the trivial solution. Geometric Intuition (Null Space) Left (Null Space) Algebraic Definition (Left Null Space) Let \\(\\F\\) be a field of scalars and let \\(\\A\\) a \\(m \\times n\\) matrix, then the left nullspace of \\(\\A\\) is actually the nullspace of its transpose: \\[ N(\\A^\\top) = \\{\\v \\in \\F^{m} ~|~ \\A^\\top\\v = \\0\\} \\] and if we take transpose to both sides of \\(\\A^\\top\\v = \\0\\) to be \\(\\left(\\A^\\top\\v \\right)^\\top = \\0^\\top\\) ; it follows we have \\(\\v^\\top\\A = \\0^\\top\\) , where the row vector \\(\\v^\\top\\) is on the left of \\(\\A\\) . Note in particular that the column space \\(N(\\A)\\) resides in the \\(\\F^{m}\\) space. Theorem (The Nullspace and Left Nullspace of a Symmetric Matrix is Equal) Given a symmetric matrix \\(\\A\\) , we know that \\(\\A^\\top = \\A\\) , and thus \\(N(\\A) = N(\\A^\\top)\\) . Orthogonal Subspaces Row and Null Space are Orthogonal and Complements each other The row space and the null space of a matrix \\(\\A \\in \\F^{m \\times n}\\) are orthogonal complements . Proof (Row Space is Orthogonal to Null Space) Orthogonal Subspace We first show that both subspaces are orthogonal. Note that the null space of a matrix \\(\\A \\in F^{m \\times n}\\) is the set of all vectors \\(\\x\\) such that \\(\\A\\x = \\0\\) . We can also write \\(\\A\\x = \\0\\) as \\[ \\A\\x = \\begin{bmatrix} \\r_1 \\cdot \\x \\\\ \\r_2 \\cdot \\x \\\\ \\vdots \\\\ \\r_m \\cdot \\x \\end{bmatrix} \\] where \\(\\cdot\\) is the dot product and \\(\\r_i\\) the row vector \\(i\\) of \\(\\A\\) . Then one can easily see that \\(\\A\\x = \\0\\) if and only if every \\(\\r_i \\cdot \\x = 0\\) . Then it immediately follows that every \\(\\r_i\\) is orthogonal to \\(\\x\\) , and consequently, the nullspace and row space of \\(\\A\\) forms an orthogonal subspace . To see this, take any vector \\(\\r \\in R(\\A)\\) , and represent this \\(\\r = \\lambda_1 \\r_1 + ... + \\lambda_m \\r_m\\) , then \\[ \\begin{aligned} \\r \\cdot \\x &= (\\lambda_1 \\r_1 + ... + \\lambda_m \\r_m) \\cdot \\x \\\\ &= \\lambda_1 \\r_1 \\cdot \\x + ... + \\lambda_m \\r_m \\cdot \\x \\\\ &= \\0 + ... + \\0 \\\\ &= \\0 \\end{aligned} \\] Orthogonal Complements Reference 1 : We have proven that the row space \\(\\newcommand{\\R}{\\mathrm{R}} \\R(A)\\) and null space \\(\\newcommand{\\N}{\\mathrm{N}} \\N(A)\\) are orthogonal to each other; that is, \\(\\newcommand{\\r}{\\vec r} \\newcommand{\\n}{\\vec n} \\forall\\r\\in\\R(A)\\ \\forall\\n\\in\\N(A): \\r\\perp\\n\\) . Next, we show that they are complements of each other: \\[\\R(A)\\cap\\N(A)=\\left\\{\\vec0\\right\\}\\] Both of these criteria must be met for two subspaces to be orthogonal complements. Proof : Suppose we take an element \\(\\v \\in \\mathrm{R}(\\A) \\cap \\N(A)\\) , this means that \\(\\newcommand{\\v}{\\vec v} \\v\\in\\N(A)\\) and \\(\\v\\in\\R(A)\\) . Recall that if \\(\\n\\in\\N(A)\\) then \\( \\(\\n\\cdot\\r=0\\) \\) where \\(\\r\\in\\R(A)\\) . Now the element we took from their intersection \\(\\v\\) has this property: \\[\\v\\cdot\\v=0=\\left\\|\\v\\right\\|^2\\] We can two ways from here, one is we know \\[\\v \\cdot \\v = \\begin{bmatrix} v_1^2 \\\\ v_2^2 \\\\ \\vdots \\\\ \\v_m^2 \\end{bmatrix}\\] and thus for this to be zero, then \\(v_i^2 = 0 \\implies v_i = 0 \\forall i\\) , hence \\(\\v\\) is the zero vector. Otherwise, since \\(\\left\\|\\v\\right\\|=0\\) , the vector \\(\\v\\) must be the zero vector. In any case, any vector \\(\\v\\) in both \\(\\R(A)\\) and \\(\\N(A)\\) must equal \\(\\vec0\\) . Therefore \\(\\R(A)\\cap\\N(A)=\\left\\{\\vec0\\right\\}\\) , and so by definition \\(\\R(A)\\) and \\(\\N(A)\\) are complementary subspaces as well as orthogonal. \\(\\blacksquare\\) Applications of Column and Null Space in Machine Learning Read Linear Algebra: Theory, Intuition, Code, 2021. (pp. 230-231) . The Four Fundamental Subspaces The Dimensionalities of Matrix Spaces References ML Wiki 2 https://math.stackexchange.com/questions/1448326/how-would-one-prove-that-the-row-space-and-null-space-are-orthogonal-compliments# \u21a9 http://mlwiki.org/index.php/Four_Fundamental_Subspaces \u21a9","title":"(old)05.0x linear algebra matrix theory matrix spaces"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#preamble","text":"Both Professor Mike and Gilbert started off with this statement: The entirety and gist of the matrix space can be summarized as answering two key questions: Given a matrix \\(\\A \\in \\F^{m \\times n}\\) and vector \\(\\b \\in \\F^{m}\\) and \\(\\0 \\in \\F^{m}\\) , then can we find a vector \\(\\x \\in \\F^{n}\\) such that \\( \\(\\A\\x = \\b\\) \\) \\( \\(\\A\\x = \\0\\) \\) From the matrix-multiplication section, we can easily re-interpret the question as: \\(\\A\\x=\\b\\) is equivalent to asking can we find linear combination of columns of \\(\\A\\) such that the sum is \\(\\b\\) ? \\(\\A\\x=\\0\\) is equivalent to asking can we find linear combination of columns of \\(\\A\\) such that the sum is \\(\\0\\) assuming \\(\\x \\neq \\0\\) .","title":"Preamble"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#column-space","text":"","title":"Column Space"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#motivation","text":"We often frame our Machine Learning problems into a linear system of equations of the form \\(\\A\\x=\\b\\) . Not every linear system of equation is easily solvable, and has a solution \\(\\x\\) exists if and only if \\(\\b\\) belongs to the column space of \\(\\A\\) . Why so? Recall in the section linear algebra and matrix multiplication the following (if you forgot go read up): A column space of a matrix \\(\\A\\) is just the set of linear combination of the columns of \\(\\A\\) , and is a subspace. We will go through it in more details, but for now, I want to introduce this idea first. As a consequence of the example prior, one should realize three things: \\(\\A\\x\\) is a combination of the columns of the matrix \\(\\A\\) . \\(\\A\\x = \\b\\) may not always have a solution \\(\\x\\) . If \\(\\A\\x = \\b\\) has a solution \\(\\x\\) , then the product \\(\\b\\) must be a linear combination of the columns of \\(\\A\\) . This has important consequences later on. For now, just know that if \\(\\A\\x = \\b\\) has a solution, then \\(\\b\\) resides in the column space of \\(\\A\\) .","title":"Motivation"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#algebraic-definition-column-space","text":"Let \\(\\F\\) be a field of scalars and let \\(\\A\\) a \\(m \\times n\\) matrix, with column vectors \\(\\a_1, \\a_2, \\cdots, \\a_n\\) . Note that a linear combination of these vectors is any vector of the form \\[c_1 \\mathbf{a}_1 + c_2 \\mathbf{a}_2 + \\cdots + c_n \\mathbf{a}_n\\] where \\(c_i\\) are scalars; Then the set of all possible linear combinations of \\(\\a_1, \\a_2, \\cdots, \\a_n\\) is called the column space of \\(\\A\\) ; In other words, the column space of \\(\\A\\) is the linear span of the vectors \\(\\a_1, \\a_2, \\cdots, \\a_n\\) . And as noted in the right multiplication method in the section matrix-vector multiplication , any linear combination of the column vectors of a matrix \\(\\A\\) can be written as the product of \\(\\A\\) with a column vector where the column vectors hold the coefficients of the linear combination: \\[ \\begin{array} {rcl} \\A \\begin{bmatrix} c_1 \\\\ \\vdots \\\\ c_n \\end{bmatrix} & = & \\begin{bmatrix} a_{11} & \\cdots & a_{1n} \\\\ \\vdots & \\ddots & \\vdots \\\\ a_{m1} & \\cdots & a_{mn} \\end{bmatrix} \\begin{bmatrix} c_1 \\\\ \\vdots \\\\ c_n \\end{bmatrix} = \\begin{bmatrix} c_1 a_{11} + \\cdots + c_{n} a_{1n} \\\\ \\vdots \\\\ c_{1} a_{m1} + \\cdots + c_{n} a_{mn} \\end{bmatrix} = c_1 \\begin{bmatrix} a_{11} \\\\ \\vdots \\\\ a_{m1} \\end{bmatrix} + \\cdots + c_n \\begin{bmatrix} a_{1n} \\\\ \\vdots \\\\ a_{mn} \\end{bmatrix} \\\\ & = & c_1 \\mathbf{v}_1 + \\cdots + c_n \\mathbf{v}_n \\end{array} \\] Therefore, the column space of \\(\\A\\) consists of all possible products \\(\\A\\x\\) for \\(\\x \\in \\F^n\\) . This is the same as the image or range of the corresponding matrix transformation in which we will learn more on in the linear transformation chapter.","title":"Algebraic Definition (Column Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#notation-column-space","text":"The column space of a matrix is denoted as \\(C(\\A)\\) , which is the space spanned by all columns of the matrix \\(\\A\\) . Note that the column space \\(C(\\A)\\) resides in the \\(\\F^{m}\\) space.","title":"Notation (Column Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#example-column-space","text":"If \\(A = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 2 & 0 \\end{bmatrix}\\) , then the column vectors are \\(\\a_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 2 \\end{bmatrix} \\quad \\a_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0\\end{bmatrix}\\) . A linear combination of \\(\\a_1, \\a_2\\) is any vector of the form \\[c_1 \\begin{bmatrix} 1 \\\\ 0 \\\\ 2 \\end{bmatrix} + c_2 \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ 2c_1 \\end{bmatrix}\\] The set of all such vectors is the column space of \\(\\A\\) and in this case, the column space is precisely the set of vectors \\((x, y, z) \\in \\R^3\\) satisfying the equation \\(z=2x\\) using Cartesian coordinates, this set is a plane through the origin in three-dimensional space.","title":"Example (Column Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#column-space-and-basis","text":"Note that the columns of the matrix \\(\\A\\) spans the column space, but may not form a basis since there is no guarantee that the columns are linearly independent. However, recall the theorem that elementary row elimination/reduction does not affect the dependency of the column vectors , therefore, we still can use row reduction to find a basis for the column space.","title":"Column Space and Basis"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#dimension-and-rank","text":"The dimension of the column space is called the rank of the matrix. Thus, if the rank of the matrix \\(\\A\\) is \\(r = n\\) , then the column space spans all of \\(\\F^{n}\\) and is a basis of \\(\\F^{n}\\) , else it is a \\(r\\) -dimensional subspace embedded in \\(\\F^{n}\\) .","title":"Dimension and Rank"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#theorem-column-space-of-mathbfa-and-mathbfamathbfatop","text":"\\(\\mathbf{A}\\) and \\(\\mathbf{A}\\mathbf{A}^\\top\\) have the same column space.","title":"Theorem (Column Space of \\(\\mathbf{A}\\) and \\(\\mathbf{A}\\mathbf{A}^\\top\\))"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#proof-column-space-of-mathbfa-and-mathbfamathbfatop","text":"We first write \\(\\B = \\A\\A^\\top\\) and use the method that if \\(M \\subseteq N\\) and \\(N \\subseteq M\\) , then \\(N=M\\) to prove this. We first show that \\(C(\\B) \\subseteq C(\\A)\\) . The column space \\(C(\\A) = \\text{span}(\\a_1, \\a_2, ..., \\a_n) \\in \\F^{m}\\) and \\(C(\\B) = \\text{span}(\\b_1, \\b_2, ..., \\b_m)\\) . We note that by the right matrix multiplication , each column of \\(\\B = \\A\\A^\\top\\) is a linear combination of the columns of the left matrix \\(\\A\\) . That is to say, we can represent each column in \\(\\B\\) as \\(\\b_i = \\lambda_1 \\a_1 + \\lambda_2 \\a_2 + \\cdots + \\lambda_n \\a_n\\) . Consequently, by definition, any column from \\(\\B\\) is an element of \\(\\text{span}(\\a_1, ..., \\a_n)\\) , and hence in the column space \\(C(\\A)\\) . Thus, for any element in \\(C(\\B)\\) , this element must be in \\(C(\\A)\\) , and thus \\(C(\\B) \\subseteq C(\\A)\\) . We then show that \\(C(\\A) \\subseteq C(\\B)\\) . Take any element from \\(C(\\A)\\) , and recall that \\(C(\\B) = \\text{span}(\\b_1, \\b_2, ..., \\b_m) = c_1 \\b_1 + c_2 \\b_2 + \\cdots + c_m \\b_m = c_1 (\\lambda_1 \\a_1 + \\lambda_2 \\a_2 + \\cdots + \\lambda_m \\a_m) + c_2 (\\lambda_1 \\a_1 + \\lambda_2 \\a_2 + \\cdots + \\lambda_m \\a_m) + ... + c_2 (\\lambda_1 \\a_1 + \\lambda_2 \\a_2 + \\cdots + \\lambda_m \\a_m) = d_1\\a_1 + d_2\\a_2 + ... + d_m \\a_m\\) for some \\(d_i \\in \\F\\) . Then any element taken from the column space of \\(\\A\\) is a linear combination of \\(\\a_1, ..., \\a_m\\) ... What can we tell? We cannot tell anything since if \\(n > m\\) , then it may be the case that an element from \\(C(\\A)\\) may not cover.","title":"Proof (Column Space of \\(\\mathbf{A}\\) and \\(\\mathbf{A}\\mathbf{A}^\\top\\))"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#the-augment-rank-algorithm-to-determine-membership-of-column-space","text":"Given a set of vectors \\(S = \\{\\v_1, \\v_2, ..., \\v_n\\}\\) , and a vector \\(\\w\\) , deduce if \\(\\w \\in \\textbf{span}(S)\\) . Assume \\(\\v_i \\in \\R^{m}\\) and \\(\\w \\in \\R^{m}\\) . Construct a matrix \\(\\mathbf{S} = \\begin{bmatrix} \\v_1 & \\v_2 & \\cdots & \\v_m \\end{bmatrix}_{m \\times n}\\) where \\(\\mathbf{S} \\in \\R^{m \\times n}\\) . Compute the rank of \\(\\mathbf{S}\\) and call it \\(s_1\\) . Horizontally concatenate \\(\\mathbf{S}\\) and \\(\\w\\) to get \\(\\mathbf{S}_w = \\mathbf{S} \\cup \\w\\) . Compute rank of \\(\\mathbf{S}_w\\) to be \\(s_2\\) . Then: If \\(s_1 = s_2\\) , this means that column space of \\(\\mathbf{S}\\) equals to the column space of \\(\\mathbf{S}_w\\) , and thus \\(\\w\\) did not alter the column space of \\(\\mathbf{S}\\) , which means \\(\\w\\) must be part of the column space of \\(\\mathbf{S}\\) , and thus in the span of \\(S\\) . If \\(s_2 > s_1\\) , then \\(\\w \\not\\in S\\) because if it is, the column space of \\(\\mathbf{S}_w\\) should not change. We can easily use this algorithm to check if a vector \\(\\w\\) is in the column space of a matrix \\(\\A\\) by setting \\(S\\) to be the set that contains all the columns of \\(\\A\\) .","title":"The \"Augment-Rank\" Algorithm to determine membership of Column Space"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#geometric-intuition","text":"Mike mentioned on Linear Algebra: Theory, Intuition, Code, 2021. (pp. 211) that we can think of the above cases geometrically. In the first case where \\(s_1 = s_2\\) , the rank is the same, this means the vector \\(\\w\\) is in the column space of \\(\\A\\) . This makes sense because when we add the new vector to the set, and yet the rank (dimension) did not change, this coincides with the idea of vector \\(\\w\\) sitting somewhere in the column space of \\(\\A\\) , and hence no new geometric directions are obatined by including this vector . In the second case where \\(s_2 > s_1\\) however, if v is outside the column space, then it points off in some other geometric dimension that is not spanned by the column space; hence, B has one extra geometric dimension not contained in A, and thus the rank is one higher. In the image below, we denote \\(C(\\A)\\) as the 1d-subspace in red, then vector \\(\\v\\) is apparently lying in the 1d-subspace, and hence a member of \\(C(\\A)\\) , then \\(\\w\\) , is slightly pointing to a different direction, and hence not in the column space of \\(\\A\\) . Fig; Column Space of A; By Hongnan G.","title":"Geometric Intuition"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#corollary","text":"If \\(\\A\\) is now a full ranked square matrix \\(m \\times m\\) , which means rank of \\(\\A\\) is \\(m\\) , then any vector \\(\\w \\in \\R^{m}\\) will be in the column space of \\(\\A\\) since the column space of \\(\\A\\) is actually a basis of the ambient subspace \\(\\F^{m}\\) .","title":"Corollary"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#theorem-row-operations-preserves-column-space","text":"Row operations will not change the dependence of the columns of a matrix.","title":"Theorem (Row Operations preserves Column Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#visualizing-column-space","text":"Courtesy of Macro Analyst's linear algebra with python . import matplotlib.pyplot as plt import numpy as np from mpl_toolkits.mplot3d import Axes3D import scipy as sp import scipy.linalg import sympy as sy sy . init_printing () Consider two matrix with \\[\\A = \\begin{bmatrix}1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{bmatrix} ,\\quad \\B = \\begin{bmatrix}3 & -1 \\\\ 2 & 4 \\\\ -1 & 1 \\end{bmatrix}\\] Then the column space \\(C(\\A)\\) is a 2d-plane given by \\[\\text{plane}(C(\\A)) = \\0 + s\\left[\\matrix{1\\cr 0\\cr 0}\\right] + t\\left[\\matrix{0\\cr 1\\cr 0}\\right]\\] and the column space \\(C(\\B)\\) is a 2d-plane given by \\[\\text{plane}(C(\\B)) = \\0 + s\\left[\\matrix{3\\cr 2\\cr -1}\\right] + t\\left[\\matrix{-1\\cr 4\\cr 1}\\right]\\] fig = plt . figure ( figsize = ( 10 , 10 )) ax = fig . add_subplot ( projection = '3d' ) s = np . linspace ( - 2 , 2 , 20 ) t = np . linspace ( - 2 , 2 , 20 ) S , T = np . meshgrid ( s , t ) X = 3 * S - T Y = 2 * S + 4 * T Z = - S + T ax . plot_wireframe ( X , Y , Z , linewidth = .5 , color = 'r' ) s = np . linspace ( - 10 , 10 , 20 ) t = np . linspace ( - 10 , 10 , 20 ) S , T = np . meshgrid ( s , t ) X = S Y = T Z = np . zeros ( S . shape ) ax . plot_wireframe ( X , Y , Z , linewidth = .5 , color = 'k' ) ax . view_init ( elev = 14 , azim = 58 )","title":"Visualizing Column Space"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#row-space","text":"","title":"Row Space"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#algebraic-definition-row-space","text":"Let \\(\\F\\) be a field of scalars and let \\(\\A\\) a \\(m \\times n\\) matrix, with row vectors \\(\\a_1, \\a_2, \\cdots, \\a_m\\) . Note that a linear combination of these vectors is any vector of the form \\[c_1 \\mathbf{a}_1 + c_2 \\mathbf{a}_2 + \\cdots + c_m \\mathbf{a}_m\\] where \\(c_i\\) are scalars; Then the set of all possible linear combinations of \\(\\a_1, \\a_2, \\cdots, \\a_m\\) is called the row space of \\(\\A\\) ; In other words, the column space of \\(\\A\\) is the linear span of the vectors \\(\\a_1, \\a_2, \\cdots, \\a_m\\) . Note in particular that the row space \\(R(\\A)\\) resides in the \\(\\F^{n}\\) space.","title":"Algebraic Definition (Row Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#notation-row-space","text":"The row space of a matrix is denoted as \\(R(\\A)\\) , which is the space spanned by all rows of the matrix \\(\\A\\) .","title":"Notation (Row Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#row-space-and-basis","text":"Note that elementary row operations do not affect the row space, consequently, row reduction can be used to find a basis for the row space.","title":"Row Space and Basis"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#dimension-and-rank_1","text":"The dimension of the row space is called the rank of the matrix. Thus, if the rank of the matrix \\(\\A\\) is \\(r = m\\) , then the column space spans all of \\(\\F^{m}\\) and is a basis of \\(\\F^{m}\\) , else it is a \\(r\\) -dimensional subspace embedded in \\(\\F^{m}\\) .","title":"Dimension and Rank"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#theorem-row-space-of-mathbfa-is-the-column-space-of-mathbfatop","text":"Given a matrix \\(\\A \\in \\F^{m \\times n}\\) , the rows of \\(\\A\\) is the columns of \\(\\A^\\top\\) , and hence the theorem follows.","title":"Theorem (Row space of \\(\\mathbf{A}\\) is the Column space of \\(\\mathbf{A}^\\top\\))"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#theorem-row-space-of-mathbfa-and-mathbfamathbfatop","text":"\\(\\mathbf{A}\\) and \\(\\mathbf{A}\\mathbf{A}^\\top\\) have the same row space.","title":"Theorem (Row Space of \\(\\mathbf{A}\\) and \\(\\mathbf{A}\\mathbf{A}^\\top\\))"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#applications-in-machine-learning","text":"The author Mike mentioned in Linear Algebra: Theory, Intuition, Code, 2021. (pp. 214) that \\(R(\\A) = R(\\A^\\top\\A)\\) is an example of dimensionality reduction where both matrices have the same row space, but \\(\\A^\\top\\A\\) might be a much smaller matrix and hence computationally more efficient.","title":"Applications in Machine Learning"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#right-null-space-kernel","text":"","title":"(Right) Null Space (Kernel)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#motivation_1","text":"To fill in.","title":"Motivation"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#algebraic-definition-null-space","text":"Let \\(\\F\\) be a field of scalars and let \\(\\A\\) a \\(m \\times n\\) matrix, with column vectors \\(\\a_1, \\a_2, \\cdots, \\a_n\\) . Note that a linear combination of these vectors is any vector of the form \\[c_1 \\mathbf{a}_1 + c_2 \\mathbf{a}_2 + \\cdots + c_n \\mathbf{a}_n\\] where \\(c_i\\) are scalars; Then the nullspace of \\(\\A\\) is the set: \\[ N(\\A) = \\{\\v \\in \\F^{n} ~|~ \\A\\v = \\0\\} \\] Note that the trivial solution \\(\\0\\) is always a solution to the nullspace. Note that if there exists a non-trivial \\(\\v\\) such that \\(\\A\\v = \\0\\) , then for any \\(\\lambda \\in F\\) , we also have \\(\\A(\\lambda\\v) = \\0\\) . Note in particular that the column space \\(N(\\A)\\) resides in the \\(\\F^{n}\\) space.","title":"Algebraic Definition (Null Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#notation-null-space","text":"The null space of a matrix is denoted as \\(N(\\A)\\) .","title":"Notation (Null Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#what-does-null-space-tell-you","text":"Now one important consequence is that, if there exists a non-trivial solution to the matrix \\(\\A\\x = \\0\\) , then what does it mean? Recall the right multiplication of columns for matrix-vector multiplication : It means that the linear combination of the columns of \\(\\A\\) can form the zero vector \\(\\0\\) . In particular, this linear combination is NON-TRIVIAL! Then the matrix \\(\\A\\) is formed by columns that are linearly dependent, and hence not full rank! We can formalize it as a theorem below.","title":"What does Null Space tell you?"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#theorem-full-ranked-matrix-has-an-empty-nullspace","text":"A matrix \\(\\A \\in \\F^{m \\times n}\\) is full rank if and only if the nullspace \\(N(\\A)\\) has only the trivial solution.","title":"Theorem (Full Ranked Matrix has an Empty Nullspace)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#geometric-intuition-null-space","text":"","title":"Geometric Intuition (Null Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#left-null-space","text":"","title":"Left (Null Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#algebraic-definition-left-null-space","text":"Let \\(\\F\\) be a field of scalars and let \\(\\A\\) a \\(m \\times n\\) matrix, then the left nullspace of \\(\\A\\) is actually the nullspace of its transpose: \\[ N(\\A^\\top) = \\{\\v \\in \\F^{m} ~|~ \\A^\\top\\v = \\0\\} \\] and if we take transpose to both sides of \\(\\A^\\top\\v = \\0\\) to be \\(\\left(\\A^\\top\\v \\right)^\\top = \\0^\\top\\) ; it follows we have \\(\\v^\\top\\A = \\0^\\top\\) , where the row vector \\(\\v^\\top\\) is on the left of \\(\\A\\) . Note in particular that the column space \\(N(\\A)\\) resides in the \\(\\F^{m}\\) space.","title":"Algebraic Definition (Left Null Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#theorem-the-nullspace-and-left-nullspace-of-a-symmetric-matrix-is-equal","text":"Given a symmetric matrix \\(\\A\\) , we know that \\(\\A^\\top = \\A\\) , and thus \\(N(\\A) = N(\\A^\\top)\\) .","title":"Theorem (The Nullspace and Left Nullspace of a Symmetric Matrix is Equal)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#orthogonal-subspaces","text":"","title":"Orthogonal Subspaces"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#row-and-null-space-are-orthogonal-and-complements-each-other","text":"The row space and the null space of a matrix \\(\\A \\in \\F^{m \\times n}\\) are orthogonal complements .","title":"Row and Null Space are Orthogonal and Complements each other"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#proof-row-space-is-orthogonal-to-null-space","text":"","title":"Proof (Row Space is Orthogonal to Null Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#orthogonal-subspace","text":"We first show that both subspaces are orthogonal. Note that the null space of a matrix \\(\\A \\in F^{m \\times n}\\) is the set of all vectors \\(\\x\\) such that \\(\\A\\x = \\0\\) . We can also write \\(\\A\\x = \\0\\) as \\[ \\A\\x = \\begin{bmatrix} \\r_1 \\cdot \\x \\\\ \\r_2 \\cdot \\x \\\\ \\vdots \\\\ \\r_m \\cdot \\x \\end{bmatrix} \\] where \\(\\cdot\\) is the dot product and \\(\\r_i\\) the row vector \\(i\\) of \\(\\A\\) . Then one can easily see that \\(\\A\\x = \\0\\) if and only if every \\(\\r_i \\cdot \\x = 0\\) . Then it immediately follows that every \\(\\r_i\\) is orthogonal to \\(\\x\\) , and consequently, the nullspace and row space of \\(\\A\\) forms an orthogonal subspace . To see this, take any vector \\(\\r \\in R(\\A)\\) , and represent this \\(\\r = \\lambda_1 \\r_1 + ... + \\lambda_m \\r_m\\) , then \\[ \\begin{aligned} \\r \\cdot \\x &= (\\lambda_1 \\r_1 + ... + \\lambda_m \\r_m) \\cdot \\x \\\\ &= \\lambda_1 \\r_1 \\cdot \\x + ... + \\lambda_m \\r_m \\cdot \\x \\\\ &= \\0 + ... + \\0 \\\\ &= \\0 \\end{aligned} \\]","title":"Orthogonal Subspace"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#orthogonal-complements","text":"Reference 1 : We have proven that the row space \\(\\newcommand{\\R}{\\mathrm{R}} \\R(A)\\) and null space \\(\\newcommand{\\N}{\\mathrm{N}} \\N(A)\\) are orthogonal to each other; that is, \\(\\newcommand{\\r}{\\vec r} \\newcommand{\\n}{\\vec n} \\forall\\r\\in\\R(A)\\ \\forall\\n\\in\\N(A): \\r\\perp\\n\\) . Next, we show that they are complements of each other: \\[\\R(A)\\cap\\N(A)=\\left\\{\\vec0\\right\\}\\] Both of these criteria must be met for two subspaces to be orthogonal complements. Proof : Suppose we take an element \\(\\v \\in \\mathrm{R}(\\A) \\cap \\N(A)\\) , this means that \\(\\newcommand{\\v}{\\vec v} \\v\\in\\N(A)\\) and \\(\\v\\in\\R(A)\\) . Recall that if \\(\\n\\in\\N(A)\\) then \\( \\(\\n\\cdot\\r=0\\) \\) where \\(\\r\\in\\R(A)\\) . Now the element we took from their intersection \\(\\v\\) has this property: \\[\\v\\cdot\\v=0=\\left\\|\\v\\right\\|^2\\] We can two ways from here, one is we know \\[\\v \\cdot \\v = \\begin{bmatrix} v_1^2 \\\\ v_2^2 \\\\ \\vdots \\\\ \\v_m^2 \\end{bmatrix}\\] and thus for this to be zero, then \\(v_i^2 = 0 \\implies v_i = 0 \\forall i\\) , hence \\(\\v\\) is the zero vector. Otherwise, since \\(\\left\\|\\v\\right\\|=0\\) , the vector \\(\\v\\) must be the zero vector. In any case, any vector \\(\\v\\) in both \\(\\R(A)\\) and \\(\\N(A)\\) must equal \\(\\vec0\\) . Therefore \\(\\R(A)\\cap\\N(A)=\\left\\{\\vec0\\right\\}\\) , and so by definition \\(\\R(A)\\) and \\(\\N(A)\\) are complementary subspaces as well as orthogonal. \\(\\blacksquare\\)","title":"Orthogonal Complements"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#applications-of-column-and-null-space-in-machine-learning","text":"Read Linear Algebra: Theory, Intuition, Code, 2021. (pp. 230-231) .","title":"Applications of Column and Null Space in Machine Learning"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#the-four-fundamental-subspaces","text":"","title":"The Four Fundamental Subspaces"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#the-dimensionalities-of-matrix-spaces","text":"","title":"The Dimensionalities of Matrix Spaces"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/%28old%2905.0x_linear_algebra_matrix_theory_matrix_spaces/#references","text":"ML Wiki 2 https://math.stackexchange.com/questions/1448326/how-would-one-prove-that-the-row-space-and-null-space-are-orthogonal-compliments# \u21a9 http://mlwiki.org/index.php/Four_Fundamental_Subspaces \u21a9","title":"References"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.02_linear_algebra_row_reduction_preserves_rank/","text":"\\[\\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\e}{\\mathbf{e}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\z}{\\mathbf{z}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\C}{\\mathbf{C}} \\newcommand{\\E}{\\mathbf{E}}\\] Preamble This theorem is important enough to derserve a page of its own. One should note that the column space of a matrix \\(\\A\\) is just the subspace spanned by its columns and row space is just the subspace spanned by its rows. We will give a formal treatment in the next section. For now, we will prove 2 key theorems. Here we prove that row elimination do not alter the linear (in)dependence of the rows and column space. One needs to first realize that any row operation on \\(\\A\\) is equivalent to applying an left multiplcation of elementary matrix \\(\\E\\) on \\(\\A\\) . Row Operations Preserves Row Space and Linear In(Depedence) Given a matrix \\(\\A \\in \\F^{m \\times n}\\) , if one applies row elimination and row operations to the matrix \\(\\A\\) , show that it does not change the row space of \\(\\A\\) , and also preserves the linear (in)dependence of the rows of \\(\\A\\) , that is equivalent to saying that row elimination/operations do not change the dimension of the row space of \\(\\A\\) . We prove this theorem first as it is relatively easier to see and visualize 1 . We first show that row elimination/operations on \\(\\A\\) preserve the row space. That is, if we represent rows of \\(\\A\\) as \\(\\v_1, \\v_2, ..., \\v_m\\) , then it suffices to prove that a row operation \\(\\E\\A\\) preserves the row space: \\[\\textbf{span}(\\v_1, \\v_2, ..., \\v_m) = \\textbf{span}(\\e_1, \\e_2, ..., \\e_m)\\] where \\(\\v_i\\) is the row \\(i\\) of \\(\\A\\) and \\(\\e_i\\) row \\(i\\) of \\(\\E\\) . Let \\(\\v_1,\\ldots,\\v_,\\) denote the rows of \\(A\\) , and let \\(V\\) be the row-space of \\(A\\) . In other words, $$ V=\\text{span}\\,{\\v_1,\\ldots,\\v_m}. $$ Firstly, the easiest type of elementary operations is permutation of rows, which amount to permuting some \\(\\v_j\\) and \\(\\v_k\\) above; such operation will not change the span of \\(\\v_1,\\ldots,\\v_m\\) . Secondly, multiplying a row with a non-zero constant does not change the row space; that is if we multiply any row of \\(\\A\\) by \\(\\lambda\\) , then: \\[\\textbf{span}(\\v_1, \\ldots, \\v_i ,\\v_m) = \\textbf{span}(\\v_1, \\ldots, \\lambda_k\\v_k, \\v_m)\\] Lastly, the row operation amounts to of replacing \\(\\v_k\\) with \\(\\v_k+\\lambda \\v_j\\) . In this case, we can write \\[ \\alpha_1\\v_1+\\cdots+\\alpha_n\\v_m=\\alpha_1\\v_1+\\cdots+\\alpha_{k-1}\\v_{k-1}+\\alpha_k(\\v_k+\\lambda \\v_j)+\\alpha_{k+1}\\v_{k+1}+\\cdots (\\alpha_j-\\alpha_k\\lambda)\\v_j+\\cdots+\\alpha_n\\v_m \\] So every linear combination of \\(\\v_1,\\ldots,\\v_m\\) is also a linear combination of \\(\\v_1,\\ldots,\\v_m\\) with $v_k$ replaced by \\(\\v_k+\\lambda \\v_j\\) . In summary, after doing any elementary operation to \\(v_1,\\ldots,v_n\\) , the span doesn't change. It follows directly that if \\(A\\) and \\(B\\) are row equivalent, since the rows of \\(B\\) can be obtained by elementary operations from the rows of \\(A\\) , the spans of their rows are equal. If \\(A\\) is invertible, then it is row equivalent to \\(I\\) , and so its row space is \\(F^n\\) . Conversely, if the row space of \\(A\\) is \\(F^n\\) , then by writing each of \\(v_1,\\ldots,v_n\\) in terms of the canonical basis, we get a recipe to go from \\(I\\) to \\(A\\) by elementary operations, and so \\(A\\) is invertible. To show that the row operations also preserve the linear (in)depedence, we just need to prove that the dimension of the column space of \\(\\A\\) before and after row operations is the same. Row Operations Preserve Column Space 1 The most important fact is that elementary row operations are realized as multiplication (on the left) by an invertible matrix. So the proof below just suffices to prove that if we have a matrix \\(\\A \\in \\F^{m \\times n}\\) , and if we apply a row operation on \\(\\A\\) as \\(\\E\\A = \\B\\) , then we need a lemma in between to show that if given a matrix \\(\\A\\) , and we do a row operation on \\(\\A\\) as \\(\\E\\A\\) where \\(\\E\\) is naturally invertible, then we suffice to show for the same column indices, \\(a_{i_1},a_{i_2},\\dots,a_{i_k}\\) are linearly independent if and only if \\(b_{i_1},b_{i_2},\\dots,b_{i_k}\\) are linearly independent. This proof is just proving that the same set of linearly independent (or dependent) columns of \\(\\A\\) coincides with the same set (indexed) columns of \\(\\B\\) . Once you know this fact, you can proceed as follows. Suppose \\(A\\) and \\(B\\) are \\(m\\times n\\) and there exists an invertible \\(m\\times m\\) matrix \\(F\\) such that \\(A=FB\\) . Denote by \\(a_1,a_2,\\dots,a_n\\) and \\(b_1,b_2,\\dots,b_n\\) the columns of \\(A\\) and \\(B\\) respectively. Consider indices \\(i_1,i_2,\\dots,i_k\\) such that \\(1\\le i_1<i_2<\\dots<i_k\\le n\\) . Then the columns \\(a_{i_1},a_{i_2},\\dots,a_{i_k}\\) are linearly independent if and only if \\(b_{i_1},b_{i_2},\\dots,b_{i_k}\\) are linearly independent. It's sufficient to prove one implication, because \\(B=F^{-1}A\\) . So, suppose the columns \\(a_{i_1},a_{i_2},\\dots,a_{i_k}\\) are linearly independent and that \\[ \\alpha_1b_{i_1}+\\alpha_2b_{i_2}+\\dots+\\alpha_kb_{i_k}=0 \\] Then we can multiply both sides by \\(F\\) and get \\[ \\alpha_1Fb_{i_1}+\\alpha_2Fb_{i_2}+\\dots+\\alpha_kFb_{i_k}=0 \\] Since \\(Fb_i=a_i\\) , by definition of matrix product, we obtain \\[ \\alpha_1a_{i_1}+\\alpha_2a_{i_2}+\\dots+\\alpha_ka_{i_k}=0 \\] so \\(\\alpha_1=\\alpha_2=\\dots=\\alpha_k=0\\) . In a similar way, we see that a column \\(a_i\\) of \\(A\\) is a linear combination of the columns \\(a_{i_1},a_{i_2},\\dots,a_{i_k}\\) if and only if \\(b_i\\) is a linear combination of \\(b_{i_1},b_{i_2},\\dots,b_{i_k}\\) , with the same coefficients . As a consequence, \\(a_{i_1},a_{i_2},\\dots,a_{i_k}\\) is a basis of the column space of \\(A\\) if and only if \\(b_{i_1},b_{i_2},\\dots,b_{i_k}\\) is a basis of the column space of \\(B\\) . In particular, the column space of \\(A\\) has the same dimension as the column space of \\(B\\) . Therefore \\(A\\) and \\(B\\) have the same column rank (the dimension is the maximum number of linearly independent columns, of course, because the columns are, by definition, generators of the column space). This has other important consequences. When you find a row echelon form \\(U\\) for \\(A\\) , it's easy to see that the pivot columns of \\(U\\) form a basis of the column space of \\(U\\) . Therefore, the columns of \\(A\\) corresponding to the pivot columns form a basis of the column space of \\(A\\) . This provides an algorithm for extracting a basis from the columns of \\(A\\) . Not only this. If \\(U\\) is the reduced row echelon form, we see that a nonpivot column is the linear combination of the pivot columns with lower column index and the coefficients in the nonpivot column are exactly those needed to write it as a linear combination. Thus the same coefficients can be used to express the columns of \\(A\\) corresponding to nonpivot columns as linear combination of the already found basis for the column space of \\(A\\) . Thus the reduced row echelon form of \\(A\\) is unique, because its entries only depend on the linear relations between the columns of \\(A\\) . Elementary row operations also preserve the row rank (dimension of the row space or maximum number of linearly independent rows). This is easier, because the row space is unchanged by elementary row operations. This is obvious if the operation is swapping two rows. If the operation is multiplying a row by a nonzero constant, then the original row is a multiple of the new row, and conversely. If the operation is of the form \\(r_i+kr_j\\) , then \\(r_i=(r_i+kr_j)-kr_j\\) , and conversely. Summary Row Operations do not change null space and row space. For null space \\(\\A\\x = \\0\\) , row eliminations also do not change the set of solutions rank of matrix \\(\\A\\) is the number of pivots after elimination Yes, the Gaussian elimination does not preserve the column space (it does preserve the row space though). However, each time you perform an elementary operation, you basically multiply your original matrix by an invertible matrix. In the language of linear transformations, you act by an isomorphisms (linear invertible operators) and the isomorphisms keep linearly independent columns linearly independent and linearly dependent columns to be linearly dependent. Note that the columns with pivots are linearly independent, and all other columns in the row reduced echelon form can be represented as linear combinations of columns with pivots. Hence the conclusion. Consider matrix \\(\\A = \\begin{bmatrix} \\a_1 \\\\ \\a_2 \\\\ \\vdots \\\\ \\a_m \\end{bmatrix}\\) where \\(\\a_i\\) is the row of \\(\\A\\) . Then one might need to think intuitively why row reduction such as Gaussian Elimination will tell us the number of linearly independent rows by counting all the non-zero rows? First, a toy example, assume that out of \\(m\\) rows, there exist \\(m-2\\) linearly independent rows \\(\\a_1, \\cdots, \\a_{m-2}\\) and the two linearly dependent rows are \\[\\a_{m-1} = \\a_1 + \\cdots + \\a_{m-2} \\quad \\a_m = \\a_1 + \\cdots + \\a_{m-1}\\] We can work out how row operations can kill (zero) out these linearly dependent rows. Firstly, if we kill \\(\\a_{m-1}\\) first by doing the row operation of \\(\\a_{m-1} - (\\a_1 + \\cdots + \\a_{m-2})\\) . Then, \\(\\a_{m}\\) will still be killed even though \\(\\a_{m-1}\\) is reduced to the zero vector because we already know \\(\\a_{m-1} = \\a_1 + \\cdots + \\a_{m-2}\\) and thus \\(\\a_m - \\left[(\\a_1 + \\cdots + a_{m-2}) + (\\a_1 + \\cdots + \\a_{m-2})\\right]\\) . How do we ensure that no other (linearly independent) rows are zeroed out? Suppose any other linearly independent rows can be zeroed out by other rows in \\(\\A\\) , then a contradiction occurred since they are linearly independent. https://math.stackexchange.com/questions/2078943/row-operations-do-not-change-the-dependency-relationships-among-columns https://math.stackexchange.com/questions/354362/why-do-elementary-row-operations-preserve-linear-dependence-between-matrix-colum?rq=1 https://math.stackexchange.com/questions/3760618/how-prove-that-the-elementary-operations-dont-change-the-rank-of-a-matrix https://math.stackexchange.com/questions/3760618/how-prove-that-the-elementary-operations-dont-change-the-rank-of-a-matrix \u21a9 \u21a9","title":"Row Reduction Preserves Rank"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.02_linear_algebra_row_reduction_preserves_rank/#preamble","text":"This theorem is important enough to derserve a page of its own. One should note that the column space of a matrix \\(\\A\\) is just the subspace spanned by its columns and row space is just the subspace spanned by its rows. We will give a formal treatment in the next section. For now, we will prove 2 key theorems. Here we prove that row elimination do not alter the linear (in)dependence of the rows and column space. One needs to first realize that any row operation on \\(\\A\\) is equivalent to applying an left multiplcation of elementary matrix \\(\\E\\) on \\(\\A\\) .","title":"Preamble"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.02_linear_algebra_row_reduction_preserves_rank/#row-operations-preserves-row-space-and-linear-indepedence","text":"Given a matrix \\(\\A \\in \\F^{m \\times n}\\) , if one applies row elimination and row operations to the matrix \\(\\A\\) , show that it does not change the row space of \\(\\A\\) , and also preserves the linear (in)dependence of the rows of \\(\\A\\) , that is equivalent to saying that row elimination/operations do not change the dimension of the row space of \\(\\A\\) . We prove this theorem first as it is relatively easier to see and visualize 1 . We first show that row elimination/operations on \\(\\A\\) preserve the row space. That is, if we represent rows of \\(\\A\\) as \\(\\v_1, \\v_2, ..., \\v_m\\) , then it suffices to prove that a row operation \\(\\E\\A\\) preserves the row space: \\[\\textbf{span}(\\v_1, \\v_2, ..., \\v_m) = \\textbf{span}(\\e_1, \\e_2, ..., \\e_m)\\] where \\(\\v_i\\) is the row \\(i\\) of \\(\\A\\) and \\(\\e_i\\) row \\(i\\) of \\(\\E\\) . Let \\(\\v_1,\\ldots,\\v_,\\) denote the rows of \\(A\\) , and let \\(V\\) be the row-space of \\(A\\) . In other words, $$ V=\\text{span}\\,{\\v_1,\\ldots,\\v_m}. $$ Firstly, the easiest type of elementary operations is permutation of rows, which amount to permuting some \\(\\v_j\\) and \\(\\v_k\\) above; such operation will not change the span of \\(\\v_1,\\ldots,\\v_m\\) . Secondly, multiplying a row with a non-zero constant does not change the row space; that is if we multiply any row of \\(\\A\\) by \\(\\lambda\\) , then: \\[\\textbf{span}(\\v_1, \\ldots, \\v_i ,\\v_m) = \\textbf{span}(\\v_1, \\ldots, \\lambda_k\\v_k, \\v_m)\\] Lastly, the row operation amounts to of replacing \\(\\v_k\\) with \\(\\v_k+\\lambda \\v_j\\) . In this case, we can write \\[ \\alpha_1\\v_1+\\cdots+\\alpha_n\\v_m=\\alpha_1\\v_1+\\cdots+\\alpha_{k-1}\\v_{k-1}+\\alpha_k(\\v_k+\\lambda \\v_j)+\\alpha_{k+1}\\v_{k+1}+\\cdots (\\alpha_j-\\alpha_k\\lambda)\\v_j+\\cdots+\\alpha_n\\v_m \\] So every linear combination of \\(\\v_1,\\ldots,\\v_m\\) is also a linear combination of \\(\\v_1,\\ldots,\\v_m\\) with $v_k$ replaced by \\(\\v_k+\\lambda \\v_j\\) . In summary, after doing any elementary operation to \\(v_1,\\ldots,v_n\\) , the span doesn't change. It follows directly that if \\(A\\) and \\(B\\) are row equivalent, since the rows of \\(B\\) can be obtained by elementary operations from the rows of \\(A\\) , the spans of their rows are equal. If \\(A\\) is invertible, then it is row equivalent to \\(I\\) , and so its row space is \\(F^n\\) . Conversely, if the row space of \\(A\\) is \\(F^n\\) , then by writing each of \\(v_1,\\ldots,v_n\\) in terms of the canonical basis, we get a recipe to go from \\(I\\) to \\(A\\) by elementary operations, and so \\(A\\) is invertible. To show that the row operations also preserve the linear (in)depedence, we just need to prove that the dimension of the column space of \\(\\A\\) before and after row operations is the same.","title":"Row Operations Preserves Row Space and Linear In(Depedence)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.02_linear_algebra_row_reduction_preserves_rank/#row-operations-preserve-column-space1","text":"The most important fact is that elementary row operations are realized as multiplication (on the left) by an invertible matrix. So the proof below just suffices to prove that if we have a matrix \\(\\A \\in \\F^{m \\times n}\\) , and if we apply a row operation on \\(\\A\\) as \\(\\E\\A = \\B\\) , then we need a lemma in between to show that if given a matrix \\(\\A\\) , and we do a row operation on \\(\\A\\) as \\(\\E\\A\\) where \\(\\E\\) is naturally invertible, then we suffice to show for the same column indices, \\(a_{i_1},a_{i_2},\\dots,a_{i_k}\\) are linearly independent if and only if \\(b_{i_1},b_{i_2},\\dots,b_{i_k}\\) are linearly independent. This proof is just proving that the same set of linearly independent (or dependent) columns of \\(\\A\\) coincides with the same set (indexed) columns of \\(\\B\\) . Once you know this fact, you can proceed as follows. Suppose \\(A\\) and \\(B\\) are \\(m\\times n\\) and there exists an invertible \\(m\\times m\\) matrix \\(F\\) such that \\(A=FB\\) . Denote by \\(a_1,a_2,\\dots,a_n\\) and \\(b_1,b_2,\\dots,b_n\\) the columns of \\(A\\) and \\(B\\) respectively. Consider indices \\(i_1,i_2,\\dots,i_k\\) such that \\(1\\le i_1<i_2<\\dots<i_k\\le n\\) . Then the columns \\(a_{i_1},a_{i_2},\\dots,a_{i_k}\\) are linearly independent if and only if \\(b_{i_1},b_{i_2},\\dots,b_{i_k}\\) are linearly independent. It's sufficient to prove one implication, because \\(B=F^{-1}A\\) . So, suppose the columns \\(a_{i_1},a_{i_2},\\dots,a_{i_k}\\) are linearly independent and that \\[ \\alpha_1b_{i_1}+\\alpha_2b_{i_2}+\\dots+\\alpha_kb_{i_k}=0 \\] Then we can multiply both sides by \\(F\\) and get \\[ \\alpha_1Fb_{i_1}+\\alpha_2Fb_{i_2}+\\dots+\\alpha_kFb_{i_k}=0 \\] Since \\(Fb_i=a_i\\) , by definition of matrix product, we obtain \\[ \\alpha_1a_{i_1}+\\alpha_2a_{i_2}+\\dots+\\alpha_ka_{i_k}=0 \\] so \\(\\alpha_1=\\alpha_2=\\dots=\\alpha_k=0\\) . In a similar way, we see that a column \\(a_i\\) of \\(A\\) is a linear combination of the columns \\(a_{i_1},a_{i_2},\\dots,a_{i_k}\\) if and only if \\(b_i\\) is a linear combination of \\(b_{i_1},b_{i_2},\\dots,b_{i_k}\\) , with the same coefficients . As a consequence, \\(a_{i_1},a_{i_2},\\dots,a_{i_k}\\) is a basis of the column space of \\(A\\) if and only if \\(b_{i_1},b_{i_2},\\dots,b_{i_k}\\) is a basis of the column space of \\(B\\) . In particular, the column space of \\(A\\) has the same dimension as the column space of \\(B\\) . Therefore \\(A\\) and \\(B\\) have the same column rank (the dimension is the maximum number of linearly independent columns, of course, because the columns are, by definition, generators of the column space). This has other important consequences. When you find a row echelon form \\(U\\) for \\(A\\) , it's easy to see that the pivot columns of \\(U\\) form a basis of the column space of \\(U\\) . Therefore, the columns of \\(A\\) corresponding to the pivot columns form a basis of the column space of \\(A\\) . This provides an algorithm for extracting a basis from the columns of \\(A\\) . Not only this. If \\(U\\) is the reduced row echelon form, we see that a nonpivot column is the linear combination of the pivot columns with lower column index and the coefficients in the nonpivot column are exactly those needed to write it as a linear combination. Thus the same coefficients can be used to express the columns of \\(A\\) corresponding to nonpivot columns as linear combination of the already found basis for the column space of \\(A\\) . Thus the reduced row echelon form of \\(A\\) is unique, because its entries only depend on the linear relations between the columns of \\(A\\) . Elementary row operations also preserve the row rank (dimension of the row space or maximum number of linearly independent rows). This is easier, because the row space is unchanged by elementary row operations. This is obvious if the operation is swapping two rows. If the operation is multiplying a row by a nonzero constant, then the original row is a multiple of the new row, and conversely. If the operation is of the form \\(r_i+kr_j\\) , then \\(r_i=(r_i+kr_j)-kr_j\\) , and conversely.","title":"Row Operations Preserve Column Space1"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.02_linear_algebra_row_reduction_preserves_rank/#summary","text":"Row Operations do not change null space and row space. For null space \\(\\A\\x = \\0\\) , row eliminations also do not change the set of solutions rank of matrix \\(\\A\\) is the number of pivots after elimination Yes, the Gaussian elimination does not preserve the column space (it does preserve the row space though). However, each time you perform an elementary operation, you basically multiply your original matrix by an invertible matrix. In the language of linear transformations, you act by an isomorphisms (linear invertible operators) and the isomorphisms keep linearly independent columns linearly independent and linearly dependent columns to be linearly dependent. Note that the columns with pivots are linearly independent, and all other columns in the row reduced echelon form can be represented as linear combinations of columns with pivots. Hence the conclusion. Consider matrix \\(\\A = \\begin{bmatrix} \\a_1 \\\\ \\a_2 \\\\ \\vdots \\\\ \\a_m \\end{bmatrix}\\) where \\(\\a_i\\) is the row of \\(\\A\\) . Then one might need to think intuitively why row reduction such as Gaussian Elimination will tell us the number of linearly independent rows by counting all the non-zero rows? First, a toy example, assume that out of \\(m\\) rows, there exist \\(m-2\\) linearly independent rows \\(\\a_1, \\cdots, \\a_{m-2}\\) and the two linearly dependent rows are \\[\\a_{m-1} = \\a_1 + \\cdots + \\a_{m-2} \\quad \\a_m = \\a_1 + \\cdots + \\a_{m-1}\\] We can work out how row operations can kill (zero) out these linearly dependent rows. Firstly, if we kill \\(\\a_{m-1}\\) first by doing the row operation of \\(\\a_{m-1} - (\\a_1 + \\cdots + \\a_{m-2})\\) . Then, \\(\\a_{m}\\) will still be killed even though \\(\\a_{m-1}\\) is reduced to the zero vector because we already know \\(\\a_{m-1} = \\a_1 + \\cdots + \\a_{m-2}\\) and thus \\(\\a_m - \\left[(\\a_1 + \\cdots + a_{m-2}) + (\\a_1 + \\cdots + \\a_{m-2})\\right]\\) . How do we ensure that no other (linearly independent) rows are zeroed out? Suppose any other linearly independent rows can be zeroed out by other rows in \\(\\A\\) , then a contradiction occurred since they are linearly independent. https://math.stackexchange.com/questions/2078943/row-operations-do-not-change-the-dependency-relationships-among-columns https://math.stackexchange.com/questions/354362/why-do-elementary-row-operations-preserve-linear-dependence-between-matrix-colum?rq=1 https://math.stackexchange.com/questions/3760618/how-prove-that-the-elementary-operations-dont-change-the-rank-of-a-matrix https://math.stackexchange.com/questions/3760618/how-prove-that-the-elementary-operations-dont-change-the-rank-of-a-matrix \u21a9 \u21a9","title":"Summary"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.03_linear_algebra_matrix_theory_matrix_spaces_row_space/","text":"\\[ \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\r}{\\mathbf{r}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\z}{\\mathbf{z}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\C}{\\mathbf{C}} \\newcommand{\\rank}{\\textbf{rank}} \\] Preamble Both Professor Mike and Gilbert started off with this statement: The entirety and gist of the matrix space can be summarized as answering two key questions: Given a matrix \\(\\A \\in \\F^{m \\times n}\\) and vector \\(\\b \\in \\F^{m}\\) and \\(\\0 \\in \\F^{m}\\) , then can we find a vector \\(\\x \\in \\F^{n}\\) such that \\(\\A\\x = \\b\\) \\(\\A\\x = \\0\\) From the matrix-multiplication section, we can easily re-interpret the question as: \\(\\A\\x=\\b\\) is equivalent to asking can we find linear combination of columns of \\(\\A\\) such that the sum is \\(\\b\\) ? \\(\\A\\x=\\0\\) is equivalent to asking can we find linear combination of columns of \\(\\A\\) such that the sum is \\(\\0\\) assuming \\(\\x \\neq \\0\\) . Row Space Algebraic Definition (Row Space) Let \\(\\F\\) be a field of scalars and let \\(\\A\\) a \\(m \\times n\\) matrix, with row vectors \\(\\r_1, \\r_2, \\cdots, \\r_m\\) . Note that a linear combination of these vectors is any vector of the form \\[c_1 \\r_1 + c_2 \\r_2 + \\cdots + c_m \\r_m\\] where \\(c_i\\) are scalars; Then the set of all possible linear combinations of \\(\\r_1, \\r_2, \\cdots, \\r_m\\) is called the row space of \\(\\A\\) ; In other words, the row space of \\(\\A\\) is the linear span of the vectors \\(\\r_1, \\r_2, \\cdots, \\r_m\\) . Notation (Row Space) The row space of a matrix is denoted as \\(R(\\A)\\) , which is the space spanned by all rows of the matrix \\(\\A\\) . Note in particular that the row space \\(R(\\A)\\) resides in the \\(\\F^{n}\\) space. Algebraic Definition (Row Rank) The row rank of \\(\\A\\) is the dimension of the row space of \\(\\A\\) . Definition (Full Row Rank) A matrix \\(\\A \\in \\F^{m \\times n}\\) is full row rank if and only if \\(\\rank(A) = m\\) . The row space spans all of \\(\\F^{m}\\) and is a basis of \\(\\F^{m}\\) , else it is a \\(r\\) -dimensional subspace embedded in \\(\\F^{m}\\) . Theorem (Row Reduction Preserves Row Space) This theorem has been proved in the section 05.02_linear_algebra_row_elimination_preserves_row_column_spaces_dependency . Theorem (Basis For Row Space) The proof is detailed in pp.127-128 of A modern introduction to linear algebra henry ricardo . Algorithm (Basis For Row Space) Note that elementary row operations do not affect the row space by our previous theorem, consequently, row reduction can be used to find a basis for the row space. The pseudo-algorithm can be detailed below: Calculate RREF of the matrix \\(\\A\\) . The pivot rows (non-zero) rows form a basis of the row space of \\(\\A\\) . Theorem (Counting Row Rank) As a consequence from the previous algorithm, the non-zero rows of RREF of \\(\\A\\) is the row rank of \\(\\A\\) . Theorem (Row Space of \\(\\mathbf{A}\\) and \\(\\mathbf{A}^\\top\\mathbf{A}\\) ) \\(\\mathbf{A}\\) and \\(\\mathbf{A}^\\top\\mathbf{A}\\) have the same row space. Applications in Machine Learning The author Mike mentioned in Linear Algebra: Theory, Intuition, Code, 2021. (pp. 214) that \\(R(\\A) = R(\\A^\\top\\A)\\) is an example of dimensionality reduction where both matrices have the same row space, but \\(\\A^\\top\\A\\) might be a much smaller matrix and hence computationally more efficient.","title":"Row Space"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.03_linear_algebra_matrix_theory_matrix_spaces_row_space/#preamble","text":"Both Professor Mike and Gilbert started off with this statement: The entirety and gist of the matrix space can be summarized as answering two key questions: Given a matrix \\(\\A \\in \\F^{m \\times n}\\) and vector \\(\\b \\in \\F^{m}\\) and \\(\\0 \\in \\F^{m}\\) , then can we find a vector \\(\\x \\in \\F^{n}\\) such that \\(\\A\\x = \\b\\) \\(\\A\\x = \\0\\) From the matrix-multiplication section, we can easily re-interpret the question as: \\(\\A\\x=\\b\\) is equivalent to asking can we find linear combination of columns of \\(\\A\\) such that the sum is \\(\\b\\) ? \\(\\A\\x=\\0\\) is equivalent to asking can we find linear combination of columns of \\(\\A\\) such that the sum is \\(\\0\\) assuming \\(\\x \\neq \\0\\) .","title":"Preamble"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.03_linear_algebra_matrix_theory_matrix_spaces_row_space/#row-space","text":"","title":"Row Space"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.03_linear_algebra_matrix_theory_matrix_spaces_row_space/#algebraic-definition-row-space","text":"Let \\(\\F\\) be a field of scalars and let \\(\\A\\) a \\(m \\times n\\) matrix, with row vectors \\(\\r_1, \\r_2, \\cdots, \\r_m\\) . Note that a linear combination of these vectors is any vector of the form \\[c_1 \\r_1 + c_2 \\r_2 + \\cdots + c_m \\r_m\\] where \\(c_i\\) are scalars; Then the set of all possible linear combinations of \\(\\r_1, \\r_2, \\cdots, \\r_m\\) is called the row space of \\(\\A\\) ; In other words, the row space of \\(\\A\\) is the linear span of the vectors \\(\\r_1, \\r_2, \\cdots, \\r_m\\) .","title":"Algebraic Definition (Row Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.03_linear_algebra_matrix_theory_matrix_spaces_row_space/#notation-row-space","text":"The row space of a matrix is denoted as \\(R(\\A)\\) , which is the space spanned by all rows of the matrix \\(\\A\\) . Note in particular that the row space \\(R(\\A)\\) resides in the \\(\\F^{n}\\) space.","title":"Notation (Row Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.03_linear_algebra_matrix_theory_matrix_spaces_row_space/#algebraic-definition-row-rank","text":"The row rank of \\(\\A\\) is the dimension of the row space of \\(\\A\\) .","title":"Algebraic Definition (Row Rank)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.03_linear_algebra_matrix_theory_matrix_spaces_row_space/#definition-full-row-rank","text":"A matrix \\(\\A \\in \\F^{m \\times n}\\) is full row rank if and only if \\(\\rank(A) = m\\) . The row space spans all of \\(\\F^{m}\\) and is a basis of \\(\\F^{m}\\) , else it is a \\(r\\) -dimensional subspace embedded in \\(\\F^{m}\\) .","title":"Definition (Full Row Rank)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.03_linear_algebra_matrix_theory_matrix_spaces_row_space/#theorem-row-reduction-preserves-row-space","text":"This theorem has been proved in the section 05.02_linear_algebra_row_elimination_preserves_row_column_spaces_dependency .","title":"Theorem (Row Reduction Preserves Row Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.03_linear_algebra_matrix_theory_matrix_spaces_row_space/#theorem-basis-for-row-space","text":"The proof is detailed in pp.127-128 of A modern introduction to linear algebra henry ricardo .","title":"Theorem (Basis For Row Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.03_linear_algebra_matrix_theory_matrix_spaces_row_space/#algorithm-basis-for-row-space","text":"Note that elementary row operations do not affect the row space by our previous theorem, consequently, row reduction can be used to find a basis for the row space. The pseudo-algorithm can be detailed below: Calculate RREF of the matrix \\(\\A\\) . The pivot rows (non-zero) rows form a basis of the row space of \\(\\A\\) .","title":"Algorithm (Basis For Row Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.03_linear_algebra_matrix_theory_matrix_spaces_row_space/#theorem-counting-row-rank","text":"As a consequence from the previous algorithm, the non-zero rows of RREF of \\(\\A\\) is the row rank of \\(\\A\\) .","title":"Theorem (Counting Row Rank)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.03_linear_algebra_matrix_theory_matrix_spaces_row_space/#theorem-row-space-of-mathbfa-and-mathbfatopmathbfa","text":"\\(\\mathbf{A}\\) and \\(\\mathbf{A}^\\top\\mathbf{A}\\) have the same row space.","title":"Theorem (Row Space of \\(\\mathbf{A}\\) and \\(\\mathbf{A}^\\top\\mathbf{A}\\))"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.03_linear_algebra_matrix_theory_matrix_spaces_row_space/#applications-in-machine-learning","text":"The author Mike mentioned in Linear Algebra: Theory, Intuition, Code, 2021. (pp. 214) that \\(R(\\A) = R(\\A^\\top\\A)\\) is an example of dimensionality reduction where both matrices have the same row space, but \\(\\A^\\top\\A\\) might be a much smaller matrix and hence computationally more efficient.","title":"Applications in Machine Learning"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.04_linear_algebra_matrix_theory_matrix_spaces_column_space/","text":"\\[ \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\z}{\\mathbf{z}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\C}{\\mathbf{C}} \\newcommand{\\rank}{\\textbf{rank}} \\] Preamble Both Professor Mike and Gilbert started off with this statement: The entirety and gist of the matrix space can be summarized as answering two key questions: Given a matrix \\(\\A \\in \\F^{m \\times n}\\) and vector \\(\\b \\in \\F^{m}\\) and \\(\\0 \\in \\F^{m}\\) , then can we find a vector \\(\\x \\in \\F^{n}\\) such that \\( \\(\\A\\x = \\b\\) \\) \\( \\(\\A\\x = \\0\\) \\) From the matrix-multiplication section, we can easily re-interpret the question as: \\(\\A\\x=\\b\\) is equivalent to asking can we find linear combination of columns of \\(\\A\\) such that the sum is \\(\\b\\) ? \\(\\A\\x=\\0\\) is equivalent to asking can we find linear combination of columns of \\(\\A\\) such that the sum is \\(\\0\\) assuming \\(\\x \\neq \\0\\) . Column Space Motivation We often frame our Machine Learning problems into a linear system of equations of the form \\(\\A\\x=\\b\\) . Not every linear system of equation is easily solvable, and has a solution \\(\\x\\) exists if and only if \\(\\b\\) belongs to the column space of \\(\\A\\) . Why so? Recall in the section linear algebra and matrix multiplication the following (if you forgot go read up): A column space of a matrix \\(\\A\\) is just the set of linear combination of the columns of \\(\\A\\) , and is a subspace. We will go through it in more details, but for now, I want to introduce this idea first. As a consequence of the example prior, one should realize three things: \\(\\A\\x\\) is a combination of the columns of the matrix \\(\\A\\) . \\(\\A\\x = \\b\\) may not always have a solution \\(\\x\\) . If \\(\\A\\x = \\b\\) has a solution \\(\\x\\) , then the product \\(\\b\\) must be a linear combination of the columns of \\(\\A\\) . This has important consequences later on. For now, just know that if \\(\\A\\x = \\b\\) has a solution, then \\(\\b\\) resides in the column space of \\(\\A\\) . Algebraic Definition (Column Space) Let \\(\\F\\) be a field of scalars and let \\(\\A\\) a \\(m \\times n\\) matrix, with column vectors \\(\\c_1, \\c_2, \\cdots, \\c_n\\) . Note that a linear combination of these vectors is any vector of the form \\[\\lambda_1 \\mathbf{c}_1 + \\lambda_2 \\mathbf{c}_2 + \\cdots + \\lambda_n \\mathbf{a}_n\\] where \\(\\lambda_i\\) are scalars; Then the set of all possible linear combinations of \\(\\c_1, \\c_2, \\cdots, \\c_n\\) is called the column space of \\(\\A\\) ; In other words, the column space of \\(\\A\\) is the linear span of the vectors \\(\\c_1, \\c_2, \\cdots, \\c_n\\) . And as noted in the right multiplication method in the section matrix-vector multiplication , any linear combination of the column vectors of a matrix \\(\\A\\) can be written as the product of \\(\\A\\) with a column vector where the column vectors hold the coefficients of the linear combination: \\[ \\begin{array} {rcl} \\A \\begin{bmatrix} c_1 \\\\ \\vdots \\\\ c_n \\end{bmatrix} & = & \\begin{bmatrix} a_{11} & \\cdots & a_{1n} \\\\ \\vdots & \\ddots & \\vdots \\\\ a_{m1} & \\cdots & a_{mn} \\end{bmatrix} \\begin{bmatrix} c_1 \\\\ \\vdots \\\\ c_n \\end{bmatrix} = \\begin{bmatrix} c_1 a_{11} + \\cdots + c_{n} a_{1n} \\\\ \\vdots \\\\ c_{1} a_{m1} + \\cdots + c_{n} a_{mn} \\end{bmatrix} = c_1 \\begin{bmatrix} a_{11} \\\\ \\vdots \\\\ a_{m1} \\end{bmatrix} + \\cdots + c_n \\begin{bmatrix} a_{1n} \\\\ \\vdots \\\\ a_{mn} \\end{bmatrix} \\\\ & = & c_1 \\mathbf{v}_1 + \\cdots + c_n \\mathbf{v}_n \\end{array} \\] Therefore, the column space of \\(\\A\\) consists of all possible products \\(\\A\\x\\) for \\(\\x \\in \\F^n\\) . This is the same as the image or range of the corresponding matrix transformation in which we will learn more on in the linear transformation chapter. Notation (Column Space) The column space of a matrix is denoted as \\(C(\\A)\\) , which is the space spanned by all columns of the matrix \\(\\A\\) . Note that the column space \\(C(\\A)\\) resides in the \\(\\F^{m}\\) space. Example (Column Space) If \\(A = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 2 & 0 \\end{bmatrix}\\) , then the column vectors are \\(\\a_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 2 \\end{bmatrix} \\quad \\a_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0\\end{bmatrix}\\) . A linear combination of \\(\\a_1, \\a_2\\) is any vector of the form \\[c_1 \\begin{bmatrix} 1 \\\\ 0 \\\\ 2 \\end{bmatrix} + c_2 \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ 2c_1 \\end{bmatrix}\\] The set of all such vectors is the column space of \\(\\A\\) and in this case, the column space is precisely the set of vectors \\((x, y, z) \\in \\R^3\\) satisfying the equation \\(z=2x\\) using Cartesian coordinates, this set is a plane through the origin in three-dimensional space. Theorem (Solvable Solutions) A system of \\(m\\) linear equations in \\(n\\) unknowns \\(\\A\\x = \\b\\) has a solution if and only if the vector \\(\\b\\) can be expressed as a linear combination of the columns of \\(\\A\\) . In other words, the system is solvable if and only if \\(\\b \\in \\textbf{Col Space}(\\A)\\) . Proof can be found in pp.125-126 A modern introduction to linear algebra by Henry Ricardo . Algebraic Definition (Column Rank) The column rank of \\(\\A\\) is the dimension of the column space of \\(\\A\\) . In other words, the column rank of \\(\\A\\) corresponds to the largest number vectors that can form a linearly indepedent set in the columns of \\(\\A\\) . This interpretation follows because the definition of dimension of a column space means given a vector space \\(V\\) which is our column space spanned by the columns of \\(\\A\\) , the dimension of the column space is the cardinality of the basis \\(\\B\\) of \\(V\\) ; and note that any basis \\(\\B\\) of \\(V\\) (our column space) is a linearly independent set which also spans \\(V\\) . So both definition is equivalent. Full Column Rank A matrix \\(\\A \\in \\F^{m \\times n}\\) is full column rank if and only if \\(\\rank(A) = n\\) . Thus, if the rank of the matrix \\(\\A\\) is \\(r = n\\) , then the column space spans all of \\(\\F^{n}\\) and is a basis of \\(\\F^{n}\\) , else it is a \\(r\\) -dimensional subspace embedded in \\(\\F^{n}\\) . Lemma (Row Operations preserves Linear Dependency of Column Space) Row operations apparently change the column space but the important part is that it preserves the column rank of a matrix. We first start off by proving the lemma to show that the columns of two row equivalent matrices satisfy the same linear dependence relations and subsequently use this to determine a basis for the column space. If an \\(m \\times n\\) matrix \\(\\A\\) with columns \\(\\a_1, \\a_2, ..., \\a_n\\) is row equivalent to matrix \\(\\B\\) with columns \\(\\b_1, \\b_2, ..., \\b_n\\) then \\[ c_1\\a_1 + c_2\\a_2 + ... + \\c_n\\a_n = \\0 \\] if and only if \\[ c_1\\b_1 + c_2\\b_2 + ... + \\c_n\\b_n = \\0 \\] Proof (Row Operations preserves Linear Dependency of Column Space) Suppose \\(\\A\\) and \\(\\B\\) are row equivalent matrices. Then we know that both have the same solution space (i.e. \\(\\A\\x=\\0\\) iff \\(\\B\\x=\\0\\) ). Denote a vector \\(\\c = \\begin{bmatrix}c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n \\end{bmatrix}\\) then we see that \\(\\A\\c=\\0\\) iff \\(\\B\\c=\\0\\) and so we see that \\[ c_1\\a_1 + c_2\\a_2 + ... + \\c_n\\a_n = \\0 \\] if and only if \\[ c_1\\b_1 + c_2\\b_2 + ... + \\c_n\\b_n = \\0 \\] Corollary (Row Operations preserves Order Linear Dependency of Column Space) As a corollary to the lemma, we note that for two row equivalent matrices \\(\\A\\) and \\(\\B\\) , the same set of linearly (in)dependen columns in \\(\\A\\) corresponds to the same set of linearly (in)dependent columns in \\(\\B\\) . Theorem (Basis For Column Space) If \\(\\A\\) is an \\(m \\times n\\) matrix and the pivots of \\(rref(\\A)\\) occur in columns \\(i_1, i_2, \\ldots, i_k\\) where \\(\\{i_1, i_2, \\ldots, i_k\\} \\subseteq \\{1, 2, \\ldots, n\\}\\) , then columns \\(i_1, i_2, \\ldots, i_k\\) of \\(\\A\\) form a basis for the column space of \\(\\A\\) . The proof is detailed in pp.130 of A modern introduction to linear algebra henry ricardo . Algorithm (Basis For Column Space) Calculate RREF of the matrix \\(\\A\\) . The pivot rows (non-zero) columns form a basis of the column space of \\(\\A\\) . Theorem (Column Space of \\(\\mathbf{A}\\) and \\(\\mathbf{A}\\mathbf{A}^\\top\\) ) \\(\\mathbf{A}\\) and \\(\\mathbf{A}\\mathbf{A}^\\top\\) have the same column space. Proof (Column Space of \\(\\mathbf{A}\\) and \\(\\mathbf{A}\\mathbf{A}^\\top\\) ) We first write \\(\\B = \\A\\A^\\top\\) and use the method that if \\(M \\subseteq N\\) and \\(N \\subseteq M\\) , then \\(N=M\\) to prove this. We first show that \\(C(\\B) \\subseteq C(\\A)\\) . The column space \\(C(\\A) = \\text{span}(\\a_1, \\a_2, ..., \\a_n) \\in \\F^{m}\\) and \\(C(\\B) = \\text{span}(\\b_1, \\b_2, ..., \\b_m)\\) . We note that by the right matrix multiplication , each column of \\(\\B = \\A\\A^\\top\\) is a linear combination of the columns of the left matrix \\(\\A\\) . That is to say, we can represent each column in \\(\\B\\) as \\(\\b_i = \\lambda_1 \\a_1 + \\lambda_2 \\a_2 + \\cdots + \\lambda_n \\a_n\\) . Consequently, by definition, any column from \\(\\B\\) is an element of \\(\\text{span}(\\a_1, ..., \\a_n)\\) , and hence in the column space \\(C(\\A)\\) . Thus, for any element in \\(C(\\B)\\) , this element must be in \\(C(\\A)\\) , and thus \\(C(\\B) \\subseteq C(\\A)\\) . We then show that \\(C(\\A) \\subseteq C(\\B)\\) . Take any element from \\(C(\\A)\\) , and recall that \\(C(\\B) = \\text{span}(\\b_1, \\b_2, ..., \\b_m) = c_1 \\b_1 + c_2 \\b_2 + \\cdots + c_m \\b_m = c_1 (\\lambda_1 \\a_1 + \\lambda_2 \\a_2 + \\cdots + \\lambda_m \\a_m) + c_2 (\\lambda_1 \\a_1 + \\lambda_2 \\a_2 + \\cdots + \\lambda_m \\a_m) + ... + c_2 (\\lambda_1 \\a_1 + \\lambda_2 \\a_2 + \\cdots + \\lambda_m \\a_m) = d_1\\a_1 + d_2\\a_2 + ... + d_m \\a_m\\) for some \\(d_i \\in \\F\\) . Then any element taken from the column space of \\(\\A\\) is a linear combination of \\(\\a_1, ..., \\a_m\\) ... What can we tell? We cannot tell anything since if \\(n > m\\) , then it may be the case that an element from \\(C(\\A)\\) may not cover. The \"Augment-Rank\" Algorithm to determine membership of Column Space Given a set of vectors \\(S = \\{\\v_1, \\v_2, ..., \\v_n\\}\\) , and a vector \\(\\w\\) , deduce if \\(\\w \\in \\textbf{span}(S)\\) . Assume \\(\\v_i \\in \\R^{m}\\) and \\(\\w \\in \\R^{m}\\) . Construct a matrix \\(\\mathbf{S} = \\begin{bmatrix} \\v_1 & \\v_2 & \\cdots & \\v_m \\end{bmatrix}_{m \\times n}\\) where \\(\\mathbf{S} \\in \\R^{m \\times n}\\) . Compute the rank of \\(\\mathbf{S}\\) and call it \\(s_1\\) . Horizontally concatenate \\(\\mathbf{S}\\) and \\(\\w\\) to get \\(\\mathbf{S}_w = \\mathbf{S} \\cup \\w\\) . Compute rank of \\(\\mathbf{S}_w\\) to be \\(s_2\\) . Then: If \\(s_1 = s_2\\) , this means that column space of \\(\\mathbf{S}\\) equals to the column space of \\(\\mathbf{S}_w\\) , and thus \\(\\w\\) did not alter the column space of \\(\\mathbf{S}\\) , which means \\(\\w\\) must be part of the column space of \\(\\mathbf{S}\\) , and thus in the span of \\(S\\) . If \\(s_2 > s_1\\) , then \\(\\w \\not\\in S\\) because if it is, the column space of \\(\\mathbf{S}_w\\) should not change. We can easily use this algorithm to check if a vector \\(\\w\\) is in the column space of a matrix \\(\\A\\) by setting \\(S\\) to be the set that contains all the columns of \\(\\A\\) . Geometric Intuition Mike mentioned on Linear Algebra: Theory, Intuition, Code, 2021. (pp. 211) that we can think of the above cases geometrically. In the first case where \\(s_1 = s_2\\) , the rank is the same, this means the vector \\(\\w\\) is in the column space of \\(\\A\\) . This makes sense because when we add the new vector to the set, and yet the rank (dimension) did not change, this coincides with the idea of vector \\(\\w\\) sitting somewhere in the column space of \\(\\A\\) , and hence no new geometric directions are obatined by including this vector . In the second case where \\(s_2 > s_1\\) however, if v is outside the column space, then it points off in some other geometric dimension that is not spanned by the column space; hence, B has one extra geometric dimension not contained in A, and thus the rank is one higher. In the image below, we denote \\(C(\\A)\\) as the 1d-subspace in red, then vector \\(\\v\\) is apparently lying in the 1d-subspace, and hence a member of \\(C(\\A)\\) , then \\(\\w\\) , is slightly pointing to a different direction, and hence not in the column space of \\(\\A\\) . Fig; Column Space of A; By Hongnan G. Corollary If \\(\\A\\) is now a full ranked square matrix \\(m \\times m\\) , which means rank of \\(\\A\\) is \\(m\\) , then any vector \\(\\w \\in \\R^{m}\\) will be in the column space of \\(\\A\\) since the column space of \\(\\A\\) is actually a basis of the ambient subspace \\(\\F^{m}\\) . Visualizing Column Space Courtesy of Macro Analyst's linear algebra with python . import matplotlib.pyplot as plt import numpy as np from mpl_toolkits.mplot3d import Axes3D import scipy as sp import scipy.linalg import sympy as sy sy . init_printing () Consider two matrix with \\[\\A = \\begin{bmatrix}1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{bmatrix} ,\\quad \\B = \\begin{bmatrix}3 & -1 \\\\ 2 & 4 \\\\ -1 & 1 \\end{bmatrix}\\] Then the column space \\(C(\\A)\\) is a 2d-plane given by \\[\\text{plane}(C(\\A)) = \\0 + s\\left[\\matrix{1\\cr 0\\cr 0}\\right] + t\\left[\\matrix{0\\cr 1\\cr 0}\\right]\\] and the column space \\(C(\\B)\\) is a 2d-plane given by \\[\\text{plane}(C(\\B)) = \\0 + s\\left[\\matrix{3\\cr 2\\cr -1}\\right] + t\\left[\\matrix{-1\\cr 4\\cr 1}\\right]\\] fig = plt . figure ( figsize = ( 10 , 10 )) ax = fig . add_subplot ( projection = '3d' ) s = np . linspace ( - 2 , 2 , 20 ) t = np . linspace ( - 2 , 2 , 20 ) S , T = np . meshgrid ( s , t ) X = 3 * S - T Y = 2 * S + 4 * T Z = - S + T ax . plot_wireframe ( X , Y , Z , linewidth = .5 , color = 'r' ) s = np . linspace ( - 10 , 10 , 20 ) t = np . linspace ( - 10 , 10 , 20 ) S , T = np . meshgrid ( s , t ) X = S Y = T Z = np . zeros ( S . shape ) ax . plot_wireframe ( X , Y , Z , linewidth = .5 , color = 'k' ) ax . view_init ( elev = 14 , azim = 58 )","title":"Column Space"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.04_linear_algebra_matrix_theory_matrix_spaces_column_space/#preamble","text":"Both Professor Mike and Gilbert started off with this statement: The entirety and gist of the matrix space can be summarized as answering two key questions: Given a matrix \\(\\A \\in \\F^{m \\times n}\\) and vector \\(\\b \\in \\F^{m}\\) and \\(\\0 \\in \\F^{m}\\) , then can we find a vector \\(\\x \\in \\F^{n}\\) such that \\( \\(\\A\\x = \\b\\) \\) \\( \\(\\A\\x = \\0\\) \\) From the matrix-multiplication section, we can easily re-interpret the question as: \\(\\A\\x=\\b\\) is equivalent to asking can we find linear combination of columns of \\(\\A\\) such that the sum is \\(\\b\\) ? \\(\\A\\x=\\0\\) is equivalent to asking can we find linear combination of columns of \\(\\A\\) such that the sum is \\(\\0\\) assuming \\(\\x \\neq \\0\\) .","title":"Preamble"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.04_linear_algebra_matrix_theory_matrix_spaces_column_space/#column-space","text":"","title":"Column Space"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.04_linear_algebra_matrix_theory_matrix_spaces_column_space/#motivation","text":"We often frame our Machine Learning problems into a linear system of equations of the form \\(\\A\\x=\\b\\) . Not every linear system of equation is easily solvable, and has a solution \\(\\x\\) exists if and only if \\(\\b\\) belongs to the column space of \\(\\A\\) . Why so? Recall in the section linear algebra and matrix multiplication the following (if you forgot go read up): A column space of a matrix \\(\\A\\) is just the set of linear combination of the columns of \\(\\A\\) , and is a subspace. We will go through it in more details, but for now, I want to introduce this idea first. As a consequence of the example prior, one should realize three things: \\(\\A\\x\\) is a combination of the columns of the matrix \\(\\A\\) . \\(\\A\\x = \\b\\) may not always have a solution \\(\\x\\) . If \\(\\A\\x = \\b\\) has a solution \\(\\x\\) , then the product \\(\\b\\) must be a linear combination of the columns of \\(\\A\\) . This has important consequences later on. For now, just know that if \\(\\A\\x = \\b\\) has a solution, then \\(\\b\\) resides in the column space of \\(\\A\\) .","title":"Motivation"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.04_linear_algebra_matrix_theory_matrix_spaces_column_space/#algebraic-definition-column-space","text":"Let \\(\\F\\) be a field of scalars and let \\(\\A\\) a \\(m \\times n\\) matrix, with column vectors \\(\\c_1, \\c_2, \\cdots, \\c_n\\) . Note that a linear combination of these vectors is any vector of the form \\[\\lambda_1 \\mathbf{c}_1 + \\lambda_2 \\mathbf{c}_2 + \\cdots + \\lambda_n \\mathbf{a}_n\\] where \\(\\lambda_i\\) are scalars; Then the set of all possible linear combinations of \\(\\c_1, \\c_2, \\cdots, \\c_n\\) is called the column space of \\(\\A\\) ; In other words, the column space of \\(\\A\\) is the linear span of the vectors \\(\\c_1, \\c_2, \\cdots, \\c_n\\) . And as noted in the right multiplication method in the section matrix-vector multiplication , any linear combination of the column vectors of a matrix \\(\\A\\) can be written as the product of \\(\\A\\) with a column vector where the column vectors hold the coefficients of the linear combination: \\[ \\begin{array} {rcl} \\A \\begin{bmatrix} c_1 \\\\ \\vdots \\\\ c_n \\end{bmatrix} & = & \\begin{bmatrix} a_{11} & \\cdots & a_{1n} \\\\ \\vdots & \\ddots & \\vdots \\\\ a_{m1} & \\cdots & a_{mn} \\end{bmatrix} \\begin{bmatrix} c_1 \\\\ \\vdots \\\\ c_n \\end{bmatrix} = \\begin{bmatrix} c_1 a_{11} + \\cdots + c_{n} a_{1n} \\\\ \\vdots \\\\ c_{1} a_{m1} + \\cdots + c_{n} a_{mn} \\end{bmatrix} = c_1 \\begin{bmatrix} a_{11} \\\\ \\vdots \\\\ a_{m1} \\end{bmatrix} + \\cdots + c_n \\begin{bmatrix} a_{1n} \\\\ \\vdots \\\\ a_{mn} \\end{bmatrix} \\\\ & = & c_1 \\mathbf{v}_1 + \\cdots + c_n \\mathbf{v}_n \\end{array} \\] Therefore, the column space of \\(\\A\\) consists of all possible products \\(\\A\\x\\) for \\(\\x \\in \\F^n\\) . This is the same as the image or range of the corresponding matrix transformation in which we will learn more on in the linear transformation chapter.","title":"Algebraic Definition (Column Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.04_linear_algebra_matrix_theory_matrix_spaces_column_space/#notation-column-space","text":"The column space of a matrix is denoted as \\(C(\\A)\\) , which is the space spanned by all columns of the matrix \\(\\A\\) . Note that the column space \\(C(\\A)\\) resides in the \\(\\F^{m}\\) space.","title":"Notation (Column Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.04_linear_algebra_matrix_theory_matrix_spaces_column_space/#example-column-space","text":"If \\(A = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 2 & 0 \\end{bmatrix}\\) , then the column vectors are \\(\\a_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 2 \\end{bmatrix} \\quad \\a_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0\\end{bmatrix}\\) . A linear combination of \\(\\a_1, \\a_2\\) is any vector of the form \\[c_1 \\begin{bmatrix} 1 \\\\ 0 \\\\ 2 \\end{bmatrix} + c_2 \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ 2c_1 \\end{bmatrix}\\] The set of all such vectors is the column space of \\(\\A\\) and in this case, the column space is precisely the set of vectors \\((x, y, z) \\in \\R^3\\) satisfying the equation \\(z=2x\\) using Cartesian coordinates, this set is a plane through the origin in three-dimensional space.","title":"Example (Column Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.04_linear_algebra_matrix_theory_matrix_spaces_column_space/#theorem-solvable-solutions","text":"A system of \\(m\\) linear equations in \\(n\\) unknowns \\(\\A\\x = \\b\\) has a solution if and only if the vector \\(\\b\\) can be expressed as a linear combination of the columns of \\(\\A\\) . In other words, the system is solvable if and only if \\(\\b \\in \\textbf{Col Space}(\\A)\\) . Proof can be found in pp.125-126 A modern introduction to linear algebra by Henry Ricardo .","title":"Theorem (Solvable Solutions)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.04_linear_algebra_matrix_theory_matrix_spaces_column_space/#algebraic-definition-column-rank","text":"The column rank of \\(\\A\\) is the dimension of the column space of \\(\\A\\) . In other words, the column rank of \\(\\A\\) corresponds to the largest number vectors that can form a linearly indepedent set in the columns of \\(\\A\\) . This interpretation follows because the definition of dimension of a column space means given a vector space \\(V\\) which is our column space spanned by the columns of \\(\\A\\) , the dimension of the column space is the cardinality of the basis \\(\\B\\) of \\(V\\) ; and note that any basis \\(\\B\\) of \\(V\\) (our column space) is a linearly independent set which also spans \\(V\\) . So both definition is equivalent.","title":"Algebraic Definition (Column Rank)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.04_linear_algebra_matrix_theory_matrix_spaces_column_space/#full-column-rank","text":"A matrix \\(\\A \\in \\F^{m \\times n}\\) is full column rank if and only if \\(\\rank(A) = n\\) . Thus, if the rank of the matrix \\(\\A\\) is \\(r = n\\) , then the column space spans all of \\(\\F^{n}\\) and is a basis of \\(\\F^{n}\\) , else it is a \\(r\\) -dimensional subspace embedded in \\(\\F^{n}\\) .","title":"Full Column Rank"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.04_linear_algebra_matrix_theory_matrix_spaces_column_space/#lemma-row-operations-preserves-linear-dependency-of-column-space","text":"Row operations apparently change the column space but the important part is that it preserves the column rank of a matrix. We first start off by proving the lemma to show that the columns of two row equivalent matrices satisfy the same linear dependence relations and subsequently use this to determine a basis for the column space. If an \\(m \\times n\\) matrix \\(\\A\\) with columns \\(\\a_1, \\a_2, ..., \\a_n\\) is row equivalent to matrix \\(\\B\\) with columns \\(\\b_1, \\b_2, ..., \\b_n\\) then \\[ c_1\\a_1 + c_2\\a_2 + ... + \\c_n\\a_n = \\0 \\] if and only if \\[ c_1\\b_1 + c_2\\b_2 + ... + \\c_n\\b_n = \\0 \\]","title":"Lemma (Row Operations preserves Linear Dependency of Column Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.04_linear_algebra_matrix_theory_matrix_spaces_column_space/#proof-row-operations-preserves-linear-dependency-of-column-space","text":"Suppose \\(\\A\\) and \\(\\B\\) are row equivalent matrices. Then we know that both have the same solution space (i.e. \\(\\A\\x=\\0\\) iff \\(\\B\\x=\\0\\) ). Denote a vector \\(\\c = \\begin{bmatrix}c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n \\end{bmatrix}\\) then we see that \\(\\A\\c=\\0\\) iff \\(\\B\\c=\\0\\) and so we see that \\[ c_1\\a_1 + c_2\\a_2 + ... + \\c_n\\a_n = \\0 \\] if and only if \\[ c_1\\b_1 + c_2\\b_2 + ... + \\c_n\\b_n = \\0 \\]","title":"Proof (Row Operations preserves Linear Dependency of Column Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.04_linear_algebra_matrix_theory_matrix_spaces_column_space/#corollary-row-operations-preserves-order-linear-dependency-of-column-space","text":"As a corollary to the lemma, we note that for two row equivalent matrices \\(\\A\\) and \\(\\B\\) , the same set of linearly (in)dependen columns in \\(\\A\\) corresponds to the same set of linearly (in)dependent columns in \\(\\B\\) .","title":"Corollary (Row Operations preserves Order Linear Dependency of Column Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.04_linear_algebra_matrix_theory_matrix_spaces_column_space/#theorem-basis-for-column-space","text":"If \\(\\A\\) is an \\(m \\times n\\) matrix and the pivots of \\(rref(\\A)\\) occur in columns \\(i_1, i_2, \\ldots, i_k\\) where \\(\\{i_1, i_2, \\ldots, i_k\\} \\subseteq \\{1, 2, \\ldots, n\\}\\) , then columns \\(i_1, i_2, \\ldots, i_k\\) of \\(\\A\\) form a basis for the column space of \\(\\A\\) . The proof is detailed in pp.130 of A modern introduction to linear algebra henry ricardo .","title":"Theorem (Basis For Column Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.04_linear_algebra_matrix_theory_matrix_spaces_column_space/#algorithm-basis-for-column-space","text":"Calculate RREF of the matrix \\(\\A\\) . The pivot rows (non-zero) columns form a basis of the column space of \\(\\A\\) .","title":"Algorithm (Basis For Column Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.04_linear_algebra_matrix_theory_matrix_spaces_column_space/#theorem-column-space-of-mathbfa-and-mathbfamathbfatop","text":"\\(\\mathbf{A}\\) and \\(\\mathbf{A}\\mathbf{A}^\\top\\) have the same column space.","title":"Theorem (Column Space of \\(\\mathbf{A}\\) and \\(\\mathbf{A}\\mathbf{A}^\\top\\))"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.04_linear_algebra_matrix_theory_matrix_spaces_column_space/#proof-column-space-of-mathbfa-and-mathbfamathbfatop","text":"We first write \\(\\B = \\A\\A^\\top\\) and use the method that if \\(M \\subseteq N\\) and \\(N \\subseteq M\\) , then \\(N=M\\) to prove this. We first show that \\(C(\\B) \\subseteq C(\\A)\\) . The column space \\(C(\\A) = \\text{span}(\\a_1, \\a_2, ..., \\a_n) \\in \\F^{m}\\) and \\(C(\\B) = \\text{span}(\\b_1, \\b_2, ..., \\b_m)\\) . We note that by the right matrix multiplication , each column of \\(\\B = \\A\\A^\\top\\) is a linear combination of the columns of the left matrix \\(\\A\\) . That is to say, we can represent each column in \\(\\B\\) as \\(\\b_i = \\lambda_1 \\a_1 + \\lambda_2 \\a_2 + \\cdots + \\lambda_n \\a_n\\) . Consequently, by definition, any column from \\(\\B\\) is an element of \\(\\text{span}(\\a_1, ..., \\a_n)\\) , and hence in the column space \\(C(\\A)\\) . Thus, for any element in \\(C(\\B)\\) , this element must be in \\(C(\\A)\\) , and thus \\(C(\\B) \\subseteq C(\\A)\\) . We then show that \\(C(\\A) \\subseteq C(\\B)\\) . Take any element from \\(C(\\A)\\) , and recall that \\(C(\\B) = \\text{span}(\\b_1, \\b_2, ..., \\b_m) = c_1 \\b_1 + c_2 \\b_2 + \\cdots + c_m \\b_m = c_1 (\\lambda_1 \\a_1 + \\lambda_2 \\a_2 + \\cdots + \\lambda_m \\a_m) + c_2 (\\lambda_1 \\a_1 + \\lambda_2 \\a_2 + \\cdots + \\lambda_m \\a_m) + ... + c_2 (\\lambda_1 \\a_1 + \\lambda_2 \\a_2 + \\cdots + \\lambda_m \\a_m) = d_1\\a_1 + d_2\\a_2 + ... + d_m \\a_m\\) for some \\(d_i \\in \\F\\) . Then any element taken from the column space of \\(\\A\\) is a linear combination of \\(\\a_1, ..., \\a_m\\) ... What can we tell? We cannot tell anything since if \\(n > m\\) , then it may be the case that an element from \\(C(\\A)\\) may not cover.","title":"Proof (Column Space of \\(\\mathbf{A}\\) and \\(\\mathbf{A}\\mathbf{A}^\\top\\))"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.04_linear_algebra_matrix_theory_matrix_spaces_column_space/#the-augment-rank-algorithm-to-determine-membership-of-column-space","text":"Given a set of vectors \\(S = \\{\\v_1, \\v_2, ..., \\v_n\\}\\) , and a vector \\(\\w\\) , deduce if \\(\\w \\in \\textbf{span}(S)\\) . Assume \\(\\v_i \\in \\R^{m}\\) and \\(\\w \\in \\R^{m}\\) . Construct a matrix \\(\\mathbf{S} = \\begin{bmatrix} \\v_1 & \\v_2 & \\cdots & \\v_m \\end{bmatrix}_{m \\times n}\\) where \\(\\mathbf{S} \\in \\R^{m \\times n}\\) . Compute the rank of \\(\\mathbf{S}\\) and call it \\(s_1\\) . Horizontally concatenate \\(\\mathbf{S}\\) and \\(\\w\\) to get \\(\\mathbf{S}_w = \\mathbf{S} \\cup \\w\\) . Compute rank of \\(\\mathbf{S}_w\\) to be \\(s_2\\) . Then: If \\(s_1 = s_2\\) , this means that column space of \\(\\mathbf{S}\\) equals to the column space of \\(\\mathbf{S}_w\\) , and thus \\(\\w\\) did not alter the column space of \\(\\mathbf{S}\\) , which means \\(\\w\\) must be part of the column space of \\(\\mathbf{S}\\) , and thus in the span of \\(S\\) . If \\(s_2 > s_1\\) , then \\(\\w \\not\\in S\\) because if it is, the column space of \\(\\mathbf{S}_w\\) should not change. We can easily use this algorithm to check if a vector \\(\\w\\) is in the column space of a matrix \\(\\A\\) by setting \\(S\\) to be the set that contains all the columns of \\(\\A\\) .","title":"The \"Augment-Rank\" Algorithm to determine membership of Column Space"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.04_linear_algebra_matrix_theory_matrix_spaces_column_space/#geometric-intuition","text":"Mike mentioned on Linear Algebra: Theory, Intuition, Code, 2021. (pp. 211) that we can think of the above cases geometrically. In the first case where \\(s_1 = s_2\\) , the rank is the same, this means the vector \\(\\w\\) is in the column space of \\(\\A\\) . This makes sense because when we add the new vector to the set, and yet the rank (dimension) did not change, this coincides with the idea of vector \\(\\w\\) sitting somewhere in the column space of \\(\\A\\) , and hence no new geometric directions are obatined by including this vector . In the second case where \\(s_2 > s_1\\) however, if v is outside the column space, then it points off in some other geometric dimension that is not spanned by the column space; hence, B has one extra geometric dimension not contained in A, and thus the rank is one higher. In the image below, we denote \\(C(\\A)\\) as the 1d-subspace in red, then vector \\(\\v\\) is apparently lying in the 1d-subspace, and hence a member of \\(C(\\A)\\) , then \\(\\w\\) , is slightly pointing to a different direction, and hence not in the column space of \\(\\A\\) . Fig; Column Space of A; By Hongnan G.","title":"Geometric Intuition"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.04_linear_algebra_matrix_theory_matrix_spaces_column_space/#corollary","text":"If \\(\\A\\) is now a full ranked square matrix \\(m \\times m\\) , which means rank of \\(\\A\\) is \\(m\\) , then any vector \\(\\w \\in \\R^{m}\\) will be in the column space of \\(\\A\\) since the column space of \\(\\A\\) is actually a basis of the ambient subspace \\(\\F^{m}\\) .","title":"Corollary"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.04_linear_algebra_matrix_theory_matrix_spaces_column_space/#visualizing-column-space","text":"Courtesy of Macro Analyst's linear algebra with python . import matplotlib.pyplot as plt import numpy as np from mpl_toolkits.mplot3d import Axes3D import scipy as sp import scipy.linalg import sympy as sy sy . init_printing () Consider two matrix with \\[\\A = \\begin{bmatrix}1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{bmatrix} ,\\quad \\B = \\begin{bmatrix}3 & -1 \\\\ 2 & 4 \\\\ -1 & 1 \\end{bmatrix}\\] Then the column space \\(C(\\A)\\) is a 2d-plane given by \\[\\text{plane}(C(\\A)) = \\0 + s\\left[\\matrix{1\\cr 0\\cr 0}\\right] + t\\left[\\matrix{0\\cr 1\\cr 0}\\right]\\] and the column space \\(C(\\B)\\) is a 2d-plane given by \\[\\text{plane}(C(\\B)) = \\0 + s\\left[\\matrix{3\\cr 2\\cr -1}\\right] + t\\left[\\matrix{-1\\cr 4\\cr 1}\\right]\\] fig = plt . figure ( figsize = ( 10 , 10 )) ax = fig . add_subplot ( projection = '3d' ) s = np . linspace ( - 2 , 2 , 20 ) t = np . linspace ( - 2 , 2 , 20 ) S , T = np . meshgrid ( s , t ) X = 3 * S - T Y = 2 * S + 4 * T Z = - S + T ax . plot_wireframe ( X , Y , Z , linewidth = .5 , color = 'r' ) s = np . linspace ( - 10 , 10 , 20 ) t = np . linspace ( - 10 , 10 , 20 ) S , T = np . meshgrid ( s , t ) X = S Y = T Z = np . zeros ( S . shape ) ax . plot_wireframe ( X , Y , Z , linewidth = .5 , color = 'k' ) ax . view_init ( elev = 14 , azim = 58 )","title":"Visualizing Column Space"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.05_linear_algebra_matrix_theory_matrix_spaces_right_nullspace/","text":"\\[ \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\z}{\\mathbf{z}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\C}{\\mathbf{C}} \\newcommand{\\rank}{\\textbf{rank}} \\] Preamble Both Professor Mike and Gilbert started off with this statement: The entirety and gist of the matrix space can be summarized as answering two key questions: Given a matrix \\(\\A \\in \\F^{m \\times n}\\) and vector \\(\\b \\in \\F^{m}\\) and \\(\\0 \\in \\F^{m}\\) , then can we find a vector \\(\\x \\in \\F^{n}\\) such that \\( \\(\\A\\x = \\b\\) \\) \\( \\(\\A\\x = \\0\\) \\) From the matrix-multiplication section, we can easily re-interpret the question as: \\(\\A\\x=\\b\\) is equivalent to asking can we find linear combination of columns of \\(\\A\\) such that the sum is \\(\\b\\) ? \\(\\A\\x=\\0\\) is equivalent to asking can we find linear combination of columns of \\(\\A\\) such that the sum is \\(\\0\\) assuming \\(\\x \\neq \\0\\) . (Right) Null Space (Kernel) Motivation To fill in. Algebraic Definition (Null Space) Let \\(\\F\\) be a field of scalars and let \\(\\A\\) a \\(m \\times n\\) matrix, then the nullspace of \\(\\A\\) is the set: \\[ N(\\A) = \\{\\v \\in \\F^{n} ~|~ \\A\\v = \\0\\} \\] Note that the trivial solution \\(\\0\\) is always a solution to the nullspace. Note that if there exists a non-trivial \\(\\v\\) such that \\(\\A\\v = \\0\\) , then for any \\(\\lambda \\in F\\) , we also have \\(\\A(\\lambda\\v) = \\0\\) . Matrix Definition (Null Space) Consider a linear map represented as a \\(m \\times n\\) matrix \\(\\A\\) with coefficients in a field \\(\\F\\) , that is operating on column vectors \\(\\x\\) with \\(n\\) components over \\(\\F\\) . The kernel of this linear map is the set of solutions to the equation \\(\\A\\x=\\0\\) where \\(\\0\\) is understood as the zero vector. The dimension of the kernel of \\(\\A\\) is called the nullity of \\(\\A\\) . In set builder notations, we recover the algebraic definition : \\[ N(\\A) = \\{\\v \\in \\F^{n} ~|~ \\A\\v = \\0\\} \\] The matrix equation is equivalent to a homogeneous system of linear equations: \\[ \\A\\x=\\mathbf{0} \\;\\;\\Leftrightarrow\\;\\; \\begin{alignat}{7} a_{11} x_1 &&\\; + \\;&& a_{12} x_2 &&\\; + \\;\\cdots\\; + \\;&& a_{1n} x_n &&\\; = \\;&&& 0 \\\\ a_{21} x_1 &&\\; + \\;&& a_{22} x_2 &&\\; + \\;\\cdots\\; + \\;&& a_{2n} x_n &&\\; = \\;&&& 0 \\\\ && && && && &&\\vdots\\ \\;&&& \\\\ a_{m1} x_1 &&\\; + \\;&& a_{m2} x_2 &&\\; + \\;\\cdots\\; + \\;&& a_{mn} x_n &&\\; = \\;&&& 0\\text{.} \\\\ \\end{alignat} \\] Thus the kernel of \\(\\A\\) is the same as the solution set to the above homogeneous equations. Notation (Null Space) The null space of a matrix is denoted as \\(N(\\A)\\) . Note in particular that the column space \\(N(\\A)\\) resides in the \\(\\F^{n}\\) space. What does Null Space tell you? Now one important consequence is that, if there exists a non-trivial solution to the matrix \\(\\A\\x = \\0\\) , then what does it mean? Recall the right multiplication of columns for matrix-vector multiplication : It means that the linear combination of the columns of \\(\\A\\) can form the zero vector \\(\\0\\) . In particular, this linear combination is NON-TRIVIAL! Then the matrix \\(\\A\\) is formed by columns that are linearly dependent, and hence not full rank! We can formalize it as a theorem below. Theorem (Full Ranked Matrix has an Empty Nullspace) A matrix \\(\\A \\in \\F^{m \\times n}\\) is full rank if and only if the nullspace \\(N(\\A)\\) has only the trivial solution. Theorem (Basis For Right Null Space) To be filled in. Geometric Intuition (Null Space) import matplotlib.pyplot as plt import numpy as np from mpl_toolkits.mplot3d import Axes3D import sympy as sy sy . init_printing () A = sy . Matrix ([[ 5 , 8 , 2 ], [ 10 , 16 , 4 ], [ 3 , 4 , 1 ]]); A \\(\\displaystyle \\left[\\begin{matrix}5 & 8 & 2\\\\10 & 16 & 4\\\\3 & 4 & 1\\end{matrix}\\right]\\) A . rref () \\(\\displaystyle \\left( \\left[\\begin{matrix}1 & 0 & 0\\\\0 & 1 & \\frac{1}{4}\\\\0 & 0 & 0\\end{matrix}\\right], \\ \\left( 0, \\ 1\\right)\\right)\\)","title":"Right Nullspace (Kernel)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.05_linear_algebra_matrix_theory_matrix_spaces_right_nullspace/#preamble","text":"Both Professor Mike and Gilbert started off with this statement: The entirety and gist of the matrix space can be summarized as answering two key questions: Given a matrix \\(\\A \\in \\F^{m \\times n}\\) and vector \\(\\b \\in \\F^{m}\\) and \\(\\0 \\in \\F^{m}\\) , then can we find a vector \\(\\x \\in \\F^{n}\\) such that \\( \\(\\A\\x = \\b\\) \\) \\( \\(\\A\\x = \\0\\) \\) From the matrix-multiplication section, we can easily re-interpret the question as: \\(\\A\\x=\\b\\) is equivalent to asking can we find linear combination of columns of \\(\\A\\) such that the sum is \\(\\b\\) ? \\(\\A\\x=\\0\\) is equivalent to asking can we find linear combination of columns of \\(\\A\\) such that the sum is \\(\\0\\) assuming \\(\\x \\neq \\0\\) .","title":"Preamble"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.05_linear_algebra_matrix_theory_matrix_spaces_right_nullspace/#right-null-space-kernel","text":"","title":"(Right) Null Space (Kernel)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.05_linear_algebra_matrix_theory_matrix_spaces_right_nullspace/#motivation","text":"To fill in.","title":"Motivation"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.05_linear_algebra_matrix_theory_matrix_spaces_right_nullspace/#algebraic-definition-null-space","text":"Let \\(\\F\\) be a field of scalars and let \\(\\A\\) a \\(m \\times n\\) matrix, then the nullspace of \\(\\A\\) is the set: \\[ N(\\A) = \\{\\v \\in \\F^{n} ~|~ \\A\\v = \\0\\} \\] Note that the trivial solution \\(\\0\\) is always a solution to the nullspace. Note that if there exists a non-trivial \\(\\v\\) such that \\(\\A\\v = \\0\\) , then for any \\(\\lambda \\in F\\) , we also have \\(\\A(\\lambda\\v) = \\0\\) .","title":"Algebraic Definition (Null Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.05_linear_algebra_matrix_theory_matrix_spaces_right_nullspace/#matrix-definition-null-space","text":"Consider a linear map represented as a \\(m \\times n\\) matrix \\(\\A\\) with coefficients in a field \\(\\F\\) , that is operating on column vectors \\(\\x\\) with \\(n\\) components over \\(\\F\\) . The kernel of this linear map is the set of solutions to the equation \\(\\A\\x=\\0\\) where \\(\\0\\) is understood as the zero vector. The dimension of the kernel of \\(\\A\\) is called the nullity of \\(\\A\\) . In set builder notations, we recover the algebraic definition : \\[ N(\\A) = \\{\\v \\in \\F^{n} ~|~ \\A\\v = \\0\\} \\] The matrix equation is equivalent to a homogeneous system of linear equations: \\[ \\A\\x=\\mathbf{0} \\;\\;\\Leftrightarrow\\;\\; \\begin{alignat}{7} a_{11} x_1 &&\\; + \\;&& a_{12} x_2 &&\\; + \\;\\cdots\\; + \\;&& a_{1n} x_n &&\\; = \\;&&& 0 \\\\ a_{21} x_1 &&\\; + \\;&& a_{22} x_2 &&\\; + \\;\\cdots\\; + \\;&& a_{2n} x_n &&\\; = \\;&&& 0 \\\\ && && && && &&\\vdots\\ \\;&&& \\\\ a_{m1} x_1 &&\\; + \\;&& a_{m2} x_2 &&\\; + \\;\\cdots\\; + \\;&& a_{mn} x_n &&\\; = \\;&&& 0\\text{.} \\\\ \\end{alignat} \\] Thus the kernel of \\(\\A\\) is the same as the solution set to the above homogeneous equations.","title":"Matrix Definition (Null Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.05_linear_algebra_matrix_theory_matrix_spaces_right_nullspace/#notation-null-space","text":"The null space of a matrix is denoted as \\(N(\\A)\\) . Note in particular that the column space \\(N(\\A)\\) resides in the \\(\\F^{n}\\) space.","title":"Notation (Null Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.05_linear_algebra_matrix_theory_matrix_spaces_right_nullspace/#what-does-null-space-tell-you","text":"Now one important consequence is that, if there exists a non-trivial solution to the matrix \\(\\A\\x = \\0\\) , then what does it mean? Recall the right multiplication of columns for matrix-vector multiplication : It means that the linear combination of the columns of \\(\\A\\) can form the zero vector \\(\\0\\) . In particular, this linear combination is NON-TRIVIAL! Then the matrix \\(\\A\\) is formed by columns that are linearly dependent, and hence not full rank! We can formalize it as a theorem below.","title":"What does Null Space tell you?"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.05_linear_algebra_matrix_theory_matrix_spaces_right_nullspace/#theorem-full-ranked-matrix-has-an-empty-nullspace","text":"A matrix \\(\\A \\in \\F^{m \\times n}\\) is full rank if and only if the nullspace \\(N(\\A)\\) has only the trivial solution.","title":"Theorem (Full Ranked Matrix has an Empty Nullspace)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.05_linear_algebra_matrix_theory_matrix_spaces_right_nullspace/#theorem-basis-for-right-null-space","text":"To be filled in.","title":"Theorem (Basis For Right Null Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.05_linear_algebra_matrix_theory_matrix_spaces_right_nullspace/#geometric-intuition-null-space","text":"import matplotlib.pyplot as plt import numpy as np from mpl_toolkits.mplot3d import Axes3D import sympy as sy sy . init_printing () A = sy . Matrix ([[ 5 , 8 , 2 ], [ 10 , 16 , 4 ], [ 3 , 4 , 1 ]]); A \\(\\displaystyle \\left[\\begin{matrix}5 & 8 & 2\\\\10 & 16 & 4\\\\3 & 4 & 1\\end{matrix}\\right]\\) A . rref () \\(\\displaystyle \\left( \\left[\\begin{matrix}1 & 0 & 0\\\\0 & 1 & \\frac{1}{4}\\\\0 & 0 & 0\\end{matrix}\\right], \\ \\left( 0, \\ 1\\right)\\right)\\)","title":"Geometric Intuition (Null Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.06_linear_algebra_matrix_theory_matrix_spaces_left_nullspace/","text":"\\[ \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\z}{\\mathbf{z}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\C}{\\mathbf{C}} \\newcommand{\\rank}{\\textbf{rank}} \\] Preamble Both Professor Mike and Gilbert started off with this statement: The entirety and gist of the matrix space can be summarized as answering two key questions: Given a matrix \\(\\A \\in \\F^{m \\times n}\\) and vector \\(\\b \\in \\F^{m}\\) and \\(\\0 \\in \\F^{m}\\) , then can we find a vector \\(\\x \\in \\F^{n}\\) such that \\( \\(\\A\\x = \\b\\) \\) \\( \\(\\A\\x = \\0\\) \\) From the matrix-multiplication section, we can easily re-interpret the question as: \\(\\A\\x=\\b\\) is equivalent to asking can we find linear combination of columns of \\(\\A\\) such that the sum is \\(\\b\\) ? \\(\\A\\x=\\0\\) is equivalent to asking can we find linear combination of columns of \\(\\A\\) such that the sum is \\(\\0\\) assuming \\(\\x \\neq \\0\\) . Left (Null Space) Algebraic Definition (Left Null Space) Let \\(\\F\\) be a field of scalars and let \\(\\A\\) a \\(m \\times n\\) matrix, then the left nullspace of \\(\\A\\) is actually the nullspace of its transpose: \\[ N(\\A^\\top) = \\{\\v \\in \\F^{m} ~|~ \\A^\\top\\v = \\0\\} \\] and if we take transpose to both sides of \\(\\A^\\top\\v = \\0\\) to be \\(\\left(\\A^\\top\\v \\right)^\\top = \\0^\\top\\) ; it follows we have \\(\\v^\\top\\A = \\0^\\top\\) , where the row vector \\(\\v^\\top\\) is on the left of \\(\\A\\) . Note in particular that the column space \\(N(\\A)\\) resides in the \\(\\F^{m}\\) space. Theorem (The Nullspace and Left Nullspace of a Symmetric Matrix is Equal) Given a symmetric matrix \\(\\A\\) , we know that \\(\\A^\\top = \\A\\) , and thus \\(N(\\A) = N(\\A^\\top)\\) .","title":"Left Nullspace"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.06_linear_algebra_matrix_theory_matrix_spaces_left_nullspace/#preamble","text":"Both Professor Mike and Gilbert started off with this statement: The entirety and gist of the matrix space can be summarized as answering two key questions: Given a matrix \\(\\A \\in \\F^{m \\times n}\\) and vector \\(\\b \\in \\F^{m}\\) and \\(\\0 \\in \\F^{m}\\) , then can we find a vector \\(\\x \\in \\F^{n}\\) such that \\( \\(\\A\\x = \\b\\) \\) \\( \\(\\A\\x = \\0\\) \\) From the matrix-multiplication section, we can easily re-interpret the question as: \\(\\A\\x=\\b\\) is equivalent to asking can we find linear combination of columns of \\(\\A\\) such that the sum is \\(\\b\\) ? \\(\\A\\x=\\0\\) is equivalent to asking can we find linear combination of columns of \\(\\A\\) such that the sum is \\(\\0\\) assuming \\(\\x \\neq \\0\\) .","title":"Preamble"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.06_linear_algebra_matrix_theory_matrix_spaces_left_nullspace/#left-null-space","text":"","title":"Left (Null Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.06_linear_algebra_matrix_theory_matrix_spaces_left_nullspace/#algebraic-definition-left-null-space","text":"Let \\(\\F\\) be a field of scalars and let \\(\\A\\) a \\(m \\times n\\) matrix, then the left nullspace of \\(\\A\\) is actually the nullspace of its transpose: \\[ N(\\A^\\top) = \\{\\v \\in \\F^{m} ~|~ \\A^\\top\\v = \\0\\} \\] and if we take transpose to both sides of \\(\\A^\\top\\v = \\0\\) to be \\(\\left(\\A^\\top\\v \\right)^\\top = \\0^\\top\\) ; it follows we have \\(\\v^\\top\\A = \\0^\\top\\) , where the row vector \\(\\v^\\top\\) is on the left of \\(\\A\\) . Note in particular that the column space \\(N(\\A)\\) resides in the \\(\\F^{m}\\) space.","title":"Algebraic Definition (Left Null Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.06_linear_algebra_matrix_theory_matrix_spaces_left_nullspace/#theorem-the-nullspace-and-left-nullspace-of-a-symmetric-matrix-is-equal","text":"Given a symmetric matrix \\(\\A\\) , we know that \\(\\A^\\top = \\A\\) , and thus \\(N(\\A) = N(\\A^\\top)\\) .","title":"Theorem (The Nullspace and Left Nullspace of a Symmetric Matrix is Equal)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.07_linear_algebra_matrix_theory_matrix_rank/","text":"\\[\\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\z}{\\mathbf{z}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\rank}{\\textbf{rank}} \\newcommand{\\C}{\\mathbf{C}}\\] Matrix Rank We will understand matrix rank in a naive manner first, we will go through it more formally in the next few sections. Algebraic Definition (Rank) 1 In linear algebra , the rank of a matrix \\(\\A\\) is the dimension of the vector space generated (or spanned ) by its columns. This corresponds to the maximal number of linearly independent columns of \\(\\A\\) . This, in turn, is identical to the dimension of the vector space spanned by its rows. Rank is thus a measure of the \\\" nondegenerateness \\\" of the system of linear equations and linear transformation encoded by \\(\\A\\) . There are multiple equivalent definitions of rank. A matrix\\'s rank is one of its most fundamental characteristics. Notation (Rank) The rank of a matrix \\(\\A\\) is commonly denoted by \\(\\rank(A)\\) . Algebraic Definition (Column Rank) The column rank of \\(\\A\\) is the dimension of the column space of \\(\\A\\) . In other words, the column rank of \\(\\A\\) corresponds to the largest number vectors that can form a linearly indepedent set in the columns of \\(\\A\\) . This interpretation follows because the definition of dimension of a column space means given a vector space \\(V\\) which is our column space spanned by the columns of \\(\\A\\) , the dimension of the column space is the cardinality of the basis \\(\\B\\) of \\(V\\) ; and note that any basis \\(\\B\\) of \\(V\\) (our column space) is a linearly independent set which also spans \\(V\\) . So both definition is equivalent. Full Column Rank A matrix \\(\\A \\in \\F^{m \\times n}\\) is full column rank if and only if \\(\\rank(A) = n\\) . Algebraic Definition (Row Rank) The row rank of \\(\\A\\) is the dimension of the row space of \\(\\A\\) . Full Row Rank A matrix \\(\\A \\in \\F^{m \\times n}\\) is full column rank if and only if \\(\\rank(A) = m\\) . Definition (Full Rank) We will soon show that the column and row rank of a matrix \\(\\A \\in \\F^{m \\times n}\\) is the same, and for a matrix to be termed full rank , it means that: \\[\\rank(\\A) = \\min{(m, n)}\\] This makes sense because if we know that for a matrix say shape \\((3, 6)\\) to have equal rank on both columns and rows, then the max rank a matrix can have for this example is \\(3\\) . Definition (Reduced Rank) If a matrix \\(\\A \\in \\F^{m \\times n}\\) is not full rank, then we call it reduced rank, or rank deficient. Geometric Definition (Rank) TBD Example (Computing Rank by Inspection) The matrix \\[\\A = \\begin{bmatrix}1&0&1\\\\-2&-3&1\\\\3&3&0\\end{bmatrix}\\] has column rank 2 since the first two columns are linearly independent , so the rank is at least 2, but since the third is a linear combination of the first two (the second subtracted from the first), the three columns are linearly dependent so the column rank is 2. The matrix \\[\\A=\\begin{bmatrix}1&1&0&2\\\\-1&-1&0&-2\\end{bmatrix}\\] has row rank 1 since row 2 is a multiple of row 1, and thus of the 2 rows, the number of linearly independent rows is 1. We will soon see that column and row rank are equivalent. Theorem (Column Rank is Row Rank) The first important theorem states that for any matrix \\(\\A \\in \\F^{m \\times n}\\) , the row rank is equivalent to the column rank . There are many proofs, we will use what is the easiest using Gaussian Elimination. For other proofs, see here 2 . Proof (Column Rank is Row Rank) Recall that row operations preserve both the column and row space of a matrix \\(\\A\\) . Thus row reduction performed on \\(\\A\\) will reveal the number of pivots. First, we assert that row reduction reveals us a basis of the row space of \\(\\A\\) by just taking the non-zero rows. Consequently, the dimension of the row space is also obtained by counting the number of pivots (non-zero rows). We thus get the row rank of the matrix \\(\\A\\) . But recall that we are counting the pivot columns to get the dimension of the row space. One immediately recalls that row reduction also preserves the linear dependency of the column space , in Theorem (Basis For Column Space) , we also deduced that the number of pivots columns corresponds to the dimension of the column space. Consequently, the dimension of the row space and column space is the same, and so is their rank. Computing Rank Naive Count If the matrix \\(\\A \\in \\F^{m \\times n}\\) is small, then eyeballing like how I did in the previous example will work. Pivots in RREF This is mentioned in Theorem (Column Rank is Row Rank) . SVD Later chapter Properties of Rank Assume \\(\\A \\in \\F^{m \\times p}\\) and \\(\\B \\in \\F^{p \\times n}\\) . Rank and Scalar Multiplication Given a matrix \\(\\A\\) , and a scalar \\(\\lambda\\) : \\[\\rank(A) = \\rank(\\lambda\\A) ,\\quad \\lambda \\neq 0\\] Geometrically, this makes sense because scaling all the columns or rows by a scalar does not change the columns or rows linear dependency. Algebraically, it is easy to see that given a set of vectors \\(\\{\\v_1, \\v_2, ..., \\v_m\\}\\) , then \\(\\lambda\\{\\v_1, \\v_2, ..., \\v_m\\}\\) does not change the linear dependency. One should recall \\[\\text{span}(\\{\\v_1, \\v_2, ..., \\v_m\\}) = \\text{span}(\\lambda\\{\\v_1, \\v_2, ..., \\v_m\\})\\] Subadditivity This property states that: \\(\\rank(\\A + \\B) \\leq \\rank(\\A) + \\rank(\\B)\\) . Rank of AB is Minimum of A or B This theorem states that: \\[ \\rank(\\A\\B) \\leq \\min{(\\rank(\\A), \\rank(\\B))} \\] Proof (Rank of AB is Minimum of A or B) Let \\(\\mathbf{C} = \\A\\B\\) to of size \\(m \\times n\\) . Then we note that: \\[ \\mathbf{C} = \\A\\begin{bmatrix}\\b_1 & \\b_2 & \\cdots & \\b_n\\end{bmatrix} \\] which implies \\[ \\mathbf{C}_i = \\A\\b_i \\] indicating that column \\(i\\) of \\(\\mathbf{C}\\) is \\(\\A\\b_i\\) . Further recall in the section on matrix multiplication that \\(\\A\\b_i\\) is the linear combination of columns of \\(\\A\\) with coefficients from \\(\\b_i\\) . Consequently, we can say that each and every column of \\(\\mathbf{C}\\) is a linear combination of columns of \\(\\A\\) , implying that the column space of \\(\\mathbf{C}\\) is a subset of the column space of \\(\\A\\) (i.e. take any column \\(\\mathbf{C}_i\\) of \\(\\mathbf{C}\\) , we show that this column is in the column space of \\(\\A\\) .) Therefore, we conclude \\[\\textbf{col}(\\mathbf{C}) \\subseteq \\textbf{col}({\\A}) \\implies \\rank(\\mathbf{C}) \\leq \\rank(\\A)\\] Without loss of generality, the same can be done for matrix \\(\\B\\) by looking at the row space of \\(\\B\\) and \\(\\mathbf{C}\\) . Equivalence of Ranks on \\(\\mathbf{A}, \\mathbf{A}^\\top, \\mathbf{A}^\\top\\mathbf{A}, \\mathbf{A}\\mathbf{A}^\\top\\) Firstly, \\(\\A\\) and \\(\\A^\\top\\) have the same rank because we can use the theorem which states the column and row rank of any matrix is the same. Rank of Random Matrices This is a refreshing topic from Mike X Cohen's Linear Algebra: Theory, Intuition, Code, 2021. (pp. 193) . In summary, if you fill up a matrix \\(\\A \\in \\F^{m \\times n}\\) with elements randomly drawn from various distributions (i.e Gaussian/Uniform etc), then this random matrix is almost always guaranteed to be full rank . The intuition is simple, consider the real space \\(\\R\\) , we are drawing elements from the continuous real space with decimal points, and assume you are creating a 3 by 2 matrix. The probability for the 3 rows or 2 columns to be linearly independent is close to 0. Python Implementation We first create full ranked matrices. import numpy as np A = np . random . standard_normal ( size = ( 3 , 6 )) # draw from standard normal with mu = 0 and sigma = 1 np . linalg . matrix_rank ( A ) == 3 True Then to create rank deficient matrices of a fixed rank, it is more challenging because we know that populating a matrix with random values from a distribution means the matrix is full rank. However, we can use the theorem that states Rank of AB is Minimum of A or B to do so. # let's say we want to create a 3x6 matrix but we only want its rank to be 2 # we know from theorem that rank(AB) <= min(rank(A), rank(B)) # we thus ensure both A and B has rank 2 (the one we want here) but we must make sure the other shape of the matrix A and B is more than 2, # which is guaranteed in the code below. reduced_rank = 2 reduced_rank_matrix_shape = ( 3 , 6 ) A = np . random . standard_normal ( size = ( reduced_rank_matrix_shape [ 0 ], reduced_rank )) B = np . random . standard_normal ( size = ( reduced_rank , reduced_rank_matrix_shape [ 1 ])) C = A @B # shape (3, 6) np . linalg . matrix_rank ( C ) 2 How to turn Rank-Deficient Matrices to Full Rank? (Machine Learning) In machine learning, we often want to work with full rank matrices. Given a rank-deficient matrix \\(\\A \\in \\F^{m \\times n}\\) , we can shift the matrix by \\(\\lambda\\) to make it full rank. Read more from Mike X Cohen's Linear Algebra: Theory, Intuition, Code, 2021. (pp. 195) . Using Rank to check if a vector in Span Given a set of vectors \\(S = \\{\\v_1, \\v_2, ..., \\v_n\\}\\) , and a vector \\(\\w\\) , deduce if \\(\\w \\in \\textbf{span}(S)\\) . Assume \\(\\v_i \\in \\R^{m}\\) and \\(\\w \\in \\R^{m}\\) . Construct a matrix \\(\\mathbf{S} = \\begin{bmatrix} \\v_1 & \\v_2 & \\cdots & \\v_m \\end{bmatrix}_{m \\times n}\\) where \\(\\mathbf{S} \\in \\R^{m \\times n}\\) . Compute the rank of \\(\\mathbf{S}\\) and call it \\(s_1\\) . Horizontally concatenate \\(\\mathbf{S}\\) and \\(\\w\\) to get \\(\\mathbf{S}_w = \\mathbf{S} \\cup \\w\\) . Compute rank of \\(\\mathbf{S}_w\\) to be \\(s_2\\) . Then: If \\(s_1 = s_2\\) , this means that column space of \\(\\mathbf{S}\\) equals to the column space of \\(\\mathbf{S}_w\\) , and thus \\(\\w\\) did not alter the column space of \\(\\mathbf{S}\\) , which means \\(\\w\\) must be part of the column space of \\(\\mathbf{S}\\) , and thus in the span of \\(S\\) . If \\(s_2 > s_1\\) , then \\(\\w \\not\\in S\\) because if it is, the column space of \\(\\mathbf{S}_w\\) should not change. https://en.wikipedia.org/wiki/Rank_(linear_algebra) \u21a9 https://math.stackexchange.com/questions/332908/looking-for-an-intuitive-explanation-why-the-row-rank-is-equal-to-the-column-ran \u21a9","title":"Matrix Rank"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.07_linear_algebra_matrix_theory_matrix_rank/#matrix-rank","text":"We will understand matrix rank in a naive manner first, we will go through it more formally in the next few sections.","title":"Matrix Rank"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.07_linear_algebra_matrix_theory_matrix_rank/#algebraic-definition-rank1","text":"In linear algebra , the rank of a matrix \\(\\A\\) is the dimension of the vector space generated (or spanned ) by its columns. This corresponds to the maximal number of linearly independent columns of \\(\\A\\) . This, in turn, is identical to the dimension of the vector space spanned by its rows. Rank is thus a measure of the \\\" nondegenerateness \\\" of the system of linear equations and linear transformation encoded by \\(\\A\\) . There are multiple equivalent definitions of rank. A matrix\\'s rank is one of its most fundamental characteristics.","title":"Algebraic Definition (Rank)1"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.07_linear_algebra_matrix_theory_matrix_rank/#notation-rank","text":"The rank of a matrix \\(\\A\\) is commonly denoted by \\(\\rank(A)\\) .","title":"Notation (Rank)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.07_linear_algebra_matrix_theory_matrix_rank/#algebraic-definition-column-rank","text":"The column rank of \\(\\A\\) is the dimension of the column space of \\(\\A\\) . In other words, the column rank of \\(\\A\\) corresponds to the largest number vectors that can form a linearly indepedent set in the columns of \\(\\A\\) . This interpretation follows because the definition of dimension of a column space means given a vector space \\(V\\) which is our column space spanned by the columns of \\(\\A\\) , the dimension of the column space is the cardinality of the basis \\(\\B\\) of \\(V\\) ; and note that any basis \\(\\B\\) of \\(V\\) (our column space) is a linearly independent set which also spans \\(V\\) . So both definition is equivalent.","title":"Algebraic Definition (Column Rank)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.07_linear_algebra_matrix_theory_matrix_rank/#full-column-rank","text":"A matrix \\(\\A \\in \\F^{m \\times n}\\) is full column rank if and only if \\(\\rank(A) = n\\) .","title":"Full Column Rank"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.07_linear_algebra_matrix_theory_matrix_rank/#algebraic-definition-row-rank","text":"The row rank of \\(\\A\\) is the dimension of the row space of \\(\\A\\) .","title":"Algebraic Definition (Row Rank)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.07_linear_algebra_matrix_theory_matrix_rank/#full-row-rank","text":"A matrix \\(\\A \\in \\F^{m \\times n}\\) is full column rank if and only if \\(\\rank(A) = m\\) .","title":"Full Row Rank"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.07_linear_algebra_matrix_theory_matrix_rank/#definition-full-rank","text":"We will soon show that the column and row rank of a matrix \\(\\A \\in \\F^{m \\times n}\\) is the same, and for a matrix to be termed full rank , it means that: \\[\\rank(\\A) = \\min{(m, n)}\\] This makes sense because if we know that for a matrix say shape \\((3, 6)\\) to have equal rank on both columns and rows, then the max rank a matrix can have for this example is \\(3\\) .","title":"Definition (Full Rank)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.07_linear_algebra_matrix_theory_matrix_rank/#definition-reduced-rank","text":"If a matrix \\(\\A \\in \\F^{m \\times n}\\) is not full rank, then we call it reduced rank, or rank deficient.","title":"Definition (Reduced Rank)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.07_linear_algebra_matrix_theory_matrix_rank/#geometric-definition-rank","text":"TBD","title":"Geometric Definition (Rank)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.07_linear_algebra_matrix_theory_matrix_rank/#example-computing-rank-by-inspection","text":"The matrix \\[\\A = \\begin{bmatrix}1&0&1\\\\-2&-3&1\\\\3&3&0\\end{bmatrix}\\] has column rank 2 since the first two columns are linearly independent , so the rank is at least 2, but since the third is a linear combination of the first two (the second subtracted from the first), the three columns are linearly dependent so the column rank is 2. The matrix \\[\\A=\\begin{bmatrix}1&1&0&2\\\\-1&-1&0&-2\\end{bmatrix}\\] has row rank 1 since row 2 is a multiple of row 1, and thus of the 2 rows, the number of linearly independent rows is 1. We will soon see that column and row rank are equivalent.","title":"Example (Computing Rank by Inspection)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.07_linear_algebra_matrix_theory_matrix_rank/#theorem-column-rank-is-row-rank","text":"The first important theorem states that for any matrix \\(\\A \\in \\F^{m \\times n}\\) , the row rank is equivalent to the column rank . There are many proofs, we will use what is the easiest using Gaussian Elimination. For other proofs, see here 2 .","title":"Theorem (Column Rank is Row Rank)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.07_linear_algebra_matrix_theory_matrix_rank/#proof-column-rank-is-row-rank","text":"Recall that row operations preserve both the column and row space of a matrix \\(\\A\\) . Thus row reduction performed on \\(\\A\\) will reveal the number of pivots. First, we assert that row reduction reveals us a basis of the row space of \\(\\A\\) by just taking the non-zero rows. Consequently, the dimension of the row space is also obtained by counting the number of pivots (non-zero rows). We thus get the row rank of the matrix \\(\\A\\) . But recall that we are counting the pivot columns to get the dimension of the row space. One immediately recalls that row reduction also preserves the linear dependency of the column space , in Theorem (Basis For Column Space) , we also deduced that the number of pivots columns corresponds to the dimension of the column space. Consequently, the dimension of the row space and column space is the same, and so is their rank.","title":"Proof (Column Rank is Row Rank)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.07_linear_algebra_matrix_theory_matrix_rank/#computing-rank","text":"","title":"Computing Rank"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.07_linear_algebra_matrix_theory_matrix_rank/#naive-count","text":"If the matrix \\(\\A \\in \\F^{m \\times n}\\) is small, then eyeballing like how I did in the previous example will work.","title":"Naive Count"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.07_linear_algebra_matrix_theory_matrix_rank/#pivots-in-rref","text":"This is mentioned in Theorem (Column Rank is Row Rank) .","title":"Pivots in RREF"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.07_linear_algebra_matrix_theory_matrix_rank/#svd","text":"Later chapter","title":"SVD"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.07_linear_algebra_matrix_theory_matrix_rank/#properties-of-rank","text":"Assume \\(\\A \\in \\F^{m \\times p}\\) and \\(\\B \\in \\F^{p \\times n}\\) .","title":"Properties of Rank"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.07_linear_algebra_matrix_theory_matrix_rank/#rank-and-scalar-multiplication","text":"Given a matrix \\(\\A\\) , and a scalar \\(\\lambda\\) : \\[\\rank(A) = \\rank(\\lambda\\A) ,\\quad \\lambda \\neq 0\\] Geometrically, this makes sense because scaling all the columns or rows by a scalar does not change the columns or rows linear dependency. Algebraically, it is easy to see that given a set of vectors \\(\\{\\v_1, \\v_2, ..., \\v_m\\}\\) , then \\(\\lambda\\{\\v_1, \\v_2, ..., \\v_m\\}\\) does not change the linear dependency. One should recall \\[\\text{span}(\\{\\v_1, \\v_2, ..., \\v_m\\}) = \\text{span}(\\lambda\\{\\v_1, \\v_2, ..., \\v_m\\})\\]","title":"Rank and Scalar Multiplication"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.07_linear_algebra_matrix_theory_matrix_rank/#subadditivity","text":"This property states that: \\(\\rank(\\A + \\B) \\leq \\rank(\\A) + \\rank(\\B)\\) .","title":"Subadditivity"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.07_linear_algebra_matrix_theory_matrix_rank/#rank-of-ab-is-minimum-of-a-or-b","text":"This theorem states that: \\[ \\rank(\\A\\B) \\leq \\min{(\\rank(\\A), \\rank(\\B))} \\]","title":"Rank of AB is Minimum of A or B"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.07_linear_algebra_matrix_theory_matrix_rank/#proof-rank-of-ab-is-minimum-of-a-or-b","text":"Let \\(\\mathbf{C} = \\A\\B\\) to of size \\(m \\times n\\) . Then we note that: \\[ \\mathbf{C} = \\A\\begin{bmatrix}\\b_1 & \\b_2 & \\cdots & \\b_n\\end{bmatrix} \\] which implies \\[ \\mathbf{C}_i = \\A\\b_i \\] indicating that column \\(i\\) of \\(\\mathbf{C}\\) is \\(\\A\\b_i\\) . Further recall in the section on matrix multiplication that \\(\\A\\b_i\\) is the linear combination of columns of \\(\\A\\) with coefficients from \\(\\b_i\\) . Consequently, we can say that each and every column of \\(\\mathbf{C}\\) is a linear combination of columns of \\(\\A\\) , implying that the column space of \\(\\mathbf{C}\\) is a subset of the column space of \\(\\A\\) (i.e. take any column \\(\\mathbf{C}_i\\) of \\(\\mathbf{C}\\) , we show that this column is in the column space of \\(\\A\\) .) Therefore, we conclude \\[\\textbf{col}(\\mathbf{C}) \\subseteq \\textbf{col}({\\A}) \\implies \\rank(\\mathbf{C}) \\leq \\rank(\\A)\\] Without loss of generality, the same can be done for matrix \\(\\B\\) by looking at the row space of \\(\\B\\) and \\(\\mathbf{C}\\) .","title":"Proof (Rank of AB is Minimum of A or B)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.07_linear_algebra_matrix_theory_matrix_rank/#equivalence-of-ranks-on-mathbfa-mathbfatop-mathbfatopmathbfa-mathbfamathbfatop","text":"Firstly, \\(\\A\\) and \\(\\A^\\top\\) have the same rank because we can use the theorem which states the column and row rank of any matrix is the same.","title":"Equivalence of Ranks on \\(\\mathbf{A}, \\mathbf{A}^\\top, \\mathbf{A}^\\top\\mathbf{A}, \\mathbf{A}\\mathbf{A}^\\top\\)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.07_linear_algebra_matrix_theory_matrix_rank/#rank-of-random-matrices","text":"This is a refreshing topic from Mike X Cohen's Linear Algebra: Theory, Intuition, Code, 2021. (pp. 193) . In summary, if you fill up a matrix \\(\\A \\in \\F^{m \\times n}\\) with elements randomly drawn from various distributions (i.e Gaussian/Uniform etc), then this random matrix is almost always guaranteed to be full rank . The intuition is simple, consider the real space \\(\\R\\) , we are drawing elements from the continuous real space with decimal points, and assume you are creating a 3 by 2 matrix. The probability for the 3 rows or 2 columns to be linearly independent is close to 0.","title":"Rank of Random Matrices"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.07_linear_algebra_matrix_theory_matrix_rank/#python-implementation","text":"We first create full ranked matrices. import numpy as np A = np . random . standard_normal ( size = ( 3 , 6 )) # draw from standard normal with mu = 0 and sigma = 1 np . linalg . matrix_rank ( A ) == 3 True Then to create rank deficient matrices of a fixed rank, it is more challenging because we know that populating a matrix with random values from a distribution means the matrix is full rank. However, we can use the theorem that states Rank of AB is Minimum of A or B to do so. # let's say we want to create a 3x6 matrix but we only want its rank to be 2 # we know from theorem that rank(AB) <= min(rank(A), rank(B)) # we thus ensure both A and B has rank 2 (the one we want here) but we must make sure the other shape of the matrix A and B is more than 2, # which is guaranteed in the code below. reduced_rank = 2 reduced_rank_matrix_shape = ( 3 , 6 ) A = np . random . standard_normal ( size = ( reduced_rank_matrix_shape [ 0 ], reduced_rank )) B = np . random . standard_normal ( size = ( reduced_rank , reduced_rank_matrix_shape [ 1 ])) C = A @B # shape (3, 6) np . linalg . matrix_rank ( C ) 2","title":"Python Implementation"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.07_linear_algebra_matrix_theory_matrix_rank/#how-to-turn-rank-deficient-matrices-to-full-rank-machine-learning","text":"In machine learning, we often want to work with full rank matrices. Given a rank-deficient matrix \\(\\A \\in \\F^{m \\times n}\\) , we can shift the matrix by \\(\\lambda\\) to make it full rank. Read more from Mike X Cohen's Linear Algebra: Theory, Intuition, Code, 2021. (pp. 195) .","title":"How to turn Rank-Deficient Matrices to Full Rank? (Machine Learning)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.07_linear_algebra_matrix_theory_matrix_rank/#using-rank-to-check-if-a-vector-in-span","text":"Given a set of vectors \\(S = \\{\\v_1, \\v_2, ..., \\v_n\\}\\) , and a vector \\(\\w\\) , deduce if \\(\\w \\in \\textbf{span}(S)\\) . Assume \\(\\v_i \\in \\R^{m}\\) and \\(\\w \\in \\R^{m}\\) . Construct a matrix \\(\\mathbf{S} = \\begin{bmatrix} \\v_1 & \\v_2 & \\cdots & \\v_m \\end{bmatrix}_{m \\times n}\\) where \\(\\mathbf{S} \\in \\R^{m \\times n}\\) . Compute the rank of \\(\\mathbf{S}\\) and call it \\(s_1\\) . Horizontally concatenate \\(\\mathbf{S}\\) and \\(\\w\\) to get \\(\\mathbf{S}_w = \\mathbf{S} \\cup \\w\\) . Compute rank of \\(\\mathbf{S}_w\\) to be \\(s_2\\) . Then: If \\(s_1 = s_2\\) , this means that column space of \\(\\mathbf{S}\\) equals to the column space of \\(\\mathbf{S}_w\\) , and thus \\(\\w\\) did not alter the column space of \\(\\mathbf{S}\\) , which means \\(\\w\\) must be part of the column space of \\(\\mathbf{S}\\) , and thus in the span of \\(S\\) . If \\(s_2 > s_1\\) , then \\(\\w \\not\\in S\\) because if it is, the column space of \\(\\mathbf{S}_w\\) should not change. https://en.wikipedia.org/wiki/Rank_(linear_algebra) \u21a9 https://math.stackexchange.com/questions/332908/looking-for-an-intuitive-explanation-why-the-row-rank-is-equal-to-the-column-ran \u21a9","title":"Using Rank to check if a vector in Span"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.08_linear_algebra_matrix_theory_matrix_spaces_summary/","text":"\\[\\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\z}{\\mathbf{z}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\rank}{\\textbf{rank}} \\newcommand{\\nullity}{\\textbf{nullity}} \\newcommand{\\C}{\\mathbf{C}}\\] The Four Fundamental Subspaces Summary We highlight some important theorems here. This is a summary of what we should know. Theorem (Row-Reduced Matrix has the same 4 Subspace Dimension as the Original Matrix) This at its core is the foundation of many matrix theories. The beauty is that given any matrix \\(\\A\\) , then its RREF matrix \\(\\mathbf{R}\\) 's 4 fundamental subspaces has the same dimension. Theorem (Row space of \\(\\mathbf{A}\\) is the Column space of \\(\\mathbf{A}^\\top\\) ) Given a matrix \\(\\A \\in \\F^{m \\times n}\\) , the rows of \\(\\A\\) is the columns of \\(\\A^\\top\\) , and hence the theorem follows. Theorem (Row rank of \\(\\mathbf{A}\\) is Column rank of \\(\\mathbf{A}\\) ) The row rank of a matrix \\(\\A\\) is equals to the column rank of \\(\\A\\) . pp 131; of A modern introduction to linear algebra henry ricardo Corollary (Transpose has the same Rank) Let \\(\\A \\in \\F^{m \\times n}\\) , then \\(\\rank(\\A) = \\rank(\\B)\\) . Theorem (Rank-Nullity Theorem) Given any matrix \\(\\A \\in \\F^{m \\times n}\\) , the Rank-Nullity Theorem states: \\[ \\rank(\\A) + \\nullity(\\A) = n \\] pp 133; of A modern introduction to linear algebra henry ricardo Dimensions of Four Subspaces Given a matrix \\(\\A \\in \\F^{m \\times n}\\) , then assume the rank of the matrix is \\(r\\) : \\(\\dim(C(\\A)) = \\dim(R(\\A)) = r\\) \\(\\dim(C(\\A^\\top)) = r = \\dim(R(\\A^\\top))\\) because the dimension of row and column space of \\(\\A\\) is equal, and so is their transpose! \\(\\dim(N(\\A)) = n - r\\) by Rank-Nullity Theorem . \\(\\dim(N(\\A^\\top)) = m - r\\) because rank of \\(\\A^\\top = r\\) and by Rank-Nullity Theorem this holds. Table of Contents Firstly, recall the solution set of a homogeneous system of linear equations remains unchanged after row operations (i.e. Gaussian-Jordan). Secondly, define rank of a matrix to be the number of pivots after RREF . So \\(\\A\\x = \\0\\) becomes \\(\\U\\x = \\0\\) . Rank of a matrix is the number of pivot columns, and therefore is the dimension of the column space","title":"The Four Fundamental Subspaces (Summary)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.08_linear_algebra_matrix_theory_matrix_spaces_summary/#the-four-fundamental-subspaces-summary","text":"We highlight some important theorems here. This is a summary of what we should know.","title":"The Four Fundamental Subspaces Summary"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.08_linear_algebra_matrix_theory_matrix_spaces_summary/#theorem-row-reduced-matrix-has-the-same-4-subspace-dimension-as-the-original-matrix","text":"This at its core is the foundation of many matrix theories. The beauty is that given any matrix \\(\\A\\) , then its RREF matrix \\(\\mathbf{R}\\) 's 4 fundamental subspaces has the same dimension.","title":"Theorem (Row-Reduced Matrix has the same 4 Subspace Dimension as the Original Matrix)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.08_linear_algebra_matrix_theory_matrix_spaces_summary/#theorem-row-space-of-mathbfa-is-the-column-space-of-mathbfatop","text":"Given a matrix \\(\\A \\in \\F^{m \\times n}\\) , the rows of \\(\\A\\) is the columns of \\(\\A^\\top\\) , and hence the theorem follows.","title":"Theorem (Row space of \\(\\mathbf{A}\\) is the Column space of \\(\\mathbf{A}^\\top\\))"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.08_linear_algebra_matrix_theory_matrix_spaces_summary/#theorem-row-rank-of-mathbfa-is-column-rank-of-mathbfa","text":"The row rank of a matrix \\(\\A\\) is equals to the column rank of \\(\\A\\) . pp 131; of A modern introduction to linear algebra henry ricardo","title":"Theorem (Row rank of \\(\\mathbf{A}\\) is Column rank of \\(\\mathbf{A}\\))"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.08_linear_algebra_matrix_theory_matrix_spaces_summary/#corollary-transpose-has-the-same-rank","text":"Let \\(\\A \\in \\F^{m \\times n}\\) , then \\(\\rank(\\A) = \\rank(\\B)\\) .","title":"Corollary (Transpose has the same Rank)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.08_linear_algebra_matrix_theory_matrix_spaces_summary/#theorem-rank-nullity-theorem","text":"Given any matrix \\(\\A \\in \\F^{m \\times n}\\) , the Rank-Nullity Theorem states: \\[ \\rank(\\A) + \\nullity(\\A) = n \\] pp 133; of A modern introduction to linear algebra henry ricardo","title":"Theorem (Rank-Nullity Theorem)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.08_linear_algebra_matrix_theory_matrix_spaces_summary/#dimensions-of-four-subspaces","text":"Given a matrix \\(\\A \\in \\F^{m \\times n}\\) , then assume the rank of the matrix is \\(r\\) : \\(\\dim(C(\\A)) = \\dim(R(\\A)) = r\\) \\(\\dim(C(\\A^\\top)) = r = \\dim(R(\\A^\\top))\\) because the dimension of row and column space of \\(\\A\\) is equal, and so is their transpose! \\(\\dim(N(\\A)) = n - r\\) by Rank-Nullity Theorem . \\(\\dim(N(\\A^\\top)) = m - r\\) because rank of \\(\\A^\\top = r\\) and by Rank-Nullity Theorem this holds.","title":"Dimensions of Four Subspaces"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.08_linear_algebra_matrix_theory_matrix_spaces_summary/#table-of-contents","text":"Firstly, recall the solution set of a homogeneous system of linear equations remains unchanged after row operations (i.e. Gaussian-Jordan). Secondly, define rank of a matrix to be the number of pivots after RREF . So \\(\\A\\x = \\0\\) becomes \\(\\U\\x = \\0\\) . Rank of a matrix is the number of pivot columns, and therefore is the dimension of the column space","title":"Table of Contents"},{"location":"reighns_ml_journey/mathematics/linear_algebra/05_system_of_linear_equations_and_matrix_theory/05.09_linear_algebra_systems_of_linear_equations_revisited/","text":"\\[\\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\z}{\\mathbf{z}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\rank}{\\textbf{rank}} \\newcommand{\\nullity}{\\textbf{nullity}} \\newcommand{\\C}{\\mathbf{C}}\\] Read pp.137 onwards of Henry's book.","title":"05.09 linear algebra systems of linear equations revisited"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.01_linear_algebra_linear_transformations/","text":"\\[ \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\e}{\\mathbf{e}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\z}{\\mathbf{z}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\C}{\\mathbf{C}} \\newcommand{\\rank}{\\textbf{rank}} \\newcommand{\\null}{\\textbf{null}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\U}{\\mathrm{U}} \\newcommand{\\V}{\\mathrm{V}} \\newcommand{\\W}{\\mathrm{W}} \\newcommand{\\L}{\\mathcal{L}} \\] Disclaimer This chapter/section borrows and reference heavily from the book Sheldon Axler: Linear Algebra Done Right, 2015 . What is Completely Determined? In the text below, you will see the phrase completely determined . This can be confusing. A vector subspace \\(\\V\\) is completely determined by the set of basis vectors . This example illustrates that, if were to know the basis vectors \\(\\v_1, ..., \\v_n\\) , we can construct the entire vector subspace \\(\\V\\) . Linear Transformation Definition (Linear Transformation/Map) Let \\(\\V\\) and \\(\\W\\) be two vector spaces over the same field \\(\\F\\) . The mapping (function) \\(T: \\V \\to \\W\\) is called a linear transformation from vector spaces \\(\\V\\) to \\(\\W\\) with the following properties: additivity: \\(T(\\u + \\v) = T(\\u) + T(\\v)\\) for all \\(\\u, \\v \\in \\V\\) . homogeneity: \\(T\\left(\\lambda(\\v)\\right) = \\lambda(T(\\v))\\) for all \\(\\lambda \\in \\F\\) and all \\(\\v \\in \\V\\) . Notation (Linear Transformation/Map) The set of all linear maps from \\(\\V\\) to \\(\\W\\) is denoted \\(\\L(\\V, \\W)\\) . To be more explicit, this set is every possible linear transformation from the vector space \\(\\V\\) to \\(\\W\\) over \\(\\F\\) . Example (Linear Transformation/Map) The Zero Mapping Let \\(\\V\\) and \\(\\W\\) be two vector subspaces in \\(\\F\\) . Then the mapping function \\(0\\) maps any element \\(\\v \\in \\V\\) to the zero vector \\(\\0_W \\in \\W\\) where \\(\\0_{\\W}\\) is the zero (additive) element in the subspace \\(\\W\\) . \\[ \\begin{eqnarray} 0: \\V & \\rightarrow & \\W \\nonumber \\\\ \\v & \\mapsto & \\0_{\\W} \\nonumber \\end{eqnarray} \\] This is a linear transformation and called the zero linear transformation from \\(\\V\\) to \\(\\W\\) . The Identity Mapping Let \\(\\V\\) be a vector subspace in \\(\\F\\) . Then the mapping function \\(I\\) maps any element \\(\\v \\in \\V\\) to the itself \\(\\v \\in \\V\\) . \\[ \\begin{eqnarray} I: \\V & \\rightarrow & \\V \\nonumber \\\\ \\v & \\mapsto & \\v \\nonumber \\end{eqnarray} \\] This is a linear transformation and called the identity linear transformation from \\(\\V\\) to \\(\\V\\) . For more examples, read here 1 . Non-Linear Mapping The mapping \\[ \\begin{eqnarray} T : \\F^{3} & \\rightarrow & \\F^{3} \\nonumber \\\\ (x, y, z) & \\mapsto & (0, x+y+z, 1) \\nonumber \\end{eqnarray} \\] is not a linear transformations assuming \\(|\\F|\\ge 3\\) . This is because note that \\(\\0\\) vector must map to \\(\\0\\) vector, if not, it is not a linear transformation because it does not fulfill homogeneity (consider \\(\\v = \\0\\) and \\(\\lambda = 2\\) , then \\(T(2(\\0)) \\neq 2(T(\\0)\\) . Theorem (Linear Transformation Maps 0 to 0) Suppose \\(T\\) is a linear transformation from \\(\\V\\) to \\(\\W\\) , then: \\[ T(\\0_\\V) = \\0_\\W \\] This is useful for checking if a linear transformation is valid because the contrapositive says: If \\(\\0_\\V\\) does not map to \\(\\0_\\W\\) , then \\(T\\) is not a linear transformation . Theorem (Equivalent Linear Transformation Definition) Let \\[T: \\V \\rightarrow \\W\\] be a map between two vector spaces over the same field \\(\\F\\) . Then the following are equivalent. \\(T\\) is a linear transformation. For any \\(\\v_1, \\v_2 \\in \\V\\) and \\(\\lambda_1, \\lambda_2 \\in \\F\\) , we have the following: \\( \\(T(\\lambda_1\\v_1 + \\lambda_2\\v_2) = \\lambda_1 T(\\v_1) + \\lambda_2 T(\\v_2)\\) \\) Theorem (Linear Transformation/Maps are entirely determined by the basis vectors) An extremely important theorem! The existence part of the next result means that we can find a linear map that takes on whatever values we wish on the vectors in a basis. The uniqueness part of the next result means that a linear map is completely determined by its values on a basis. - Sheldon Axler: Linear Algebra Done Right, 2015. pp. 54 Let \\(\\V\\) and \\(\\W\\) be vector spaces over \\(\\F\\) . Suppose \\(\\v_1, ..., \\v_n\\) is a basis of \\(\\V\\) and \\(\\w_1, ..., \\w_n\\) be a set of arbitrary vectors in \\(\\W\\) . Then there exists an unique linear map \\(T: \\V \\to \\W\\) such that \\[ T(\\v_j) = \\w_j , \\quad \\forall j = 1, ..., n \\] This implies that if we know the basis vectors of \\(\\V\\) and Proof (Linear Transformation/Maps are entirely determined by the basis vectors) This mathematical proof is non-trivial and requires two components: We need to prove the existence of \\(T\\) , that such a linear transformation \\(T\\) exists (i.e. fulfilling the properties in the definition). We need to show uniqueness of such \\(T\\) , that is equivalent to asking: given another mapping \\(T^{'}: \\V \\to \\W\\) and assume that \\(T(\\v) = T^{'}(\\v) , \\forall i\\) , then \\(T = T^{'}\\) . Neither parts of the proof are trivial, and can be referenced here 2 . I will be more verbose in this proof to be clear to myself. Existence To show existence , we need to show that given the basis vectors \\(\\v_1, ..., \\v_n\\) of \\(\\V\\) and any set of arbitrary vectors \\(\\w_1, ..., \\w_n\\) of \\(\\W\\) , the mapping \\(T\\) indeed maps \\(\\v_j\\) to \\(\\w_j\\) where \\(j = 1, ..., n\\) , specifically, the basis vectors of \\(\\V\\) all have a mapping to the corresponding vectors in \\(\\W\\) . The implicit assumption in this part is that the map \\(T\\) is also well-defined , which we will outline below. Define and construct \\(T: \\V \\to \\W\\) where we want to map any element \\(\\v \\in \\V\\) to some \\(\\w \\in \\W\\) , define \\(\\v = c_1 \\v_1 + ... + c_n \\v_n\\) and the corresponding \\(\\w = c_1 \\w_1 + ... + c_n \\w_n\\) . \\[ T(c_1 \\v_1 + ... + c_n \\v_n) = c_1 \\w_1 + ... + c_n \\w_n \\] At this stage we are not assuming anything about \\(T\\) 's linearity, we are just constructing a linear equation in hope that this coincides with what we want (i.e. showing the existence of such \\(T\\) ). Note also \\(c\\) is arbitrary in \\(\\F\\) . The domain \\(\\V\\) is well defined because for any \\(\\v \\in \\V\\) , \\(\\v\\) can be written as \\(\\v = c_1 \\v_1 + ... + c_n \\v_n\\) and has an unique representation. Consequently, the mapping is well defined. Next, we just need to show that the additivity and homogeneity properties to complete the proof on existence. Uniqueness We can show uniqueness by: given another mapping \\(T^{'}: \\V \\to \\W\\) and assume that \\(T(\\v) = T^{'}(\\v) , \\forall i\\) , then \\(T = T^{'}\\) . Example (Linear Transformation/Maps are entirely determined by the basis vectors) https://math.stackexchange.com/questions/4114034/is-a-linear-map-uniquely-determined-on-v-or-on-a-basis-of-v Suppose I have a transformation \\(T: \\mathbb{R}^2 \\to \\mathbb{R}\\) . Pick a basis \\(B=\\{\\e_1=(1,0), \\e_2=(0,1) \\}\\) . Then \\(T\\) is completely specified by the values \\(\\w_1=T(\\e_1),\\w_2=T(\\e_2)\\) . To elaborate from the post, the OP mentioned that now he understood that once \\(T\\) is determined by \\(\\w_j\\) on \\(\\v_j\\) , then this \\(T\\) is unique on \\(\\V\\) . My understanding of determined is that we need to know both \\(\\v_1, \\v_2\\) (the basis vectors of \\(\\V = \\mathbb{R}^2\\) ) and the output \\(\\w_1 = T(\\e_1), \\w_2 = T(\\e_2)\\) . Consequently, we need to pick any vectors \\(\\w_1, \\w_2 \\in \\W\\) , say \\(\\w_1 = (1, 1), \\w_2 = (2, 0)\\) then only can we say that this particular mapping \\(T\\) is uniquely and entirely determined by \\(\\w_j\\) on \\(\\v_j\\) ? Algebraic Operations on Linear Transformation Definition (Addition and Scalar-Multiplication) Suppose \\(S, T \\in \\L(\\V, \\W)\\) and \\(\\lambda \\in \\F\\) . We define the addition \\(S + T\\) as: \\( \\((S+T)(\\v) = S(\\v) + T(\\v)\\) \\) And define the scalar-multiplication \\(\\lambda T\\) as: \\( \\(\\lambda T(\\v) = T(\\lambda\\v)\\) \\) for all \\(\\v \\in \\V\\) . In particular, both \\(S+T\\) and \\(\\lambda T\\) are in itself linear transformations . Corollary (The set of Linear Transformations is a Vector Space) \\(\\L(\\V, \\W)\\) is a vector space . Definition (Composite of Linear Transformations) Let \\(S\\) and \\(T\\) be linear transformations in \\(\\L(\\V, \\U)\\) and \\(\\L(\\U, \\W)\\) respectively: \\[ S: \\V \\to \\U \\] \\[ T: \\U \\to \\W \\] then the composite map is also a linear transformation given by: \\[ S \\circ T: \\V \\to \\W \\] where \\((S \\circ T)(\\v) = S \\circ (T(\\v)\\) . Properties of Linear Transformations Associativity Identity Distributive Properties Non-Commutative Sheldon Axler: Linear Algebra Done Right, 2015. pp. 52-53 \u21a9 Sheldon Axler: Linear Algebra Done Right, 2015. pp. 54 \u21a9","title":"Linear Transformation"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.01_linear_algebra_linear_transformations/#disclaimer","text":"This chapter/section borrows and reference heavily from the book Sheldon Axler: Linear Algebra Done Right, 2015 .","title":"Disclaimer"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.01_linear_algebra_linear_transformations/#what-is-completely-determined","text":"In the text below, you will see the phrase completely determined . This can be confusing. A vector subspace \\(\\V\\) is completely determined by the set of basis vectors . This example illustrates that, if were to know the basis vectors \\(\\v_1, ..., \\v_n\\) , we can construct the entire vector subspace \\(\\V\\) .","title":"What is Completely Determined?"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.01_linear_algebra_linear_transformations/#linear-transformation","text":"","title":"Linear Transformation"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.01_linear_algebra_linear_transformations/#definition-linear-transformationmap","text":"Let \\(\\V\\) and \\(\\W\\) be two vector spaces over the same field \\(\\F\\) . The mapping (function) \\(T: \\V \\to \\W\\) is called a linear transformation from vector spaces \\(\\V\\) to \\(\\W\\) with the following properties: additivity: \\(T(\\u + \\v) = T(\\u) + T(\\v)\\) for all \\(\\u, \\v \\in \\V\\) . homogeneity: \\(T\\left(\\lambda(\\v)\\right) = \\lambda(T(\\v))\\) for all \\(\\lambda \\in \\F\\) and all \\(\\v \\in \\V\\) .","title":"Definition (Linear Transformation/Map)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.01_linear_algebra_linear_transformations/#notation-linear-transformationmap","text":"The set of all linear maps from \\(\\V\\) to \\(\\W\\) is denoted \\(\\L(\\V, \\W)\\) . To be more explicit, this set is every possible linear transformation from the vector space \\(\\V\\) to \\(\\W\\) over \\(\\F\\) .","title":"Notation (Linear Transformation/Map)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.01_linear_algebra_linear_transformations/#example-linear-transformationmap","text":"","title":"Example (Linear Transformation/Map)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.01_linear_algebra_linear_transformations/#the-zero-mapping","text":"Let \\(\\V\\) and \\(\\W\\) be two vector subspaces in \\(\\F\\) . Then the mapping function \\(0\\) maps any element \\(\\v \\in \\V\\) to the zero vector \\(\\0_W \\in \\W\\) where \\(\\0_{\\W}\\) is the zero (additive) element in the subspace \\(\\W\\) . \\[ \\begin{eqnarray} 0: \\V & \\rightarrow & \\W \\nonumber \\\\ \\v & \\mapsto & \\0_{\\W} \\nonumber \\end{eqnarray} \\] This is a linear transformation and called the zero linear transformation from \\(\\V\\) to \\(\\W\\) .","title":"The Zero Mapping"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.01_linear_algebra_linear_transformations/#the-identity-mapping","text":"Let \\(\\V\\) be a vector subspace in \\(\\F\\) . Then the mapping function \\(I\\) maps any element \\(\\v \\in \\V\\) to the itself \\(\\v \\in \\V\\) . \\[ \\begin{eqnarray} I: \\V & \\rightarrow & \\V \\nonumber \\\\ \\v & \\mapsto & \\v \\nonumber \\end{eqnarray} \\] This is a linear transformation and called the identity linear transformation from \\(\\V\\) to \\(\\V\\) . For more examples, read here 1 .","title":"The Identity Mapping"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.01_linear_algebra_linear_transformations/#non-linear-mapping","text":"The mapping \\[ \\begin{eqnarray} T : \\F^{3} & \\rightarrow & \\F^{3} \\nonumber \\\\ (x, y, z) & \\mapsto & (0, x+y+z, 1) \\nonumber \\end{eqnarray} \\] is not a linear transformations assuming \\(|\\F|\\ge 3\\) . This is because note that \\(\\0\\) vector must map to \\(\\0\\) vector, if not, it is not a linear transformation because it does not fulfill homogeneity (consider \\(\\v = \\0\\) and \\(\\lambda = 2\\) , then \\(T(2(\\0)) \\neq 2(T(\\0)\\) .","title":"Non-Linear Mapping"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.01_linear_algebra_linear_transformations/#theorem-linear-transformation-maps-0-to-0","text":"Suppose \\(T\\) is a linear transformation from \\(\\V\\) to \\(\\W\\) , then: \\[ T(\\0_\\V) = \\0_\\W \\] This is useful for checking if a linear transformation is valid because the contrapositive says: If \\(\\0_\\V\\) does not map to \\(\\0_\\W\\) , then \\(T\\) is not a linear transformation .","title":"Theorem (Linear Transformation Maps 0 to 0)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.01_linear_algebra_linear_transformations/#theorem-equivalent-linear-transformation-definition","text":"Let \\[T: \\V \\rightarrow \\W\\] be a map between two vector spaces over the same field \\(\\F\\) . Then the following are equivalent. \\(T\\) is a linear transformation. For any \\(\\v_1, \\v_2 \\in \\V\\) and \\(\\lambda_1, \\lambda_2 \\in \\F\\) , we have the following: \\( \\(T(\\lambda_1\\v_1 + \\lambda_2\\v_2) = \\lambda_1 T(\\v_1) + \\lambda_2 T(\\v_2)\\) \\)","title":"Theorem (Equivalent Linear Transformation Definition)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.01_linear_algebra_linear_transformations/#theorem-linear-transformationmaps-are-entirely-determined-by-the-basis-vectors","text":"An extremely important theorem! The existence part of the next result means that we can find a linear map that takes on whatever values we wish on the vectors in a basis. The uniqueness part of the next result means that a linear map is completely determined by its values on a basis. - Sheldon Axler: Linear Algebra Done Right, 2015. pp. 54 Let \\(\\V\\) and \\(\\W\\) be vector spaces over \\(\\F\\) . Suppose \\(\\v_1, ..., \\v_n\\) is a basis of \\(\\V\\) and \\(\\w_1, ..., \\w_n\\) be a set of arbitrary vectors in \\(\\W\\) . Then there exists an unique linear map \\(T: \\V \\to \\W\\) such that \\[ T(\\v_j) = \\w_j , \\quad \\forall j = 1, ..., n \\] This implies that if we know the basis vectors of \\(\\V\\) and","title":"Theorem (Linear Transformation/Maps are entirely determined by the basis vectors)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.01_linear_algebra_linear_transformations/#proof-linear-transformationmaps-are-entirely-determined-by-the-basis-vectors","text":"This mathematical proof is non-trivial and requires two components: We need to prove the existence of \\(T\\) , that such a linear transformation \\(T\\) exists (i.e. fulfilling the properties in the definition). We need to show uniqueness of such \\(T\\) , that is equivalent to asking: given another mapping \\(T^{'}: \\V \\to \\W\\) and assume that \\(T(\\v) = T^{'}(\\v) , \\forall i\\) , then \\(T = T^{'}\\) . Neither parts of the proof are trivial, and can be referenced here 2 . I will be more verbose in this proof to be clear to myself. Existence To show existence , we need to show that given the basis vectors \\(\\v_1, ..., \\v_n\\) of \\(\\V\\) and any set of arbitrary vectors \\(\\w_1, ..., \\w_n\\) of \\(\\W\\) , the mapping \\(T\\) indeed maps \\(\\v_j\\) to \\(\\w_j\\) where \\(j = 1, ..., n\\) , specifically, the basis vectors of \\(\\V\\) all have a mapping to the corresponding vectors in \\(\\W\\) . The implicit assumption in this part is that the map \\(T\\) is also well-defined , which we will outline below. Define and construct \\(T: \\V \\to \\W\\) where we want to map any element \\(\\v \\in \\V\\) to some \\(\\w \\in \\W\\) , define \\(\\v = c_1 \\v_1 + ... + c_n \\v_n\\) and the corresponding \\(\\w = c_1 \\w_1 + ... + c_n \\w_n\\) . \\[ T(c_1 \\v_1 + ... + c_n \\v_n) = c_1 \\w_1 + ... + c_n \\w_n \\] At this stage we are not assuming anything about \\(T\\) 's linearity, we are just constructing a linear equation in hope that this coincides with what we want (i.e. showing the existence of such \\(T\\) ). Note also \\(c\\) is arbitrary in \\(\\F\\) . The domain \\(\\V\\) is well defined because for any \\(\\v \\in \\V\\) , \\(\\v\\) can be written as \\(\\v = c_1 \\v_1 + ... + c_n \\v_n\\) and has an unique representation. Consequently, the mapping is well defined. Next, we just need to show that the additivity and homogeneity properties to complete the proof on existence. Uniqueness We can show uniqueness by: given another mapping \\(T^{'}: \\V \\to \\W\\) and assume that \\(T(\\v) = T^{'}(\\v) , \\forall i\\) , then \\(T = T^{'}\\) .","title":"Proof (Linear Transformation/Maps are entirely determined by the basis vectors)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.01_linear_algebra_linear_transformations/#example-linear-transformationmaps-are-entirely-determined-by-the-basis-vectors","text":"https://math.stackexchange.com/questions/4114034/is-a-linear-map-uniquely-determined-on-v-or-on-a-basis-of-v Suppose I have a transformation \\(T: \\mathbb{R}^2 \\to \\mathbb{R}\\) . Pick a basis \\(B=\\{\\e_1=(1,0), \\e_2=(0,1) \\}\\) . Then \\(T\\) is completely specified by the values \\(\\w_1=T(\\e_1),\\w_2=T(\\e_2)\\) . To elaborate from the post, the OP mentioned that now he understood that once \\(T\\) is determined by \\(\\w_j\\) on \\(\\v_j\\) , then this \\(T\\) is unique on \\(\\V\\) . My understanding of determined is that we need to know both \\(\\v_1, \\v_2\\) (the basis vectors of \\(\\V = \\mathbb{R}^2\\) ) and the output \\(\\w_1 = T(\\e_1), \\w_2 = T(\\e_2)\\) . Consequently, we need to pick any vectors \\(\\w_1, \\w_2 \\in \\W\\) , say \\(\\w_1 = (1, 1), \\w_2 = (2, 0)\\) then only can we say that this particular mapping \\(T\\) is uniquely and entirely determined by \\(\\w_j\\) on \\(\\v_j\\) ?","title":"Example (Linear Transformation/Maps are entirely determined by the basis vectors)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.01_linear_algebra_linear_transformations/#algebraic-operations-on-linear-transformation","text":"","title":"Algebraic Operations on Linear Transformation"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.01_linear_algebra_linear_transformations/#definition-addition-and-scalar-multiplication","text":"Suppose \\(S, T \\in \\L(\\V, \\W)\\) and \\(\\lambda \\in \\F\\) . We define the addition \\(S + T\\) as: \\( \\((S+T)(\\v) = S(\\v) + T(\\v)\\) \\) And define the scalar-multiplication \\(\\lambda T\\) as: \\( \\(\\lambda T(\\v) = T(\\lambda\\v)\\) \\) for all \\(\\v \\in \\V\\) . In particular, both \\(S+T\\) and \\(\\lambda T\\) are in itself linear transformations .","title":"Definition (Addition and Scalar-Multiplication)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.01_linear_algebra_linear_transformations/#corollary-the-set-of-linear-transformations-is-a-vector-space","text":"\\(\\L(\\V, \\W)\\) is a vector space .","title":"Corollary (The set of Linear Transformations is a Vector Space)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.01_linear_algebra_linear_transformations/#definition-composite-of-linear-transformations","text":"Let \\(S\\) and \\(T\\) be linear transformations in \\(\\L(\\V, \\U)\\) and \\(\\L(\\U, \\W)\\) respectively: \\[ S: \\V \\to \\U \\] \\[ T: \\U \\to \\W \\] then the composite map is also a linear transformation given by: \\[ S \\circ T: \\V \\to \\W \\] where \\((S \\circ T)(\\v) = S \\circ (T(\\v)\\) .","title":"Definition (Composite of Linear Transformations)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.01_linear_algebra_linear_transformations/#properties-of-linear-transformations","text":"Associativity Identity Distributive Properties Non-Commutative Sheldon Axler: Linear Algebra Done Right, 2015. pp. 52-53 \u21a9 Sheldon Axler: Linear Algebra Done Right, 2015. pp. 54 \u21a9","title":"Properties of Linear Transformations"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.02_linear_algebra_linear_transformations_nullspace/","text":"\\[ \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\e}{\\mathbf{e}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\z}{\\mathbf{z}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\C}{\\mathbf{C}} \\newcommand{\\rank}{\\textbf{rank}} \\newcommand{\\null}{\\textbf{null}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\U}{\\mathrm{U}} \\newcommand{\\V}{\\mathrm{V}} \\newcommand{\\W}{\\mathrm{W}} \\newcommand{\\L}{\\mathcal{L}} \\] Null Space Definition (Nullspace of \\(T\\) ) For any \\(T: \\V \\to \\W \\in L(\\V, \\W)\\) , the nullspace of \\(T\\) denoted as \\(\\null(T)\\) is the subset of vectors in \\(\\V\\) that are mapped to the zero vector \\(\\0_\\W\\) by \\(T\\) . \\[ \\null(T) = \\{\\v \\in \\V ~|~ T(\\v) = \\0_\\W\\} \\] Definition (Nullspace of \\(T\\) is nullspace of \\(\\A\\) ) As per title. Example (Nullspace of Differentiation) Let \\(T \\in \\L(\\F[\\z], \\F[\\z])\\) be the differentiation map \\(T(f(\\z)) = f^{'}(\\z)\\) . Then the nullspace of \\(T\\) is: \\[ \\null(T) = \\{f \\in \\F[\\z] ~|~ f(\\z) \\text{ is constant}\\} \\] which is apparent because the only way a function's derivative is zero is if the function is a constant. Theorem (Nullspace is a Subspace) Given any \\(T: \\V \\to \\W\\) in \\(\\L(\\V, \\W)\\) , the nullspace of \\(T\\) is a subspace of \\(\\V\\) . We have already seen that in matrix theory. Definition (Injective) Many of us are familiar with what is an injective map , for those who aren't, here is the definition: A function or mapping \\(f: X \\to Y\\) is injective if and only if \\(T(x) = T(y) \\implies x = y\\) . Note the contrapositive applies here as well: If \\(x \\neq y \\implies T(x) \\neq T(y)\\) then \\(f\\) is not njective . Theorem (Nullspace and Injectivity) Let \\(T \\in \\L(\\V, \\W)\\) , then \\(T\\) is injective if and only if the \\(\\null(T) = \\{\\0\\}\\) . In matrix terminology: Let \\(T_{\\A} \\in \\L(\\V, \\W)\\) , then \\(T_{\\A}\\) is injective if and only if the \\(\\null(T_{\\A}) = \\{\\0\\}\\) if and only if the nullity of \\(\\A\\) is zero (i.e. dimension of the nullspace of the matrix \\(\\A\\) is zero). Intuition (Nullspace and Injectivity) Proving this theorem alone shows you understand the algebraic logic, but we can go one step further and try to get more intuition of what this theorem means. Well, for one, we know that \\(\\null(T) = \\0\\) means that the matrix \\(\\A\\) associated with this \\(T\\) is full-rank, that is to say, \\(\\A\\x = \\0\\) has only the trivial solution. We can immediately have a corollary. Corollary (Nullspace and Injectivity implies Full-Rank) If \\(T\\) is an injective map, then the matrix associated with \\(T\\) is full-rank. Example (Injective Maps) Let \\(T: \\F[\\z] \\to \\F[\\z]\\) be the differentiation map. Note \\(T(f(\\z)) = f^{'}(\\z)\\) . The differentiation map \\(f(\\z) \\mapsto f^{'}(\\z)\\) is not injective because if you take \\(T(f(\\z)) = T(g(\\z))\\) , we must recover \\(f(\\z) = g(\\z)\\) , but in fact this is not true. Set \\(g(\\z) = f(\\z) + c\\) where \\(c \\neq 0 \\in \\F\\) is a constant, then \\(f(\\z) \\neq g(\\z)\\) by construction and hence not injective.","title":"Nullspace"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.02_linear_algebra_linear_transformations_nullspace/#null-space","text":"","title":"Null Space"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.02_linear_algebra_linear_transformations_nullspace/#definition-nullspace-of-t","text":"For any \\(T: \\V \\to \\W \\in L(\\V, \\W)\\) , the nullspace of \\(T\\) denoted as \\(\\null(T)\\) is the subset of vectors in \\(\\V\\) that are mapped to the zero vector \\(\\0_\\W\\) by \\(T\\) . \\[ \\null(T) = \\{\\v \\in \\V ~|~ T(\\v) = \\0_\\W\\} \\]","title":"Definition (Nullspace of \\(T\\))"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.02_linear_algebra_linear_transformations_nullspace/#definition-nullspace-of-t-is-nullspace-of-a","text":"As per title.","title":"Definition (Nullspace of \\(T\\) is nullspace of \\(\\A\\))"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.02_linear_algebra_linear_transformations_nullspace/#example-nullspace-of-differentiation","text":"Let \\(T \\in \\L(\\F[\\z], \\F[\\z])\\) be the differentiation map \\(T(f(\\z)) = f^{'}(\\z)\\) . Then the nullspace of \\(T\\) is: \\[ \\null(T) = \\{f \\in \\F[\\z] ~|~ f(\\z) \\text{ is constant}\\} \\] which is apparent because the only way a function's derivative is zero is if the function is a constant.","title":"Example (Nullspace of Differentiation)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.02_linear_algebra_linear_transformations_nullspace/#theorem-nullspace-is-a-subspace","text":"Given any \\(T: \\V \\to \\W\\) in \\(\\L(\\V, \\W)\\) , the nullspace of \\(T\\) is a subspace of \\(\\V\\) . We have already seen that in matrix theory.","title":"Theorem (Nullspace is a Subspace)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.02_linear_algebra_linear_transformations_nullspace/#definition-injective","text":"Many of us are familiar with what is an injective map , for those who aren't, here is the definition: A function or mapping \\(f: X \\to Y\\) is injective if and only if \\(T(x) = T(y) \\implies x = y\\) . Note the contrapositive applies here as well: If \\(x \\neq y \\implies T(x) \\neq T(y)\\) then \\(f\\) is not njective .","title":"Definition (Injective)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.02_linear_algebra_linear_transformations_nullspace/#theorem-nullspace-and-injectivity","text":"Let \\(T \\in \\L(\\V, \\W)\\) , then \\(T\\) is injective if and only if the \\(\\null(T) = \\{\\0\\}\\) . In matrix terminology: Let \\(T_{\\A} \\in \\L(\\V, \\W)\\) , then \\(T_{\\A}\\) is injective if and only if the \\(\\null(T_{\\A}) = \\{\\0\\}\\) if and only if the nullity of \\(\\A\\) is zero (i.e. dimension of the nullspace of the matrix \\(\\A\\) is zero).","title":"Theorem (Nullspace and Injectivity)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.02_linear_algebra_linear_transformations_nullspace/#intuition-nullspace-and-injectivity","text":"Proving this theorem alone shows you understand the algebraic logic, but we can go one step further and try to get more intuition of what this theorem means. Well, for one, we know that \\(\\null(T) = \\0\\) means that the matrix \\(\\A\\) associated with this \\(T\\) is full-rank, that is to say, \\(\\A\\x = \\0\\) has only the trivial solution. We can immediately have a corollary.","title":"Intuition (Nullspace and Injectivity)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.02_linear_algebra_linear_transformations_nullspace/#corollary-nullspace-and-injectivity-implies-full-rank","text":"If \\(T\\) is an injective map, then the matrix associated with \\(T\\) is full-rank.","title":"Corollary (Nullspace and Injectivity implies Full-Rank)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.02_linear_algebra_linear_transformations_nullspace/#example-injective-maps","text":"Let \\(T: \\F[\\z] \\to \\F[\\z]\\) be the differentiation map. Note \\(T(f(\\z)) = f^{'}(\\z)\\) . The differentiation map \\(f(\\z) \\mapsto f^{'}(\\z)\\) is not injective because if you take \\(T(f(\\z)) = T(g(\\z))\\) , we must recover \\(f(\\z) = g(\\z)\\) , but in fact this is not true. Set \\(g(\\z) = f(\\z) + c\\) where \\(c \\neq 0 \\in \\F\\) is a constant, then \\(f(\\z) \\neq g(\\z)\\) by construction and hence not injective.","title":"Example (Injective Maps)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.03_linear_algebra_linear_transformations_ranges/","text":"\\[ \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\e}{\\mathbf{e}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\z}{\\mathbf{z}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\C}{\\mathbf{C}} \\newcommand{\\rank}{\\textbf{rank}} \\newcommand{\\null}{\\textbf{null}} \\newcommand{\\range}{\\textbf{range}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\U}{\\mathrm{U}} \\newcommand{\\V}{\\mathrm{V}} \\newcommand{\\W}{\\mathrm{W}} \\newcommand{\\L}{\\mathcal{L}} \\] Range Definition (Range of \\(T\\) ) For any \\(T: \\V \\to \\W \\in L(\\V, \\W)\\) , the range of \\(T\\) denoted as \\(\\range(T)\\) is the subset of vectors in \\(\\W\\) that are in the image of \\(T\\) , more concretely: \\[ \\range(T) = \\{T(\\v) ~|~ \\v \\in \\V\\} = \\{\\w \\in \\W ~|~ \\exists \\v \\in \\V \\text{ s.t. } T(\\v) = \\w\\} \\] Definition (Range of T is column space of A) Example (Range of Differentiation) Let \\(T \\in \\L(\\F[\\z], \\F[\\z])\\) be the differentiation map \\(T(f(\\z)) = f^{'}(\\z)\\) . Then the range of \\(T\\) is: \\[ \\range(T) = \\F[\\z] \\] since for every polynomial \\(q \\in \\F[\\z]\\) , there exists a \\(p \\in \\F[\\z]\\) such that \\(p^{'} = q\\) . Theorem (Range is a Subspace) Given any \\(T: \\V \\to \\W\\) in \\(\\L(\\V, \\W)\\) , the range of \\(T\\) is a subspace of \\(\\W\\) . We have already seen that in matrix theory. Definition (Surjective) Many of us are familiar with what is an surjective map , for those who aren't, here is the definition: A function or mapping \\(f: X \\to Y\\) is surjective if and only if \\(f\\) is a function that maps an element \\(x \\in X\\) to every element \\(y \\in Y\\) . In other words, for every \\(y \\in Y\\) , there exists an \\(x \\in X\\) such that \\(f(x) = y\\) . Theorem (Range and Surjectivity) Let \\(T: \\V \\to \\W \\in \\L(\\V, \\W)\\) , then \\(T\\) is surjective if and only if the \\(\\range(T) = \\W\\) . In matrix terminology: Let \\(T_{\\A} \\in \\L(\\V, \\W)\\) , then \\(T_{\\A}\\) is surjective if and only if the rank of \\(\\A\\) is \\(m\\) .","title":"Range"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.03_linear_algebra_linear_transformations_ranges/#range","text":"","title":"Range"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.03_linear_algebra_linear_transformations_ranges/#definition-range-of-t","text":"For any \\(T: \\V \\to \\W \\in L(\\V, \\W)\\) , the range of \\(T\\) denoted as \\(\\range(T)\\) is the subset of vectors in \\(\\W\\) that are in the image of \\(T\\) , more concretely: \\[ \\range(T) = \\{T(\\v) ~|~ \\v \\in \\V\\} = \\{\\w \\in \\W ~|~ \\exists \\v \\in \\V \\text{ s.t. } T(\\v) = \\w\\} \\]","title":"Definition (Range of \\(T\\))"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.03_linear_algebra_linear_transformations_ranges/#definition-range-of-t-is-column-space-of-a","text":"","title":"Definition (Range of T is column space of A)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.03_linear_algebra_linear_transformations_ranges/#example-range-of-differentiation","text":"Let \\(T \\in \\L(\\F[\\z], \\F[\\z])\\) be the differentiation map \\(T(f(\\z)) = f^{'}(\\z)\\) . Then the range of \\(T\\) is: \\[ \\range(T) = \\F[\\z] \\] since for every polynomial \\(q \\in \\F[\\z]\\) , there exists a \\(p \\in \\F[\\z]\\) such that \\(p^{'} = q\\) .","title":"Example (Range of Differentiation)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.03_linear_algebra_linear_transformations_ranges/#theorem-range-is-a-subspace","text":"Given any \\(T: \\V \\to \\W\\) in \\(\\L(\\V, \\W)\\) , the range of \\(T\\) is a subspace of \\(\\W\\) . We have already seen that in matrix theory.","title":"Theorem (Range is a Subspace)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.03_linear_algebra_linear_transformations_ranges/#definition-surjective","text":"Many of us are familiar with what is an surjective map , for those who aren't, here is the definition: A function or mapping \\(f: X \\to Y\\) is surjective if and only if \\(f\\) is a function that maps an element \\(x \\in X\\) to every element \\(y \\in Y\\) . In other words, for every \\(y \\in Y\\) , there exists an \\(x \\in X\\) such that \\(f(x) = y\\) .","title":"Definition (Surjective)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.03_linear_algebra_linear_transformations_ranges/#theorem-range-and-surjectivity","text":"Let \\(T: \\V \\to \\W \\in \\L(\\V, \\W)\\) , then \\(T\\) is surjective if and only if the \\(\\range(T) = \\W\\) . In matrix terminology: Let \\(T_{\\A} \\in \\L(\\V, \\W)\\) , then \\(T_{\\A}\\) is surjective if and only if the rank of \\(\\A\\) is \\(m\\) .","title":"Theorem (Range and Surjectivity)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.04_linear_algebra_linear_transformations_homomorphism/","text":"\\[ \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\e}{\\mathbf{e}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\z}{\\mathbf{z}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\C}{\\mathbf{C}} \\newcommand{\\rank}{\\textbf{rank}} \\newcommand{\\null}{\\textbf{null}} \\newcommand{\\range}{\\textbf{range}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\U}{\\mathrm{U}} \\newcommand{\\V}{\\mathrm{V}} \\newcommand{\\W}{\\mathrm{W}} \\newcommand{\\L}{\\mathcal{L}} \\] Homomorphisms Definition (Homomorphisms) Extracted from here , we have the following definition: Linear maps between vector spaces are called vector space homomorphisms , and instead of \\(\\L(\\V, \\W)\\) , we see the following notation: \\[ \\text{Hom}_{\\F}(\\V, \\W) = \\{T: \\V \\to \\W ~|~ T \\text{ is a linear map}\\} \\] Definition (Isomorphism and Bijective Linear Map) A homomorphism \\(T: \\V \\to \\W\\) is an isomorphism if and only if \\(T\\) is a bijective map , that is, \\(T\\) is both injective and surjective . Definition (Monomorphism and Bijective Linear Map) A homomorphism \\(T: \\V \\to \\W\\) is an monomorphism if and only if \\(T\\) is an injective map . Definition (Epimorphism and Bijective Linear Map) A homomorphism \\(T: \\V \\to \\W\\) is an epimorphism if and only if \\(T\\) is an surjective map . Definition (Endomorphism and Bijective Linear Map) A homomorphism \\(T: \\V \\to \\W\\) is an endomorphism if and only if \\(\\V = \\W\\) . Definition (Automorphism and Bijective Linear Map) A homomorphism \\(T: \\V \\to \\W\\) is an automorphism if and only if \\(\\V = \\W\\) and \\(T\\) is bijective .","title":"Homomorphism"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.04_linear_algebra_linear_transformations_homomorphism/#homomorphisms","text":"","title":"Homomorphisms"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.04_linear_algebra_linear_transformations_homomorphism/#definition-homomorphisms","text":"Extracted from here , we have the following definition: Linear maps between vector spaces are called vector space homomorphisms , and instead of \\(\\L(\\V, \\W)\\) , we see the following notation: \\[ \\text{Hom}_{\\F}(\\V, \\W) = \\{T: \\V \\to \\W ~|~ T \\text{ is a linear map}\\} \\]","title":"Definition (Homomorphisms)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.04_linear_algebra_linear_transformations_homomorphism/#definition-isomorphism-and-bijective-linear-map","text":"A homomorphism \\(T: \\V \\to \\W\\) is an isomorphism if and only if \\(T\\) is a bijective map , that is, \\(T\\) is both injective and surjective .","title":"Definition (Isomorphism and Bijective Linear Map)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.04_linear_algebra_linear_transformations_homomorphism/#definition-monomorphism-and-bijective-linear-map","text":"A homomorphism \\(T: \\V \\to \\W\\) is an monomorphism if and only if \\(T\\) is an injective map .","title":"Definition (Monomorphism and Bijective Linear Map)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.04_linear_algebra_linear_transformations_homomorphism/#definition-epimorphism-and-bijective-linear-map","text":"A homomorphism \\(T: \\V \\to \\W\\) is an epimorphism if and only if \\(T\\) is an surjective map .","title":"Definition (Epimorphism and Bijective Linear Map)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.04_linear_algebra_linear_transformations_homomorphism/#definition-endomorphism-and-bijective-linear-map","text":"A homomorphism \\(T: \\V \\to \\W\\) is an endomorphism if and only if \\(\\V = \\W\\) .","title":"Definition (Endomorphism and Bijective Linear Map)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.04_linear_algebra_linear_transformations_homomorphism/#definition-automorphism-and-bijective-linear-map","text":"A homomorphism \\(T: \\V \\to \\W\\) is an automorphism if and only if \\(\\V = \\W\\) and \\(T\\) is bijective .","title":"Definition (Automorphism and Bijective Linear Map)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.05_linear_algebra_linear_transformations_fundamental_theorem/","text":"\\[ \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\e}{\\mathbf{e}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\z}{\\mathbf{z}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\C}{\\mathbf{C}} \\newcommand{\\rank}{\\textbf{rank}} \\newcommand{\\null}{\\textbf{null}} \\newcommand{\\range}{\\textbf{range}} \\newcommand{\\dim}{\\textbf{dim}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\U}{\\mathrm{U}} \\newcommand{\\V}{\\mathrm{V}} \\newcommand{\\W}{\\mathrm{W}} \\newcommand{\\L}{\\mathcal{L}} \\] The Fundamental Theorem of Linear Transformation/Maps Definition (Nullity-Rank Theorem revisited) Let \\(T: \\V \\to \\W\\) be a linear transformation between vector spaces over a field \\(\\F\\) where we assume \\(\\V\\) is finite-dimensional and \\(\\range(T)\\) is finite as a consequence. Then \\[ \\dim_{\\F}(\\V) = \\dim\\left[\\null(T)\\right] + \\dim\\left[\\range(T)\\right] \\] In particular, if \\(\\A \\in M_{m \\times n}(\\F)\\) then the theorem recovers the nullity-rank theorem : \\[ \\text{nullity}(\\A) + \\text{rank}(\\A) = n \\] where \\(n\\) is the number of columns of \\(\\A\\) . Corollary (A map to a smaller dimensional space is not injective) A simple corollary 1 is that no linear map from a finite-dimensional vector space to a \"smaller\" vector space can be injective, where smaller means dimension. Suppose \\(\\V\\) and \\(\\W\\) are finite-dimensional vector spaces such that \\(\\dim(\\V) > \\dim(\\W)\\) , then no linear map from \\(\\V\\) to \\(\\W\\) is injective . Corollary (A map to a smaller dimensional space is not injective) A simple corollary 2 shows that no linear map from a finite-dimensional vector space to a \"bigger\" vector space can be surjective, where \"bigger\" is measured by dimension. Suppose \\(\\V\\) and \\(\\W\\) are finite-dimensional vector spaces such that \\(\\dim(\\V) < \\dim(\\W)\\) , then no linear map from \\(\\V\\) to \\(\\W\\) is surjective . Homogeneous System of Linear Equations Quite important, revisit pp.66-67 of Linear Algebra done right. Inhomogeneous system of linear equations Sheldon Axler: Linear Algebra Done Right, 2015; pp.64 \u21a9 Sheldon Axler: Linear Algebra Done Right, 2015; pp.64 \u21a9","title":"Fundamental Theorem"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.05_linear_algebra_linear_transformations_fundamental_theorem/#the-fundamental-theorem-of-linear-transformationmaps","text":"","title":"The Fundamental Theorem of Linear Transformation/Maps"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.05_linear_algebra_linear_transformations_fundamental_theorem/#definition-nullity-rank-theorem-revisited","text":"Let \\(T: \\V \\to \\W\\) be a linear transformation between vector spaces over a field \\(\\F\\) where we assume \\(\\V\\) is finite-dimensional and \\(\\range(T)\\) is finite as a consequence. Then \\[ \\dim_{\\F}(\\V) = \\dim\\left[\\null(T)\\right] + \\dim\\left[\\range(T)\\right] \\] In particular, if \\(\\A \\in M_{m \\times n}(\\F)\\) then the theorem recovers the nullity-rank theorem : \\[ \\text{nullity}(\\A) + \\text{rank}(\\A) = n \\] where \\(n\\) is the number of columns of \\(\\A\\) .","title":"Definition (Nullity-Rank Theorem revisited)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.05_linear_algebra_linear_transformations_fundamental_theorem/#corollary-a-map-to-a-smaller-dimensional-space-is-not-injective","text":"A simple corollary 1 is that no linear map from a finite-dimensional vector space to a \"smaller\" vector space can be injective, where smaller means dimension. Suppose \\(\\V\\) and \\(\\W\\) are finite-dimensional vector spaces such that \\(\\dim(\\V) > \\dim(\\W)\\) , then no linear map from \\(\\V\\) to \\(\\W\\) is injective .","title":"Corollary (A map to a smaller dimensional space is not injective)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.05_linear_algebra_linear_transformations_fundamental_theorem/#corollary-a-map-to-a-smaller-dimensional-space-is-not-injective_1","text":"A simple corollary 2 shows that no linear map from a finite-dimensional vector space to a \"bigger\" vector space can be surjective, where \"bigger\" is measured by dimension. Suppose \\(\\V\\) and \\(\\W\\) are finite-dimensional vector spaces such that \\(\\dim(\\V) < \\dim(\\W)\\) , then no linear map from \\(\\V\\) to \\(\\W\\) is surjective .","title":"Corollary (A map to a smaller dimensional space is not injective)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.05_linear_algebra_linear_transformations_fundamental_theorem/#homogeneous-system-of-linear-equations","text":"Quite important, revisit pp.66-67 of Linear Algebra done right.","title":"Homogeneous System of Linear Equations"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.05_linear_algebra_linear_transformations_fundamental_theorem/#inhomogeneous-system-of-linear-equations","text":"Sheldon Axler: Linear Algebra Done Right, 2015; pp.64 \u21a9 Sheldon Axler: Linear Algebra Done Right, 2015; pp.64 \u21a9","title":"Inhomogeneous system of linear equations"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.06_linear_algebra_linear_transformations_matrix/","text":"\\[ \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\e}{\\mathbf{e}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\z}{\\mathbf{z}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\C}{\\mathbf{C}} \\newcommand{\\rank}{\\textbf{rank}} \\newcommand{\\null}{\\textbf{null}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\U}{\\mathrm{U}} \\newcommand{\\V}{\\mathrm{V}} \\newcommand{\\W}{\\mathrm{W}} \\newcommand{\\L}{\\mathcal{L}} \\] Matrix and Linear Transformations This is a big topic and has its own spot in this chapter. We let \\[ \\A=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix} \\] Definition (Matrix is a Linear Transformation Naive) Let \\(\\A = (\\a_{ij}) \\in \\F^{m \\times n}\\) be an \\(m \\times n\\) matrix with entries in a field \\(\\F\\) . Let us first define the matrix-vector multiplication \\(\\A\\x = \\b\\) where \\(\\x \\in \\F^{n}\\) and \\(\\b \\in \\F^{m}\\) , and specifically, let \\(\\F^n\\) and \\(\\F^m\\) be our vector spaces \\(\\V\\) and \\(\\W\\) respectively. Define the map \\[T_{\\A}: \\F_{c}^{n} \\rightarrow F_{c}^{m}\\] \\[\\x = \\begin{bmatrix} x_{1}\\\\ x_{2}\\\\ x_{3}\\\\ \\vdots\\\\ x_{n} \\end{bmatrix} \\mapsto \\A\\x \\] Then \\(T_{\\A}\\) is a linear transformation (associated with the matrix \\(\\A\\) ) where we are mapping (sending) the vector \\(\\x\\) from \\(n\\) dimensional space to the vector \\(\\b = \\A\\x\\) in the \\(m\\) dimensional space . Important for readers: \\(T_{\\A}(\\x) = \\A\\x\\) . Proof (Matrix is a Linear Transformation) This definition deserves a proof, simply, because one should really internalize this and not gloss it over. Recall carefully that matrix operations such as: \\(\\A(\\x_1 + \\x_2) = \\A\\x_1 \\A\\x_2\\) \\(\\lambda(\\A\\x) = \\A(\\lambda(\\x))\\) are well-defined . With this, this directly carries over to our linear transformation , for any vectors \\(\\x_1, \\x_2 \\in \\F^n\\) and any \\(\\lambda \\in \\F\\) : additivity: \\(T_{\\A}(\\x_1 + \\x_2) = T_{\\A}(\\x_1) + T_{\\A}(\\x_2)\\) for all \\(\\x_1, \\x_2 \\in \\V=\\F^n\\) . homogeneity: \\(T_{\\A}\\left(\\lambda(\\x)\\right) = \\lambda(T_{\\A}(\\x))\\) for all \\(\\lambda \\in \\F\\) and all \\(\\x \\in \\V=\\F^n\\) . Definition (Matrix is a Linear Transformation Proper) The proper definition of matrix being a linear transformation is given below. Note that we will see that every linear transformation \\(T: \\V \\to \\W\\) can be defined and encoded by a matrix \\(\\A\\) , and conversely, every matrix also defines such a linear transformation . Let \\(T \\in \\L(\\V, \\W)\\) where \\(\\V\\) and \\(\\W\\) are finite-dimensional vector spaces, in particular, they are \\(n\\) and \\(m\\) dimensional respectively.. Since every vector space has a basis, we can define \\(\\v_1, ..., \\v_n\\) and \\(\\w_1, ..., \\w_m\\) to be the basis of \\(\\V\\) and \\(\\W\\) respectively . More concretely, every vector \\(\\v \\in \\V\\) is uniquely determined by the coefficients \\(c_1, ..., c_n \\in \\F\\) , that is, any vector \\(\\v\\) in the vector space \\(\\V\\) can be uniquely determined by the linear combination below: \\[ \\v = c_1 \\v_1 + ... + c_n \\v_n , \\quad \\forall \\v \\in \\V \\] If \\(T: \\V \\to \\W\\) is a linear transformation , then it follows that: \\[ T(c_1 \\v_1 + ... + c_n \\v_n) = c_1 T(\\v_1) + ... + c_nT(\\v_n) \\] by additivity . This implies that the mapping \\(T\\) is entirely determined by the vectors \\(T(\\v_1), ..., T(\\v_n)\\) . Now, we turn out attention to the basis vectors of \\(\\W\\) . We see that we can represent each vector \\(T(\\v_j)\\) as: \\[ T(\\v_j) = a_{1j} \\w_1 + ... + a_{mj}\\w_m \\] where \\(a_{ij}\\) is unique scalars in \\(\\F\\) as well. Thus the mapping \\(T\\) is also entirely determined by the values \\(a_{ij}\\) where \\(i\\) is from \\(1\\) to \\(m\\) . Last but not least, we define the matrix \\(\\A\\) associated with the mapping \\(T\\) with respect to these bases is the \\(m \\times n\\) matrix \\(T_{\\A}\\) whose entries \\(a_{ij}\\) are defined as: \\[ T(\\v_j) = a_{1j}\\w_1 + ... + a_{mj}\\w_m \\] As a consequence, the \\(j\\) -th column of \\(\\A\\) contains the coefficients of the \\(j\\) -th basis vector \\(\\v_j\\) where each element of \\(\\v_j\\) are the coefficients of the vector \\(\\w_j = T(\\v_j)\\) . \\[ \\A=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix} \\] This is a mouthful, so we understand it better with examples. Examples (Matrix is a Linear Transformation Proper) Standard Basis Linear Transformation Let \\(T \\in \\L(\\F^2, \\F^3)\\) be defined by: \\[ T(x, y) = (x+3y, 2x+5y, 7x+9y) \\] Then the \\(T_{\\A}\\) with respect to the standard bases in \\(\\F^2\\) and \\(\\F^3\\) is: Let \\(\\v_1 = [1, 0], \\quad \\v_2 = [0, 1]\\) be the basis of \\(\\F^2\\) and we see that \\[ \\w_1 = T(\\v_1) = (1, 2, 7), \\quad \\w_2 = T(\\v_2) = (3, 5, 9) \\] Construct the matrix \\[ \\A = \\begin{bmatrix} 1 & 3 \\\\ 2 & 5 \\\\ 7 & 9 \\end{bmatrix} \\] Example (Rotational Matrix) Define the following setup: 2 dimensional real space: \\(\\R^2\\) Define our domain and co-domain to be the same 2d-space (i.e. \\(\\V = \\W = \\R^2\\) ) Define basis vectors from \\(\\V\\) to be the standard basis \\(\\e_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\) and \\(\\e_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\) Now let me tell you first that a rotation 90 degrees anti-clockwise is a valid linear transformation and you can draw it to see geometrically that this rotation maps our basis vector \\(\\e_1\\) and \\(\\e_2\\) to \\(T(\\e_1) = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\) and \\(T(\\e_2) = \\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix}\\) (you can check visually). Then the theorem of linear transformation tells us that if we know the set of basis vectors from the domain \\(\\V\\) , and correspondingly the mapping of these basis vectors (i.e in this case \\(T(\\e_1)\\) and \\(T(\\e_2)\\) ), we can then say that our mapping (rotational) is entirely determined and uniquely defined in \\(\\V\\) by \\(\\W\\) . In laymen, given any vector \\(\\v \\in \\V\\) , we can now fully determine where \\(\\v\\) is going to go to in \\(\\W\\) (in this case the same space) via the rotational mapping \\(T\\) . Note that the mapping \\(T\\) can also be in terms of matrix \\(\\A = \\begin{bmatrix}T(\\e_1) & T(\\e_2) \\end{bmatrix} = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}\\) Thus, by the theorem of linear transformation, if we take an arbitrary vector say \\(\\v = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\) , then visually you know the 90 degrees rotation will go to \\(T(\\v) = \\begin{bmatrix} -2 \\\\ 1 \\end{bmatrix}\\) , but using algebra we can assert this is true by \\(T(\\v) = 1 \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} + 2 \\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} -2 \\\\ 1 \\end{bmatrix}\\) Lastly, recall the matrix-vector multiplication \\(\\A\\v = \\b\\) , then \\(\\A\\v = \\b\\) is the linear transformation where \\(\\A\\) is the rotational mapping in which the columns are \\(T(\\e_i)\\) , and \\(\\v\\) is any vector in the domain. Now we have a algebraic formula to denote the mapping. The Four Fundamental Subspaces Revisited We can now frame our Four (Favourite) Fundamental Subspaces in the lingo of linear transformation . TBD write this after section 3.B in LADR. Nullspace: The kernel of \\(T_{\\A}\\) equals the null space (or kernel) of \\(A\\) $$ \\text{Ker}(T_{A}) = \\text{Ker}(A) = \\text{Null}(A) = {\\x \\in F_{c}^{n} ~|~ A\\x = 0} \\subseteq F_{c}^{n}$$ The range (or image) of \\(T_{A}\\) equals the range of \\(A\\) : \\[R(T_{A}) = T_{A}(F_{c}^{n}) = R(A) : \\{A\\x ~|~ \\x \\in F_{c}^{n}\\} \\subseteq F_{c}^{m}\\] Note in particular, the Range of \\(A\\) is just the Column Space of \\(A\\) , that is if \\(A = (\\a_1,...,\\a_n)\\) , then \\( \\(\\text{Range}(A) = \\text{Span}\\{\\a_1,...,\\a_n\\}\\) \\) References https://math.stackexchange.com/questions/4376735/what-is-meant-by-the-linear-mapping-t-is-entirely-and-uniquely-determined-on/4376761#4376761 https://math.stackexchange.com/questions/4114034/is-a-linear-map-uniquely-determined-on-v-or-on-a-basis-of-v","title":"Linear Transformation and Matrix"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.06_linear_algebra_linear_transformations_matrix/#matrix-and-linear-transformations","text":"This is a big topic and has its own spot in this chapter. We let \\[ \\A=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix} \\]","title":"Matrix and Linear Transformations"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.06_linear_algebra_linear_transformations_matrix/#definition-matrix-is-a-linear-transformation-naive","text":"Let \\(\\A = (\\a_{ij}) \\in \\F^{m \\times n}\\) be an \\(m \\times n\\) matrix with entries in a field \\(\\F\\) . Let us first define the matrix-vector multiplication \\(\\A\\x = \\b\\) where \\(\\x \\in \\F^{n}\\) and \\(\\b \\in \\F^{m}\\) , and specifically, let \\(\\F^n\\) and \\(\\F^m\\) be our vector spaces \\(\\V\\) and \\(\\W\\) respectively. Define the map \\[T_{\\A}: \\F_{c}^{n} \\rightarrow F_{c}^{m}\\] \\[\\x = \\begin{bmatrix} x_{1}\\\\ x_{2}\\\\ x_{3}\\\\ \\vdots\\\\ x_{n} \\end{bmatrix} \\mapsto \\A\\x \\] Then \\(T_{\\A}\\) is a linear transformation (associated with the matrix \\(\\A\\) ) where we are mapping (sending) the vector \\(\\x\\) from \\(n\\) dimensional space to the vector \\(\\b = \\A\\x\\) in the \\(m\\) dimensional space . Important for readers: \\(T_{\\A}(\\x) = \\A\\x\\) .","title":"Definition (Matrix is a Linear Transformation Naive)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.06_linear_algebra_linear_transformations_matrix/#proof-matrix-is-a-linear-transformation","text":"This definition deserves a proof, simply, because one should really internalize this and not gloss it over. Recall carefully that matrix operations such as: \\(\\A(\\x_1 + \\x_2) = \\A\\x_1 \\A\\x_2\\) \\(\\lambda(\\A\\x) = \\A(\\lambda(\\x))\\) are well-defined . With this, this directly carries over to our linear transformation , for any vectors \\(\\x_1, \\x_2 \\in \\F^n\\) and any \\(\\lambda \\in \\F\\) : additivity: \\(T_{\\A}(\\x_1 + \\x_2) = T_{\\A}(\\x_1) + T_{\\A}(\\x_2)\\) for all \\(\\x_1, \\x_2 \\in \\V=\\F^n\\) . homogeneity: \\(T_{\\A}\\left(\\lambda(\\x)\\right) = \\lambda(T_{\\A}(\\x))\\) for all \\(\\lambda \\in \\F\\) and all \\(\\x \\in \\V=\\F^n\\) .","title":"Proof (Matrix is a Linear Transformation)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.06_linear_algebra_linear_transformations_matrix/#definition-matrix-is-a-linear-transformation-proper","text":"The proper definition of matrix being a linear transformation is given below. Note that we will see that every linear transformation \\(T: \\V \\to \\W\\) can be defined and encoded by a matrix \\(\\A\\) , and conversely, every matrix also defines such a linear transformation . Let \\(T \\in \\L(\\V, \\W)\\) where \\(\\V\\) and \\(\\W\\) are finite-dimensional vector spaces, in particular, they are \\(n\\) and \\(m\\) dimensional respectively.. Since every vector space has a basis, we can define \\(\\v_1, ..., \\v_n\\) and \\(\\w_1, ..., \\w_m\\) to be the basis of \\(\\V\\) and \\(\\W\\) respectively . More concretely, every vector \\(\\v \\in \\V\\) is uniquely determined by the coefficients \\(c_1, ..., c_n \\in \\F\\) , that is, any vector \\(\\v\\) in the vector space \\(\\V\\) can be uniquely determined by the linear combination below: \\[ \\v = c_1 \\v_1 + ... + c_n \\v_n , \\quad \\forall \\v \\in \\V \\] If \\(T: \\V \\to \\W\\) is a linear transformation , then it follows that: \\[ T(c_1 \\v_1 + ... + c_n \\v_n) = c_1 T(\\v_1) + ... + c_nT(\\v_n) \\] by additivity . This implies that the mapping \\(T\\) is entirely determined by the vectors \\(T(\\v_1), ..., T(\\v_n)\\) . Now, we turn out attention to the basis vectors of \\(\\W\\) . We see that we can represent each vector \\(T(\\v_j)\\) as: \\[ T(\\v_j) = a_{1j} \\w_1 + ... + a_{mj}\\w_m \\] where \\(a_{ij}\\) is unique scalars in \\(\\F\\) as well. Thus the mapping \\(T\\) is also entirely determined by the values \\(a_{ij}\\) where \\(i\\) is from \\(1\\) to \\(m\\) . Last but not least, we define the matrix \\(\\A\\) associated with the mapping \\(T\\) with respect to these bases is the \\(m \\times n\\) matrix \\(T_{\\A}\\) whose entries \\(a_{ij}\\) are defined as: \\[ T(\\v_j) = a_{1j}\\w_1 + ... + a_{mj}\\w_m \\] As a consequence, the \\(j\\) -th column of \\(\\A\\) contains the coefficients of the \\(j\\) -th basis vector \\(\\v_j\\) where each element of \\(\\v_j\\) are the coefficients of the vector \\(\\w_j = T(\\v_j)\\) . \\[ \\A=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix} \\] This is a mouthful, so we understand it better with examples.","title":"Definition (Matrix is a Linear Transformation Proper)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.06_linear_algebra_linear_transformations_matrix/#examples-matrix-is-a-linear-transformation-proper","text":"","title":"Examples (Matrix is a Linear Transformation Proper)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.06_linear_algebra_linear_transformations_matrix/#standard-basis-linear-transformation","text":"Let \\(T \\in \\L(\\F^2, \\F^3)\\) be defined by: \\[ T(x, y) = (x+3y, 2x+5y, 7x+9y) \\] Then the \\(T_{\\A}\\) with respect to the standard bases in \\(\\F^2\\) and \\(\\F^3\\) is: Let \\(\\v_1 = [1, 0], \\quad \\v_2 = [0, 1]\\) be the basis of \\(\\F^2\\) and we see that \\[ \\w_1 = T(\\v_1) = (1, 2, 7), \\quad \\w_2 = T(\\v_2) = (3, 5, 9) \\] Construct the matrix \\[ \\A = \\begin{bmatrix} 1 & 3 \\\\ 2 & 5 \\\\ 7 & 9 \\end{bmatrix} \\]","title":"Standard Basis Linear Transformation"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.06_linear_algebra_linear_transformations_matrix/#example-rotational-matrix","text":"Define the following setup: 2 dimensional real space: \\(\\R^2\\) Define our domain and co-domain to be the same 2d-space (i.e. \\(\\V = \\W = \\R^2\\) ) Define basis vectors from \\(\\V\\) to be the standard basis \\(\\e_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\) and \\(\\e_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\) Now let me tell you first that a rotation 90 degrees anti-clockwise is a valid linear transformation and you can draw it to see geometrically that this rotation maps our basis vector \\(\\e_1\\) and \\(\\e_2\\) to \\(T(\\e_1) = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\) and \\(T(\\e_2) = \\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix}\\) (you can check visually). Then the theorem of linear transformation tells us that if we know the set of basis vectors from the domain \\(\\V\\) , and correspondingly the mapping of these basis vectors (i.e in this case \\(T(\\e_1)\\) and \\(T(\\e_2)\\) ), we can then say that our mapping (rotational) is entirely determined and uniquely defined in \\(\\V\\) by \\(\\W\\) . In laymen, given any vector \\(\\v \\in \\V\\) , we can now fully determine where \\(\\v\\) is going to go to in \\(\\W\\) (in this case the same space) via the rotational mapping \\(T\\) . Note that the mapping \\(T\\) can also be in terms of matrix \\(\\A = \\begin{bmatrix}T(\\e_1) & T(\\e_2) \\end{bmatrix} = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}\\) Thus, by the theorem of linear transformation, if we take an arbitrary vector say \\(\\v = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\) , then visually you know the 90 degrees rotation will go to \\(T(\\v) = \\begin{bmatrix} -2 \\\\ 1 \\end{bmatrix}\\) , but using algebra we can assert this is true by \\(T(\\v) = 1 \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} + 2 \\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} -2 \\\\ 1 \\end{bmatrix}\\) Lastly, recall the matrix-vector multiplication \\(\\A\\v = \\b\\) , then \\(\\A\\v = \\b\\) is the linear transformation where \\(\\A\\) is the rotational mapping in which the columns are \\(T(\\e_i)\\) , and \\(\\v\\) is any vector in the domain. Now we have a algebraic formula to denote the mapping.","title":"Example (Rotational Matrix)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.06_linear_algebra_linear_transformations_matrix/#the-four-fundamental-subspaces-revisited","text":"We can now frame our Four (Favourite) Fundamental Subspaces in the lingo of linear transformation . TBD write this after section 3.B in LADR. Nullspace: The kernel of \\(T_{\\A}\\) equals the null space (or kernel) of \\(A\\) $$ \\text{Ker}(T_{A}) = \\text{Ker}(A) = \\text{Null}(A) = {\\x \\in F_{c}^{n} ~|~ A\\x = 0} \\subseteq F_{c}^{n}$$ The range (or image) of \\(T_{A}\\) equals the range of \\(A\\) : \\[R(T_{A}) = T_{A}(F_{c}^{n}) = R(A) : \\{A\\x ~|~ \\x \\in F_{c}^{n}\\} \\subseteq F_{c}^{m}\\] Note in particular, the Range of \\(A\\) is just the Column Space of \\(A\\) , that is if \\(A = (\\a_1,...,\\a_n)\\) , then \\( \\(\\text{Range}(A) = \\text{Span}\\{\\a_1,...,\\a_n\\}\\) \\)","title":"The Four Fundamental Subspaces Revisited"},{"location":"reighns_ml_journey/mathematics/linear_algebra/06_linear_transformation/06.06_linear_algebra_linear_transformations_matrix/#references","text":"https://math.stackexchange.com/questions/4376735/what-is-meant-by-the-linear-mapping-t-is-entirely-and-uniquely-determined-on/4376761#4376761 https://math.stackexchange.com/questions/4114034/is-a-linear-map-uniquely-determined-on-v-or-on-a-basis-of-v","title":"References"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.01_motivation/","text":"Motivation of Chapter This chapter is essential in Machine Learning context. Below is an image of the workflow of this chapter, taken from the book \"Mathematics for Machine Learning\". Fig 2.3: 3 of the same vectors with different starting coordinates; By Hongnan G.","title":"Motivation"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.01_motivation/#motivation-of-chapter","text":"This chapter is essential in Machine Learning context. Below is an image of the workflow of this chapter, taken from the book \"Mathematics for Machine Learning\". Fig 2.3: 3 of the same vectors with different starting coordinates; By Hongnan G.","title":"Motivation of Chapter"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.02_norms/","text":"\\[ \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\r}{\\mathbf{r}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\z}{\\mathbf{z}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\C}{\\mathbf{C}} \\newcommand{\\rank}{\\textbf{rank}} \\] Norms 1 Algebraic Definition (Norms) Algebraic Definition (Manhattan Norm) Geometric Definition (Manhattan Norm) Algebraic Definition (Euclidean Norm) Geometric Definition (Euclidean Norm) Algebraic Definition (LP Norm) Norms: Cambridge University Press: Mathematics for Machine Learning, 2020. (pp. 71-72) \u21a9","title":"Norms"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.02_norms/#norms1","text":"","title":"Norms1"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.02_norms/#algebraic-definition-norms","text":"","title":"Algebraic Definition (Norms)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.02_norms/#algebraic-definition-manhattan-norm","text":"","title":"Algebraic Definition (Manhattan Norm)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.02_norms/#geometric-definition-manhattan-norm","text":"","title":"Geometric Definition (Manhattan Norm)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.02_norms/#algebraic-definition-euclidean-norm","text":"","title":"Algebraic Definition (Euclidean Norm)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.02_norms/#geometric-definition-euclidean-norm","text":"","title":"Geometric Definition (Euclidean Norm)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.02_norms/#algebraic-definition-lp-norm","text":"Norms: Cambridge University Press: Mathematics for Machine Learning, 2020. (pp. 71-72) \u21a9","title":"Algebraic Definition (LP Norm)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.03_inner_products/","text":"\\[ \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\r}{\\mathbf{r}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\z}{\\mathbf{z}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\C}{\\mathbf{C}} \\newcommand{\\rank}{\\textbf{rank}} \\] Inner Products 1 Inner Products: Cambridge University Press: Mathematics for Machine Learning, 2020. (pp. 72-74) \u21a9","title":"Inner Product Spaces"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.03_inner_products/#inner-products1","text":"Inner Products: Cambridge University Press: Mathematics for Machine Learning, 2020. (pp. 72-74) \u21a9","title":"Inner Products1"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.04_lengths_and_distances/","text":"\\[ \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\r}{\\mathbf{r}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\z}{\\mathbf{z}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\C}{\\mathbf{C}} \\newcommand{\\rank}{\\textbf{rank}} \\] Lengths and Distances 1 Lengths and Distances: Cambridge University Press: Mathematics for Machine Learning, 2020. (pp. 75-76) \u21a9","title":"Lengths and Distances"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.04_lengths_and_distances/#lengths-and-distances1","text":"Lengths and Distances: Cambridge University Press: Mathematics for Machine Learning, 2020. (pp. 75-76) \u21a9","title":"Lengths and Distances1"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.05_angles_and_orthogonality/","text":"\\[ \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\r}{\\mathbf{r}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\z}{\\mathbf{z}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\C}{\\mathbf{C}} \\newcommand{\\rank}{\\textbf{rank}} \\] Angles and Orthogonality 1 Calculating Angles Python In Euclidean space , a Euclidean vector is a geometric object that possesses both a magnitude and a direction. A vector can be pictured as an arrow. Its magnitude is its length, and its direction is the direction to which the arrow points. The magnitude of a vector a is denoted by \\(\\left\\| \\mathbf{a} \\right\\|\\) . The dot product of two Euclidean vectors a and b is defined by \\[\\mathbf{a}\\cdot\\mathbf{b}=\\|\\mathbf{a}\\|\\ \\|\\mathbf{b}\\|\\cos\\theta ,\\] where \\(\\theta\\) is the angle between \\(\\a\\) and \\(\\b\\) . And thus to find angle we can just: \\[\\theta = \\cos^{-1}\\left(\\dfrac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\|\\ \\|\\mathbf{b}\\|}\\right)\\] Fig: Calculate Angle Between Vectors; By Hongnan G. import numpy as np import math def calculate_angle ( a , b , c ): x1 , y1 = a x2 , y2 = b x3 , y3 = c angle = math . degrees ( math . atan2 ( y3 - y2 , x3 - x2 ) - math . atan2 ( y1 - y2 , x1 - x2 ) ) return angle + 360 if angle < 0 else angle def calculate_angle_using_dot_prod ( a : np . ndarray , b : np . ndarray , c : np . ndarray , return_as_degrees : bool = True , ): \"\"\"Takes in three points and calculates the angle between them using dot product. Let angle ABC as the angle between vectors AB and BC. This function calculates the angle ABC using the dot product formula: .. math:: BA = a - b BC = c - b \\cos(angle(ABC)) = \\dfrac{(BA \\cdot BC)}{(|BA||BC|)} Args: a (np.ndarray): Point a corresponding to A. b (np.ndarray): Point b corresponding to B. c (np.ndarray): Point c corresponding to C. return_as_degrees (bool): Returns angle in degrees if True else radians. Default: True. Shape: a (np.ndarray): (2, ) b (np.ndarray): (2, ) c (np.ndarray): (2, ) Examples: >>> import numpy as np >>> a = (6, 0) >>> b = (0, 0) >>> c = (6, 6) >>> calculate_angle_using_dot_prod(a,b,c) 45.0 \"\"\" # turn the points into numpy arrays a = np . asarray ( a ) b = np . asarray ( b ) c = np . asarray ( c ) ba = a - b bc = c - b cosine_angle = np . dot ( ba , bc ) / ( np . linalg . norm ( ba ) * np . linalg . norm ( bc ) ) angle = np . arccos ( cosine_angle ) if return_as_degrees : return np . degrees ( angle ) return angle a = ( 6 , 0 ) b = ( 0 , 0 ) c = ( 6 , 6 ) print ( calculate_angle ( a , b , c )) print ( calculate_angle_using_dot_prod ( a , b , c )) 45.0 45.0 https://stackoverflow.com/questions/58953047/issue-with-finding-angle-between-3-points-in-python https://math.stackexchange.com/questions/361412/finding-the-angle-between-three-points https://stackoverflow.com/questions/14066933/direct-way-of-computing-clockwise-angle-between-2-vectors Angles and Orthogonality: Cambridge University Press: Mathematics for Machine Learning, 2020. (pp. 76-78) \u21a9","title":"Angles and Orthogoanlity"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.05_angles_and_orthogonality/#angles-and-orthogonality1","text":"","title":"Angles and Orthogonality1"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.05_angles_and_orthogonality/#calculating-angles-python","text":"In Euclidean space , a Euclidean vector is a geometric object that possesses both a magnitude and a direction. A vector can be pictured as an arrow. Its magnitude is its length, and its direction is the direction to which the arrow points. The magnitude of a vector a is denoted by \\(\\left\\| \\mathbf{a} \\right\\|\\) . The dot product of two Euclidean vectors a and b is defined by \\[\\mathbf{a}\\cdot\\mathbf{b}=\\|\\mathbf{a}\\|\\ \\|\\mathbf{b}\\|\\cos\\theta ,\\] where \\(\\theta\\) is the angle between \\(\\a\\) and \\(\\b\\) . And thus to find angle we can just: \\[\\theta = \\cos^{-1}\\left(\\dfrac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\|\\ \\|\\mathbf{b}\\|}\\right)\\] Fig: Calculate Angle Between Vectors; By Hongnan G. import numpy as np import math def calculate_angle ( a , b , c ): x1 , y1 = a x2 , y2 = b x3 , y3 = c angle = math . degrees ( math . atan2 ( y3 - y2 , x3 - x2 ) - math . atan2 ( y1 - y2 , x1 - x2 ) ) return angle + 360 if angle < 0 else angle def calculate_angle_using_dot_prod ( a : np . ndarray , b : np . ndarray , c : np . ndarray , return_as_degrees : bool = True , ): \"\"\"Takes in three points and calculates the angle between them using dot product. Let angle ABC as the angle between vectors AB and BC. This function calculates the angle ABC using the dot product formula: .. math:: BA = a - b BC = c - b \\cos(angle(ABC)) = \\dfrac{(BA \\cdot BC)}{(|BA||BC|)} Args: a (np.ndarray): Point a corresponding to A. b (np.ndarray): Point b corresponding to B. c (np.ndarray): Point c corresponding to C. return_as_degrees (bool): Returns angle in degrees if True else radians. Default: True. Shape: a (np.ndarray): (2, ) b (np.ndarray): (2, ) c (np.ndarray): (2, ) Examples: >>> import numpy as np >>> a = (6, 0) >>> b = (0, 0) >>> c = (6, 6) >>> calculate_angle_using_dot_prod(a,b,c) 45.0 \"\"\" # turn the points into numpy arrays a = np . asarray ( a ) b = np . asarray ( b ) c = np . asarray ( c ) ba = a - b bc = c - b cosine_angle = np . dot ( ba , bc ) / ( np . linalg . norm ( ba ) * np . linalg . norm ( bc ) ) angle = np . arccos ( cosine_angle ) if return_as_degrees : return np . degrees ( angle ) return angle a = ( 6 , 0 ) b = ( 0 , 0 ) c = ( 6 , 6 ) print ( calculate_angle ( a , b , c )) print ( calculate_angle_using_dot_prod ( a , b , c )) 45.0 45.0 https://stackoverflow.com/questions/58953047/issue-with-finding-angle-between-3-points-in-python https://math.stackexchange.com/questions/361412/finding-the-angle-between-three-points https://stackoverflow.com/questions/14066933/direct-way-of-computing-clockwise-angle-between-2-vectors Angles and Orthogonality: Cambridge University Press: Mathematics for Machine Learning, 2020. (pp. 76-78) \u21a9","title":"Calculating Angles Python"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.06_orthogonal_subspaces/","text":"\\[ \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\q}{\\mathbf{q}} \\newcommand{\\r}{\\mathbf{r}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\z}{\\mathbf{z}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\C}{\\mathbf{C}} \\newcommand{\\Q}{\\mathbf{Q}} \\newcommand{\\U}{\\mathbf{U}} \\newcommand{\\V}{\\mathbf{V}} \\newcommand{\\W}{\\mathbf{W}} \\newcommand{\\rank}{\\textbf{rank}} \\newcommand{\\span}{\\textbf{span}} \\] Orthogonal Subspaces Definition (Orthogonal Subspaces) Note that is does not make sense to talk about the Orthogonality of one subspace. Given any two vector subspaces \\(\\V\\) and \\(\\W\\) , both necessarily have the same dimension in the sense that any elements in either subspace should have the same length, if not the dot (inner) product is not well defined, then we say that \\(\\V\\) and \\(\\W\\) are orthogonal subspaces if \\(\\v^\\top \\w = \\0\\) for all \\(\\v \\in \\V\\) and \\(\\w \\in \\W\\) . Example (Orthogonal Subspaces) The Nullspace and the Row Space are orthogonal subspaces. We will soon prove this. Theorem (Dimension implies Non-Orthogonality) This may be best explained using an example so that it sticks to memory intuitively. Consider \\(\\R^3\\) space and take 2 subspaces \\(\\V, \\W \\subseteq \\R^3\\) where both are 2D-planes (spanned by 2 linearly independent vectors) embedded in the \\(\\R^3\\) space. Then \\(\\V\\) and \\(\\W\\) can never be orthogonal to each other. The intuition is that no matter how you position 2 such 2D-planes, they can never be placed completely orthogonal to each other. This idea leads us to define another core concept, Orthogonal Complement. Once defined, you will then be able to prove why this is so. Definition (Orthogonal Subset) Let \\(\\Q \\subseteq \\V\\) be a subset in a subspace , then \\(\\Q\\) is an orthogonal subset of \\(\\V\\) if every element in \\(\\Q\\) is mutually orthogonal to each other, that is, \\(\\forall \\q_i, \\q_j \\in \\Q\\) , we have \\(\\q_i \\perp \\q_j\\) . Theorem (Orthogonal Subset is Linearly Independent) Let \\(\\V\\) be an inner product space , then any orthogonal subset of \\(\\V\\) consisting of non-zero vectors is linearly independent . Proof (Orthogonal Subset is Linearly Independent) 1 Intuitively, this must be true, just image the standard basis in \\(\\R^3\\) space, we can easily verify they are an orthogonal subset , and also have the intuition that these 3 vectors must be linearly independent. Orthogonal Complements Definition (Orthogonal Complements) Let \\(\\V \\in \\F^{D}\\) be a \\(D\\) -dimensional subspace and let \\(\\U \\subset \\V\\) be a \\(M\\) -dimensional subspace. Denote the set of vectors \\(\\{\\w_i\\}\\) to be all vectors that are orthogonal to \\(\\U\\) , then this set of vectors, denoted \\(\\U^\\perp\\) , is the orthogonal complement to \\(\\U\\) and is defined as: \\[ \\U^\\perp = \\left\\{\\w \\in \\V ~|~ \\langle \\w, \\u \\rangle = \\0 \\quad \\forall \\u \\in \\U \\right\\} \\] Theorem (Properties of Orthogonal Complements) Let \\(\\U\\) be a subset of a finite-dimensional inner product space \\(\\V\\) , then we have: Theorem (Intersection of Orthogonal Complements is Zero) \\[ \\U \\cap \\U^\\perp = \\{\\0\\} \\] The proof is relatively simple, consider a contradiction that there is some element \\(\\u\\) in the intersection of \\(\\U \\cap \\U^\\perp\\) that is not \\(\\0\\) , then \\(\\u \\in \\U\\) and \\(\\u \\in \\U^\\perp\\) , by the definition of Orthogonal Complement , we must have \\(\\u \\cdot \\u = \\0\\) , which is a contradiction since no such \\(\\u\\) can fulfill this. Theorem (Orthogonal Complement is a Subspace) If \\(\\U\\) is a non-empty subset of a finite-dimensional inner product space \\(\\V\\) , then \\(\\U^\\perp\\) is a subspace . Theorem (Element of Orthogonal Complement is Orthogonal to every Linear Combination of U) \\(\\U^\\perp = (\\span(\\U))^\\perp\\) which means a vector \\(\\u\\) belonging to the orthogonal complement of \\(\\U\\) if and only if \\(\\u\\) is orthogonal to every linear combination of vectors from \\(\\U\\) . Theorem (Othogonal Complement's Complement is Itself) \\[ (\\U^\\perp)^\\perp = \\U \\] Theorem (The Fundamental Theorem of Linear Algebra Part 2: Orthogonal Complements and The Four Fundamental Subspaces) We state straightaway that the row space and the kernel (right nullspace) are orthogonal complements, with some preliminary intuition below. More specifically, one should note that the row space of a matrix A is not only orthogonal to its nullspace, it also serves as a complement to it such that the union of them is the whole R^n space itself. I will now denote nullspace as N(A) and row space as R(A), with the associated matrix being m by n. Before the concept of orthogonality, one may take a while to realize that R(A) UNION N(A) = R^n. Even though the rank-nullity theorem tells us that their dimension of the nullspace and row space is n, it did not specifically say that the union of them is the whole Rn space (though intuitively they are if you stare at it long enough). But the main point is, with the orthogonal concept introduced, every vector in the nullspace is \"perpendicular\" to those in the row space, and that means that any vector n in N(A) is of a \"different axis/dimension\" as those in R(A), and this indicates linear independence of them. Now this means the basis vectors of N(A) and R(A) are linearly independent, and hence when unioned, they make up the whole R^n space. Row space and Null Space are Orthogonal Complements The row space and the null space of a matrix \\(\\A \\in \\F^{m \\times n}\\) are orthogonal complements . Proof (Orthogonal Subspace) We first show that both subspaces are orthogonal. Note that the null space of a matrix \\(\\A \\in F^{m \\times n}\\) is the set of all vectors \\(\\x\\) such that \\(\\A\\x = \\0\\) . We can also write \\(\\A\\x = \\0\\) as \\[ \\A\\x = \\begin{bmatrix} \\r_1 \\cdot \\x \\\\ \\r_2 \\cdot \\x \\\\ \\vdots \\\\ \\r_m \\cdot \\x \\end{bmatrix} \\] where \\(\\cdot\\) is the dot product and \\(\\r_i\\) the row vector \\(i\\) of \\(\\A\\) . Then one can easily see that \\(\\A\\x = \\0\\) if and only if every \\(\\r_i \\cdot \\x = 0\\) . Then it immediately follows that every \\(\\r_i\\) is orthogonal to \\(\\x\\) , and consequently, the nullspace and row space of \\(\\A\\) forms an orthogonal subspace . To see this, take any vector \\(\\r \\in R(\\A)\\) , and represent this \\(\\r = \\lambda_1 \\r_1 + ... + \\lambda_m \\r_m\\) , then \\[ \\begin{aligned} \\r \\cdot \\x &= (\\lambda_1 \\r_1 + ... + \\lambda_m \\r_m) \\cdot \\x \\\\ &= \\lambda_1 \\r_1 \\cdot \\x + ... + \\lambda_m \\r_m \\cdot \\x \\\\ &= \\0 + ... + \\0 \\\\ &= \\0 \\end{aligned} \\] So we have proved that if we take any element \\(\\r\\) in the row space of \\(\\A\\) , then \\(\\r\\x = \\0\\) for every \\(\\x \\in N(\\A)\\) . Since the dot product \\(\\cdot\\) is commutative, we do not need to show that for every element \\(\\x \\in N(\\A)\\) , it is orthogonal to every element in the row space of \\(\\A\\) . Proof (Orthogonal Complements) 2 We have proven that the row space \\(\\newcommand{\\R}{\\mathrm{R}} \\R(A)\\) and null space \\(\\newcommand{\\N}{\\mathrm{N}} \\N(A)\\) are orthogonal to each other; that is, \\(\\newcommand{\\r}{\\vec r} \\newcommand{\\n}{\\vec n} \\forall\\r\\in\\R(A)\\ \\forall\\n\\in\\N(A): \\r\\perp\\n\\) . Next, we show that they are complements of each other: \\[\\R(A)\\cap\\N(A)=\\left\\{\\vec0\\right\\}\\] Both of these criteria must be met for two subspaces to be orthogonal complements. Proof : Suppose we take an element \\(\\v \\in \\mathrm{R}(\\A) \\cap \\N(A)\\) , this means that \\(\\newcommand{\\v}{\\vec v} \\v\\in\\N(A)\\) and \\(\\v\\in\\R(A)\\) . Recall that if \\(\\n\\in\\N(A)\\) then \\( \\(\\n\\cdot\\r=0\\) \\) where \\(\\r\\in\\R(A)\\) . Now the element we took from their intersection \\(\\v\\) has this property: \\[\\v\\cdot\\v=0=\\left\\|\\v\\right\\|^2\\] We can use two ways from here, one is we know \\[\\v \\cdot \\v = \\begin{bmatrix} v_1^2 \\\\ v_2^2 \\\\ \\vdots \\\\ \\v_m^2 \\end{bmatrix}\\] and thus for this to be zero, then \\(v_i^2 = 0 \\implies v_i = 0 \\forall i\\) , hence \\(\\v\\) is the zero vector. Otherwise, since \\(\\left\\|\\v\\right\\|=0\\) , the vector \\(\\v\\) must be the zero vector. In any case, any vector \\(\\v\\) in both \\(\\R(A)\\) and \\(\\N(A)\\) must equal \\(\\vec0\\) . Therefore \\(\\R(A)\\cap\\N(A)=\\left\\{\\vec0\\right\\}\\) , and so by definition \\(\\R(A)\\) and \\(\\N(A)\\) are complementary subspaces as well as orthogonal. \\(\\blacksquare\\) Column space and Left Null Space are Orthogonal Complements This proof is similar with the previous one. Important Note Note that at this junction, we have to remind ourselves that given two subspaces \\(\\U\\) and \\(\\U^\\perp\\) in a vector space \\(\\V \\in \\F^n\\) , we can only tell that their intersection is zero, meaning they are disjoin, but we are not sure whether the union of them actually spans the whole \\(\\V\\) or not. Intuitively, it should, but we should soon see why in proof. Note the intuition is given earlier in The Fundamental Theorem of Linear Algebra Part 2. Theorem (Subspace and its Orthogonal Complement Forms a Disjoint Union) Let \\(\\V \\in \\F^n\\) be of \\(n\\) dimensions. Let \\(\\U\\) be a subspace of \\(\\V\\) and \\(\\U^\\perp\\) be the Orthogonal Complement of \\(\\U\\) , then we have \\[ \\U \\sqcup \\U^\\perp = \\V \\] In other words, the basis of \\(\\U\\) and \\(\\U^\\perp\\) span \\(\\V\\) . We do not use a formal proof here, instead we reason out with some geometric intuition. We should use the example of the row space and the kernel with symbols \\(R(\\A)\\) and \\(N(\\A)\\) respectively. First, the Rank-Nullity Theorem tells us that the dimensions of \\(R(\\A)\\) and \\(N(\\A)\\) is \\(n\\) . This helps because if they do not add up to \\(n\\) , then their basis certainly do not span \\(\\V\\) . Then, since Orthogonality implies that every vector is pairwise orthogonal, it means that geomtrically, both subspaces' basis vectors are all independent. You can think perpendicular means that the basis vectors all point in different axis. Thus, both subspace have pairwise linearly independent vectors. Finally, since the basis vectors of \\(R(\\A)\\) and \\(N(\\A)\\) when unioned together, are linearly independent, and the cardinality is \\(n\\) , it follows that this basis spans \\(\\V\\) . \\(\\blacksquare\\) Theorem (The Orthogonal Decomposition Theorem) 3 Let \\(\\U\\) be a subspace of a finite-dimensional inner product space \\(\\V\\) , then any vector \\(\\v \\in \\V\\) can be written uniquely in the form \\(\\v = \\u_1 + \\u_2\\) where \\(\\u_1 \\in \\U\\) and \\(\\u_2 \\in \\U^\\perp\\) . In direct sum , we have: \\[ \\V = \\U \\oplus \\U^\\perp \\] Corollary (Dimensions of Orthogonal Complements) 3 If \\(\\U\\) is a subsapce of an \\(n\\) -dimensional inner product space \\(\\V\\) , then \\(\\dim(\\U) + \\dim(\\U^\\perp) = n\\) . Proof (Dimensions of Orthogonal Complements) 3 Corollary (Dimensions of Four Fundamental Subspaces Revisited) 4 Theorem (Unique Row Space Solution to \\(\\mathbf{Ax=b}\\) ) Given any \\(\\b \\in C(\\A)\\) , there exists an unique member \\(\\r_0 \\in R(\\A)\\) such that \\(\\r_0\\) is a solution to \\(\\A\\x=\\b\\) , and this \\(\\r_0\\) is the solution (special) and no other solution can have a smaller length in the sense that \\(||\\x|| \\geq ||\\r_0||\\) for any solution \\(\\x\\) . Visualization (Row Space and Null Space Orthogonal) The plots and contents below are entirely credited to MacroAnalyst's GitHub Repo 5 . import matplotlib.pyplot as plt import numpy as np from mpl_toolkits.mplot3d import Axes3D import sympy as sy sy . init_printing () A = sy . Matrix ([[ 5 , 8 , 2 ], [ 10 , 16 , 4 ], [ 3 , 4 , 1 ]]); A \\(\\displaystyle \\left[\\begin{matrix}5 & 8 & 2\\\\10 & 16 & 4\\\\3 & 4 & 1\\end{matrix}\\right]\\) A . rref () \\(\\displaystyle \\left( \\left[\\begin{matrix}1 & 0 & 0\\\\0 & 1 & \\frac{1}{4}\\\\0 & 0 & 0\\end{matrix}\\right], \\ \\left( 0, \\ 1\\right)\\right)\\) The basis of row space of \\(A\\) is \\((1, 0, 0)\\) and \\((0, 1, .25)\\) .And the \\(\\text{Row}A\\) is \\[ \\text{Row}A= s\\left[ \\begin{matrix} 1 \\\\ 0\\\\ 0 \\end{matrix} \\right]+ t\\left[ \\begin{matrix} 0 \\\\ 1\\\\ 0.25 \\end{matrix} \\right] \\] The \\(\\text{Nul}A\\) is $$ \\left[ \\begin{matrix} x_1 \\ x_2\\ x_3 \\end{matrix} \\right]= x_3 \\left[ \\begin{matrix} 0 \\ -.25\\ 1 \\end{matrix} \\right] $$ Now we can visualize their relations geometrically. Again keep in mind that Matplotlib does not render 3D properly, so you need some imagination as well. Here is what we observe. The \\(\\text{Row}A\\) is a plane and \\(\\text{Nul}A\\) is a line which is perpendicular to the plane. It is easy to grasp the idea if you notice that in a homogeneous system \\(Ab = \\mathbf{0}\\) , it breaks down into many dot products \\[ Ab =\\left[ \\begin{matrix} A_{1i}\\cdot b \\\\ A_{2i}\\cdot b\\\\ A_{3i}\\cdot b \\end{matrix} \\right] \\] where \\(A_{1i}, A_{2i}, A_{3i}\\) are the rows of \\(A\\) . In later chapters we will prove when the dot product of two vectors equals zero, which means geometrically they are perpendicular. % matplotlib inline s = np . linspace ( - 1 , 1 , 10 ) t = np . linspace ( - 1 , 1 , 10 ) S , T = np . meshgrid ( s , t ) X = S Y = T Z = T * .25 fig = plt . figure ( figsize = ( 8 , 8 )) ax = fig . add_subplot ( 111 , projection = '3d' ) ax . plot_surface ( X , Y , Z , alpha = .9 , cmap = plt . cm . coolwarm ) x3 = np . linspace ( - 1 , 1 , 10 ) x1 = 0 * x3 x2 = - .25 * x3 ax . plot ( x1 , x2 , x3 , lw = 5 ) ax . set_xlabel ( 'x-axis' , size = 18 ) ax . set_ylabel ( 'y-axis' , size = 18 ) ax . set_zlabel ( 'z-axis' , size = 18 ) ax . axis ([ - 1 , 1 , - 1 , 1 ]) ax . text ( x = 1 , y = - 1 , z = - .25 , s = r '$Row\\ A$' , size = 17 ) ax . text ( 0 , - .25 , 1 , s = r '$Nul\\ A$' , size = 17 ) ax . view_init ( 7 , 20 ) plt . show () Orthogonal Subset is Linearly Independent: Henry Ricardo: A Modern Introduction to Linear Algebra, 2009. (pp. 477) \u21a9 Row space and Null space are Orthogonal Complements: How would one prove that the row space and null space are orthogonal compliments of each other? \u21a9 Proof (Dimensions of Orthogonal Complements): Henry Ricardo: A Modern Introduction to Linear Algebra, 2009. (pp. 524) \u21a9 \u21a9 \u21a9 (Dimensions of Four Fundamental Subspaces Revisited): Henry Ricardo: A Modern Introduction to Linear Algebra, 2009. (pp. 525) \u21a9 Row Space and Null Space Orthogonal: Row Space and Null Space Orthogonal \u21a9","title":"Orthogonal Subspace"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.06_orthogonal_subspaces/#orthogonal-subspaces","text":"","title":"Orthogonal Subspaces"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.06_orthogonal_subspaces/#definition-orthogonal-subspaces","text":"Note that is does not make sense to talk about the Orthogonality of one subspace. Given any two vector subspaces \\(\\V\\) and \\(\\W\\) , both necessarily have the same dimension in the sense that any elements in either subspace should have the same length, if not the dot (inner) product is not well defined, then we say that \\(\\V\\) and \\(\\W\\) are orthogonal subspaces if \\(\\v^\\top \\w = \\0\\) for all \\(\\v \\in \\V\\) and \\(\\w \\in \\W\\) .","title":"Definition (Orthogonal Subspaces)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.06_orthogonal_subspaces/#example-orthogonal-subspaces","text":"The Nullspace and the Row Space are orthogonal subspaces. We will soon prove this.","title":"Example (Orthogonal Subspaces)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.06_orthogonal_subspaces/#theorem-dimension-implies-non-orthogonality","text":"This may be best explained using an example so that it sticks to memory intuitively. Consider \\(\\R^3\\) space and take 2 subspaces \\(\\V, \\W \\subseteq \\R^3\\) where both are 2D-planes (spanned by 2 linearly independent vectors) embedded in the \\(\\R^3\\) space. Then \\(\\V\\) and \\(\\W\\) can never be orthogonal to each other. The intuition is that no matter how you position 2 such 2D-planes, they can never be placed completely orthogonal to each other. This idea leads us to define another core concept, Orthogonal Complement. Once defined, you will then be able to prove why this is so.","title":"Theorem (Dimension implies Non-Orthogonality)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.06_orthogonal_subspaces/#definition-orthogonal-subset","text":"Let \\(\\Q \\subseteq \\V\\) be a subset in a subspace , then \\(\\Q\\) is an orthogonal subset of \\(\\V\\) if every element in \\(\\Q\\) is mutually orthogonal to each other, that is, \\(\\forall \\q_i, \\q_j \\in \\Q\\) , we have \\(\\q_i \\perp \\q_j\\) .","title":"Definition (Orthogonal Subset)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.06_orthogonal_subspaces/#theorem-orthogonal-subset-is-linearly-independent","text":"Let \\(\\V\\) be an inner product space , then any orthogonal subset of \\(\\V\\) consisting of non-zero vectors is linearly independent .","title":"Theorem (Orthogonal Subset is Linearly Independent)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.06_orthogonal_subspaces/#proof-orthogonal-subset-is-linearly-independent1","text":"Intuitively, this must be true, just image the standard basis in \\(\\R^3\\) space, we can easily verify they are an orthogonal subset , and also have the intuition that these 3 vectors must be linearly independent.","title":"Proof (Orthogonal Subset is Linearly Independent)1"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.06_orthogonal_subspaces/#orthogonal-complements","text":"","title":"Orthogonal Complements"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.06_orthogonal_subspaces/#definition-orthogonal-complements","text":"Let \\(\\V \\in \\F^{D}\\) be a \\(D\\) -dimensional subspace and let \\(\\U \\subset \\V\\) be a \\(M\\) -dimensional subspace. Denote the set of vectors \\(\\{\\w_i\\}\\) to be all vectors that are orthogonal to \\(\\U\\) , then this set of vectors, denoted \\(\\U^\\perp\\) , is the orthogonal complement to \\(\\U\\) and is defined as: \\[ \\U^\\perp = \\left\\{\\w \\in \\V ~|~ \\langle \\w, \\u \\rangle = \\0 \\quad \\forall \\u \\in \\U \\right\\} \\]","title":"Definition (Orthogonal Complements)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.06_orthogonal_subspaces/#theorem-properties-of-orthogonal-complements","text":"Let \\(\\U\\) be a subset of a finite-dimensional inner product space \\(\\V\\) , then we have:","title":"Theorem (Properties of Orthogonal Complements)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.06_orthogonal_subspaces/#theorem-intersection-of-orthogonal-complements-is-zero","text":"\\[ \\U \\cap \\U^\\perp = \\{\\0\\} \\] The proof is relatively simple, consider a contradiction that there is some element \\(\\u\\) in the intersection of \\(\\U \\cap \\U^\\perp\\) that is not \\(\\0\\) , then \\(\\u \\in \\U\\) and \\(\\u \\in \\U^\\perp\\) , by the definition of Orthogonal Complement , we must have \\(\\u \\cdot \\u = \\0\\) , which is a contradiction since no such \\(\\u\\) can fulfill this.","title":"Theorem (Intersection of Orthogonal Complements is Zero)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.06_orthogonal_subspaces/#theorem-orthogonal-complement-is-a-subspace","text":"If \\(\\U\\) is a non-empty subset of a finite-dimensional inner product space \\(\\V\\) , then \\(\\U^\\perp\\) is a subspace .","title":"Theorem (Orthogonal Complement is a Subspace)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.06_orthogonal_subspaces/#theorem-element-of-orthogonal-complement-is-orthogonal-to-every-linear-combination-of-u","text":"\\(\\U^\\perp = (\\span(\\U))^\\perp\\) which means a vector \\(\\u\\) belonging to the orthogonal complement of \\(\\U\\) if and only if \\(\\u\\) is orthogonal to every linear combination of vectors from \\(\\U\\) .","title":"Theorem (Element of Orthogonal Complement is Orthogonal to every Linear Combination of U)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.06_orthogonal_subspaces/#theorem-othogonal-complements-complement-is-itself","text":"\\[ (\\U^\\perp)^\\perp = \\U \\]","title":"Theorem (Othogonal Complement's Complement is Itself)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.06_orthogonal_subspaces/#theorem-the-fundamental-theorem-of-linear-algebra-part-2-orthogonal-complements-and-the-four-fundamental-subspaces","text":"We state straightaway that the row space and the kernel (right nullspace) are orthogonal complements, with some preliminary intuition below. More specifically, one should note that the row space of a matrix A is not only orthogonal to its nullspace, it also serves as a complement to it such that the union of them is the whole R^n space itself. I will now denote nullspace as N(A) and row space as R(A), with the associated matrix being m by n. Before the concept of orthogonality, one may take a while to realize that R(A) UNION N(A) = R^n. Even though the rank-nullity theorem tells us that their dimension of the nullspace and row space is n, it did not specifically say that the union of them is the whole Rn space (though intuitively they are if you stare at it long enough). But the main point is, with the orthogonal concept introduced, every vector in the nullspace is \"perpendicular\" to those in the row space, and that means that any vector n in N(A) is of a \"different axis/dimension\" as those in R(A), and this indicates linear independence of them. Now this means the basis vectors of N(A) and R(A) are linearly independent, and hence when unioned, they make up the whole R^n space.","title":"Theorem (The Fundamental Theorem of Linear Algebra Part 2: Orthogonal Complements and The Four Fundamental Subspaces)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.06_orthogonal_subspaces/#row-space-and-null-space-are-orthogonal-complements","text":"The row space and the null space of a matrix \\(\\A \\in \\F^{m \\times n}\\) are orthogonal complements .","title":"Row space and Null Space are Orthogonal Complements"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.06_orthogonal_subspaces/#proof-orthogonal-subspace","text":"We first show that both subspaces are orthogonal. Note that the null space of a matrix \\(\\A \\in F^{m \\times n}\\) is the set of all vectors \\(\\x\\) such that \\(\\A\\x = \\0\\) . We can also write \\(\\A\\x = \\0\\) as \\[ \\A\\x = \\begin{bmatrix} \\r_1 \\cdot \\x \\\\ \\r_2 \\cdot \\x \\\\ \\vdots \\\\ \\r_m \\cdot \\x \\end{bmatrix} \\] where \\(\\cdot\\) is the dot product and \\(\\r_i\\) the row vector \\(i\\) of \\(\\A\\) . Then one can easily see that \\(\\A\\x = \\0\\) if and only if every \\(\\r_i \\cdot \\x = 0\\) . Then it immediately follows that every \\(\\r_i\\) is orthogonal to \\(\\x\\) , and consequently, the nullspace and row space of \\(\\A\\) forms an orthogonal subspace . To see this, take any vector \\(\\r \\in R(\\A)\\) , and represent this \\(\\r = \\lambda_1 \\r_1 + ... + \\lambda_m \\r_m\\) , then \\[ \\begin{aligned} \\r \\cdot \\x &= (\\lambda_1 \\r_1 + ... + \\lambda_m \\r_m) \\cdot \\x \\\\ &= \\lambda_1 \\r_1 \\cdot \\x + ... + \\lambda_m \\r_m \\cdot \\x \\\\ &= \\0 + ... + \\0 \\\\ &= \\0 \\end{aligned} \\] So we have proved that if we take any element \\(\\r\\) in the row space of \\(\\A\\) , then \\(\\r\\x = \\0\\) for every \\(\\x \\in N(\\A)\\) . Since the dot product \\(\\cdot\\) is commutative, we do not need to show that for every element \\(\\x \\in N(\\A)\\) , it is orthogonal to every element in the row space of \\(\\A\\) .","title":"Proof (Orthogonal Subspace)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.06_orthogonal_subspaces/#proof-orthogonal-complements2","text":"We have proven that the row space \\(\\newcommand{\\R}{\\mathrm{R}} \\R(A)\\) and null space \\(\\newcommand{\\N}{\\mathrm{N}} \\N(A)\\) are orthogonal to each other; that is, \\(\\newcommand{\\r}{\\vec r} \\newcommand{\\n}{\\vec n} \\forall\\r\\in\\R(A)\\ \\forall\\n\\in\\N(A): \\r\\perp\\n\\) . Next, we show that they are complements of each other: \\[\\R(A)\\cap\\N(A)=\\left\\{\\vec0\\right\\}\\] Both of these criteria must be met for two subspaces to be orthogonal complements. Proof : Suppose we take an element \\(\\v \\in \\mathrm{R}(\\A) \\cap \\N(A)\\) , this means that \\(\\newcommand{\\v}{\\vec v} \\v\\in\\N(A)\\) and \\(\\v\\in\\R(A)\\) . Recall that if \\(\\n\\in\\N(A)\\) then \\( \\(\\n\\cdot\\r=0\\) \\) where \\(\\r\\in\\R(A)\\) . Now the element we took from their intersection \\(\\v\\) has this property: \\[\\v\\cdot\\v=0=\\left\\|\\v\\right\\|^2\\] We can use two ways from here, one is we know \\[\\v \\cdot \\v = \\begin{bmatrix} v_1^2 \\\\ v_2^2 \\\\ \\vdots \\\\ \\v_m^2 \\end{bmatrix}\\] and thus for this to be zero, then \\(v_i^2 = 0 \\implies v_i = 0 \\forall i\\) , hence \\(\\v\\) is the zero vector. Otherwise, since \\(\\left\\|\\v\\right\\|=0\\) , the vector \\(\\v\\) must be the zero vector. In any case, any vector \\(\\v\\) in both \\(\\R(A)\\) and \\(\\N(A)\\) must equal \\(\\vec0\\) . Therefore \\(\\R(A)\\cap\\N(A)=\\left\\{\\vec0\\right\\}\\) , and so by definition \\(\\R(A)\\) and \\(\\N(A)\\) are complementary subspaces as well as orthogonal. \\(\\blacksquare\\)","title":"Proof (Orthogonal Complements)2"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.06_orthogonal_subspaces/#column-space-and-left-null-space-are-orthogonal-complements","text":"This proof is similar with the previous one.","title":"Column space and Left Null Space are Orthogonal Complements"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.06_orthogonal_subspaces/#important-note","text":"Note that at this junction, we have to remind ourselves that given two subspaces \\(\\U\\) and \\(\\U^\\perp\\) in a vector space \\(\\V \\in \\F^n\\) , we can only tell that their intersection is zero, meaning they are disjoin, but we are not sure whether the union of them actually spans the whole \\(\\V\\) or not. Intuitively, it should, but we should soon see why in proof. Note the intuition is given earlier in The Fundamental Theorem of Linear Algebra Part 2.","title":"Important Note"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.06_orthogonal_subspaces/#theorem-subspace-and-its-orthogonal-complement-forms-a-disjoint-union","text":"Let \\(\\V \\in \\F^n\\) be of \\(n\\) dimensions. Let \\(\\U\\) be a subspace of \\(\\V\\) and \\(\\U^\\perp\\) be the Orthogonal Complement of \\(\\U\\) , then we have \\[ \\U \\sqcup \\U^\\perp = \\V \\] In other words, the basis of \\(\\U\\) and \\(\\U^\\perp\\) span \\(\\V\\) . We do not use a formal proof here, instead we reason out with some geometric intuition. We should use the example of the row space and the kernel with symbols \\(R(\\A)\\) and \\(N(\\A)\\) respectively. First, the Rank-Nullity Theorem tells us that the dimensions of \\(R(\\A)\\) and \\(N(\\A)\\) is \\(n\\) . This helps because if they do not add up to \\(n\\) , then their basis certainly do not span \\(\\V\\) . Then, since Orthogonality implies that every vector is pairwise orthogonal, it means that geomtrically, both subspaces' basis vectors are all independent. You can think perpendicular means that the basis vectors all point in different axis. Thus, both subspace have pairwise linearly independent vectors. Finally, since the basis vectors of \\(R(\\A)\\) and \\(N(\\A)\\) when unioned together, are linearly independent, and the cardinality is \\(n\\) , it follows that this basis spans \\(\\V\\) . \\(\\blacksquare\\)","title":"Theorem (Subspace and its Orthogonal Complement Forms a Disjoint Union)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.06_orthogonal_subspaces/#theorem-the-orthogonal-decomposition-theorem3","text":"Let \\(\\U\\) be a subspace of a finite-dimensional inner product space \\(\\V\\) , then any vector \\(\\v \\in \\V\\) can be written uniquely in the form \\(\\v = \\u_1 + \\u_2\\) where \\(\\u_1 \\in \\U\\) and \\(\\u_2 \\in \\U^\\perp\\) . In direct sum , we have: \\[ \\V = \\U \\oplus \\U^\\perp \\]","title":"Theorem (The Orthogonal Decomposition Theorem)3"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.06_orthogonal_subspaces/#corollary-dimensions-of-orthogonal-complements3","text":"If \\(\\U\\) is a subsapce of an \\(n\\) -dimensional inner product space \\(\\V\\) , then \\(\\dim(\\U) + \\dim(\\U^\\perp) = n\\) .","title":"Corollary (Dimensions of Orthogonal Complements)3"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.06_orthogonal_subspaces/#proof-dimensions-of-orthogonal-complements3","text":"","title":"Proof (Dimensions of Orthogonal Complements)3"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.06_orthogonal_subspaces/#corollary-dimensions-of-four-fundamental-subspaces-revisited4","text":"","title":"Corollary (Dimensions of Four Fundamental Subspaces Revisited)4"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.06_orthogonal_subspaces/#theorem-unique-row-space-solution-to-mathbfaxb","text":"Given any \\(\\b \\in C(\\A)\\) , there exists an unique member \\(\\r_0 \\in R(\\A)\\) such that \\(\\r_0\\) is a solution to \\(\\A\\x=\\b\\) , and this \\(\\r_0\\) is the solution (special) and no other solution can have a smaller length in the sense that \\(||\\x|| \\geq ||\\r_0||\\) for any solution \\(\\x\\) .","title":"Theorem (Unique Row Space Solution to \\(\\mathbf{Ax=b}\\))"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.06_orthogonal_subspaces/#visualization-row-space-and-null-space-orthogonal","text":"The plots and contents below are entirely credited to MacroAnalyst's GitHub Repo 5 . import matplotlib.pyplot as plt import numpy as np from mpl_toolkits.mplot3d import Axes3D import sympy as sy sy . init_printing () A = sy . Matrix ([[ 5 , 8 , 2 ], [ 10 , 16 , 4 ], [ 3 , 4 , 1 ]]); A \\(\\displaystyle \\left[\\begin{matrix}5 & 8 & 2\\\\10 & 16 & 4\\\\3 & 4 & 1\\end{matrix}\\right]\\) A . rref () \\(\\displaystyle \\left( \\left[\\begin{matrix}1 & 0 & 0\\\\0 & 1 & \\frac{1}{4}\\\\0 & 0 & 0\\end{matrix}\\right], \\ \\left( 0, \\ 1\\right)\\right)\\) The basis of row space of \\(A\\) is \\((1, 0, 0)\\) and \\((0, 1, .25)\\) .And the \\(\\text{Row}A\\) is \\[ \\text{Row}A= s\\left[ \\begin{matrix} 1 \\\\ 0\\\\ 0 \\end{matrix} \\right]+ t\\left[ \\begin{matrix} 0 \\\\ 1\\\\ 0.25 \\end{matrix} \\right] \\] The \\(\\text{Nul}A\\) is $$ \\left[ \\begin{matrix} x_1 \\ x_2\\ x_3 \\end{matrix} \\right]= x_3 \\left[ \\begin{matrix} 0 \\ -.25\\ 1 \\end{matrix} \\right] $$ Now we can visualize their relations geometrically. Again keep in mind that Matplotlib does not render 3D properly, so you need some imagination as well. Here is what we observe. The \\(\\text{Row}A\\) is a plane and \\(\\text{Nul}A\\) is a line which is perpendicular to the plane. It is easy to grasp the idea if you notice that in a homogeneous system \\(Ab = \\mathbf{0}\\) , it breaks down into many dot products \\[ Ab =\\left[ \\begin{matrix} A_{1i}\\cdot b \\\\ A_{2i}\\cdot b\\\\ A_{3i}\\cdot b \\end{matrix} \\right] \\] where \\(A_{1i}, A_{2i}, A_{3i}\\) are the rows of \\(A\\) . In later chapters we will prove when the dot product of two vectors equals zero, which means geometrically they are perpendicular. % matplotlib inline s = np . linspace ( - 1 , 1 , 10 ) t = np . linspace ( - 1 , 1 , 10 ) S , T = np . meshgrid ( s , t ) X = S Y = T Z = T * .25 fig = plt . figure ( figsize = ( 8 , 8 )) ax = fig . add_subplot ( 111 , projection = '3d' ) ax . plot_surface ( X , Y , Z , alpha = .9 , cmap = plt . cm . coolwarm ) x3 = np . linspace ( - 1 , 1 , 10 ) x1 = 0 * x3 x2 = - .25 * x3 ax . plot ( x1 , x2 , x3 , lw = 5 ) ax . set_xlabel ( 'x-axis' , size = 18 ) ax . set_ylabel ( 'y-axis' , size = 18 ) ax . set_zlabel ( 'z-axis' , size = 18 ) ax . axis ([ - 1 , 1 , - 1 , 1 ]) ax . text ( x = 1 , y = - 1 , z = - .25 , s = r '$Row\\ A$' , size = 17 ) ax . text ( 0 , - .25 , 1 , s = r '$Nul\\ A$' , size = 17 ) ax . view_init ( 7 , 20 ) plt . show () Orthogonal Subset is Linearly Independent: Henry Ricardo: A Modern Introduction to Linear Algebra, 2009. (pp. 477) \u21a9 Row space and Null space are Orthogonal Complements: How would one prove that the row space and null space are orthogonal compliments of each other? \u21a9 Proof (Dimensions of Orthogonal Complements): Henry Ricardo: A Modern Introduction to Linear Algebra, 2009. (pp. 524) \u21a9 \u21a9 \u21a9 (Dimensions of Four Fundamental Subspaces Revisited): Henry Ricardo: A Modern Introduction to Linear Algebra, 2009. (pp. 525) \u21a9 Row Space and Null Space Orthogonal: Row Space and Null Space Orthogonal \u21a9","title":"Visualization (Row Space and Null Space Orthogonal)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.07_orthogonal_projections/","text":"\\[ \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\r}{\\mathbf{r}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\z}{\\mathbf{z}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\C}{\\mathbf{C}} \\newcommand{\\P}{\\mathbf{P}} \\newcommand{\\U}{\\mathbf{U}} \\newcommand{\\V}{\\mathbf{V}} \\newcommand{\\rank}{\\textbf{rank}} \\] Orthogonal Projections Most derivations are written in my Ipad. Algebraic Definition (Orthogonal Projections) Let \\(\\V\\) be the ambient vector space. A projection on a vector subspace \\(\\U \\subseteq \\V\\) is a linear mapping \\(\\pi: \\V \\to \\U\\) such that \\(\\pi^2 = \\pi\\) . If \\(\\V\\) is an inner product space , then \\(\\pi\\) can be called an orthogonal projection . Definition (Projection Matrix) We can verify that \\(\\pi\\) is indeed a linear transformaton . Recall that linear transformation can be expressed by transformation matrices . Thus we define projection matrices to be \\(\\P_\\pi\\) such that \\(\\P_\\pi^2 = \\P\\) . Derivation of Projection onto a Line \\(U\\) 1 Projection onto the x-axis In page 84, the author said if the vector \\(\\x\\) is of unit length, then projecting on the horizontal axis yields a projection vector to be \\(\\cos(\\omega)\\) . This may be confusing at first if you derive it using formula since we see that \\[ \\pi_{U}(\\x) = \\dfrac{\\b^\\top\\x}{|| \\b ||^2}\\b = \\dfrac{||\\b|| ||\\x|| \\cos(\\omega)}{||\\b||^2} \\b = \\cos(\\omega) ||\\x|| \\dfrac{\\b}{||\\b||} \\overset{||\\x|| = 1}{=} \\cos(\\omega) \\dfrac{\\b}{||\\b||} \\] where did the unit vector \\(\\hat{\\b} = \\frac{\\b}{||\\b||}\\) go? The confusion lies in two folds, one is author mentioned that this is only true when projecting onto the horizontal axis (x-axis), and secondly, the abuse of notation of vector where I misunderstood \\(\\cos(\\omega)\\) as the \"projection vector\". In fact, if we are projecting on the horizontal axis, then the basis vector \\(\\b\\) is just \\(\\begin{bmatrix}1 \\\\0 \\end{bmatrix}\\) and we have the projection vector to be actually \\[ \\pi_{U}(\\x) = \\cos(\\omega) \\dfrac{\\b}{||\\b||} = \\cos(\\omega) \\begin{bmatrix}1 \\\\0 \\end{bmatrix} = \\begin{bmatrix}\\cos(\\omega) \\\\0 \\end{bmatrix} \\] and so when mentioned loosely, we can say that the projection vector is just \\(\\cos(\\omega)\\) . Derivation of Projection onto a General Subspace \\(U\\) 2 3 The derivation using orthogonal complement provides a more intuitive understanding! Projection Onto Lower Dimensional Subspace The example here will tell you that given a vector \\(\\x \\in \\R^3\\) , it can be projected onto a lower dimensional subspace with minimal information loss since their distance is lowest. One thing to not get confused is that the projected vector is still in \\(\\R^3\\) , but it exists in a lower dimensional subspace \\(\\U \\subset \\R^3\\) embedded in \\(\\R^3\\) with 2 dimensions. Python Plot (A Visualization of Projection) Suppose we have \\(\\u = \\begin{bmatrix} 4 \\\\ 5 \\end{bmatrix}\\) , \\(\\v = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}\\) . Consider the subspace \\(\\U\\) spanned by \\(\\v\\) . We first plot the graph without the projection vector. The plots and contents (including the visualization of the decomposition) below are entirely credited to MacroAnalyst's GitHub Repo 5 . import matplotlib.pyplot as plt import matplotlib as mpl import numpy as np from mpl_toolkits.mplot3d import Axes3D import scipy as sp import scipy.linalg import scipy.spatial import sympy as sy sy . init_printing () fig , ax = plt . subplots ( figsize = ( 12 , 12 )) vects = np . array ([[ 4 , 5 ], [ 2 , 1 ]]) colr = [ \"red\" , \"blue\" ] cordt = [ \"$(4, 5)$\" , \"$(2, 1)$\" ] vec_name = [ \"$\\mathbf {u} $\" , \"$\\mathbf {v} $\" ] for i in range ( 2 ): ax . arrow ( 0 , 0 , vects [ i ][ 0 ], vects [ i ][ 1 ], color = colr [ i ], width = 0.03 , length_includes_head = True , head_width = 0.1 , # default: 3*width head_length = 0.2 , overhang = 0.4 , ) ax . text ( x = vects [ i ][ 0 ], y = vects [ i ][ 1 ], s = cordt [ i ], size = 15 ) ax . text ( x = vects [ i ][ 0 ] / 2 , y = vects [ i ][ 1 ] / 2 , s = vec_name [ i ], size = 22 ) ################################### Subspace L ############################ x = np . linspace ( 0 , 8.1 ) y = 1 / 2 * x ax . plot ( x , y , lw = 3 , color = \"red\" , alpha = 0.5 ) ax . text ( x = 6.5 , y = 3 , s = \"$L = \\operatorname{Span(\\mathbf {v} )}$\" , size = 19 ) ax . axis ([ 0 , 8 , 0 , 8 ]) ax . grid () Next, we plot the projection vector \\(\\pi_{\\U}(\\u)\\) onto \\(\\U\\) . u = np . array ([ 4 , 5 ]) v = np . array ([ 2 , 1 ]) alpha = ( u @ v ) / ( v @ v ) # the lambda coordinates print ( alpha ) proj_vec = alpha * v print ( proj_vec ) 2.6 [5.2 2.6] fig , ax = plt . subplots ( figsize = ( 12 , 12 )) vects = np . array ([[ 4 , 5 ], [ 2 , 1 ], [ 5.2 , 2.6 ]]) colr = [ \"red\" , \"blue\" , \"green\" ] cordt = [ \"$(4, 5)$\" , \"$(2, 1)$\" , \"(5.2, 2.6)\" ] vec_name = [ \"$\\mathbf {u} $\" , \"$\\mathbf {v} $\" , r \"$\\pi_{\\mathbf {U} }(\\mathbf {u} ) = \\alpha\\mathbf {v} $\" , ] for i in range ( 3 ): ax . arrow ( 0 , 0 , vects [ i ][ 0 ], vects [ i ][ 1 ], color = colr [ i ], width = 0.03 , length_includes_head = True , head_width = 0.1 , # default: 3*width head_length = 0.2 , overhang = 0.4 , zorder =- i , ) ax . text ( x = vects [ i ][ 0 ], y = vects [ i ][ 1 ], s = cordt [ i ], size = 19 ) ax . text ( x = vects [ i ][ 0 ] / 2 , y = vects [ i ][ 1 ] / 2 , s = vec_name [ i ], size = 22 ) ##################################### Components of y orthogonal to u ########################## point1 = [ 4 , 5 ] point2 = [ 5.2 , 2.6 ] line1 = np . array ([ point1 , point2 ]) ax . plot ( line1 [:, 0 ], line1 [:, 1 ], c = \"k\" , lw = 3.5 , alpha = 0.5 , ls = \"--\" ) ax . text ( 4.7 , 3.8 , \"$\\mathbf {z} $\" , size = 22 ) ################################### Subspace L ############################ x = np . linspace ( 0 , 8.1 ) y = 1 / 2 * x ax . plot ( x , y , lw = 3 , color = \"red\" , alpha = 0.5 , zorder =- 3 ) ax . text ( x = 6.5 , y = 3 , s = \"$\\mathbf {U} = \\operatorname{Span(\\mathbf {v} )}$\" , size = 19 ) ax . axis ([ 0 , 8 , 0 , 8 ]) ax . grid () #################################### Formula ################################ ax . text ( x = 1 , y = 7 , s = r \"$(\\mathbf {u} - \\alpha \\mathbf {v} )\\cdot \\mathbf {v} = 0$\" , size = 22 , color = \"b\" , ) ax . text ( x = 1 , y = 6.5 , s = r \"$\\alpha = \\frac{\\mathbf {u} \\cdot \\mathbf {v} }{\\mathbf {v} \\cdot \\mathbf {v} }$\" , size = 22 , color = \"b\" , ) ax . text ( x = 1 , y = 6 , s = r \"$\\operatorname {proj} _{\\mathbf {U} }\\mathbf {u} =\\frac{\\mathbf {u} \\cdot\\mathbf {v} }{\\mathbf {v} \\cdot\\mathbf {v} }\\mathbf {v} $\" , size = 22 , color = \"b\" , ) plt . show () Python Plot (A Visualization of Orthogonal Decomposition) To generalize the orthogonal projection in the higher dimension \\(\\mathbb{R}^n\\) , we summarize the idea into the orthogonal decomposition theorem . Let \\(W\\) be a subspace of \\(\\mathbb{R}^{n}\\) . Then each \\(\\mathbf{y}\\) in \\(\\mathbb{R}^{n}\\) can be written uniquely in the form $$ \\mathbf{y}=\\hat{\\mathbf{y}}+\\mathbf{z} $$ where \\(\\hat{\\mathbf{y}}\\) is in \\(W\\) and \\(\\mathbf{z}\\) is in \\(W^{\\perp} .\\) In fact, if \\(\\left\\{\\mathbf{u}_{1}, \\ldots, \\mathbf{u}_{p}\\right\\}\\) is any orthogonal basis of \\(W,\\) then $$ \\hat{\\mathbf{y}}=\\frac{\\mathbf{y} \\cdot \\mathbf{u} {1}}{\\mathbf{u} \\cdot \\mathbf{u} {1}} \\mathbf{u} +\\cdots+\\frac{\\mathbf{y} \\cdot \\mathbf{u} {p}}{\\mathbf{u} \\cdot \\mathbf{u} {p}} \\mathbf{u} $$ and \\(\\mathbf{z}=\\mathbf{y}-\\hat{\\mathbf{y}}\\) . In \\(\\mathbb{R}^{2}\\) , we project \\(\\mathbf{y}\\) onto subspace \\(L\\) which is spanned by \\(\\mathbf{u}\\) , here we generalize the formula for \\(\\mathbb{R}^{n}\\) , that \\(\\mathbf{y}\\) is projected onto \\(W\\) which is spanned by \\(\\left\\{\\mathbf{u}_{1}, \\ldots, \\mathbf{u}_{p}\\right\\}\\) . A Visual Example in \\(\\mathbb{R}^{3}\\) A subspace \\(W=\\operatorname{Span}\\left\\{\\mathbf{u}_{1}, \\mathbf{u}_{2}\\right\\}\\) , and a vector \\(\\mathbf{y}\\) is not in \\(W\\) , decompose \\(\\mathbf{y}\\) into \\(\\hat{\\mathbf{y}} + \\mathbf{z}\\) , and plot them. where \\[\\mathbf{u}_{1}=\\left[\\begin{array}{r} 2 \\\\ 5 \\\\ -1 \\end{array}\\right], \\mathbf{u}_{2}=\\left[\\begin{array}{r} -2 \\\\ 1 \\\\ 1 \\end{array}\\right], \\text { and } \\mathbf{y}=\\left[\\begin{array}{l} 1 \\\\ 2 \\\\ 3 \\end{array}\\right]\\] The projection onto \\(W\\) in \\(\\mathbb{R}^3\\) is \\[ \\hat{\\mathbf{y}}=\\frac{\\mathbf{y} \\cdot \\mathbf{u}_{1}}{\\mathbf{u}_{1} \\cdot \\mathbf{u}_{1}} \\mathbf{u}_{1}+\\frac{\\mathbf{y} \\cdot \\mathbf{u}_{2}}{\\mathbf{u}_{2} \\cdot \\mathbf{u}_{2}} \\mathbf{u}_{2}=\\hat{\\mathbf{y}}_{1}+\\hat{\\mathbf{y}}_{2} \\] The codes for plotting are quite redundent, however exceedingly intuitive. ######################## Subspace W ############################## s = np . linspace ( - .5 , .5 , 10 ) t = np . linspace ( - .5 , .5 , 10 ) S , T = np . meshgrid ( s , t ) X1 = 2 * S - 2 * T X2 = 5 * S + T X3 = - S + T fig = plt . figure ( figsize = ( 7 , 7 )) ax = fig . add_subplot ( projection = '3d' ) ax . plot_wireframe ( X1 , X2 , X3 , linewidth = 1.5 , alpha = .3 ) ########################### vector y ############################### y = np . array ([ 1 , 2 , 3 ]) u1 , u2 = np . array ([ 2 , 5 , - 1 ]), np . array ([ - 2 , 1 , 1 ]) vec = np . array ([[ 0 , 0 , 0 , y [ 0 ], y [ 1 ], y [ 2 ]]]) X , Y , Z , U , V , W = zip ( * vec ) ax . quiver ( X , Y , Z , U , V , W , length = 1 , normalize = False , color = 'red' , alpha = .6 , arrow_length_ratio = .08 , pivot = 'tail' , linestyles = 'solid' , linewidths = 3 ) ax . text ( y [ 0 ], y [ 1 ], y [ 2 ], '$\\mathbf {y} $' , size = 15 ) ########################### vector u1 and u2 ############################### vec = np . array ([[ 0 , 0 , 0 , u1 [ 0 ], u1 [ 1 ], u1 [ 2 ]]]) X , Y , Z , U , V , W = zip ( * vec ) ax . quiver ( X , Y , Z , U , V , W , length = 1 , normalize = False , color = 'red' , alpha = .6 , arrow_length_ratio = .08 , pivot = 'tail' , linestyles = 'solid' , linewidths = 3 ) vec = np . array ([[ 0 , 0 , 0 , u2 [ 0 ], u2 [ 1 ], u2 [ 2 ]]]) X , Y , Z , U , V , W = zip ( * vec ) ax . quiver ( X , Y , Z , U , V , W , length = 1 , normalize = False , color = 'red' , alpha = .6 , arrow_length_ratio = .08 , pivot = 'tail' , linestyles = 'solid' , linewidths = 3 ) ax . text ( u1 [ 0 ], u1 [ 1 ], u1 [ 2 ], '$\\mathbf {u} _1$' , size = 15 ) ax . text ( u2 [ 0 ], u2 [ 1 ], u2 [ 2 ], '$\\mathbf {u} _2$' , size = 15 ) ########################### yhat ############################### alpha1 = ( y @u1 ) / ( u1 @u1 ) alpha2 = ( y @u2 ) / ( u2 @u2 ) yhat1 = alpha1 * u1 yhat2 = alpha2 * u2 yhat = yhat1 + yhat2 vec = np . array ([[ 0 , 0 , 0 , yhat1 [ 0 ], yhat1 [ 1 ], yhat1 [ 2 ]]]) X , Y , Z , U , V , W = zip ( * vec ) ax . quiver ( X , Y , Z , U , V , W , length = 1 , normalize = False , color = 'blue' , alpha = .6 , arrow_length_ratio = .08 , pivot = 'tail' , linestyles = 'solid' , linewidths = 3 , zorder = 3 ) vec = np . array ([[ 0 , 0 , 0 , yhat2 [ 0 ], yhat2 [ 1 ], yhat2 [ 2 ]]]) X , Y , Z , U , V , W = zip ( * vec ) ax . quiver ( X , Y , Z , U , V , W , length = 1 , normalize = False , color = 'blue' , alpha = .6 , arrow_length_ratio = .08 , pivot = 'tail' , linestyles = 'solid' , linewidths = 3 , zorder = 3 ) vec = np . array ([[ 0 , 0 , 0 , yhat [ 0 ], yhat [ 1 ], yhat [ 2 ]]]) X , Y , Z , U , V , W = zip ( * vec ) ax . quiver ( X , Y , Z , U , V , W , length = 1 , normalize = False , color = 'pink' , alpha = 1 , arrow_length_ratio = .12 , pivot = 'tail' , linestyles = 'solid' , linewidths = 3 , zorder = 3 ) ax . text ( yhat1 [ 0 ], yhat1 [ 1 ], yhat1 [ 2 ], '$\\hat{\\mathbf {y} }_1$' , size = 15 ) ax . text ( yhat2 [ 0 ], yhat2 [ 1 ], yhat2 [ 2 ], '$\\hat{\\mathbf {y} }_2$' , size = 15 ) ax . text ( x = yhat [ 0 ], y = yhat [ 1 ], z = yhat [ 2 ], s = r '$\\hat{\\mathbf {y} }=\\frac{\\mathbf {y} \\cdot \\mathbf {u} _ {1} }{\\mathbf {u} _ {1} \\cdot \\mathbf {u} _ {1} } \\mathbf {u} _ {1} +\\frac{\\mathbf {y} \\cdot \\mathbf {u} _ {2} }{\\mathbf {u} _ {2} \\cdot \\mathbf {u} _ {2} } \\mathbf {u} _ {2} =\\hat{\\mathbf {y} }_ {1} +\\hat{\\mathbf {y} }_ {2} $' , size = 15 ) ########################### z ############################### z = y - yhat vec = np . array ([[ yhat [ 0 ], yhat [ 1 ], yhat [ 2 ], z [ 0 ], z [ 1 ], z [ 2 ]]]) X , Y , Z , U , V , W = zip ( * vec ) ax . quiver ( X , Y , Z , U , V , W , length = 1 , normalize = False , color = 'green' , alpha = .6 , arrow_length_ratio = .08 , pivot = 'tail' , linestyles = 'solid' , linewidths = 3 ) ############################ Dashed Line #################### line1 = np . array ([ y , yhat1 ]) ax . plot ( line1 [:, 0 ], line1 [:, 1 ], line1 [:, 2 ], c = 'b' , lw = 3.5 , alpha = 0.5 , ls = '--' ) line2 = np . array ([ y , yhat2 ]) ax . plot ( line2 [:, 0 ], line2 [:, 1 ], line2 [:, 2 ], c = 'b' , lw = 2.5 , alpha = 0.5 , ls = '--' ) line3 = np . array ([ yhat , yhat2 ]) ax . plot ( line3 [:, 0 ], line3 [:, 1 ], line3 [:, 2 ], c = 'b' , lw = 2.5 , alpha = 0.5 , ls = '--' ) line4 = np . array ([ yhat , yhat1 ]) ax . plot ( line4 [:, 0 ], line4 [:, 1 ], line4 [:, 2 ], c = 'b' , lw = 2.5 , alpha = 0.5 , ls = '--' ) ############################# View Angel ax . view_init ( elev =- 6 , azim = 12 ) Derivation of Projection onto a Line: Cambridge University Press: Mathematics for Machine Learning, 2020. (pp. 82-85) \u21a9 Derivation of Projection onto a General Subspace: Cambridge University Press: Mathematics for Machine Learning, 2020. (pp. 85-88) \u21a9 Derivation of Projection onto a General Subspace: Khan's Academy Projection onto a General Subspace \u21a9 Projection Onto Lower Dimensional Subspace: Cambridge University Press: Mathematics for Machine Learning, 2020. (pp. 87-88) \u21a9 Plotting Projections in Python: Plot Projections in Python \u21a9","title":"Orthogonal Projection"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.07_orthogonal_projections/#orthogonal-projections","text":"Most derivations are written in my Ipad.","title":"Orthogonal Projections"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.07_orthogonal_projections/#algebraic-definition-orthogonal-projections","text":"Let \\(\\V\\) be the ambient vector space. A projection on a vector subspace \\(\\U \\subseteq \\V\\) is a linear mapping \\(\\pi: \\V \\to \\U\\) such that \\(\\pi^2 = \\pi\\) . If \\(\\V\\) is an inner product space , then \\(\\pi\\) can be called an orthogonal projection .","title":"Algebraic Definition (Orthogonal Projections)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.07_orthogonal_projections/#definition-projection-matrix","text":"We can verify that \\(\\pi\\) is indeed a linear transformaton . Recall that linear transformation can be expressed by transformation matrices . Thus we define projection matrices to be \\(\\P_\\pi\\) such that \\(\\P_\\pi^2 = \\P\\) .","title":"Definition (Projection Matrix)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.07_orthogonal_projections/#derivation-of-projection-onto-a-line-u1","text":"","title":"Derivation of Projection onto a Line \\(U\\)1"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.07_orthogonal_projections/#projection-onto-the-x-axis","text":"In page 84, the author said if the vector \\(\\x\\) is of unit length, then projecting on the horizontal axis yields a projection vector to be \\(\\cos(\\omega)\\) . This may be confusing at first if you derive it using formula since we see that \\[ \\pi_{U}(\\x) = \\dfrac{\\b^\\top\\x}{|| \\b ||^2}\\b = \\dfrac{||\\b|| ||\\x|| \\cos(\\omega)}{||\\b||^2} \\b = \\cos(\\omega) ||\\x|| \\dfrac{\\b}{||\\b||} \\overset{||\\x|| = 1}{=} \\cos(\\omega) \\dfrac{\\b}{||\\b||} \\] where did the unit vector \\(\\hat{\\b} = \\frac{\\b}{||\\b||}\\) go? The confusion lies in two folds, one is author mentioned that this is only true when projecting onto the horizontal axis (x-axis), and secondly, the abuse of notation of vector where I misunderstood \\(\\cos(\\omega)\\) as the \"projection vector\". In fact, if we are projecting on the horizontal axis, then the basis vector \\(\\b\\) is just \\(\\begin{bmatrix}1 \\\\0 \\end{bmatrix}\\) and we have the projection vector to be actually \\[ \\pi_{U}(\\x) = \\cos(\\omega) \\dfrac{\\b}{||\\b||} = \\cos(\\omega) \\begin{bmatrix}1 \\\\0 \\end{bmatrix} = \\begin{bmatrix}\\cos(\\omega) \\\\0 \\end{bmatrix} \\] and so when mentioned loosely, we can say that the projection vector is just \\(\\cos(\\omega)\\) .","title":"Projection onto the x-axis"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.07_orthogonal_projections/#derivation-of-projection-onto-a-general-subspace-u23","text":"The derivation using orthogonal complement provides a more intuitive understanding!","title":"Derivation of Projection onto a General Subspace \\(U\\)23"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.07_orthogonal_projections/#projection-onto-lower-dimensional-subspace","text":"The example here will tell you that given a vector \\(\\x \\in \\R^3\\) , it can be projected onto a lower dimensional subspace with minimal information loss since their distance is lowest. One thing to not get confused is that the projected vector is still in \\(\\R^3\\) , but it exists in a lower dimensional subspace \\(\\U \\subset \\R^3\\) embedded in \\(\\R^3\\) with 2 dimensions.","title":"Projection Onto Lower Dimensional Subspace"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.07_orthogonal_projections/#python-plot-a-visualization-of-projection","text":"Suppose we have \\(\\u = \\begin{bmatrix} 4 \\\\ 5 \\end{bmatrix}\\) , \\(\\v = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}\\) . Consider the subspace \\(\\U\\) spanned by \\(\\v\\) . We first plot the graph without the projection vector. The plots and contents (including the visualization of the decomposition) below are entirely credited to MacroAnalyst's GitHub Repo 5 . import matplotlib.pyplot as plt import matplotlib as mpl import numpy as np from mpl_toolkits.mplot3d import Axes3D import scipy as sp import scipy.linalg import scipy.spatial import sympy as sy sy . init_printing () fig , ax = plt . subplots ( figsize = ( 12 , 12 )) vects = np . array ([[ 4 , 5 ], [ 2 , 1 ]]) colr = [ \"red\" , \"blue\" ] cordt = [ \"$(4, 5)$\" , \"$(2, 1)$\" ] vec_name = [ \"$\\mathbf {u} $\" , \"$\\mathbf {v} $\" ] for i in range ( 2 ): ax . arrow ( 0 , 0 , vects [ i ][ 0 ], vects [ i ][ 1 ], color = colr [ i ], width = 0.03 , length_includes_head = True , head_width = 0.1 , # default: 3*width head_length = 0.2 , overhang = 0.4 , ) ax . text ( x = vects [ i ][ 0 ], y = vects [ i ][ 1 ], s = cordt [ i ], size = 15 ) ax . text ( x = vects [ i ][ 0 ] / 2 , y = vects [ i ][ 1 ] / 2 , s = vec_name [ i ], size = 22 ) ################################### Subspace L ############################ x = np . linspace ( 0 , 8.1 ) y = 1 / 2 * x ax . plot ( x , y , lw = 3 , color = \"red\" , alpha = 0.5 ) ax . text ( x = 6.5 , y = 3 , s = \"$L = \\operatorname{Span(\\mathbf {v} )}$\" , size = 19 ) ax . axis ([ 0 , 8 , 0 , 8 ]) ax . grid () Next, we plot the projection vector \\(\\pi_{\\U}(\\u)\\) onto \\(\\U\\) . u = np . array ([ 4 , 5 ]) v = np . array ([ 2 , 1 ]) alpha = ( u @ v ) / ( v @ v ) # the lambda coordinates print ( alpha ) proj_vec = alpha * v print ( proj_vec ) 2.6 [5.2 2.6] fig , ax = plt . subplots ( figsize = ( 12 , 12 )) vects = np . array ([[ 4 , 5 ], [ 2 , 1 ], [ 5.2 , 2.6 ]]) colr = [ \"red\" , \"blue\" , \"green\" ] cordt = [ \"$(4, 5)$\" , \"$(2, 1)$\" , \"(5.2, 2.6)\" ] vec_name = [ \"$\\mathbf {u} $\" , \"$\\mathbf {v} $\" , r \"$\\pi_{\\mathbf {U} }(\\mathbf {u} ) = \\alpha\\mathbf {v} $\" , ] for i in range ( 3 ): ax . arrow ( 0 , 0 , vects [ i ][ 0 ], vects [ i ][ 1 ], color = colr [ i ], width = 0.03 , length_includes_head = True , head_width = 0.1 , # default: 3*width head_length = 0.2 , overhang = 0.4 , zorder =- i , ) ax . text ( x = vects [ i ][ 0 ], y = vects [ i ][ 1 ], s = cordt [ i ], size = 19 ) ax . text ( x = vects [ i ][ 0 ] / 2 , y = vects [ i ][ 1 ] / 2 , s = vec_name [ i ], size = 22 ) ##################################### Components of y orthogonal to u ########################## point1 = [ 4 , 5 ] point2 = [ 5.2 , 2.6 ] line1 = np . array ([ point1 , point2 ]) ax . plot ( line1 [:, 0 ], line1 [:, 1 ], c = \"k\" , lw = 3.5 , alpha = 0.5 , ls = \"--\" ) ax . text ( 4.7 , 3.8 , \"$\\mathbf {z} $\" , size = 22 ) ################################### Subspace L ############################ x = np . linspace ( 0 , 8.1 ) y = 1 / 2 * x ax . plot ( x , y , lw = 3 , color = \"red\" , alpha = 0.5 , zorder =- 3 ) ax . text ( x = 6.5 , y = 3 , s = \"$\\mathbf {U} = \\operatorname{Span(\\mathbf {v} )}$\" , size = 19 ) ax . axis ([ 0 , 8 , 0 , 8 ]) ax . grid () #################################### Formula ################################ ax . text ( x = 1 , y = 7 , s = r \"$(\\mathbf {u} - \\alpha \\mathbf {v} )\\cdot \\mathbf {v} = 0$\" , size = 22 , color = \"b\" , ) ax . text ( x = 1 , y = 6.5 , s = r \"$\\alpha = \\frac{\\mathbf {u} \\cdot \\mathbf {v} }{\\mathbf {v} \\cdot \\mathbf {v} }$\" , size = 22 , color = \"b\" , ) ax . text ( x = 1 , y = 6 , s = r \"$\\operatorname {proj} _{\\mathbf {U} }\\mathbf {u} =\\frac{\\mathbf {u} \\cdot\\mathbf {v} }{\\mathbf {v} \\cdot\\mathbf {v} }\\mathbf {v} $\" , size = 22 , color = \"b\" , ) plt . show ()","title":"Python Plot (A Visualization of Projection)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.07_orthogonal_projections/#python-plot-a-visualization-of-orthogonal-decomposition","text":"To generalize the orthogonal projection in the higher dimension \\(\\mathbb{R}^n\\) , we summarize the idea into the orthogonal decomposition theorem . Let \\(W\\) be a subspace of \\(\\mathbb{R}^{n}\\) . Then each \\(\\mathbf{y}\\) in \\(\\mathbb{R}^{n}\\) can be written uniquely in the form $$ \\mathbf{y}=\\hat{\\mathbf{y}}+\\mathbf{z} $$ where \\(\\hat{\\mathbf{y}}\\) is in \\(W\\) and \\(\\mathbf{z}\\) is in \\(W^{\\perp} .\\) In fact, if \\(\\left\\{\\mathbf{u}_{1}, \\ldots, \\mathbf{u}_{p}\\right\\}\\) is any orthogonal basis of \\(W,\\) then $$ \\hat{\\mathbf{y}}=\\frac{\\mathbf{y} \\cdot \\mathbf{u} {1}}{\\mathbf{u} \\cdot \\mathbf{u} {1}} \\mathbf{u} +\\cdots+\\frac{\\mathbf{y} \\cdot \\mathbf{u} {p}}{\\mathbf{u} \\cdot \\mathbf{u} {p}} \\mathbf{u} $$ and \\(\\mathbf{z}=\\mathbf{y}-\\hat{\\mathbf{y}}\\) . In \\(\\mathbb{R}^{2}\\) , we project \\(\\mathbf{y}\\) onto subspace \\(L\\) which is spanned by \\(\\mathbf{u}\\) , here we generalize the formula for \\(\\mathbb{R}^{n}\\) , that \\(\\mathbf{y}\\) is projected onto \\(W\\) which is spanned by \\(\\left\\{\\mathbf{u}_{1}, \\ldots, \\mathbf{u}_{p}\\right\\}\\) .","title":"Python Plot (A Visualization of Orthogonal Decomposition)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.07_orthogonal_projections/#a-visual-example-in-mathbbr3","text":"A subspace \\(W=\\operatorname{Span}\\left\\{\\mathbf{u}_{1}, \\mathbf{u}_{2}\\right\\}\\) , and a vector \\(\\mathbf{y}\\) is not in \\(W\\) , decompose \\(\\mathbf{y}\\) into \\(\\hat{\\mathbf{y}} + \\mathbf{z}\\) , and plot them. where \\[\\mathbf{u}_{1}=\\left[\\begin{array}{r} 2 \\\\ 5 \\\\ -1 \\end{array}\\right], \\mathbf{u}_{2}=\\left[\\begin{array}{r} -2 \\\\ 1 \\\\ 1 \\end{array}\\right], \\text { and } \\mathbf{y}=\\left[\\begin{array}{l} 1 \\\\ 2 \\\\ 3 \\end{array}\\right]\\] The projection onto \\(W\\) in \\(\\mathbb{R}^3\\) is \\[ \\hat{\\mathbf{y}}=\\frac{\\mathbf{y} \\cdot \\mathbf{u}_{1}}{\\mathbf{u}_{1} \\cdot \\mathbf{u}_{1}} \\mathbf{u}_{1}+\\frac{\\mathbf{y} \\cdot \\mathbf{u}_{2}}{\\mathbf{u}_{2} \\cdot \\mathbf{u}_{2}} \\mathbf{u}_{2}=\\hat{\\mathbf{y}}_{1}+\\hat{\\mathbf{y}}_{2} \\] The codes for plotting are quite redundent, however exceedingly intuitive. ######################## Subspace W ############################## s = np . linspace ( - .5 , .5 , 10 ) t = np . linspace ( - .5 , .5 , 10 ) S , T = np . meshgrid ( s , t ) X1 = 2 * S - 2 * T X2 = 5 * S + T X3 = - S + T fig = plt . figure ( figsize = ( 7 , 7 )) ax = fig . add_subplot ( projection = '3d' ) ax . plot_wireframe ( X1 , X2 , X3 , linewidth = 1.5 , alpha = .3 ) ########################### vector y ############################### y = np . array ([ 1 , 2 , 3 ]) u1 , u2 = np . array ([ 2 , 5 , - 1 ]), np . array ([ - 2 , 1 , 1 ]) vec = np . array ([[ 0 , 0 , 0 , y [ 0 ], y [ 1 ], y [ 2 ]]]) X , Y , Z , U , V , W = zip ( * vec ) ax . quiver ( X , Y , Z , U , V , W , length = 1 , normalize = False , color = 'red' , alpha = .6 , arrow_length_ratio = .08 , pivot = 'tail' , linestyles = 'solid' , linewidths = 3 ) ax . text ( y [ 0 ], y [ 1 ], y [ 2 ], '$\\mathbf {y} $' , size = 15 ) ########################### vector u1 and u2 ############################### vec = np . array ([[ 0 , 0 , 0 , u1 [ 0 ], u1 [ 1 ], u1 [ 2 ]]]) X , Y , Z , U , V , W = zip ( * vec ) ax . quiver ( X , Y , Z , U , V , W , length = 1 , normalize = False , color = 'red' , alpha = .6 , arrow_length_ratio = .08 , pivot = 'tail' , linestyles = 'solid' , linewidths = 3 ) vec = np . array ([[ 0 , 0 , 0 , u2 [ 0 ], u2 [ 1 ], u2 [ 2 ]]]) X , Y , Z , U , V , W = zip ( * vec ) ax . quiver ( X , Y , Z , U , V , W , length = 1 , normalize = False , color = 'red' , alpha = .6 , arrow_length_ratio = .08 , pivot = 'tail' , linestyles = 'solid' , linewidths = 3 ) ax . text ( u1 [ 0 ], u1 [ 1 ], u1 [ 2 ], '$\\mathbf {u} _1$' , size = 15 ) ax . text ( u2 [ 0 ], u2 [ 1 ], u2 [ 2 ], '$\\mathbf {u} _2$' , size = 15 ) ########################### yhat ############################### alpha1 = ( y @u1 ) / ( u1 @u1 ) alpha2 = ( y @u2 ) / ( u2 @u2 ) yhat1 = alpha1 * u1 yhat2 = alpha2 * u2 yhat = yhat1 + yhat2 vec = np . array ([[ 0 , 0 , 0 , yhat1 [ 0 ], yhat1 [ 1 ], yhat1 [ 2 ]]]) X , Y , Z , U , V , W = zip ( * vec ) ax . quiver ( X , Y , Z , U , V , W , length = 1 , normalize = False , color = 'blue' , alpha = .6 , arrow_length_ratio = .08 , pivot = 'tail' , linestyles = 'solid' , linewidths = 3 , zorder = 3 ) vec = np . array ([[ 0 , 0 , 0 , yhat2 [ 0 ], yhat2 [ 1 ], yhat2 [ 2 ]]]) X , Y , Z , U , V , W = zip ( * vec ) ax . quiver ( X , Y , Z , U , V , W , length = 1 , normalize = False , color = 'blue' , alpha = .6 , arrow_length_ratio = .08 , pivot = 'tail' , linestyles = 'solid' , linewidths = 3 , zorder = 3 ) vec = np . array ([[ 0 , 0 , 0 , yhat [ 0 ], yhat [ 1 ], yhat [ 2 ]]]) X , Y , Z , U , V , W = zip ( * vec ) ax . quiver ( X , Y , Z , U , V , W , length = 1 , normalize = False , color = 'pink' , alpha = 1 , arrow_length_ratio = .12 , pivot = 'tail' , linestyles = 'solid' , linewidths = 3 , zorder = 3 ) ax . text ( yhat1 [ 0 ], yhat1 [ 1 ], yhat1 [ 2 ], '$\\hat{\\mathbf {y} }_1$' , size = 15 ) ax . text ( yhat2 [ 0 ], yhat2 [ 1 ], yhat2 [ 2 ], '$\\hat{\\mathbf {y} }_2$' , size = 15 ) ax . text ( x = yhat [ 0 ], y = yhat [ 1 ], z = yhat [ 2 ], s = r '$\\hat{\\mathbf {y} }=\\frac{\\mathbf {y} \\cdot \\mathbf {u} _ {1} }{\\mathbf {u} _ {1} \\cdot \\mathbf {u} _ {1} } \\mathbf {u} _ {1} +\\frac{\\mathbf {y} \\cdot \\mathbf {u} _ {2} }{\\mathbf {u} _ {2} \\cdot \\mathbf {u} _ {2} } \\mathbf {u} _ {2} =\\hat{\\mathbf {y} }_ {1} +\\hat{\\mathbf {y} }_ {2} $' , size = 15 ) ########################### z ############################### z = y - yhat vec = np . array ([[ yhat [ 0 ], yhat [ 1 ], yhat [ 2 ], z [ 0 ], z [ 1 ], z [ 2 ]]]) X , Y , Z , U , V , W = zip ( * vec ) ax . quiver ( X , Y , Z , U , V , W , length = 1 , normalize = False , color = 'green' , alpha = .6 , arrow_length_ratio = .08 , pivot = 'tail' , linestyles = 'solid' , linewidths = 3 ) ############################ Dashed Line #################### line1 = np . array ([ y , yhat1 ]) ax . plot ( line1 [:, 0 ], line1 [:, 1 ], line1 [:, 2 ], c = 'b' , lw = 3.5 , alpha = 0.5 , ls = '--' ) line2 = np . array ([ y , yhat2 ]) ax . plot ( line2 [:, 0 ], line2 [:, 1 ], line2 [:, 2 ], c = 'b' , lw = 2.5 , alpha = 0.5 , ls = '--' ) line3 = np . array ([ yhat , yhat2 ]) ax . plot ( line3 [:, 0 ], line3 [:, 1 ], line3 [:, 2 ], c = 'b' , lw = 2.5 , alpha = 0.5 , ls = '--' ) line4 = np . array ([ yhat , yhat1 ]) ax . plot ( line4 [:, 0 ], line4 [:, 1 ], line4 [:, 2 ], c = 'b' , lw = 2.5 , alpha = 0.5 , ls = '--' ) ############################# View Angel ax . view_init ( elev =- 6 , azim = 12 ) Derivation of Projection onto a Line: Cambridge University Press: Mathematics for Machine Learning, 2020. (pp. 82-85) \u21a9 Derivation of Projection onto a General Subspace: Cambridge University Press: Mathematics for Machine Learning, 2020. (pp. 85-88) \u21a9 Derivation of Projection onto a General Subspace: Khan's Academy Projection onto a General Subspace \u21a9 Projection Onto Lower Dimensional Subspace: Cambridge University Press: Mathematics for Machine Learning, 2020. (pp. 87-88) \u21a9 Plotting Projections in Python: Plot Projections in Python \u21a9","title":" A Visual Example in \\(\\mathbb{R}^{3}\\)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.08_orthogonal_matrices_and_basis/","text":"\\[ \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\q}{\\mathbf{q}} \\newcommand{\\r}{\\mathbf{r}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\z}{\\mathbf{z}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\C}{\\mathbf{C}} \\newcommand{\\I}{\\mathbf{I}} \\newcommand{\\P}{\\mathbf{P}} \\newcommand{\\Q}{\\mathbf{Q}} \\newcommand{\\U}{\\mathbf{U}} \\newcommand{\\V}{\\mathbf{V}} \\newcommand{\\rank}{\\textbf{rank}} \\] Orthogonal Matrix Definition (Orthogonal Matrix) We mentioned this before in the matrix chapter. An orthogonal matrix is a square matrix \\(\\Q\\) with real entries whose columns and rows are orthogonal unit vectors (that is, orthonormal vectors). This means: All columns of the matrix are pairwise orthogonal, which means for any \\(\\q_i, \\q_j\\) of the columns \\(\\Q\\) , we have \\(\\q_i \\cdot \\q_j = \\0\\) Each column of the matrix has unit length and are therefore unit vectors: \\(\\Vert \\q_i \\Vert = \\1\\) . From the above, we can deduce a notation: \\[ \\Q_i \\cdot \\Q_j = \\begin{cases} 1\\hspace{0.5cm} \\text{if } i=j \\\\ 0\\hspace{0.5cm} \\text{if } i \\neq j \\end{cases} \\] To make the notation/expression more compact, we can write that a matrix \\(\\Q\\) is orthogonal if: \\[ \\Q^\\top \\Q = \\I \\] Theorem (Equivalence of Transpose and Inverse in Orthogonal Matrix) For any orthogonal matrix \\(\\Q\\) , we have its transpose equals to its inverse: \\[ \\Q^\\top = \\Q^{-1} \\] The proof is relatively easy since by definition \\(\\Q^\\top\\Q = \\I \\implies \\Q^\\top\\Q = \\Q^{-1}\\Q\\) . This result isn't trivial as computing inverse is significantly more difficult than computing transpose! General Form of 2 by 2 Orthogonal Matrix Given any angle \\(\\theta\\) , we can construct the 2 by 2 orthogonal matrix \\(\\Q\\) : \\[ \\Q = \\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{bmatrix} \\] This is easy to see as we just need to verify the two properties: \\(-\\cos(\\theta)\\sin(\\theta) + \\sin(\\theta)\\cos(\\theta) = 0\\) ; The length of each column is unit length of \\(1\\) by the property of \\(\\sin(\\theta)^2 + \\cos(\\theta)^2 = 1\\) . Rectangular Orthogonal Matrices 1 Orthogonal and Orthonormal Basis 2 Orthogonal Basis Orthonormal Basis Rectangular Orthogonal Matrices: Mike X Cohen: Linear Algebra: Theory, Intuition, Code, 2021. (pp. 372-373) \u21a9 Orthogonal and Orthonormal Basis: Henry Ricardo: A Modern Introduction to Linear Algebra, 2009. (pp. 477-480) \u21a9","title":"Orthogonal Matrices and Basis"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.08_orthogonal_matrices_and_basis/#orthogonal-matrix","text":"","title":"Orthogonal Matrix"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.08_orthogonal_matrices_and_basis/#definition-orthogonal-matrix","text":"We mentioned this before in the matrix chapter. An orthogonal matrix is a square matrix \\(\\Q\\) with real entries whose columns and rows are orthogonal unit vectors (that is, orthonormal vectors). This means: All columns of the matrix are pairwise orthogonal, which means for any \\(\\q_i, \\q_j\\) of the columns \\(\\Q\\) , we have \\(\\q_i \\cdot \\q_j = \\0\\) Each column of the matrix has unit length and are therefore unit vectors: \\(\\Vert \\q_i \\Vert = \\1\\) . From the above, we can deduce a notation: \\[ \\Q_i \\cdot \\Q_j = \\begin{cases} 1\\hspace{0.5cm} \\text{if } i=j \\\\ 0\\hspace{0.5cm} \\text{if } i \\neq j \\end{cases} \\] To make the notation/expression more compact, we can write that a matrix \\(\\Q\\) is orthogonal if: \\[ \\Q^\\top \\Q = \\I \\]","title":"Definition (Orthogonal Matrix)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.08_orthogonal_matrices_and_basis/#theorem-equivalence-of-transpose-and-inverse-in-orthogonal-matrix","text":"For any orthogonal matrix \\(\\Q\\) , we have its transpose equals to its inverse: \\[ \\Q^\\top = \\Q^{-1} \\] The proof is relatively easy since by definition \\(\\Q^\\top\\Q = \\I \\implies \\Q^\\top\\Q = \\Q^{-1}\\Q\\) . This result isn't trivial as computing inverse is significantly more difficult than computing transpose!","title":"Theorem (Equivalence of Transpose and Inverse in Orthogonal Matrix)"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.08_orthogonal_matrices_and_basis/#general-form-of-2-by-2-orthogonal-matrix","text":"Given any angle \\(\\theta\\) , we can construct the 2 by 2 orthogonal matrix \\(\\Q\\) : \\[ \\Q = \\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{bmatrix} \\] This is easy to see as we just need to verify the two properties: \\(-\\cos(\\theta)\\sin(\\theta) + \\sin(\\theta)\\cos(\\theta) = 0\\) ; The length of each column is unit length of \\(1\\) by the property of \\(\\sin(\\theta)^2 + \\cos(\\theta)^2 = 1\\) .","title":"General Form of 2 by 2 Orthogonal Matrix"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.08_orthogonal_matrices_and_basis/#rectangular-orthogonal-matrices-1","text":"","title":"Rectangular Orthogonal Matrices 1"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.08_orthogonal_matrices_and_basis/#orthogonal-and-orthonormal-basis-2","text":"","title":"Orthogonal and Orthonormal Basis 2"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.08_orthogonal_matrices_and_basis/#orthogonal-basis","text":"","title":"Orthogonal Basis"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.08_orthogonal_matrices_and_basis/#orthonormal-basis","text":"Rectangular Orthogonal Matrices: Mike X Cohen: Linear Algebra: Theory, Intuition, Code, 2021. (pp. 372-373) \u21a9 Orthogonal and Orthonormal Basis: Henry Ricardo: A Modern Introduction to Linear Algebra, 2009. (pp. 477-480) \u21a9","title":"Orthonormal Basis"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.09_gram_schmidt_orthogonalization/","text":"\\[ \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\r}{\\mathbf{r}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\z}{\\mathbf{z}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}} \\newcommand{\\A}{\\mathbf{A}} \\newcommand{\\B}{\\mathbf{B}} \\newcommand{\\C}{\\mathbf{C}} \\newcommand{\\P}{\\mathbf{P}} \\newcommand{\\U}{\\mathbf{U}} \\newcommand{\\V}{\\mathbf{V}} \\newcommand{\\rank}{\\textbf{rank}} \\] Gram-Schmidt The motivation of Gram-Schmidt is clear when we realize working with orthogonal/orthonormal vectors yield desirable properties. For example, a basis vector \\(\\B = \\{\\b_1, \\b_2, \\ldots, \\b_n\\}\\) of a \\(\\R^n\\) space can be transformed into a set of orthogonal vectors for easier processing. With this in mind, we go through a method to do so. Geometric Intuition (Gram-Schmidt) 1 2 We leave the heavy lifting in the two references. But the intuition can be simply derived in \\(\\R^2\\) space. Consider \\(2\\) basis vectors \\(\\{\\u, \\v\\}\\) of \\(\\R^2\\) , then fix the vector \\(\\u\\) , and find the projection of the vector \\(\\u\\) on \\(\\v\\) . Recall this is found by: \\[ \\textbf{proj}_{\\u}(\\v) = \\dfrac{\\u \\cdot \\v}{\\u \\cdot \\u} \\u \\] Then \\(\\v- \\textbf{proj}_{\\u}(\\v)\\) is guaranteed to be orthogonal to \\(\\u\\) by definition and this method holds for any \\(\\u, \\v\\) in \\(\\R^n\\) where \\(\\u \\neq \\0\\) . In \\(\\R^2\\) space, we have already turned \\(\\u, \\v\\) to \\(\\u, \\v- \\textbf{proj}_{\\u}(\\v)\\) where the latter is a set of orthogonal vectors that is also a basis of \\(\\R^2\\) . Note one should be clear by now that orthogonal vectors like these are guanranteed to be linearly independent and since the cardinality is \\(2\\) , it spans the \\(R^2\\) space. To further make them orthonormal , we further divide them by their length as a way of normalization and that is all. For higher dimensions, the process is iteratively applied. QR Decomposition Geometric Intuition of Gram-Schmidt: Henry Ricardo: A Modern Introduction to Linear Algebra, 2009. (pp. 483-485) \u21a9 Geometric Intuition of Gram-Schmidt: Cambridge University Press: Mathematics for Machine Learning, 2020. (pp. 89-90) \u21a9","title":"Gram-Schmidt Orthogonalization"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.09_gram_schmidt_orthogonalization/#gram-schmidt","text":"The motivation of Gram-Schmidt is clear when we realize working with orthogonal/orthonormal vectors yield desirable properties. For example, a basis vector \\(\\B = \\{\\b_1, \\b_2, \\ldots, \\b_n\\}\\) of a \\(\\R^n\\) space can be transformed into a set of orthogonal vectors for easier processing. With this in mind, we go through a method to do so.","title":"Gram-Schmidt"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.09_gram_schmidt_orthogonalization/#geometric-intuition-gram-schmidt1-2","text":"We leave the heavy lifting in the two references. But the intuition can be simply derived in \\(\\R^2\\) space. Consider \\(2\\) basis vectors \\(\\{\\u, \\v\\}\\) of \\(\\R^2\\) , then fix the vector \\(\\u\\) , and find the projection of the vector \\(\\u\\) on \\(\\v\\) . Recall this is found by: \\[ \\textbf{proj}_{\\u}(\\v) = \\dfrac{\\u \\cdot \\v}{\\u \\cdot \\u} \\u \\] Then \\(\\v- \\textbf{proj}_{\\u}(\\v)\\) is guaranteed to be orthogonal to \\(\\u\\) by definition and this method holds for any \\(\\u, \\v\\) in \\(\\R^n\\) where \\(\\u \\neq \\0\\) . In \\(\\R^2\\) space, we have already turned \\(\\u, \\v\\) to \\(\\u, \\v- \\textbf{proj}_{\\u}(\\v)\\) where the latter is a set of orthogonal vectors that is also a basis of \\(\\R^2\\) . Note one should be clear by now that orthogonal vectors like these are guanranteed to be linearly independent and since the cardinality is \\(2\\) , it spans the \\(R^2\\) space. To further make them orthonormal , we further divide them by their length as a way of normalization and that is all. For higher dimensions, the process is iteratively applied.","title":"Geometric Intuition (Gram-Schmidt)1 2"},{"location":"reighns_ml_journey/mathematics/linear_algebra/08_analytic_geometry/08.09_gram_schmidt_orthogonalization/#qr-decomposition","text":"Geometric Intuition of Gram-Schmidt: Henry Ricardo: A Modern Introduction to Linear Algebra, 2009. (pp. 483-485) \u21a9 Geometric Intuition of Gram-Schmidt: Cambridge University Press: Mathematics for Machine Learning, 2020. (pp. 89-90) \u21a9","title":"QR Decomposition"},{"location":"reighns_ml_journey/mathematics/probability_statistics_theory/day-1-binomial-distribution/","text":"66 Days of Data [04/07/2021] Day 2 This is slightly modified from the original module in Datacamp, as I find it better to put the concept of covariance later on. Quick Navigation * [Dependencies](#1) * [Configurations](#2) * [Seeding](#3) * [Loading Iris Data](#4) * [Quantitative Exploratory Data Analysis](#5) * [Descripte Analytics](#51) * [Mean](#511) * [Median](#512) * [Mode](#513) * [Range](#514) * [Population Variance](#515) * [Population Standard Deviation](#516) * [Percentiles](#517) * [Box and Whiskers Plot](#52) Dependencies # !pip install -U -q scikit-learn==0.24.2 import os import random import sys import seaborn as sns import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import pandas as pd import sklearn import torch import scipy from tqdm import tqdm from sklearn.datasets import * assert sys . version_info >= ( 3 , 5 ) assert sklearn . __version__ >= \"0.20\" Configurations % matplotlib inline sns . set ( style = \"ticks\" ) plt . style . use ( \"dark_background\" ) # mpl.rc('axes', labelsize=15) # mpl.rc('xtick', labelsize=12) # mpl.rc('ytick', labelsize=12) # Where to save the figures PROJECT_ROOT_DIR = \".\" CHAPTER_ID = \"Quantitative Exploratory Data Analysis\" IMAGES_PATH = os . path . join ( PROJECT_ROOT_DIR , \"images\" , CHAPTER_ID ) os . makedirs ( IMAGES_PATH , exist_ok = True ) def save_fig ( fig_id , tight_layout = True , fig_extension = \"png\" , resolution = 300 ): path = os . path . join ( IMAGES_PATH , fig_id + \".\" + fig_extension ) print ( \"Saving figure\" , fig_id ) if tight_layout : plt . tight_layout () plt . savefig ( path , format = fig_extension , dpi = resolution ) def ecdf ( data ): \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\" # Number of data points: n n = len ( data ) # x-data for the ECDF: x x = np . sort ( data ) # y-data for the ECDF: y y = np . arange ( start = 1 , stop = n + 1 ) / n return x , y Seeding def seed_all ( seed : int = 1930 ): \"\"\"Seed all random number generators.\"\"\" print ( \"Using Seed Number {} \" . format ( seed )) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) # set PYTHONHASHSEED env var at fixed value torch . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) torch . cuda . manual_seed ( seed ) # pytorch (both CPU and CUDA) np . random . seed ( seed ) # for numpy pseudo-random generator random . seed ( seed ) # set fixed value for python built-in pseudo-random generator torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = False torch . backends . cudnn . enabled = False def seed_worker ( _worker_id ): \"\"\"Seed a worker with the given ID.\"\"\" worker_seed = torch . initial_seed () % 2 ** 32 np . random . seed ( worker_seed ) random . seed ( worker_seed ) seed_all ( 42 ) In [probability](probability_theory \"wikilink\") and [statistics](statistics \"wikilink\"), a **probability mass function** is a function that gives the probability that a [discrete random variable](discrete_random_variable \"wikilink\") is exactly equal to some value.[^1] Sometimes it is also known as the discrete density function. The probability mass function is often the primary means of defining a [discrete probability distribution](discrete_probability_distribution \"wikilink\"), and such functions exist for either [scalar](Scalar_variable \"wikilink\") or [multivariate random variables](multivariate_random_variable \"wikilink\") whose [domain](Domain_of_a_function \"wikilink\") is discrete. A probability mass function differs from a [probability density function](probability_density_function \"wikilink\") (PDF) in that the latter is associated with continuous rather than discrete random variables. A PDF must be [integrated](integration_(mathematics) \"wikilink\") over an interval to yield a probability.[^2] The value of the random variable having the largest probability mass is called the [mode](mode_(statistics) \"wikilink\"). ## Introduction to Inferential Statistics ### Review of Probability Theory #### Properties of expected values and variances Let $X$ be an arbitrary random variable, and $a$ and $c$ are two constants, then we have - $\\mathbb{E}(c) = c$; - $\\mathbb{E}(aX+c) = a\\mathbb{E}(X) + c$. If $\\{a_1, a_2, ..., a_n\\}$ is a sequence of constants, and $\\{X_1, X_2, ..., X_n\\}$ is a sequence of random variables, then we have - $\\mathbb{E}\\left(\\sum\\limits_{i=1}^na_iX_i\\right) = \\sum\\limits_{i=1}^na_i\\mathbb{E}(X_i)$ As a special case that each $a_i=1$, then the equation above can be written as $\\mathbb{E}\\left(\\sum_{i=1}^nX_i\\right)=\\sum_{i=1}^n\\mathbb{E}(X_i)$. Let $X$ and $Y$ be two random variables, and $a$, $b$, and $c$ are three constants, then we have - $\\text{Var}(c) = 0$; - $\\text{Var}(aX+c) = a^2\\text{Var}(X)$; - $\\text{Var}(aX+bY+c) = a^2\\text{Var}(X) + b^2\\text{Var}(Y) + 2ab\\text{Cov}(X, Y)$. In a special case that $X$ and $Y$ are uncorrelated, the covariance between $X$ and $Y$ is zero, so the last equation can be written as $\\text{Var}(aX+bY+c) = a^2\\text{Var}(X) + b^2\\text{Var}(Y)$. Such a result can be extend to the following case - $\\text{Var}\\left(\\sum\\limits_{i=1}^na_iX_i\\right) = \\sum\\limits_{i=1}^na_i^2\\text{Var}(X_i)$, where $\\{a_1, a_2, ..., a_n\\}$ is a sequence of constants, and $\\{X_1, X_2, ..., X_n\\}$ is a sequence of **pairwise uncorrelated** random variables. #### Discrete random variables and their distributions A random variable $X$ is defined to be **discrete** if its possible outcomes are finite or countable. A few examples are given below. 1. The result of rolling a dice (discrete uniform distribution) 2. The preference of one customer for Coke or Pepsi (Bernoulli distribution) 3. Among 10 customers, the number of people who prefer Coke over Pepsi (Binomial distribution) 4. The number of patients arriving in an emergency room within a fixed time interval (Poisson distribution) Notes: For a discrete random variable $X$ with $k$ possible outcomes $x_j$, the probability mass function (PMF) is given by: \\begin{align} P(X=x_j) = p_j, \\text{ for each }j=1, 2, ..., k, \\end{align} where $p_j$ is the probability of the outcome $x_j$, and all $p_i$ must satisfy \\begin{cases} 0\\leq p_i \\leq 1 \\\\ \\sum_{j=1}^kp_j = 1 \\end{cases} Example 4: Suppose that in Singapore, $65\\%$ of customers prefer Coke, while the remaining $35\\%$ prefer Pepsi. Now we randomly survey 10 customers, among which the number of customers who prefer Coke is denoted by a discrete random variable $X$. Plot the PMF of $X$. ### [Probability Mass Function] Definition: For a discrete random variable $X$ with $k$ possible outcomes $x_j$, the probability mass function (PMF) is given by: \\begin{align} P(X=x_j) = p_X(x_j), \\text{ for each }j=1, 2, ..., k, \\end{align} where $p_X(x_j)$ is the probability of the outcome $x_j$, and all $p_X(x_i)$ must satisfy \\begin{cases} 0\\leq p_X(x_i) \\leq 1 \\\\ \\sum_{j=1}^kp_j = 1 \\end{cases} The probability mass function $f$ for outcomes $k$ and probabilities $p$ is: ### $$ pmf(k; p) = \\begin{cases} p & \\text{for }k=1 \\\\ q=(1-p) & \\text{for }k=0 \\\\ \\end{cases} $$ ### [Bernoulli Trials] **Note Bernoulli Trial is not the same Bernoulli Distribution, in terms of definition**. Definition: The performance of a fixed number of trials with fixed probability of success on each trial is known as a Bernoulli trial. Independent repeated trials of an experiment with exactly two possible outcomes are called Bernoulli trials. Call one of the outcomes \"success\" and the other outcome \"failure\". Let $p$ be the probability of success in a Bernoulli trial, and $q$ be the probability of failure. Then the probability of success and the probability of failure sum to one, since these are complementary events: \"success\" and \"failure\" are mutually exclusive and exhaustive. Thus one has the following relations: \\begin{align} p = 1 - q, \\quad \\quad q = 1 - p, \\quad \\quad p + q = 1. \\end{align} [Random variables](Random_variable \"wikilink\") describing Bernoulli trials are often encoded using the convention that 1 = \\\"success\\\", 0 = \\\"failure\\\". Closely related to a Bernoulli trial is a binomial experiment, which consists of a fixed number $n$ of [statistically independent](statistically_independent \"wikilink\") Bernoulli trials, each with a probability of success $p$, and counts the number of successes. A random variable corresponding to a binomial is denoted by $B(n,p)$, and is said to have a *[binomial distribution](binomial_distribution \"wikilink\")*. The probability of exactly $k$ successes in the experiment $B(n,p)$ is given by: $$P(k)={n \\choose k} p^k q^{n-k}$$ where ${n \\choose k}$ is a [binomial coefficient](binomial_coefficient \"wikilink\"). Bernoulli trials may also lead to [negative binomial distributions](negative_binomial_distribution \"wikilink\") (which count the number of successes in a series of repeated Bernoulli trials until a specified number of failures are seen), as well as various other distributions. When multiple Bernoulli trials are performed, each with its own probability of success, these are sometimes referred to as [Poisson trials](Poisson_trial \"wikilink\").[^3] [Example 1] tossing coins: Consider the simple experiment where a fair coin is tossed four times. Find the probability that exactly two of the tosses result in heads. [Solution]: For this experiment, let a heads be defined as a *success* and a tails as a *failure.* Because the coin is assumed to be fair, the probability of success is $p = \\tfrac{1}{2}$. Thus the probability of failure, $q$ is given by $$q = 1 - p = 1 - \\tfrac{1}{2} = \\tfrac{1}{2}$$ Using the equation above, the probability of exactly two tosses out of four total tosses resulting in a heads is given by: $$\\begin{align} P(2) &= {4 \\choose 2} p^{2} q^{4-2} \\\\ &= 6 \\times \\left(\\tfrac{1}{2}\\right)^2 \\times \\left(\\tfrac{1}{2}\\right)^2 \\\\ &= \\dfrac {3}{8}. \\end{align}$$ Example 1: Suppose that in Singapore, $65\\%$ of customers prefer Coke, while the remaining $35\\%$ prefer Pepsi. Now we randomly survey 10 customers, among which the number of customers who prefer Coke is denoted by a discrete random variable $X$. Plot the PMF of $X$. def perform_bernoulli_trials ( n , p ): \"\"\" Perform n Bernoulli trials with success probability p and return number of successes. That is to say: If I toss coin 10 times, and landed head 7 times, then success rate is 70% -> this is what I should return. But note this is binary. \"\"\" # Initialize number of successes: n_success n_success = 0 # Perform trials for i in range ( n ): # Choose random number between zero and one: random_number random_number = np . random . random ( size = 1 ) # If less than p, it's a success so add one to n_success if random_number < p : n_success += 1 return n_success # Initialize the number of defaults: n_defaults n_defaults = np . empty ( shape = 100000 ) # Compute the number of defaults for 1000 times # this is to say, every 100 loans, we find the number of defaults for i in tqdm ( range ( len ( n_defaults ))): n_defaults [ i ] = perform_bernoulli_trials ( n = 100 , p = 0.05 ) # Plot the histogram with default number of bins, density=True is normalized; label your axes _ = plt . hist ( x = n_defaults , density = True ) _ = plt . hist ( x = n_defaults , density = True , ) _ = plt . xlabel ( 'number of defaults out of 100 loans' ) _ = plt . ylabel ( 'probability' ) # save save_fig ( \"Bernoulli Trial\" ) # Show the plot plt . show () # from the plot, seems 4-8 is quite common, when I do on kaggle, bin it nicely for visuals~ len ( n_defaults ) # Compute ECDF: x, y x , y = ecdf ( n_defaults ) # Plot the ECDF with labeled axes _ = plt . plot ( x , y , marker = '.' , linestyle = 'none' ) _ = plt . xlabel ( xlabel = 'number of defaults' ) _ = plt . ylabel ( ylabel = 'percentage' ) # Show the plot plt . show () # Compute the number of 100-loan simulations with 10 or more defaults: n_lose_money default_more_than_ten_bool = np . where ( n_defaults >= 10 , True , False ) n_lose_money = np . sum ( default_more_than_ten_bool , axis = None ) # Compute and print probability of losing money print ( 'Probability of losing money =' , n_lose_money / len ( n_defaults )) # How to intepret this as bernoulli? # As we might expect, we most likely get 5/100 defaults. But we still have about a 2% chance of getting 10 or more defaults out of 100 loans. # This means given n=100, and p = 0.05 of default, we draw 100000 samples from this distribution. This is basically performing bernoulli function 100 times. n_defaults = np . random . binomial ( n = 100 , p = 0.05 , size = 1000000000 ) # Compute CDF: x, y x , y = ecdf ( n_defaults ) # Plot the CDF with axis labels _ = plt . plot ( x , y , marker = '.' , linestyle = 'none' ) _ = plt . xlabel ( xlabel = \"number of defaults\" ) _ = plt . ylabel ( ylabel = 'ECDF' ) # Show the plot plt . show () # Compute the number of 100-loan simulations with 10 or more defaults: n_lose_money default_more_than_ten_bool = np . where ( n_defaults >= 10 , True , False ) n_lose_money = np . sum ( default_more_than_ten_bool , axis = None ) # Compute and print probability of losing money print ( 'Probability of losing money =' , n_lose_money / len ( n_defaults )) ### [Bernoulli Distribution] PMF of Bernoulli Distribution: For a discrete random variable $X$ with $k$ possible outcomes $x_j$, the probability mass function (PMF) is given by: \\begin{align} P(X=x_j) = p_X(x_j), \\text{ for each }j=1, 2, ..., k, \\end{align} where $p_X(x_j)$ is the probability of the outcome $x_j$, and all $p_X(x_i)$ must satisfy \\begin{cases} 0\\leq p_X(x_i) \\leq 1 \\\\ \\sum_{j=1}^kp_j = 1 \\end{cases} In [probability theory](probability_theory \"wikilink\") and [statistics](statistics \"wikilink\"), the **Bernoulli distribution**, named after Swiss mathematician [Jacob Bernoulli](Jacob_Bernoulli \"wikilink\"),[^1] is the [discrete probability distribution](discrete_probability_distribution \"wikilink\") of a [random variable](random_variable \"wikilink\") which takes the value 1 with probability $p$ and the value 0 with probability $q = 1-p$. Less formally, it can be thought of as a model for the set of possible outcomes of any single [experiment](experiment \"wikilink\") that asks a [yes--no question](yes\u2013no_question \"wikilink\"). Such questions lead to [outcomes](outcome_(probability) \"wikilink\") that are [boolean](boolean-valued_function \"wikilink\")-valued: a single [bit](bit \"wikilink\") whose value is success/[yes](yes_and_no \"wikilink\")/[true](truth \"wikilink\")/[one](one \"wikilink\") with [probability](probability \"wikilink\") *p* and failure/no/[false](false_(logic) \"wikilink\")/[zero](zero \"wikilink\") with probability *q*. It can be used to represent a (possibly biased) [coin toss](coin_toss \"wikilink\") where 1 and 0 would represent \\\"heads\\\" and \\\"tails\\\" (or vice versa), respectively, and *p* would be the probability of the coin landing on heads or tails, respectively. In particular, unfair coins would have $p \\neq 1/2.$ The Bernoulli distribution is a special case of the [binomial distribution](binomial_distribution \"wikilink\") where a single trial is conducted (so *n* would be 1 for such a binomial distribution). It is also a special case of the **two-point distribution**, for which the possible outcomes need not be 0 and 1. ## Properties If $X$ is a random variable with this distribution, then: $$\\Pr(X=1) = p = 1 - \\Pr(X=0) = 1 - q.$$ The [probability mass function](probability_mass_function \"wikilink\") $f$ of this distribution, over possible outcomes *k*, is $$f(k;p) = \\begin{cases} p & \\text{if }k=1, \\\\ q = 1-p & \\text {if } k = 0. \\end{cases}$$[^2] This can also be expressed as $$f(k;p) = p^k (1-p)^{1-k} \\quad \\text{for } k\\in\\{0,1\\}$$ or as $$f(k;p)=pk+(1-p)(1-k) \\quad \\text{for } k\\in\\{0,1\\}.$$ The Bernoulli distribution is a special case of the [binomial distribution](binomial_distribution \"wikilink\") with $n = 1.$[^3] The [kurtosis](kurtosis \"wikilink\") goes to infinity for high and low values of $p,$ but for $p=1/2$ the two-point distributions including the Bernoulli distribution have a lower [excess kurtosis](excess_kurtosis \"wikilink\") than any other probability distribution, namely \u22122. The Bernoulli distributions for $0 \\le p \\le 1$ form an [exponential family](exponential_family \"wikilink\"). The [maximum likelihood estimator](maximum_likelihood_estimator \"wikilink\") of $p$ based on a random sample is the [sample mean](sample_mean \"wikilink\"). ## Mean The [expected value](expected_value \"wikilink\") of a Bernoulli random variable $X$ is $$\\operatorname{E}\\left(X\\right)=p$$ This is due to the fact that for a Bernoulli distributed random variable $X$ with $\\Pr(X=1)=p$ and $\\Pr(X=0)=q$ we find $$\\operatorname{E}[X] = \\Pr(X=1)\\cdot 1 + \\Pr(X=0)\\cdot 0 = p \\cdot 1 + q\\cdot 0 = p.$$[^4] ## Variance The [variance](variance \"wikilink\") of a Bernoulli distributed $X$ is $$\\operatorname{Var}[X] = pq = p(1-p)$$ We first find $$\\operatorname{E}[X^2] = \\Pr(X=1)\\cdot 1^2 + \\Pr(X=0)\\cdot 0^2 = p \\cdot 1^2 + q\\cdot 0^2 = p = \\operatorname{E}[X]$$ From this follows $$\\operatorname{Var}[X] = \\operatorname{E}[X^2]-\\operatorname{E}[X]^2 = \\operatorname{E}[X]-\\operatorname{E}[X]^2 = p-p^2 = p(1-p) = pq$$[^5] With this result it is easy to prove that, for any Bernoulli distribution, its variance will have a value inside $[0,1/4]$. ## Skewness The [skewness](skewness \"wikilink\") is $\\frac{q-p}{\\sqrt{pq}}=\\frac{1-2p}{\\sqrt{pq}}$. When we take the standardized Bernoulli distributed random variable $\\frac{X-\\operatorname{E}[X]}{\\sqrt{\\operatorname{Var}[X]}}$ we find that this random variable attains $\\frac{q}{\\sqrt{pq}}$ with probability $p$ and attains $-\\frac{p}{\\sqrt{pq}}$ with probability $q$. Thus we get $$\\begin{align} \\gamma_1 &= \\operatorname{E} \\left[\\left(\\frac{X-\\operatorname{E}[X]}{\\sqrt{\\operatorname{Var}[X]}}\\right)^3\\right] \\\\ &= p \\cdot \\left(\\frac{q}{\\sqrt{pq}}\\right)^3 + q \\cdot \\left(-\\frac{p}{\\sqrt{pq}}\\right)^3 \\\\ &= \\frac{1}{\\sqrt{pq}^3} \\left(pq^3-qp^3\\right) \\\\ &= \\frac{pq}{\\sqrt{pq}^3} (q-p) \\\\ &= \\frac{q-p}{\\sqrt{pq}} \\end{align}$$ ## Higher moments and cumulants {#higher_moments_and_cumulants} The raw moments are all equal due to the fact that $1^k=1$ and $0^k=0$. $$\\operatorname{E}[X^k] = \\Pr(X=1)\\cdot 1^k + \\Pr(X=0)\\cdot 0^k = p \\cdot 1 + q\\cdot 0 = p = \\operatorname{E}[X].$$ The central moment of order $k$ is given by $$\\mu_k =(1-p)(-p)^k +p(1-p)^k.$$ The first six central moments are $$\\begin{align} \\mu_1 &= 0, \\\\ \\mu_2 &= p(1-p), \\\\ \\mu_3 &= p(1-p)(1-2p), \\\\ \\mu_4 &= p(1-p)(1-3p(1-p)), \\\\ \\mu_5 &= p(1-p)(1-2p)(1-2p(1-p)), \\\\ \\mu_6 &= p(1-p)(1-5p(1-p)(1-p(1-p))). \\end{align}$$ The higher central moments can be expressed more compactly in terms of $\\mu_2$ and $\\mu_3$ $$\\begin{align} \\mu_4 &= \\mu_2 (1-3\\mu_2 ), \\\\ \\mu_5 &= \\mu_3 (1-2\\mu_2 ), \\\\ \\mu_6 &= \\mu_2 (1-5\\mu_2 (1-\\mu_2 )). \\end{align}$$ The first six cumulants are $$\\begin{align} \\kappa_1 &= p, \\\\ \\kappa_2 &= \\mu_2 , \\\\ \\kappa_3 &= \\mu_3 , \\\\ \\kappa_4 &= \\mu_2 (1-6\\mu_2 ), \\\\ \\kappa_5 &= \\mu_3 (1-12\\mu_2 ), \\\\ \\kappa_6 &= \\mu_2 (1-30\\mu_2 (1-4\\mu_2 )). \\end{align}$$ ### Binomial Distribution In [probability theory](probability_theory \"wikilink\") and [statistics](statistics \"wikilink\"), the **[binomial](Binomial_coefficient \"wikilink\") distribution** with parameters *n* and *p* is the [discrete probability distribution](discrete_probability_distribution \"wikilink\") of the number of successes in a sequence of *n* [independent](statistical_independence \"wikilink\") [experiments](experiment_(probability_theory) \"wikilink\"), each asking a [yes--no question](yes\u2013no_question \"wikilink\"), and each with its own [Boolean](boolean-valued_function \"wikilink\")-valued [outcome](outcome_(probability) \"wikilink\"): *success* (with probability *p*) or *failure* (with probability *q* = 1 \u2212 *p*). A single success/failure experiment is also called a [Bernoulli trial](Bernoulli_trial \"wikilink\") or Bernoulli experiment, and a sequence of outcomes is called a [Bernoulli process](Bernoulli_process \"wikilink\"); for a single trial, i.e., *n* = 1, the binomial distribution is a [Bernoulli distribution](Bernoulli_distribution \"wikilink\"). The binomial distribution is the basis for the popular [binomial test](binomial_test \"wikilink\") of [statistical significance](statistical_significance \"wikilink\"). The binomial distribution is frequently used to model the number of successes in a sample of size *n* drawn [with replacement](with_replacement \"wikilink\") from a population of size *N*. If the sampling is carried out without replacement, the draws are not independent and so the resulting distribution is a [hypergeometric distribution](hypergeometric_distribution \"wikilink\"), not a binomial one. However, for *N* much larger than *n*, the binomial distribution remains a good approximation, and is widely used. In particular, The binomial distribution is directly related to the Bernoullli distribution: it is **the sum of positive outcomes of a Bernoulli distributed random variable**. Whereas the Bernoulli distribution represented a single binary outcome and it's probability of occuring, the binomial has a parameter $n$ for the number of \"trials\". It is important to note that each trial, or event, must be independent and have the same probability of success in order to be represented with the binomial distribution. Run simulation to be close to the result. Means to say if the coin flip is TRULY a binomial distribution. Then simulating enough times will ultimately converge to what the binomial distribution will say , like say prob of 5 heads out of ten Toss is such and such ### [Binomial Distribution] PMF of Binomial Distribution: For a discrete random variable $X$ which follows the binomial distribution, with parameters $n \\in \\mathbb{N}$, $p \\in [0,1]$, representing number of trials, and the probability of success respectively. Then, the probability mass function (PMF) of $X$ is given by: \\begin{align} pmf(k;n,p) = P(X = k) = \\binom n k p^k(1-p)^{n-k} \\end{align} for $k = 0, 1, 2, ..., n$, where $$\\binom{n}{k} =\\frac{n!}{k!(n-k)!}$$ is the [binomial coefficient](binomial_coefficient \"wikilink\"), hence the name of the distribution. Take note that the probability mass function above also means the probability of getting exactly $k$ successes in $n$ independent Bernoulli trials. Interpretation: The formula can be understood as follows: *k* successes occur with probability *p*^*k*^ and *n* \u2212 *k* failures occur with probability (1 \u2212 *p*)^*n* \u2212 *k*^. However, the *k* successes can occur anywhere among the *n* trials, and there are $\\binom{n}{k}$ different ways of distributing *k* successes in a sequence of *n* trials. [Example 1] Classical Coin Toss: Consider the simple experiment where a fair coin is tossed ten times. Find the probability that exactly two of the tosses result in heads. [Solution]: We can do this using just probability (which I will not go into because I don't like probability). But if the question added a assumption, that says this experiment follows a binomial distribution, then I am very happy since we have an out-of-the-box formula to calculate such questions! To reframe the question, let us define the random variable $X$ to represent the number of heads you get (note since the question asked us about on \"tosses\" and \"heads\", it is thus natural for the random variable to be defined as such. Furthermore, there are 10 tosses, and X can take on integers 0 to 10). For this experiment, let a heads be defined as a *success* and a tails as a *failure.* Because the coin is assumed to be fair, the probability of success is $p = \\tfrac{1}{2}$. Thus the probability of failure, $q$ is given by $$q = 1 - p = 1 - \\tfrac{1}{2} = \\tfrac{1}{2}$$ Random Variable $X$ n=10 p=0.5 k=2 Using the formula defined above, the probability of exactly two tosses out of ten total tosses resulting in a heads is given by: $$\\begin{align} P(k=2) &= {10 \\choose 2} p^{2} q^{10-2} \\\\ \\end{align}$$ [reference](https://towardsdatascience.com/fun-with-the-binomial-distribution-96a5ecabf65b) ## Definitions ### Probability mass function {#probability_mass_function} ### Example Suppose a [biased coin](fair_coin \"wikilink\") comes up heads with probability 0.3 when tossed. The probability of seeing exactly 4 heads in 6 tosses is $$f(4,6,0.3) = \\binom{6}{4}0.3^4 (1-0.3)^{6-4}= 0.059535.$$ ### Cumulative distribution function {#cumulative_distribution_function} The [cumulative distribution function](cumulative_distribution_function \"wikilink\") can be expressed as: $$F(k;n,p) = \\Pr(X \\le k) = \\sum_{i=0}^{\\lfloor k \\rfloor} {n\\choose i}p^i(1-p)^{n-i},$$ where $\\lfloor k\\rfloor$ is the \\\"floor\\\" under *k*, i.e. the [greatest integer](greatest_integer \"wikilink\") less than or equal to *k*. It can also be represented in terms of the [regularized incomplete beta function](regularized_incomplete_beta_function \"wikilink\"), as follows:[^2] $$\\begin{align} F(k;n,p) & = \\Pr(X \\le k) \\\\ &= I_{1-p}(n-k, k+1) \\\\ & = (n-k) {n \\choose k} \\int_0^{1-p} t^{n-k-1} (1-t)^k \\, dt. \\end{align}$$ which is equivalent to the [cumulative distribution function](cumulative_distribution_function \"wikilink\") of the [`{{mvar|F}}`{=mediawiki}-distribution](F-distribution \"wikilink\"):[^3] $$F(k;n,p) = F_{F\\text{-distribution}}\\left(x=\\frac{1-p}{p}\\frac{k+1}{n-k};d_1=2(n-k),d_2=2(k+1)\\right).$$ Some closed-form bounds for the cumulative distribution function are given [below](#Tail_bounds \"wikilink\"). ## Properties ### Expected value and variance {#expected_value_and_variance} If *X* \\~ *B*(*n*, *p*), that is, *X* is a binomially distributed random variable, n being the total number of experiments and p the probability of each experiment yielding a successful result, then the [expected value](expected_value \"wikilink\") of *X* is:[^4] $$\\operatorname{E}[X] = np.$$ This follows from the linearity of the expected value along with the fact that `{{mvar|X}}`{=mediawiki} is the sum of `{{mvar|n}}`{=mediawiki} identical Bernoulli random variables, each with expected value `{{mvar|p}}`{=mediawiki}. In other words, if $X_1, \\ldots, X_n$ are identical (and independent) Bernoulli random variables with parameter `{{mvar|p}}`{=mediawiki}, then $X = X_1 + \\cdots + X_n$ and $$\\operatorname{E}[X] = \\operatorname{E}[X_1 + \\cdots + X_n] = \\operatorname{E}[X_1] + \\cdots + \\operatorname{E}[X_n] = p + \\cdots + p = np.$$ The [variance](variance \"wikilink\") is: $$\\operatorname{Var}(X) = np(1 - p).$$ This similarly follows from the fact that the variance of a sum of independent random variables is the sum of the variances. ### Higher moments {#higher_moments} The first 6 [central moments](central_moment \"wikilink\"), defined as $\\mu _{c}=\\operatorname {E} \\left[(X-\\operatorname {E} [X])^{c}\\right]$, are given by $$\\begin{align} \\mu_1 &= 0, \\\\ \\mu_2 &= np(1-p),\\\\ \\mu_3 &= np(1-p)(1-2p),\\\\ \\mu_4 &= np(1-p)(1+(3n-6)p(1-p)),\\\\ \\mu_5 &= np(1-p)(1-2p)(1+(10n-12)p(1-p)),\\\\ \\mu_6 &= np(1-p)(1-30p(1-p)(1-4p(1-p))+5np(1-p)(5-26p(1-p))+15n^2 p^2 (1-p)^2). \\end{align}$$ The non-central moments satisfy $$\\begin{align} \\operatorname {E}[X] &= np, \\\\ \\operatorname {E}[X^2] &= np(1-p)+n^2p^2, \\end{align}$$ and in general [^5] $$\\operatorname {E}[X^c] = \\sum_{k=0}^c \\left\\{ {c \\atop k} \\right\\} n^{\\underline{k}} p^k,$$ where $\\textstyle \\left\\{{c\\atop k}\\right\\}$ are the [Stirling numbers of the second kind](Stirling_numbers_of_the_second_kind \"wikilink\"), and $n^{\\underline{k}} = n(n-1)\\cdots(n-k+1)$ is the $k$th [falling power](Falling_and_rising_factorials \"wikilink\") of $n$. A simple bound [^6] follows by bounding the Binomial moments via the [higher Poisson moments](Poisson_distribution#Higher_moments \"wikilink\"): $$\\operatorname {E}[X^c] \\le \\left(\\frac{c}{\\log(c/(np)+1)}\\right)^c \\le (np)^c \\exp(c^2/(2np)).$$ ### Mode Usually the [mode](mode_(statistics) \"wikilink\") of a binomial *B*(*n*,\u2009*p*) distribution is equal to $\\lfloor (n+1)p\\rfloor$, where $\\lfloor\\cdot\\rfloor$ is the [floor function](floor_function \"wikilink\"). However, when (*n* + 1)*p* is an integer and *p* is neither 0 nor 1, then the distribution has two modes: (*n* + 1)*p* and (*n* + 1)*p* \u2212 1. When *p* is equal to 0 or 1, the mode will be 0 and *n* correspondingly. These cases can be summarized as follows: : ` `{=html}\\\\text{mode} = ` \\begin{cases}`\\ ` \\lfloor (n+1)\\,p\\rfloor & \\text{if }(n+1)p\\text{ is 0 or a noninteger}, \\\\`\\ ` (n+1)\\,p\\ \\text{ and }\\ (n+1)\\,p - 1 &\\text{if }(n+1)p\\in\\{1,\\dots,n\\}, \\\\`\\ ` n & \\text{if }(n+1)p = n + 1.`\\ ` \\end{cases}`` `{=html} **Proof:** Let $$f(k)=\\binom nk p^k q^{n-k}.$$ For $p=0$ only $f(0)$ has a nonzero value with $f(0)=1$. For $p=1$ we find $f(n)=1$ and $f(k)=0$ for $k\\neq n$. This proves that the mode is 0 for $p=0$ and $n$ for $p=1$. Let $0 < p < 1$. We find $$\\frac{f(k+1)}{f(k)} = \\frac{(n-k)p}{(k+1)(1-p)}$$. From this follows $$\\begin{align} k > (n+1)p-1 \\Rightarrow f(k+1) < f(k) \\\\ k = (n+1)p-1 \\Rightarrow f(k+1) = f(k) \\\\ k < (n+1)p-1 \\Rightarrow f(k+1) > f(k) \\end{align}$$ So when $(n+1)p-1$ is an integer, then $(n+1)p-1$ and $(n+1)p$ is a mode. In the case that $(n+1)p-1\\notin \\Z$, then only $\\lfloor (n+1)p-1\\rfloor+1=\\lfloor (n+1)p\\rfloor$ is a mode.[^7] ### Median In general, there is no single formula to find the [median](median \"wikilink\") for a binomial distribution, and it may even be non-unique. However several special results have been established: - If *np* is an integer, then the mean, median, and mode coincide and equal *np*.[^8][^9] - Any median *m* must lie within the interval \u230a*np*\u230b \u2264 *m* \u2264 \u2308*np*\u2309.[^10] - A median *m* cannot lie too far away from the mean: `{{nowrap||''m'' \u2212 ''np''| \u2264 min{\u2009ln 2, max{''p'', 1 \u2212 ''p''}\u2009}}`{=mediawiki}}.[^11] - The median is unique and equal to *m* = [round](Rounding \"wikilink\")(*np*) when \\|*m* \u2212 *np*\\| \u2264 min{*p*, 1 \u2212 *p*} (except for the case when *p* = `{{sfrac|1|2}}`{=mediawiki} and *n* is odd).[^12] - When *p* is a rational number (with the exception of *p* = 1/2 and *n* odd) the median is unique.[^13] - When *p* = 1/2 and *n* is odd, any number *m* in the interval `{{sfrac|1|2}}`{=mediawiki}(*n* \u2212 1) \u2264 *m* \u2264 `{{sfrac|1|2}}`{=mediawiki}(*n* + 1) is a median of the binomial distribution. If *p* = 1/2 and *n* is even, then *m* = *n*/2 is the unique median. ### Tail bounds {#tail_bounds} For *k* \u2264 *np*, upper bounds can be derived for the lower tail of the cumulative distribution function $F(k;n,p) = \\Pr(X \\le k)$, the probability that there are at most *k* successes. Since $\\Pr(X \\ge k) = F(n-k;n,1-p)$, these bounds can also be seen as bounds for the upper tail of the cumulative distribution function for *k* \u2265 *np*. [Hoeffding\\'s inequality](Hoeffding's_inequality \"wikilink\") yields the simple bound $$F(k;n,p) \\leq \\exp\\left(-2 n\\left(p-\\frac{k}{n}\\right)^2\\right), \\!$$ which is however not very tight. In particular, for *p* = 1, we have that *F*(*k*;*n*,*p*) = 0 (for fixed *k*, *n* with *k* \\ < *n*), but Hoeffding\\'s bound evaluates to a positive constant. A sharper bound can be obtained from the [Chernoff bound](Chernoff_bound \"wikilink\"):[^14] $$F(k;n,p) \\leq \\exp\\left(-nD\\left(\\frac{k}{n}\\parallel p\\right)\\right)$$ where *D*(*a* \\|\\| *p*) is the [relative entropy (or Kullback-Leibler divergence)](Kullback\u2013Leibler_divergence \"wikilink\") between an *a*-coin and a *p*-coin (i.e. between the Bernoulli(*a*) and Bernoulli(*p*) distribution): $$D(a\\parallel p)=(a)\\log\\frac{a}{p}+(1-a)\\log\\frac{1-a}{1-p}. \\!$$ Asymptotically, this bound is reasonably tight; see [^15] for details. One can also obtain *lower* bounds on the tail $F(k;n,p)$, known as anti-concentration bounds. By approximating the binomial coefficient with Stirling\\'s formula it can be shown that[^16] $$F(k;n,p) \\geq \\frac{1}{\\sqrt{8n\\tfrac{k}{n}(1-\\tfrac{k}{n})}} \\exp\\left(-nD\\left(\\frac{k}{n}\\parallel p\\right)\\right),$$ which implies the simpler but looser bound $$F(k;n,p) \\geq \\frac1{\\sqrt{2n}} \\exp\\left(-nD\\left(\\frac{k}{n}\\parallel p\\right)\\right).$$ For *p* = 1/2 and *k* \u2265 3*n*/8 for even *n*, it is possible to make the denominator constant:[^17] $$F(k;n,\\tfrac{1}{2}) \\geq \\frac{1}{15} \\exp\\left(- 16n \\left(\\frac{1}{2} -\\frac{k}{n}\\right)^2\\right). \\!$$ ## Related distributions {#related_distributions} ### Sums of binomials {#sums_of_binomials} If *X* \\~ B(*n*, *p*) and *Y* \\~ B(*m*, *p*) are independent binomial variables with the same probability *p*, then *X* + *Y* is again a binomial variable; its distribution is *Z=X+Y* \\~ B(*n+m*, *p*): $$\\begin{align} \\operatorname P(Z=k) &= \\sum_{i=0}^k\\left[\\binom{n}i p^i (1-p)^{n-i}\\right]\\left[\\binom{m}{k-i} p^{k-i} (1-p)^{m-k+i}\\right]\\\\ &= \\binom{n+m}k p^k (1-p)^{n+m-k} \\end{align}$$ However, if *X* and *Y* do not have the same probability *p*, then the variance of the sum will be [smaller than the variance of a binomial variable](Binomial_sum_variance_inequality \"wikilink\") distributed as $B(n+m, \\bar{p}).\\,$ ### Poisson binomial distribution {#poisson_binomial_distribution} The binomial distribution is a special case of the [Poisson binomial distribution](Poisson_binomial_distribution \"wikilink\"), or [general binomial distribution](general_binomial_distribution \"wikilink\"), which is the distribution of a sum of *n* independent non-identical [Bernoulli trials](Bernoulli_trials \"wikilink\") B(*p~i~*).[^18] ### Ratio of two binomial distributions {#ratio_of_two_binomial_distributions} This result was first derived by Katz and coauthors in 1978.[^19] Let *X* \\~ B(*n*,*p*~1~) and *Y* \\~ B(*m*,*p*~2~) be independent. Let *T* = (*X*/*n*)/(*Y*/*m*). Then log(*T*) is approximately normally distributed with mean log(*p*~1~/*p*~2~) and variance ((1/*p*~1~) \u2212 1)/*n* + ((1/*p*~2~) \u2212 1)/*m*. ### Conditional binomials {#conditional_binomials} If *X* \\~ B(*n*, *p*) and *Y* \\| *X* \\~ B(*X*, *q*) (the conditional distribution of *Y*, given *X*), then *Y* is a simple binomial random variable with distribution *Y* \\~ B(*n*, *pq*). For example, imagine throwing *n* balls to a basket *U~X~* and taking the balls that hit and throwing them to another basket *U~Y~*. If *p* is the probability to hit *U~X~* then *X* \\~ B(*n*, *p*) is the number of balls that hit *U~X~*. If *q* is the probability to hit *U~Y~* then the number of balls that hit *U~Y~* is *Y* \\~ B(*X*, *q*) and therefore *Y* \\~ B(*n*, *pq*). {{hidden begin|style=width:60%|ta1=center|border=1px #aaa solid|title=[Proof]}} Since $X \\sim B(n, p)$ and $Y \\sim B(X, q)$, by the [law of total probability](law_of_total_probability \"wikilink\"), $$\\begin{align} \\Pr[Y = m] &= \\sum_{k = m}^{n} \\Pr[Y = m \\mid X = k] \\Pr[X = k] \\\\[2pt] &= \\sum_{k=m}^n \\binom{n}{k} \\binom{k}{m} p^k q^m (1-p)^{n-k} (1-q)^{k-m} \\end{align}$$ Since $\\tbinom{n}{k} \\tbinom{k}{m} = \\tbinom{n}{m} \\tbinom{n-m}{k-m},$ the equation above can be expressed as $$\\Pr[Y = m] = \\sum_{k=m}^{n} \\binom{n}{m} \\binom{n-m}{k-m} p^k q^m (1-p)^{n-k} (1-q)^{k-m}$$ Factoring $p^k = p^m p^{k-m}$ and pulling all the terms that don\\'t depend on $k$ out of the sum now yields $$\\begin{align} \\Pr[Y = m] &= \\binom{n}{m} p^m q^m \\left( \\sum_{k=m}^n \\binom{n-m}{k-m} p^{k-m} (1-p)^{n-k} (1-q)^{k-m} \\right) \\\\[2pt] &= \\binom{n}{m} (pq)^m \\left( \\sum_{k=m}^n \\binom{n-m}{k-m} \\left(p(1-q)\\right)^{k-m} (1-p)^{n-k} \\right) \\end{align}$$ After substituting $i = k - m$ in the expression above, we get $$\\Pr[Y = m] = \\binom{n}{m} (pq)^m \\left( \\sum_{i=0}^{n-m} \\binom{n-m}{i} (p - pq)^i (1-p)^{n-m - i} \\right)$$ Notice that the sum (in the parentheses) above equals $(p - pq + 1 - p)^{n-m}$ by the [binomial theorem](binomial_theorem \"wikilink\"). Substituting this in finally yields $$\\begin{align} \\Pr[Y=m] &= \\binom{n}{m} (pq)^m (p - pq + 1 - p)^{n-m}\\\\[4pt] &= \\binom{n}{m} (pq)^m (1-pq)^{n-m} \\end{align}$$ and thus $Y \\sim B(n, pq)$ as desired. `{{hidden end}}`{=mediawiki} ### Bernoulli distribution {#bernoulli_distribution} The [Bernoulli distribution](Bernoulli_distribution \"wikilink\") is a special case of the binomial distribution, where *n* = 1. Symbolically, *X* \\~ B(1, *p*) has the same meaning as *X* \\~ Bernoulli(*p*). Conversely, any binomial distribution, B(*n*, *p*), is the distribution of the sum of *n* [Bernoulli trials](Bernoulli_trials \"wikilink\"), Bernoulli(*p*), each with the same probability *p*.[^20] ### Normal approximation {#normal_approximation} ![Binomial [probability mass function](probability_mass_function \"wikilink\") and normal [probability density function](probability_density_function \"wikilink\") approximation for *n* = 6 and *p* = 0.5](Binomial_Distribution.svg \"Binomial probability mass function and normal probability density function approximation for n = 6 and p = 0.5\"){width=\"250\"} If *n* is large enough, then the skew of the distribution is not too great. In this case a reasonable approximation to B(*n*, *p*) is given by the [normal distribution](normal_distribution \"wikilink\") $$\\mathcal{N}(np,\\,np(1-p)),$$ and this basic approximation can be improved in a simple way by using a suitable [continuity correction](continuity_correction \"wikilink\"). The basic approximation generally improves as *n* increases (at least 20) and is better when *p* is not near to 0 or 1.[^21] Various [rules of thumb](Rule_of_thumb \"wikilink\") may be used to decide whether *n* is large enough, and *p* is far enough from the extremes of zero or one: - One rule[^22] is that for `{{nowrap|''n'' > 5}}`{=mediawiki} the normal approximation is adequate if the absolute value of the skewness is strictly less than 1/3; that is, if $$\\frac{|1-2p|}{\\sqrt{np(1-p)}}=\\frac1{\\sqrt{n}}\\left|\\sqrt{\\frac{1-p}p}-\\sqrt{\\frac{p}{1-p}}\\,\\right| < \\frac13.$$ - A stronger rule states that the normal approximation is appropriate only if everything within 3 standard deviations of its mean is within the range of possible values; that is, only if $$\\mu\\pm3\\sigma=np\\pm3\\sqrt{np(1-p)}\\in(0,n).$$ : This 3-standard-deviation rule is equivalent to the following conditions, which also imply the first rule above. $$n>9 \\left(\\frac{1-p}{p} \\right)\\quad\\text{and}\\quad n>9\\left(\\frac{p}{1-p}\\right).$$ {{hidden begin|style=width:66%|ta1=center|border=1px #aaa solid|title=[Proof]}} The rule $np\\pm3\\sqrt{np(1-p)}\\in(0,n)$ is totally equivalent to request that $$np-3\\sqrt{np(1-p)}>0\\quad\\text{and}\\quad np+3\\sqrt{np(1-p)} 3\\sqrt{np(1-p)}\\quad\\text{and}\\quad n(1-p)>3\\sqrt{np(1-p)}.$$ Since $0 9 \\left(\\frac{1-p}p\\right) \\quad\\text{and}\\quad n>9 \\left(\\frac{p}{1-p}\\right).$$ Notice that these conditions automatically imply that $n>9$. On the other hand, apply again the square root and divide by 3, $$\\frac{\\sqrt{n}}3>\\sqrt{\\frac{1-p}p}>0 \\quad \\text{and} \\quad \\frac{\\sqrt{n}}3 > \\sqrt{\\frac{p}{1-p}}>0.$$ Subtracting the second set of inequalities from the first one yields: $$\\frac{\\sqrt{n}}3>\\sqrt{\\frac{1-p}p}-\\sqrt{\\frac{p}{1-p}}>-\\frac{\\sqrt{n}}3;$$ and so, the desired first rule is satisfied, $$\\left|\\sqrt{\\frac{1-p}p}-\\sqrt{\\frac{p}{1-p}}\\,\\right| < \\frac{\\sqrt{n}}3.$$ `{{hidden end}}`{=mediawiki} - Another commonly used rule is that both values $np$ and $n(1-p)$ must be greater than or equal to 5. However, the specific number varies from source to source, and depends on how good an approximation one wants. In particular, if one uses 9 instead of 5, the rule implies the results stated in the previous paragraphs. {{hidden begin|style=width:66%|ta1=center|border=1px #aaa solid|title=[Proof]}} Assume that both values $np$ and $n(1-p)$ are greater than 9. Since $0 < p < 1$, we easily have that $$np\\geq9>9(1-p)\\quad\\text{and}\\quad n(1-p)\\geq9>9p.$$ We only have to divide now by the respective factors $p$ and $1-p$, to deduce the alternative form of the 3-standard-deviation rule: $$n>9 \\left(\\frac{1-p}p\\right) \\quad\\text{and}\\quad n>9 \\left(\\frac{p}{1-p}\\right).$$ `{{hidden end}}`{=mediawiki} The following is an example of applying a [continuity correction](continuity_correction \"wikilink\"). Suppose one wishes to calculate Pr(*X* \u2264 8) for a binomial random variable *X*. If *Y* has a distribution given by the normal approximation, then Pr(*X* \u2264 8) is approximated by Pr(*Y* \u2264 8.5). The addition of 0.5 is the continuity correction; the uncorrected normal approximation gives considerably less accurate results. This approximation, known as [de Moivre--Laplace theorem](de_Moivre\u2013Laplace_theorem \"wikilink\"), is a huge time-saver when undertaking calculations by hand (exact calculations with large *n* are very onerous); historically, it was the first use of the normal distribution, introduced in [Abraham de Moivre](Abraham_de_Moivre \"wikilink\")\\'s book *[The Doctrine of Chances](The_Doctrine_of_Chances \"wikilink\")* in 1738. Nowadays, it can be seen as a consequence of the [central limit theorem](central_limit_theorem \"wikilink\") since B(*n*, *p*) is a sum of *n* independent, identically distributed [Bernoulli variables](Bernoulli_distribution \"wikilink\") with parameter *p*. This fact is the basis of a [hypothesis test](hypothesis_test \"wikilink\"), a \\\"proportion z-test\\\", for the value of *p* using *x/n*, the sample proportion and estimator of *p*, in a [common test statistic](common_test_statistics \"wikilink\").[^23] For example, suppose one randomly samples *n* people out of a large population and ask them whether they agree with a certain statement. The proportion of people who agree will of course depend on the sample. If groups of *n* people were sampled repeatedly and truly randomly, the proportions would follow an approximate normal distribution with mean equal to the true proportion *p* of agreement in the population and with standard deviation $\\sigma = \\sqrt{\\frac{p(1-p)}{n}}$ ### Poisson approximation {#poisson_approximation} The binomial distribution converges towards the [Poisson distribution](Poisson_distribution \"wikilink\") as the number of trials goes to infinity while the product *np* remains fixed or at least *p* tends to zero. Therefore, the Poisson distribution with parameter *\u03bb* = *np* can be used as an approximation to B(*n*, *p*) of the binomial distribution if *n* is sufficiently large and *p* is sufficiently small. According to two rules of thumb, this approximation is good if *n* \u2265 20 and *p* \u2264 0.05, or if *n* \u2265 100 and *np* \u2264 10.[^24] Concerning the accuracy of Poisson approximation, see Novak,[^25] ch. 4, and references therein. ### Limiting distributions {#limiting_distributions} - *[Poisson limit theorem](Poisson_limit_theorem \"wikilink\")*: As *n* approaches \u221e and *p* approaches 0 with the product *np* held fixed, the Binomial(*n*, *p*) distribution approaches the [Poisson distribution](Poisson_distribution \"wikilink\") with [expected value](expected_value \"wikilink\") *\u03bb = np*.[^26] - *[de Moivre--Laplace theorem](de_Moivre\u2013Laplace_theorem \"wikilink\")*: As *n* approaches \u221e while *p* remains fixed, the distribution of $$\\frac{X-np}{\\sqrt{np(1-p)}}$$ : approaches the [normal distribution](normal_distribution \"wikilink\") with expected value 0 and [variance](variance \"wikilink\") 1.`{{citation needed|date=May 2012}}`{=mediawiki} This result is sometimes loosely stated by saying that the distribution of *X* is [asymptotically normal](Asymptotic_normality \"wikilink\") with expected value *np* and [variance](variance \"wikilink\") *np*(1 \u2212 *p*). This result is a specific case of the [central limit theorem](central_limit_theorem \"wikilink\"). ### Beta distribution {#beta_distribution} The binomial distribution and beta distribution are different views of the same model of repeated Bernoulli trials. The binomial distribution is the [PMF](Probability_mass_function \"wikilink\") of `{{mvar|k}}`{=mediawiki} successes given `{{mvar|n}}`{=mediawiki} independent events each with a probability `{{mvar|p}}`{=mediawiki} of success. Mathematically, when `{{math|1=''\u03b1'' = ''k'' + 1}}`{=mediawiki} and `{{math|1=''\u03b2'' = ''n'' \u2212 ''k'' + 1}}`{=mediawiki}, the beta distribution and the binomial distribution are related by a factor of `{{math|''n'' + 1}}`{=mediawiki}: $$\\operatorname{Beta}(p;\\alpha;\\beta) = (n+1)\\operatorname{Binom}(k;n;p)$$ [Beta distributions](Beta_distribution \"wikilink\") also provide a family of [prior probability distributions](prior_distribution \"wikilink\") for binomial distributions in [Bayesian inference](Bayesian_inference \"wikilink\"):[^27] $$P(p;\\alpha,\\beta) = \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{\\mathrm{B}(\\alpha,\\beta)}.$$ Given a uniform prior, the posterior distribution for the probability of success `{{mvar|p}}`{=mediawiki} given `{{mvar|n}}`{=mediawiki} independent events with `{{mvar|k}}`{=mediawiki} observed successes is a beta distribution.[^28] ## Statistical Inference {#statistical_inference} ### Estimation of parameters {#estimation_of_parameters} {{see also|Beta distribution#Bayesian inference}} When *n* is known, the parameter *p* can be estimated using the proportion of successes: $\\widehat{p} = \\frac{x}{n}.$ This estimator is found using [maximum likelihood estimator](maximum_likelihood_estimator \"wikilink\") and also the [method of moments](method_of_moments_(statistics) \"wikilink\"). This estimator is [unbiased](Bias_of_an_estimator \"wikilink\") and uniformly with [minimum variance](Minimum-variance_unbiased_estimator \"wikilink\"), proven using [Lehmann--Scheff\u00e9 theorem](Lehmann\u2013Scheff\u00e9_theorem \"wikilink\"), since it is based on a [minimal sufficient](minimal_sufficient \"wikilink\") and [complete](Completeness_(statistics) \"wikilink\") statistic (i.e.: *x*). It is also [consistent](Consistent_estimator \"wikilink\") both in probability and in [MSE](Mean_squared_error \"wikilink\"). A closed form [Bayes estimator](Bayes_estimator \"wikilink\") for *p* also exists when using the [Beta distribution](Beta_distribution \"wikilink\") as a [conjugate](Conjugate_prior \"wikilink\") [prior distribution](prior_distribution \"wikilink\"). When using a general $\\operatorname{Beta}(\\alpha, \\beta)$ as a prior, the [posterior mean](Bayes_estimator#Posterior_mean \"wikilink\") estimator is: $\\widehat{p_b} = \\frac{x+\\alpha}{n+\\alpha+\\beta}$. The Bayes estimator is [asymptotically efficient](Asymptotic_efficiency_(Bayes) \"wikilink\") and as the sample size approaches infinity (*n* \u2192 \u221e), it approaches the [MLE](Maximum_likelihood_estimation \"wikilink\") solution. The Bayes estimator is [biased](Bias_of_an_estimator \"wikilink\") (how much depends on the priors), [admissible](Bayes_estimator#Admissibility \"wikilink\") and [consistent](Consistent_estimator \"wikilink\") in probability. For the special case of using the [standard uniform distribution](standard_uniform_distribution \"wikilink\") as a [non-informative prior](non-informative_prior \"wikilink\") ($\\operatorname{Beta}(\\alpha=1, \\beta=1) = U(0,1)$), the posterior mean estimator becomes $\\widehat{p_b} = \\frac{x+1}{n+2}$ (a [posterior mode](Bayes_estimator#Posterior_mode \"wikilink\") should just lead to the standard estimator). This method is called the [rule of succession](rule_of_succession \"wikilink\"), which was introduced in the 18th century by [Pierre-Simon Laplace](Pierre-Simon_Laplace \"wikilink\"). When estimating *p* with very rare events and a small *n* (e.g.: if x=0), then using the standard estimator leads to $\\widehat{p} = 0,$ which sometimes is unrealistic and undesirable. In such cases there are various alternative estimators.[^29] One way is to use the Bayes estimator, leading to: $\\widehat{p_b} = \\frac{1}{n+2}$). Another method is to use the upper bound of the [confidence interval](confidence_interval \"wikilink\") obtained using the [rule of three](Rule_of_three_(statistics) \"wikilink\"): $\\widehat{p_{\\text{rule of 3}}} = \\frac{3}{n}$) ### Confidence intervals {#confidence_intervals} {{Main|Binomial proportion confidence interval}} Even for quite large values of *n*, the actual distribution of the mean is significantly nonnormal.[^30] Because of this problem several methods to estimate confidence intervals have been proposed. In the equations for confidence intervals below, the variables have the following meaning: - *n*~1~ is the number of successes out of *n*, the total number of trials - $\\widehat{p\\,} = \\frac{n_1}{n}$ is the proportion of successes - $z$ is the $1 - \\tfrac{1}{2}\\alpha$ [quantile](quantile \"wikilink\") of a [standard normal distribution](standard_normal_distribution \"wikilink\") (i.e., [probit](probit \"wikilink\")) corresponding to the target error rate $\\alpha$. For example, for a 95% confidence level the error $\\alpha$ = 0.05, so $1 - \\tfrac{1}{2}\\alpha$ = 0.975 and $z$ = 1.96. #### Wald method {#wald_method} : : $\\widehat{p\\,} \\pm z \\sqrt{ \\frac{ \\widehat{p\\,} ( 1 -\\widehat{p\\,} )}{ n } } .$ <!-- --> : A [continuity correction](continuity_correction \"wikilink\") of 0.5/*n* may be added.`{{clarify|date=July 2012}}`{=mediawiki} #### Agresti--Coull method {#agresticoull_method} [^31] : : $\\tilde{p} \\pm z \\sqrt{ \\frac{ \\tilde{p} ( 1 - \\tilde{p} )}{ n + z^2 } }$ <!-- --> : Here the estimate of *p* is modified to <!-- --> : : $\\tilde{p}= \\frac{ n_1 + \\frac{1}{2} z^2}{ n + z^2 }$ <!-- --> : This method works well for $n>10$ and $n_1\\neq 0,n$.[^32] See here for $n\\leq 10$. [^33] For $n_1 = 0,n$ use the Wilson (score) method below. #### Arcsine method {#arcsine_method} [^34] : $\\sin^2 \\left(\\arcsin \\left(\\sqrt{\\widehat{p\\,}}\\right) \\pm \\frac{z}{2\\sqrt{n}} \\right).$ #### Wilson (score) method {#wilson_score_method} {{Main|Binomial proportion confidence interval#Wilson score interval}} The notation in the formula below differs from the previous formulas in two respects:[^35] - Firstly, *z*~*x*~ has a slightly different interpretation in the formula below: it has its ordinary meaning of \\'the *x*th quantile of the standard normal distribution\\', rather than being a shorthand for \\'the (1 \u2212 *x*)-th quantile\\'. - Secondly, this formula does not use a plus-minus to define the two bounds. Instead, one may use $z = z_{\\alpha / 2}$ to get the lower bound, or use $z = z_{1 - \\alpha/2}$ to get the upper bound. For example: for a 95% confidence level the error $\\alpha$ = 0.05, so one gets the lower bound by using $z = z_{\\alpha/2} = z_{0.025} = - 1.96$, and one gets the upper bound by using $z = z_{1 - \\alpha/2} = z_{0.975} = 1.96$. : : ` `{=html}\\\\frac{ ` \\widehat{p\\,} + \\frac{z^2}{2n} + z`\\ ` \\sqrt{`\\ ` \\frac{\\widehat{p\\,}(1 - \\widehat{p\\,})}{n} +`\\ ` \\frac{z^2}{4 n^2}`\\ ` }` }{ ` 1 + \\frac{z^2}{n}` }` `{=html}[^36] #### Comparison The exact ([Clopper--Pearson](Binomial_proportion_confidence_interval#Clopper\u2013Pearson_interval \"wikilink\")) method is the most conservative.[^37] The Wald method, although commonly recommended in textbooks, is the most biased.`{{clarify|reason=what sense of bias is this|date=July 2012}}`{=mediawiki} ## Computational methods {#computational_methods} ### Generating binomial random variates {#generating_binomial_random_variates} Methods for [random number generation](random_number_generation \"wikilink\") where the [marginal distribution](marginal_distribution \"wikilink\") is a binomial distribution are well-established.[^38][^39] One way to generate random samples from a binomial distribution is to use an inversion algorithm. To do so, one must calculate the probability that `{{math|1=Pr(''X'' = ''k'')}}`{=mediawiki} for all values `{{mvar|k}}`{=mediawiki} from `{{math|0}}`{=mediawiki} through `{{mvar|n}}`{=mediawiki}. (These probabilities should sum to a value close to one, in order to encompass the entire sample space.) Then by using a [pseudorandom number generator](pseudorandom_number_generator \"wikilink\") to generate samples uniformly between 0 and 1, one can transform the calculated samples into discrete numbers by using the probabilities calculated in the first step. ## History This distribution was derived by [Jacob Bernoulli](Jacob_Bernoulli \"wikilink\"). He considered the case where *p* = *r*/(*r* + *s*) where *p* is the probability of success and *r* and *s* are positive integers. [Blaise Pascal](Blaise_Pascal \"wikilink\") had earlier considered the case where *p* = 1/2. ## See also {#see_also} {{Portal|Mathematics}} - [Logistic regression](Logistic_regression \"wikilink\") - [Multinomial distribution](Multinomial_distribution \"wikilink\") - [Negative binomial distribution](Negative_binomial_distribution \"wikilink\") - [Beta-binomial distribution](Beta-binomial_distribution \"wikilink\") - Binomial measure, an example of a [multifractal](Multifractal_system \"wikilink\") [measure](measure_(mathematics) \"wikilink\").[^40] - [Statistical mechanics](Statistical_mechanics \"wikilink\") - [Piling-up lemma](Piling-up_lemma \"wikilink\"), the resulting probability when [XOR](XOR \"wikilink\")-ing independent Boolean variables ## References {{reflist|colwidth=30em}} ## Further reading {#further_reading} - ```{=mediawiki} {{cite book |first=Werner Z. |last=Hirsch |title=Introduction to Modern Statistics |location=New York |publisher=MacMillan |year=1957 |chapter=Binomial Distribution\u2014Success or Failure, How Likely Are They? |pages=140\u2013153 |chapter-url=https://books.google.com/books?id=KostAAAAIAAJ&pg=PA140 }} ``` - ```{=mediawiki} {{cite book |first1=John |last1=Neter |first2=William |last2=Wasserman |first3=G. A. |last3=Whitmore |title=Applied Statistics |location=Boston |publisher=Allyn & Bacon |edition=Third |year=1988 |isbn=0-205-10328-6 |pages=185\u2013192 }} ``` ## External links {#external_links} {{Commons category|Binomial distributions}} - Interactive graphic: [Univariate Distribution Relationships](http://www.math.wm.edu/~leemis/chart/UDR/UDR.html) - [Binomial distribution formula calculator](http://www.fxsolver.com/browse/formulas/Binomial+distribution) - Difference of two binomial variables: [X-Y](https://math.stackexchange.com/q/1065487) or [\\|X-Y\\|](https://math.stackexchange.com/q/562119) - [Querying the binomial probability distribution in WolframAlpha](http://www.wolframalpha.com/input/?i=Prob+x+%3E+19+if+x+is+binomial+with+n+%3D+36++and+p+%3D+.6) {{-}} `{{ProbDistributions|discrete-finite}}`{=mediawiki} {{DEFAULTSORT:Binomial Distribution}} [Category:Discrete distributions](Category:Discrete_distributions \"wikilink\") [Category:Factorial and binomial topics](Category:Factorial_and_binomial_topics \"wikilink\") [Category:Conjugate prior distributions](Category:Conjugate_prior_distributions \"wikilink\") [Category:Exponential family distributions](Category:Exponential_family_distributions \"wikilink\") [2] Quantitative Exploratory Data Analysis Descriptive Statistics Mean ```numpy.mean(a, axis=None, dtype=None)``` * a: array containing numbers whose mean is required * axis: axis or axes along which the means are computed, * default is to compute the mean of the flattened array * dtype: type of data to be used in calculations # get versicolor's petal length versicolor_petal_length = df_iris . loc [ df_iris [ 'species' ] == 'versicolor' ][ 'petal length (cm)' ] . to_numpy ( dtype = np . float64 , copy = False ) # Compute the mean: mean_length_vers mean_length_vers = np . mean ( versicolor_petal_length , axis = None , dtype = np . float16 ) # Print the result with some nice formatting print ( 'Iris. versicolor:' , mean_length_vers , 'cm' ) Median ```numpy.median(a, axis=None, out=None)``` * a: array containing numbers whose median is required * axis: axis or axes along which the median is computed * default is to compute the median of the flattened array * out: alternative output array to place the result, must have the same shape and buffer length as the expected output. # Compute the median: median_length_vers median_length_vers = np . median ( versicolor_petal_length , axis = None ) # Print the result with some nice formatting print ( 'Iris. versicolor:' , median_length_vers , 'cm' ) Mode **The mode is the most frequently occurring number.** scipy . stats . mstats . mode ( a , axis = 0 ) * a: array containing numbers whose mode is required * axis: axis or axes along which the mode is computed * default is 0 * if None, compute the mode of the flattened array It returns (```mode: array of modal values, count: array of counts for each mode```). # Compute the mode: mode_length_vers mode_length_vers = scipy . stats . mstats . mode ( versicolor_petal_length , axis = None ) # Print the result with some nice formatting print ( 'Iris. versicolor:' , mode_length_vers , 'cm' ) Range ```numpy.ptp(a, axis=None, out=None)``` 'ptp' stands for 'peak to peak'. * a: array containing numbers whose range is required * axis: axis or axes along which the range is computed, * default is to compute the range of the flattened array. It returns a new array with the result. # Compute the range: range_length_vers range_length_vers = np . ptp ( versicolor_petal_length , axis = None ) # Print the result with some nice formatting print ( 'Iris. versicolor:' , range_length_vers , 'cm' ) Population Variance Variance can be calculated in python using different libraries like numpy, pandas, and statistics. numpy.var(a, axis=None, dtype=None, ddof=0) * a: array containing numbers whose variance is required * axis: axis or axes along which the variances are computed, default is to compute the mean of the flattened array * ddof : int, optional * ddof stands for delta degrees of freedom. It is the divisor used in the calculation, which is N - ddof, where N is the number of elements. * The default value of ddof is 0. **Formula** $$\\sigma^2 = \\frac{\\displaystyle\\sum_{i=1}^{n}(x_i - \\mu)^2} {n}$$ # Array of differences to mean: differences differences = versicolor_petal_length - \\ np . mean ( versicolor_petal_length , axis = None ) # Square the differences: diff_sq diff_sq = np . square ( differences ) # Compute the mean square difference: variance_explicit variance_explicit = np . sum ( np . square ( versicolor_petal_length - np . mean ( versicolor_petal_length , axis = None ))) / len ( versicolor_petal_length ) # Compute the variance using NumPy: variance_np variance_np = np . var ( versicolor_petal_length , axis = None ) # Print the results print ( variance_explicit , variance_np ) Note that the unit of variance is not the same as the dataset, for example, if we are looking at the Iris dataset's versicolor petal length, which is in cm, and its variance is 0.2164 cm squared. So it is not easy to interpret the variance and hence we can look at the standard deviation, which square roots the variance, to make it the same unit as the dataset. With standard deviation, we have 0.465188 cm. Population Standard Deviation The standard deviation is the square root of variance. numpy.std(a, axis=None, dtype=None, ddof=0) Parameters are the same as ```numpy.var()```. * a: array containing numbers whose standard deviation is required * axis: axis or axes along which the standard deviations are computed, default is to compute the mean of the flattened array * ddof : int, optional * ddof stands for delta degrees of freedom. It is the divisor used in the calculation, which is N - ddof, where N is the number of elements. * The default value of ddof is 0. **Formula** $$\\sigma = \\sqrt{\\frac{\\displaystyle\\sum_{i=1}^{n}(x_i - \\mu)^2} {n}}$$ Because the variance is the average of the distances from the mean _squared_, the standard deviation tells us approximately, on average, the distance of numbers in a distribution from the mean of the distribution. In the **Mean** section, we see that the average petal length is of 4.26 cm, our standard deviation tells us that ***on average***, each data point in the distribution (population) is around 0.465188~ cm away from the mean. # Compute the variance: variance variance = np . var ( versicolor_petal_length , axis = None ) # Print the square root of the variance print ( np . sqrt ( variance )) # Print the standard deviation print ( np . std ( versicolor_petal_length , axis = None )) Percentile # Specify array of percentiles: percentiles percentiles = np . array ([ 2.5 , 25 , 50 , 75 , 97.5 ]) # Compute percentiles: ptiles_vers ptiles_vers = np . percentile ( a = versicolor_petal_length , q = percentiles , axis = None ) # Print the result print ( ptiles_vers ) def ecdf ( data ): \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\" # Number of data points: n n = len ( data ) # x-data for the ECDF: x x = np . sort ( data ) # y-data for the ECDF: y y = np . arange ( start = 1 , stop = n + 1 ) / n return x , y # Plot the ECDF x_vers , y_vers = ecdf ( versicolor_petal_length ) _ = plt . plot ( x_vers , y_vers , \".\" ) _ = plt . xlabel ( 'petal length (cm)' ) _ = plt . ylabel ( 'ECDF' ) # Overlay percentiles as red diamonds. _ = plt . plot ( ptiles_vers , percentiles / 100 , marker = 'D' , color = 'red' , linestyle = 'none' ) # Save the plot save_fig ( \"[Versicolor] Percentile + ECDF Plot on Petal Length\" ) # Show the plot plt . show () Skewness scipy.stats.skew(a, axis=0) For normally distributed data, the skewness should be about 0. A skewness value > 0 means that there is more weight in the left tail of the distribution. Skewness refers to the lack of symmetry in a distribution of data. [Technical note: we will be talking about skewness here only in the context of _unimodal_ distributions.] ![image.png](attachment:e5188fd8-9bb3-422f-8121-a459a129189f.png) A *positive-skewed* distribution is one whose right tail is longer or fatter than its left. Conversely, a *negative-skewed* distribution is one whose left tail is longer or fatter than its right. Symmetric distributions have no skewness! #### Skewness and measures of central tendency The mean, median, and mode are affected by skewness. When a distribution is symmetric, the mean, median, and mode are the same. > Symmetric: mean == median == mode When a distribution is negatively skewed, the mean is less than the median, which is less than the mode. > Negative skew: mean < median < mode When a distribution is positively skewed, the mean is greater than the median, which is greater than the mode. > Positive skew: mode < median < mean You\u2019ve learned numerical measures of center, spread, and outliers, but what about **measures of shape**? The histogram can give you a general idea of the shape, but two numerical measures of shape give a more precise evaluation: **skewness tells you the amount and direction of skew** (departure from horizontal symmetry), and **kurtosis tells you how tall and sharp the central peak is**, relative to a standard bell curve. Why do we care? One application is **testing for normality**: many statistics inferences require that a distribution be normal or nearly normal. A normal distribution has skewness and excess kurtosis of 0, so if your distribution is close to those values then it is probably close to normal. # Compute the range: range_length_vers skew_length_vers = scipy . stats . skew ( versicolor_petal_length , axis = None ) # Print the result with some nice formatting print ( 'Iris. versicolor:' , skew_length_vers , 'cm' ) Indeed, our skew is negative, indicating the data is in a slightly negative skewed distribution, where the left tail is longer. A natural question that follows is how to calculate skewness? How is the value -0.588158~ quantified? _ = sns . displot ( x = versicolor_petal_length , kind = 'kde' ) _ = plt . xlabel ( xlabel = 'petal length (cm)' ) _ = sns . displot ( x = versicolor_petal_length , kind = 'hist' ) _ = plt . xlabel ( xlabel = 'petal length (cm)' ) _ = sns . displot ( x = versicolor_petal_length , kind = 'ecdf' ) _ = plt . xlabel ( xlabel = 'petal length (cm)' ) Box and Whiskers Plot Making a box plot for the petal lengths is unnecessary because the iris data set is not too large and the bee swarm plot works fine. Furthermore, A box and whisker plot\u2014also called a box plot\u2014displays the five-number summary of a set of data. The five-number summary is the minimum, first quartile, median, third quartile, and maximum. In a box plot, we draw a box from the first quartile to the third quartile. A vertical/horizontal line goes through the box at the median. The whiskers go from each quartile to the minimum or maximum. # Create box plot with Seaborn's default settings _ = sns . boxplot ( x = 'species' , y = 'petal length (cm)' , data = df_iris ) # Label the axes _ = plt . xlabel ( xlabel = 'species' ) _ = plt . ylabel ( ylabel = 'petal length (cm)' ) # Save the plot save_fig ( \"[Versicolor] Box and Whiskers Plot on Petal Length\" ) # Show the plot plt . show () To understand the graph above, let us just look at **versicolor** for consistency. In simple terms, the ECDF of this species can be interpreted as follows: what percentage of **versicolor** have a **petal length** of less than 4 cm? By eyeballing, we can tell that about $20\\%$ just by pinpointing which value on the y-axis (ECDF) corresponds to 4 cm on the x-axis (petal length).","title":"Day 1 binomial distribution"},{"location":"reighns_ml_journey/mathematics/probability_statistics_theory/day-1-graphical-exploratory-data-analysis/","text":"66 Days of Data [03/07/2021] Day 1 Quick Navigation * [Dependencies](#1) * [Configurations](#2) * [Seeding](#3) * [Loading Iris Data](#4) * [Graphical Exploratory Data Analysis](#5) * [Histogram](#51) * [Bee Swarm Plot](#52) * [Empirical Cumulative Distribution Function](#53) * [ECDF vs CDF](#531) Dependencies # !pip install -U -q scikit-learn==0.24.2 import os import random import sys import seaborn as sns import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import pandas as pd import sklearn import torch from sklearn.datasets import * assert sys . version_info >= ( 3 , 5 ) assert sklearn . __version__ >= \"0.20\" Configurations np . random . seed ( 42 ) % matplotlib inline sns . set ( style = \"ticks\" ) plt . style . use ( \"dark_background\" ) # mpl.rc('axes', labelsize=15) # mpl.rc('xtick', labelsize=12) # mpl.rc('ytick', labelsize=12) # Where to save the figures PROJECT_ROOT_DIR = \".\" CHAPTER_ID = \"Graphical Exploratory Data Analysis\" IMAGES_PATH = os . path . join ( PROJECT_ROOT_DIR , \"images\" , CHAPTER_ID ) os . makedirs ( IMAGES_PATH , exist_ok = True ) def save_fig ( fig_id , tight_layout = True , fig_extension = \"png\" , resolution = 300 ): path = os . path . join ( IMAGES_PATH , fig_id + \".\" + fig_extension ) print ( \"Saving figure\" , fig_id ) if tight_layout : plt . tight_layout () plt . savefig ( path , format = fig_extension , dpi = resolution ) Seeding def seed_all ( seed : int = 1930 ): \"\"\"Seed all random number generators.\"\"\" print ( \"Using Seed Number {} \" . format ( seed )) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) # set PYTHONHASHSEED env var at fixed value torch . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) torch . cuda . manual_seed ( seed ) # pytorch (both CPU and CUDA) np . random . seed ( seed ) # for numpy pseudo-random generator random . seed ( seed ) # set fixed value for python built-in pseudo-random generator torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = False torch . backends . cudnn . enabled = False def seed_worker ( _worker_id ): \"\"\"Seed a worker with the given ID.\"\"\" worker_seed = torch . initial_seed () % 2 ** 32 np . random . seed ( worker_seed ) random . seed ( worker_seed ) seed_all ( 1992 ) Using Seed Number 1992 Loading Iris Data # import iris and load as dataframe iris = load_iris ( return_X_y = False , as_frame = True ) # make a copy of iris df_iris = iris . frame . copy () # change target to species and map 0,1,2 to species df_iris . columns = [ 'sepal length (cm)' , 'sepal width (cm)' , 'petal length (cm)' , 'petal width (cm)' , 'species' ] species_mapping = { 0 : 'setosa' , 1 : 'versicolor' , 2 : 'virginica' } df_iris [ 'species' ] = df_iris [ 'species' ] . map ( species_mapping ) [1] Graphical Exploratory Data Analysis Histogram - Used for continuous data while bar charts are used for categorical data. - To construct a histogram, the first step is to \"bin\" (or \"bucket\") the range of values\u2014that is, divide the entire range of values into a series of intervals\u2014and then count how many values fall into each interval. We used the square root rule to calculate the number of bins in the histogram below. It turns out that `int(n_bins)=7`. It is a nice number because if I set the x-tick step size to be 0.3, we can easily see that each interval contains how many such values. For example, we know that the `max(versicolor_petal_length)=5.1` and the `min(versicolor_petal_length)=3`. Therefore, the range is 2.1. We can divide 2.1 into 7 bins, where each interval is 0.3. This is why I chose the x-tick step size to be 0.3 earlier. From the graph, it is now easy to interpret that there are 1 count from 3-3.3, 4 counts from 3.3-3.6 and so on. You can uncomment `_ = plt.hist(x=versicolor_petal_length, bins=20, edgecolor='blue', linewidth=1.2)` below to see that when you increase the number of bins, your data representation seemingly change, which is one of the drawbacks of histograms. # get versicolor's petal length versicolor_petal_length = df_iris . loc [ df_iris [ 'species' ] == 'versicolor' ][ 'petal length (cm)' ] . to_numpy ( dtype = np . float16 , copy = False ) # Compute number of data points: n_data n_data = len ( versicolor_petal_length ) # Number of bins is the square root of number of data points: n_bins n_bins = np . sqrt ( n_data ) # Convert number of bins to integer: n_bins n_bins = int ( n_bins ) # Plot the histogram _ = plt . hist ( x = versicolor_petal_length , bins = n_bins , edgecolor = 'red' , linewidth = 1.2 ) # _ = plt.hist(x=versicolor_petal_length, bins=20, edgecolor='blue', linewidth=1.2) # Label axes _ = plt . xlabel ( 'petal length (cm)' ) _ = plt . ylabel ( 'count' ) # setting x and y ticks to show a more granular level of x and y range. # note carefully that np.arange does not include the stop value, except when my step size is in float (like the case below) _ = plt . xticks ( np . arange ( start = min ( versicolor_petal_length ), stop = max ( versicolor_petal_length ), step = 0.3 )) _ = plt . yticks ( np . arange ( start = 0 , stop = 18 , step = 2 )) # Save the plot save_fig ( \"[Versicolor] Histogram Plot on Petal Length\" ) # Show histogram plt . show () Saving figure [Versicolor] Histogram Plot on Petal Length ![png](day-1-graphical-exploratory-data-analysis_files/day-1-graphical-exploratory-data-analysis_14_1.png) We can tell that there are more data clustered in between 3.9-4.8 cm. Bee Swarm Plot An additional problem with histograms is that we are not plotting all of the data. We are sweeping the data into bins, and losing their actual values. This means that we are not on a granular level when reading off histogram plots, instead, we are grouping data in terms of bins, like from this range to that range, how many data are within the said intervals. This can lead to what we mentioned on the point above - binning bias. Bee Swarm Plot addresses this issue by plotting all data points of Iris dataset where x-axis is the species, and y-axis the petal length. This diagram below attempts to show all data points, linking back to the histogram previously, we can tell that for versicolor species, there are more data points clustered in between 4-4.75 cm (roughly the same as what we saw in histogram). # get petal species and petal length petal_species = df_iris [ 'species' ] petal_length = df_iris [ 'petal length (cm)' ] # _ = sns.swarmplot(x=petal_species, y = petal_length) same as below # note that change marker size from 5 to 3 to avoid the plot warning on marker size too big _ = sns . swarmplot ( x = 'species' , y = 'petal length (cm)' , data = df_iris , size = 3 ) # Label the axes _ = plt . xlabel ( xlabel = 'species' ) _ = plt . ylabel ( ylabel = 'petal length (cm)' ) _ = plt . xlabel ( 'petal length (cm)' ) _ = plt . ylabel ( 'count' ) # setting x and y ticks to show a more granular level of x and y range. _ = plt . yticks ( np . arange ( start = 0 , stop = max ( petal_length ) + 1 , step = 1 )) # Save the plot save_fig ( \"[Iris] Bee Swarm Plot on Petal Length\" ) # Show the plot plt . show () Saving figure [Iris] Bee Swarm Plot on Petal Length ![png](day-1-graphical-exploratory-data-analysis_files/day-1-graphical-exploratory-data-analysis_18_1.png) Empirical Cumulative Distribution Function An ECDF is an estimator of the Cumulative Distribution Function. To read it, it is similar to the idea of percentiles. To code it in python: 1. sort the data in ascending order 2. calculate the number of samples in the data 3. calculate the y-axis in a binning manner: start from 0, with increment of 1/n until it reaches 1. def ecdf ( data ): \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\" # Number of data points: n n = len ( data ) # x-data for the ECDF: x x = np . sort ( data ) # y-data for the ECDF: y y = np . arange ( start = 1 , stop = n + 1 ) / n return x , y versicolor_petal_length = df_iris . loc [ df_iris [ 'species' ] == 'versicolor' ][ 'petal length (cm)' ] . to_numpy ( dtype = np . float16 , copy = False ) setosa_petal_length = df_iris . loc [ df_iris [ 'species' ] == 'setosa' ][ 'petal length (cm)' ] . to_numpy ( dtype = np . float16 , copy = False ) virginica_petal_length = df_iris . loc [ df_iris [ 'species' ] == 'virginica' ][ 'petal length (cm)' ] . to_numpy ( dtype = np . float16 , copy = False ) # Compute ECDFs x_set , y_set = ecdf ( setosa_petal_length ) x_vers , y_vers = ecdf ( versicolor_petal_length ) x_virg , y_virg = ecdf ( virginica_petal_length ) # Plot all ECDFs on the same plot _ = plt . plot ( x_set , y_set , marker = \".\" , linestyle = 'none' ) _ = plt . plot ( x_vers , y_vers , marker = \".\" , linestyle = 'none' ) _ = plt . plot ( x_virg , y_virg , marker = \".\" , linestyle = 'none' ) # Annotate the plot plt . legend (( 'setosa' , 'versicolor' , 'virginica' ), loc = 'lower right' ) _ = plt . xlabel ( 'petal length (cm)' ) _ = plt . ylabel ( 'ECDF' ) # Save the plot save_fig ( \"[Iris] ECDF Plot on Petal Length\" ) # Display the plot plt . show () Saving figure [Iris] ECDF Plot on Petal Length ![png](day-1-graphical-exploratory-data-analysis_files/day-1-graphical-exploratory-data-analysis_22_1.png) To understand the graph above, let us just look at **versicolor** for consistency. In simple terms, the ECDF of this species can be interpreted as follows: what percentage of **versicolor** have a **petal length** of less than 4 cm? By eyeballing, we can tell that about $20\\%$ just by pinpointing which value on the y-axis (ECDF) corresponds to 4 cm on the x-axis (petal length). ECDF vs CDF Let $X$ be a random variable. - The cumulative distribution function $F(x)$ gives the $P(X \\leq x)$. - An empirical cumulative distribution function function $G(x)$ gives $P(X \\leq x)$ based on the observations in your sample. The distinction is which probability measure is used. For the empirical CDF, you use the probability measure defined by the frequency counts in an empirical sample. **Simple example (coin flip):** Let $X$ be a random variable denoting the result of a single coin flip where $X=1$ denotes heads and $X=0$ denotes tails. The CDF for a fair coin is given by: $$ F(x) = \\left\\{ \\begin{array}{ll} 0 & \\text{for } x < 0\\\\ \\frac{1}{2} & \\text{for } 0 \\leq x < 1 \\\\1 & \\text{for } 1 \\leq x \\end{array} \\right. $$ If you flipped 2 heads and 1 tail, the empirical CDF would be: $$ G(x) = \\left\\{ \\begin{array}{ll} 0 & \\text{for } x < 0\\\\ \\frac{2}{3} & \\text{for } 0 \\leq x < 1 \\\\1 & \\text{for } 1 \\leq x \\end{array} \\right. $$ The empirical CDF would reflect that in your sample, $2/3$ of your flips were heads. **Another example ($F$ is CDF for normal distribution):** Let $X$ be a normally distributed random variable with mean $0$ and standard deviation $1$. The CDF is given by: $$F(x) = \\int_{-\\infty}^x \\frac{1}{\\sqrt{2\\pi}} e^{\\frac{-x^2}{2}}$$ Let's say you had 3 IID draws and obtained the values $x_1 < x_2 < x_3$. The empirical CDF would be: $$ G(y) = \\left\\{ \\begin{array}{ll} 0 & \\text{for } y < x_1\\\\ \\frac{1}{3} & \\text{for } x_1 \\leq y < x_2 \\\\\\frac{2}{3} & \\text{for } x_2 \\leq y < x_3 \\\\1 & \\text{for } x_3 \\leq y \\end{array} \\right. $$ With enough IID draws (and certain regularity conditions are satisfied), the empirical CDF would converge on the underlying CDF of the population.","title":"Day 1 graphical exploratory data analysis"},{"location":"reighns_ml_journey/mathematics/probability_statistics_theory/day-2-quantitative-exploratory-data-analysis/","text":"66 Days of Data [04/07/2021] Day 2 This is slightly modified from the original module in Datacamp, as I find it better to put the concept of covariance later on. Quick Navigation * [Dependencies](#1) * [Configurations](#2) * [Seeding](#3) * [Loading Iris Data](#4) * [Quantitative Exploratory Data Analysis](#5) * [Descripte Analytics](#51) * [Mean](#511) * [Median](#512) * [Mode](#513) * [Range](#514) * [Population Variance](#515) * [Population Standard Deviation](#516) * [Percentiles](#517) * [Box and Whiskers Plot](#52) Dependencies # !pip install -U -q scikit-learn==0.24.2 import os import random import sys import seaborn as sns import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import pandas as pd import sklearn import torch import scipy from sklearn.datasets import * assert sys . version_info >= ( 3 , 5 ) assert sklearn . __version__ >= \"0.20\" Configurations np . random . seed ( 42 ) % matplotlib inline sns . set ( style = \"ticks\" ) plt . style . use ( \"dark_background\" ) # mpl.rc('axes', labelsize=15) # mpl.rc('xtick', labelsize=12) # mpl.rc('ytick', labelsize=12) # Where to save the figures PROJECT_ROOT_DIR = \".\" CHAPTER_ID = \"Quantitative Exploratory Data Analysis\" IMAGES_PATH = os . path . join ( PROJECT_ROOT_DIR , \"images\" , CHAPTER_ID ) os . makedirs ( IMAGES_PATH , exist_ok = True ) def save_fig ( fig_id , tight_layout = True , fig_extension = \"png\" , resolution = 300 ): path = os . path . join ( IMAGES_PATH , fig_id + \".\" + fig_extension ) print ( \"Saving figure\" , fig_id ) if tight_layout : plt . tight_layout () plt . savefig ( path , format = fig_extension , dpi = resolution ) Seeding def seed_all ( seed : int = 1930 ): \"\"\"Seed all random number generators.\"\"\" print ( \"Using Seed Number {} \" . format ( seed )) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) # set PYTHONHASHSEED env var at fixed value torch . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) torch . cuda . manual_seed ( seed ) # pytorch (both CPU and CUDA) np . random . seed ( seed ) # for numpy pseudo-random generator random . seed ( seed ) # set fixed value for python built-in pseudo-random generator torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = False torch . backends . cudnn . enabled = False def seed_worker ( _worker_id ): \"\"\"Seed a worker with the given ID.\"\"\" worker_seed = torch . initial_seed () % 2 ** 32 np . random . seed ( worker_seed ) random . seed ( worker_seed ) seed_all ( 1992 ) Using Seed Number 1992 Loading Iris Data # import iris and load as dataframe iris = load_iris ( return_X_y = False , as_frame = True ) # make a copy of iris df_iris = iris . frame . copy () # change target to species and map 0,1,2 to species df_iris . columns = [ 'sepal length (cm)' , 'sepal width (cm)' , 'petal length (cm)' , 'petal width (cm)' , 'species' ] species_mapping = { 0 : 'setosa' , 1 : 'versicolor' , 2 : 'virginica' } df_iris [ 'species' ] = df_iris [ 'species' ] . map ( species_mapping ) [2] Quantitative Exploratory Data Analysis Descriptive Statistics Mean ```numpy.mean(a, axis=None, dtype=None)``` * a: array containing numbers whose mean is required * axis: axis or axes along which the means are computed, * default is to compute the mean of the flattened array * dtype: type of data to be used in calculations # get versicolor's petal length versicolor_petal_length = df_iris . loc [ df_iris [ 'species' ] == 'versicolor' ][ 'petal length (cm)' ] . to_numpy ( dtype = np . float64 , copy = False ) # Compute the mean: mean_length_vers mean_length_vers = np . mean ( versicolor_petal_length , axis = None , dtype = np . float16 ) # Print the result with some nice formatting print ( 'Iris. versicolor:' , mean_length_vers , 'cm' ) Iris. versicolor: 4.26 cm Median ```numpy.median(a, axis=None, out=None)``` * a: array containing numbers whose median is required * axis: axis or axes along which the median is computed * default is to compute the median of the flattened array * out: alternative output array to place the result, must have the same shape and buffer length as the expected output. # Compute the median: median_length_vers median_length_vers = np . median ( versicolor_petal_length , axis = None ) # Print the result with some nice formatting print ( 'Iris. versicolor:' , median_length_vers , 'cm' ) Iris. versicolor: 4.35 cm Mode **The mode is the most frequently occurring number.** scipy . stats . mstats . mode ( a , axis = 0 ) * a: array containing numbers whose mode is required * axis: axis or axes along which the mode is computed * default is 0 * if None, compute the mode of the flattened array It returns (```mode: array of modal values, count: array of counts for each mode```). # Compute the mode: mode_length_vers mode_length_vers = scipy . stats . mstats . mode ( versicolor_petal_length , axis = None ) # Print the result with some nice formatting print ( 'Iris. versicolor:' , mode_length_vers , 'cm' ) Iris. versicolor: ModeResult(mode=array([4.5]), count=array([7.])) cm Range ```numpy.ptp(a, axis=None, out=None)``` 'ptp' stands for 'peak to peak'. * a: array containing numbers whose range is required * axis: axis or axes along which the range is computed, * default is to compute the range of the flattened array. It returns a new array with the result. # Compute the range: range_length_vers range_length_vers = np . ptp ( versicolor_petal_length , axis = None ) # Print the result with some nice formatting print ( 'Iris. versicolor:' , range_length_vers , 'cm' ) Iris. versicolor: 2.0999999999999996 cm Population Variance Variance can be calculated in python using different libraries like numpy, pandas, and statistics. numpy.var(a, axis=None, dtype=None, ddof=0) * a: array containing numbers whose variance is required * axis: axis or axes along which the variances are computed, default is to compute the mean of the flattened array * ddof : int, optional * ddof stands for delta degrees of freedom. It is the divisor used in the calculation, which is N - ddof, where N is the number of elements. * The default value of ddof is 0. **Formula** $$\\sigma^2 = \\frac{\\displaystyle\\sum_{i=1}^{n}(x_i - \\mu)^2} {n}$$ # Array of differences to mean: differences differences = versicolor_petal_length - \\ np . mean ( versicolor_petal_length , axis = None ) # Square the differences: diff_sq diff_sq = np . square ( differences ) # Compute the mean square difference: variance_explicit variance_explicit = np . sum ( np . square ( versicolor_petal_length - np . mean ( versicolor_petal_length , axis = None ))) / len ( versicolor_petal_length ) # Compute the variance using NumPy: variance_np variance_np = np . var ( versicolor_petal_length , axis = None ) # Print the results print ( variance_explicit , variance_np ) 0.21640000000000004 0.21640000000000004 Note that the unit of variance is not the same as the dataset, for example, if we are looking at the Iris dataset's versicolor petal length, which is in cm, and its variance is 0.2164 cm squared. So it is not easy to interpret the variance and hence we can look at the standard deviation, which square roots the variance, to make it the same unit as the dataset. With standard deviation, we have 0.465188 cm. Population Standard Deviation The standard deviation is the square root of variance. numpy.std(a, axis=None, dtype=None, ddof=0) Parameters are the same as ```numpy.var()```. * a: array containing numbers whose standard deviation is required * axis: axis or axes along which the standard deviations are computed, default is to compute the mean of the flattened array * ddof : int, optional * ddof stands for delta degrees of freedom. It is the divisor used in the calculation, which is N - ddof, where N is the number of elements. * The default value of ddof is 0. **Formula** $$\\sigma = \\sqrt{\\frac{\\displaystyle\\sum_{i=1}^{n}(x_i - \\mu)^2} {n}}$$ Because the variance is the average of the distances from the mean _squared_, the standard deviation tells us approximately, on average, the distance of numbers in a distribution from the mean of the distribution. In the **Mean** section, we see that the average petal length is of 4.26 cm, our standard deviation tells us that ***on average***, each data point in the distribution (population) is around 0.465188~ cm away from the mean. # Compute the variance: variance variance = np . var ( versicolor_petal_length , axis = None ) # Print the square root of the variance print ( np . sqrt ( variance )) # Print the standard deviation print ( np . std ( versicolor_petal_length , axis = None )) 0.4651881339845203 0.4651881339845203 Percentile # Specify array of percentiles: percentiles percentiles = np . array ([ 2.5 , 25 , 50 , 75 , 97.5 ]) # Compute percentiles: ptiles_vers ptiles_vers = np . percentile ( a = versicolor_petal_length , q = percentiles , axis = None ) # Print the result print ( ptiles_vers ) [3.3 4. 4.35 4.6 4.9775] def ecdf ( data ): \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\" # Number of data points: n n = len ( data ) # x-data for the ECDF: x x = np . sort ( data ) # y-data for the ECDF: y y = np . arange ( start = 1 , stop = n + 1 ) / n return x , y # Plot the ECDF x_vers , y_vers = ecdf ( versicolor_petal_length ) _ = plt . plot ( x_vers , y_vers , \".\" ) _ = plt . xlabel ( 'petal length (cm)' ) _ = plt . ylabel ( 'ECDF' ) # Overlay percentiles as red diamonds. _ = plt . plot ( ptiles_vers , percentiles / 100 , marker = 'D' , color = 'red' , linestyle = 'none' ) # Save the plot save_fig ( \"[Versicolor] Percentile + ECDF Plot on Petal Length\" ) # Show the plot plt . show () Saving figure [Versicolor] Percentile + ECDF Plot on Petal Length ![png](day-2-quantitative-exploratory-data-analysis_files/day-2-quantitative-exploratory-data-analysis_39_1.png) Skewness scipy.stats.skew(a, axis=0) For normally distributed data, the skewness should be about 0. A skewness value > 0 means that there is more weight in the left tail of the distribution. Skewness refers to the lack of symmetry in a distribution of data. [Technical note: we will be talking about skewness here only in the context of _unimodal_ distributions.] ![image.png](attachment:e5188fd8-9bb3-422f-8121-a459a129189f.png) A *positive-skewed* distribution is one whose right tail is longer or fatter than its left. Conversely, a *negative-skewed* distribution is one whose left tail is longer or fatter than its right. Symmetric distributions have no skewness! #### Skewness and measures of central tendency The mean, median, and mode are affected by skewness. When a distribution is symmetric, the mean, median, and mode are the same. > Symmetric: mean == median == mode When a distribution is negatively skewed, the mean is less than the median, which is less than the mode. > Negative skew: mean < median < mode When a distribution is positively skewed, the mean is greater than the median, which is greater than the mode. > Positive skew: mode < median < mean You\u2019ve learned numerical measures of center, spread, and outliers, but what about **measures of shape**? The histogram can give you a general idea of the shape, but two numerical measures of shape give a more precise evaluation: **skewness tells you the amount and direction of skew** (departure from horizontal symmetry), and **kurtosis tells you how tall and sharp the central peak is**, relative to a standard bell curve. Why do we care? One application is **testing for normality**: many statistics inferences require that a distribution be normal or nearly normal. A normal distribution has skewness and excess kurtosis of 0, so if your distribution is close to those values then it is probably close to normal. # Compute the range: range_length_vers skew_length_vers = scipy . stats . skew ( versicolor_petal_length , axis = None ) # Print the result with some nice formatting print ( 'Iris. versicolor:' , skew_length_vers , 'cm' ) Iris. versicolor: -0.5881586743962586 cm Indeed, our skew is negative, indicating the data is in a slightly negative skewed distribution, where the left tail is longer. A natural question that follows is how to calculate skewness? How is the value -0.588158~ quantified? _ = sns . displot ( x = versicolor_petal_length , kind = 'kde' ) _ = plt . xlabel ( xlabel = 'petal length (cm)' ) _ = sns . displot ( x = versicolor_petal_length , kind = 'hist' ) _ = plt . xlabel ( xlabel = 'petal length (cm)' ) _ = sns . displot ( x = versicolor_petal_length , kind = 'ecdf' ) _ = plt . xlabel ( xlabel = 'petal length (cm)' ) ![png](day-2-quantitative-exploratory-data-analysis_files/day-2-quantitative-exploratory-data-analysis_46_0.png) ![png](day-2-quantitative-exploratory-data-analysis_files/day-2-quantitative-exploratory-data-analysis_46_1.png) ![png](day-2-quantitative-exploratory-data-analysis_files/day-2-quantitative-exploratory-data-analysis_46_2.png) Box and Whiskers Plot Making a box plot for the petal lengths is unnecessary because the iris data set is not too large and the bee swarm plot works fine. Furthermore, A box and whisker plot\u2014also called a box plot\u2014displays the five-number summary of a set of data. The five-number summary is the minimum, first quartile, median, third quartile, and maximum. In a box plot, we draw a box from the first quartile to the third quartile. A vertical/horizontal line goes through the box at the median. The whiskers go from each quartile to the minimum or maximum. # Create box plot with Seaborn's default settings _ = sns . boxplot ( x = 'species' , y = 'petal length (cm)' , data = df_iris ) # Label the axes _ = plt . xlabel ( xlabel = 'species' ) _ = plt . ylabel ( ylabel = 'petal length (cm)' ) # Save the plot save_fig ( \"[Versicolor] Box and Whiskers Plot on Petal Length\" ) # Show the plot plt . show () Saving figure [Versicolor] Box and Whiskers Plot on Petal Length ![png](day-2-quantitative-exploratory-data-analysis_files/day-2-quantitative-exploratory-data-analysis_49_1.png) To understand the graph above, let us just look at **versicolor** for consistency. In simple terms, the ECDF of this species can be interpreted as follows: what percentage of **versicolor** have a **petal length** of less than 4 cm? By eyeballing, we can tell that about $20\\%$ just by pinpointing which value on the y-axis (ECDF) corresponds to 4 cm on the x-axis (petal length).","title":"Day 2 quantitative exploratory data analysis"},{"location":"reighns_ml_journey/mathematics/probability_statistics_theory/day-3-probabilistically-discrete-variables/","text":"66 Days of Data [05/07/2021] Day 3 This is slightly modified from the original module in Datacamp, as I find it better to put the concept of covariance later on. Quick Navigation * [Dependencies](#1) * [Configurations](#2) * [Seeding](#3) * [Loading Iris Data](#4) * [Stastical Inferences](#5) * [Descripte Analytics](#51) * [Mean](#511) * [Median](#512) * [Mode](#513) * [Range](#514) * [Population Variance](#515) * [Population Standard Deviation](#516) * [Percentiles](#517) * [Box and Whiskers Plot](#52) Dependencies # !pip install -U -q scikit-learn==0.24.2 import os import random import sys import seaborn as sns import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import pandas as pd import sklearn import torch import scipy from sklearn.datasets import * assert sys . version_info >= ( 3 , 5 ) assert sklearn . __version__ >= \"0.20\" Configurations np . random . seed ( 42 ) % matplotlib inline sns . set ( style = \"ticks\" ) plt . style . use ( \"dark_background\" ) # mpl.rc('axes', labelsize=15) # mpl.rc('xtick', labelsize=12) # mpl.rc('ytick', labelsize=12) # Where to save the figures PROJECT_ROOT_DIR = \".\" CHAPTER_ID = \"Quantitative Exploratory Data Analysis\" IMAGES_PATH = os . path . join ( PROJECT_ROOT_DIR , \"images\" , CHAPTER_ID ) os . makedirs ( IMAGES_PATH , exist_ok = True ) def save_fig ( fig_id , tight_layout = True , fig_extension = \"png\" , resolution = 300 ): path = os . path . join ( IMAGES_PATH , fig_id + \".\" + fig_extension ) print ( \"Saving figure\" , fig_id ) if tight_layout : plt . tight_layout () plt . savefig ( path , format = fig_extension , dpi = resolution ) Seeding def seed_all ( seed : int = 1930 ): \"\"\"Seed all random number generators.\"\"\" print ( \"Using Seed Number {} \" . format ( seed )) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) # set PYTHONHASHSEED env var at fixed value torch . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) torch . cuda . manual_seed ( seed ) # pytorch (both CPU and CUDA) np . random . seed ( seed ) # for numpy pseudo-random generator random . seed ( seed ) # set fixed value for python built-in pseudo-random generator torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = False torch . backends . cudnn . enabled = False def seed_worker ( _worker_id ): \"\"\"Seed a worker with the given ID.\"\"\" worker_seed = torch . initial_seed () % 2 ** 32 np . random . seed ( worker_seed ) random . seed ( worker_seed ) seed_all ( 1992 ) Using Seed Number 1992 Loading Iris Data {{short description|Any experiment with two possible random outcomes}} {{Probability fundamentals}} ![Graphs of probability *P* of **not** observing independent events each of probability *p* after *n* Bernoulli trials vs *np* for various *p*. Three examples are shown:\\ **Blue curve**: Throwing a 6-sided die 6 times gives 33.5% chance that 6 (or any other given number) never turns up; it can be observed that as *n* increases, the probability of a 1/*n*-chance event never appearing after *n* tries rapidly converges to *0*.\\ **Grey curve**: To get 50-50 chance of throwing a [Yahtzee](Yahtzee \"wikilink\") (5 cubic dice all showing the same number) requires 0.69 \u00d7 1296 \\~ 898 throws.\\ **Green curve**: Drawing a card from a deck of playing cards without jokers 100 (1.92 \u00d7 52) times with replacement gives 85.7% chance of drawing the ace of spades at least once.](Bernoulli_trial_progression.svg \"fig:Graphs of probability P of not observing independent events each of probability p after n Bernoulli trials vs np for various p. Three examples are shown: Blue curve: Throwing a 6-sided die 6 times gives 33.5% chance that 6 (or any other given number) never turns up; it can be observed that as n increases, the probability of a 1/n-chance event never appearing after n tries rapidly converges to 0. Grey curve: To get 50-50 chance of throwing a Yahtzee (5 cubic dice all showing the same number) requires 0.69 \u00d7 1296 ~ 898 throws. Green curve: Drawing a card from a deck of playing cards without jokers 100 (1.92 \u00d7 52) times with replacement gives 85.7% chance of drawing the ace of spades at least once.\"){width=\"400\"} In the theory of [probability](probability \"wikilink\") and [statistics](statistics \"wikilink\"), a **Bernoulli trial** (or **binomial trial**) is a random [experiment](Experiment_(probability_theory) \"wikilink\") with exactly two possible [outcomes](Outcome_(probability) \"wikilink\"), \\\"success\\\" and \\\"failure\\\", in which the probability of success is the same every time the experiment is conducted.[^1] It is named after [Jacob Bernoulli](Jacob_Bernoulli \"wikilink\"), a 17th-century Swiss mathematician, who analyzed them in his *[Ars Conjectandi](Ars_Conjectandi \"wikilink\")* (1713).[^2] The mathematical formalisation of the Bernoulli trial is known as the [Bernoulli process](Bernoulli_process \"wikilink\"). This article offers an elementary introduction to the concept, whereas the article on the Bernoulli process offers a more advanced treatment. Since a Bernoulli trial has only two possible outcomes, it can be framed as some \\\"yes or no\\\" question. For example: - Is the top card of a shuffled deck an ace? - Was the newborn child a girl? (See [human sex ratio](human_sex_ratio \"wikilink\").) Therefore, success and failure are merely labels for the two outcomes, and should not be construed literally. The term \\\"success\\\" in this sense consists in the result meeting specified conditions, not in any moral judgement. More generally, given any [probability space](probability_space \"wikilink\"), for any [event](Event_(probability_theory) \"wikilink\") (set of outcomes), one can define a Bernoulli trial, corresponding to whether the event occurred or not (event or [complementary event](complementary_event \"wikilink\")). Examples of Bernoulli trials include: - Flipping a coin. In this context, obverse (\\\"heads\\\") conventionally denotes success and reverse (\\\"tails\\\") denotes failure. A [fair coin](fair_coin \"wikilink\") has the probability of success 0.5 by definition. In this case there are exactly two possible outcomes. - Rolling a `{{dice}}`{=mediawiki}, where a six is \\\"success\\\" and everything else a \\\"failure\\\". In this case there are six possible outcomes, and the event is a six; the complementary event \\\"not a six\\\" corresponds to the other five possible outcomes. - In conducting a political [opinion poll](opinion_poll \"wikilink\"), choosing a voter at random to ascertain whether that voter will vote \\\"yes\\\" in an upcoming referendum. ## Definition Independent repeated trials of an experiment with exactly two possible outcomes are called Bernoulli trials. Call one of the outcomes \\\"success\\\" and the other outcome \\\"failure\\\". Let $p$ be the probability of success in a Bernoulli trial, and $q$ be the probability of failure. Then the probability of success and the probability of failure sum to one, since these are complementary events: \\\"success\\\" and \\\"failure\\\" are [mutually exclusive](mutually_exclusive \"wikilink\") and [exhaustive](Collectively_exhaustive_events \"wikilink\"). Thus one has the following relations: $$p = 1 - q, \\quad \\quad q = 1 - p, \\quad \\quad p + q = 1.$$ Alternatively, these can be stated in terms of [odds](Odds_(statistics) \"wikilink\"): given probability *p* of success and *q* of failure, the *odds for* are $p:q$ and the *odds against* are $q:p.$ These can also be expressed as numbers, by dividing, yielding the odds for, $o_f$, and the odds against, $o_a:$, $$\\begin{align} o_f &= p/q = p/(1-p) = (1-q)/q\\\\ o_a &= q/p = (1-p)/p = q/(1-q) \\end{align}$$ These are [multiplicative inverses](multiplicative_inverse \"wikilink\"), so they multiply to 1, with the following relations: $$o_f = 1/o_a, \\quad o_a = 1/o_f, \\quad o_f \\cdot o_a = 1.$$ In the case that a Bernoulli trial is representing an event from finitely many [equally likely outcomes](equally_likely_outcomes \"wikilink\"), where *S* of the outcomes are success and *F* of the outcomes are failure, the odds for are $S:F$ and the odds against are $F:S.$ This yields the following formulas for probability and odds: $$\\begin{align} p &= S/(S+F)\\\\ q &= F/(S+F)\\\\ o_f &= S/F\\\\ o_a &= F/S \\end{align}$$ Note that here the odds are computed by dividing the number of outcomes, not the probabilities, but the proportion is the same, since these ratios only differ by multiplying both terms by the same constant factor. [Random variables](Random_variable \"wikilink\") describing Bernoulli trials are often encoded using the convention that 1 = \\\"success\\\", 0 = \\\"failure\\\". Closely related to a Bernoulli trial is a binomial experiment, which consists of a fixed number $n$ of [statistically independent](statistically_independent \"wikilink\") Bernoulli trials, each with a probability of success $p$, and counts the number of successes. A random variable corresponding to a binomial is denoted by $B(n,p)$, and is said to have a *[binomial distribution](binomial_distribution \"wikilink\")*. The probability of exactly $k$ successes in the experiment $B(n,p)$ is given by: $$P(k)={n \\choose k} p^k q^{n-k}$$ where ${n \\choose k}$ is a [binomial coefficient](binomial_coefficient \"wikilink\"). Bernoulli trials may also lead to [negative binomial distributions](negative_binomial_distribution \"wikilink\") (which count the number of successes in a series of repeated Bernoulli trials until a specified number of failures are seen), as well as various other distributions. When multiple Bernoulli trials are performed, each with its own probability of success, these are sometimes referred to as [Poisson trials](Poisson_trial \"wikilink\").[^3] ## Example: tossing coins {#example_tossing_coins} Consider the simple experiment where a fair coin is tossed four times. Find the probability that exactly two of the tosses result in heads. ### Solution For this experiment, let a heads be defined as a *success* and a tails as a *failure.* Because the coin is assumed to be fair, the probability of success is $p = \\tfrac{1}{2}$. Thus the probability of failure, $q$, is given by $$q = 1 - p = 1 - \\tfrac{1}{2} = \\tfrac{1}{2}$$. Using the equation above, the probability of exactly two tosses out of four total tosses resulting in a heads is given by: $$\\begin{align} P(2) &= {4 \\choose 2} p^{2} q^{4-2} \\\\ &= 6 \\times \\left(\\tfrac{1}{2}\\right)^2 \\times \\left(\\tfrac{1}{2}\\right)^2 \\\\ &= \\dfrac {3}{8}. \\end{align}$$ ## See also {#see_also} - [Bernoulli scheme](Bernoulli_scheme \"wikilink\") - [Bernoulli sampling](Bernoulli_sampling \"wikilink\") - [Bernoulli distribution](Bernoulli_distribution \"wikilink\") - [Binomial distribution](Binomial_distribution \"wikilink\") - [Binomial coefficient](Binomial_coefficient \"wikilink\") - [Binomial proportion confidence interval](Binomial_proportion_confidence_interval \"wikilink\") - [Poisson sampling](Poisson_sampling \"wikilink\") - [Sampling design](Sampling_design \"wikilink\") - [Coin flipping](Coin_flipping \"wikilink\") - [Jacob Bernoulli](Jacob_Bernoulli \"wikilink\") - [Fisher\\'s exact test](Fisher's_exact_test \"wikilink\") - [Boschloo\\'s test](Boschloo's_test \"wikilink\") ## References {{reflist}} ## External links {#external_links} {{Commonscat}} - ```{=mediawiki} {{springer|title=Bernoulli trials|id=p/b015690}} ``` - ```{=mediawiki} {{cite web|url=http://www.math.uah.edu/stat/applets/BinomialTimelineExperiment.html|title=Simulation of n Bernoulli trials|publisher=math.uah.edu|access-date=2014-01-21}} ``` ```{=mediawiki} {{DEFAULTSORT:Bernoulli Trial}} [Category:Discrete distributions](Category:Discrete_distributions \"wikilink\") [Category:Coin flipping](Category:Coin_flipping \"wikilink\") [Category:Experiment (probability theory)](Category:Experiment_(probability_theory) \"wikilink\") [^1]: `{{cite encyclopedia | last = Papoulis | first = A. | contribution = Bernoulli Trials | title = Probability, Random Variables, and Stochastic Processes | edition = 2nd | location = New York | publisher = [[McGraw-Hill]] | pages = 57\u201363 | year = 1984}}`{=mediawiki} [^2]: James Victor Uspensky: *Introduction to Mathematical Probability*, McGraw-Hill, New York 1937, page 45 [^3]: [Rajeev Motwani](Rajeev_Motwani \"wikilink\") and P. Raghavan. Randomized Algorithms. Cambridge University Press, New York (NY), 1995, p.67-68 ```python def perform_bernoulli_trials(n, p): \"\"\" Perform n Bernoulli trials with success probability p and return number of successes. That is to say: If I toss coin 10 times, and landed head 7 times, then success rate is 70% -> this is what I should return. But note this is binary. \"\"\" # Initialize number of successes: n_success n_success = 0 # Perform trials for i in range(n): # Choose random number between zero and one: random_number random_number = np.random.random(size=1) # If less than p, it's a success so add one to n_success if random_number < p: n_success += 1 return n_success def perform_bernoulli_trials(n, p): \"\"\" Perform n Bernoulli trials with success probability p and return number of successes. That is to say: If I toss coin 10 times, and landed head 7 times, then success rate is 70% -> this is what I should return. But note this is binary. \"\"\" # Initialize number of successes: n_success n_success = 0 # Perform trials,# Choose random number between zero and one: random_number # If less than p, it's a success so add one to n_success random_number_arr = [True if np.random.random(size=n) < p else False] n_success = np.sum(random_number_arr, axis = None) return n_success # import iris and load as dataframe iris = load_iris ( return_X_y = False , as_frame = True ) # make a copy of iris df_iris = iris . frame . copy () # change target to species and map 0,1,2 to species df_iris . columns = [ 'sepal length (cm)' , 'sepal width (cm)' , 'petal length (cm)' , 'petal width (cm)' , 'species' ] species_mapping = { 0 : 'setosa' , 1 : 'versicolor' , 2 : 'virginica' } df_iris [ 'species' ] = df_iris [ 'species' ] . map ( species_mapping ) [2] Quantitative Exploratory Data Analysis Descriptive Statistics Mean ```numpy.mean(a, axis=None, dtype=None)``` * a: array containing numbers whose mean is required * axis: axis or axes along which the means are computed, * default is to compute the mean of the flattened array * dtype: type of data to be used in calculations # get versicolor's petal length versicolor_petal_length = df_iris . loc [ df_iris [ 'species' ] == 'versicolor' ][ 'petal length (cm)' ] . to_numpy ( dtype = np . float64 , copy = False ) # Compute the mean: mean_length_vers mean_length_vers = np . mean ( versicolor_petal_length , axis = None , dtype = np . float16 ) # Print the result with some nice formatting print ( 'Iris. versicolor:' , mean_length_vers , 'cm' ) Iris. versicolor: 4.26 cm Median ```numpy.median(a, axis=None, out=None)``` * a: array containing numbers whose median is required * axis: axis or axes along which the median is computed * default is to compute the median of the flattened array * out: alternative output array to place the result, must have the same shape and buffer length as the expected output. # Compute the median: median_length_vers median_length_vers = np . median ( versicolor_petal_length , axis = None ) # Print the result with some nice formatting print ( 'Iris. versicolor:' , median_length_vers , 'cm' ) Iris. versicolor: 4.35 cm Mode **The mode is the most frequently occurring number.** scipy . stats . mstats . mode ( a , axis = 0 ) * a: array containing numbers whose mode is required * axis: axis or axes along which the mode is computed * default is 0 * if None, compute the mode of the flattened array It returns (```mode: array of modal values, count: array of counts for each mode```). # Compute the mode: mode_length_vers mode_length_vers = scipy . stats . mstats . mode ( versicolor_petal_length , axis = None ) # Print the result with some nice formatting print ( 'Iris. versicolor:' , mode_length_vers , 'cm' ) Iris. versicolor: ModeResult(mode=array([4.5]), count=array([7.])) cm Range ```numpy.ptp(a, axis=None, out=None)``` 'ptp' stands for 'peak to peak'. * a: array containing numbers whose range is required * axis: axis or axes along which the range is computed, * default is to compute the range of the flattened array. It returns a new array with the result. # Compute the range: range_length_vers range_length_vers = np . ptp ( versicolor_petal_length , axis = None ) # Print the result with some nice formatting print ( 'Iris. versicolor:' , range_length_vers , 'cm' ) Iris. versicolor: 2.0999999999999996 cm Population Variance Variance can be calculated in python using different libraries like numpy, pandas, and statistics. numpy.var(a, axis=None, dtype=None, ddof=0) * a: array containing numbers whose variance is required * axis: axis or axes along which the variances are computed, default is to compute the mean of the flattened array * ddof : int, optional * ddof stands for delta degrees of freedom. It is the divisor used in the calculation, which is N - ddof, where N is the number of elements. * The default value of ddof is 0. **Formula** $$\\sigma^2 = \\frac{\\displaystyle\\sum_{i=1}^{n}(x_i - \\mu)^2} {n}$$ # Array of differences to mean: differences differences = versicolor_petal_length - \\ np . mean ( versicolor_petal_length , axis = None ) # Square the differences: diff_sq diff_sq = np . square ( differences ) # Compute the mean square difference: variance_explicit variance_explicit = np . sum ( np . square ( versicolor_petal_length - np . mean ( versicolor_petal_length , axis = None ))) / len ( versicolor_petal_length ) # Compute the variance using NumPy: variance_np variance_np = np . var ( versicolor_petal_length , axis = None ) # Print the results print ( variance_explicit , variance_np ) 0.21640000000000004 0.21640000000000004 Note that the unit of variance is not the same as the dataset, for example, if we are looking at the Iris dataset's versicolor petal length, which is in cm, and its variance is 0.2164 cm squared. So it is not easy to interpret the variance and hence we can look at the standard deviation, which square roots the variance, to make it the same unit as the dataset. With standard deviation, we have 0.465188 cm. Population Standard Deviation The standard deviation is the square root of variance. numpy.std(a, axis=None, dtype=None, ddof=0) Parameters are the same as ```numpy.var()```. * a: array containing numbers whose standard deviation is required * axis: axis or axes along which the standard deviations are computed, default is to compute the mean of the flattened array * ddof : int, optional * ddof stands for delta degrees of freedom. It is the divisor used in the calculation, which is N - ddof, where N is the number of elements. * The default value of ddof is 0. **Formula** $$\\sigma = \\sqrt{\\frac{\\displaystyle\\sum_{i=1}^{n}(x_i - \\mu)^2} {n}}$$ Because the variance is the average of the distances from the mean _squared_, the standard deviation tells us approximately, on average, the distance of numbers in a distribution from the mean of the distribution. In the **Mean** section, we see that the average petal length is of 4.26 cm, our standard deviation tells us that ***on average***, each data point in the distribution (population) is around 0.465188~ cm away from the mean. # Compute the variance: variance variance = np . var ( versicolor_petal_length , axis = None ) # Print the square root of the variance print ( np . sqrt ( variance )) # Print the standard deviation print ( np . std ( versicolor_petal_length , axis = None )) 0.4651881339845203 0.4651881339845203 Percentile # Specify array of percentiles: percentiles percentiles = np . array ([ 2.5 , 25 , 50 , 75 , 97.5 ]) # Compute percentiles: ptiles_vers ptiles_vers = np . percentile ( a = versicolor_petal_length , q = percentiles , axis = None ) # Print the result print ( ptiles_vers ) [3.3 4. 4.35 4.6 4.9775] def ecdf ( data ): \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\" # Number of data points: n n = len ( data ) # x-data for the ECDF: x x = np . sort ( data ) # y-data for the ECDF: y y = np . arange ( start = 1 , stop = n + 1 ) / n return x , y # Plot the ECDF x_vers , y_vers = ecdf ( versicolor_petal_length ) _ = plt . plot ( x_vers , y_vers , \".\" ) _ = plt . xlabel ( 'petal length (cm)' ) _ = plt . ylabel ( 'ECDF' ) # Overlay percentiles as red diamonds. _ = plt . plot ( ptiles_vers , percentiles / 100 , marker = 'D' , color = 'red' , linestyle = 'none' ) # Save the plot save_fig ( \"[Versicolor] Percentile + ECDF Plot on Petal Length\" ) # Show the plot plt . show () Saving figure [Versicolor] Percentile + ECDF Plot on Petal Length ![png](day-3-probabilistically-discrete-variables_files/day-3-probabilistically-discrete-variables_41_1.png) Skewness scipy.stats.skew(a, axis=0) For normally distributed data, the skewness should be about 0. A skewness value > 0 means that there is more weight in the left tail of the distribution. Skewness refers to the lack of symmetry in a distribution of data. [Technical note: we will be talking about skewness here only in the context of _unimodal_ distributions.] ![image.png](attachment:e5188fd8-9bb3-422f-8121-a459a129189f.png) A *positive-skewed* distribution is one whose right tail is longer or fatter than its left. Conversely, a *negative-skewed* distribution is one whose left tail is longer or fatter than its right. Symmetric distributions have no skewness! #### Skewness and measures of central tendency The mean, median, and mode are affected by skewness. When a distribution is symmetric, the mean, median, and mode are the same. > Symmetric: mean == median == mode When a distribution is negatively skewed, the mean is less than the median, which is less than the mode. > Negative skew: mean < median < mode When a distribution is positively skewed, the mean is greater than the median, which is greater than the mode. > Positive skew: mode < median < mean You\u2019ve learned numerical measures of center, spread, and outliers, but what about **measures of shape**? The histogram can give you a general idea of the shape, but two numerical measures of shape give a more precise evaluation: **skewness tells you the amount and direction of skew** (departure from horizontal symmetry), and **kurtosis tells you how tall and sharp the central peak is**, relative to a standard bell curve. Why do we care? One application is **testing for normality**: many statistics inferences require that a distribution be normal or nearly normal. A normal distribution has skewness and excess kurtosis of 0, so if your distribution is close to those values then it is probably close to normal. # Compute the range: range_length_vers skew_length_vers = scipy . stats . skew ( versicolor_petal_length , axis = None ) # Print the result with some nice formatting print ( 'Iris. versicolor:' , skew_length_vers , 'cm' ) Iris. versicolor: -0.5881586743962586 cm Indeed, our skew is negative, indicating the data is in a slightly negative skewed distribution, where the left tail is longer. A natural question that follows is how to calculate skewness? How is the value -0.588158~ quantified? _ = sns . displot ( x = versicolor_petal_length , kind = 'kde' ) _ = plt . xlabel ( xlabel = 'petal length (cm)' ) _ = sns . displot ( x = versicolor_petal_length , kind = 'hist' ) _ = plt . xlabel ( xlabel = 'petal length (cm)' ) _ = sns . displot ( x = versicolor_petal_length , kind = 'ecdf' ) _ = plt . xlabel ( xlabel = 'petal length (cm)' ) ![png](day-3-probabilistically-discrete-variables_files/day-3-probabilistically-discrete-variables_48_0.png) ![png](day-3-probabilistically-discrete-variables_files/day-3-probabilistically-discrete-variables_48_1.png) ![png](day-3-probabilistically-discrete-variables_files/day-3-probabilistically-discrete-variables_48_2.png) Box and Whiskers Plot Making a box plot for the petal lengths is unnecessary because the iris data set is not too large and the bee swarm plot works fine. Furthermore, A box and whisker plot\u2014also called a box plot\u2014displays the five-number summary of a set of data. The five-number summary is the minimum, first quartile, median, third quartile, and maximum. In a box plot, we draw a box from the first quartile to the third quartile. A vertical/horizontal line goes through the box at the median. The whiskers go from each quartile to the minimum or maximum. # Create box plot with Seaborn's default settings _ = sns . boxplot ( x = 'species' , y = 'petal length (cm)' , data = df_iris ) # Label the axes _ = plt . xlabel ( xlabel = 'species' ) _ = plt . ylabel ( ylabel = 'petal length (cm)' ) # Save the plot save_fig ( \"[Versicolor] Box and Whiskers Plot on Petal Length\" ) # Show the plot plt . show () Saving figure [Versicolor] Box and Whiskers Plot on Petal Length ![png](day-3-probabilistically-discrete-variables_files/day-3-probabilistically-discrete-variables_51_1.png) To understand the graph above, let us just look at **versicolor** for consistency. In simple terms, the ECDF of this species can be interpreted as follows: what percentage of **versicolor** have a **petal length** of less than 4 cm? By eyeballing, we can tell that about $20\\%$ just by pinpointing which value on the y-axis (ECDF) corresponds to 4 cm on the x-axis (petal length).","title":"Day 3 probabilistically discrete variables"},{"location":"reighns_ml_journey/projects/LTA_road_cracks_detection/notebooks/walkthrough/","text":"\\[ \\newcommand{\\M}{\\mathcal{M}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\yhat}{\\mathbf{\\hat{y}}} \\newcommand{\\iou}{\\textbf{IOU}} \\] Data Collection and Label Original Dataset 500 images of size \\(4032 \\times 3024\\) provided by LTA: 150 positive class 1 where 1 means defective; 350 negative class 0 where 0 means non-defective; Class labels are subjected to changes depending on model used (i.e. F-RCNN treats background as 0 while Yolo does not care) There is some slight class imbalance: We checked if we can garner more data; We were given the green light; We collected about 1500 more photos. Labelling The total number of images is \\(2000\\) ; We need to label all of them as labels were not provided; Shortage of manpower prompted us to think of ways to reduce manual labour. Semi-Supervised Learning Semi-supervised learning is an approach to machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training. Semi-supervised learning falls between unsupervised learning (with no labeled training data) and supervised learning (with only labeled training data). It is a special instance of weak supervision. High-level overview We manually labour \\(500\\) images of 50-50 ratio with class balanced; We train on the \\(500\\) images using an Object Detection Model \\(\\M_1\\) until convergence (i.e. an acceptable result say MAP > 0.8, but more importantly, we do not care the class accuracy, rather, we want the localization error to be low); We use \\(\\M_1\\) and perform inference on the remaining \\(1500\\) images so that \\(\\M_1\\) can return us each image's bounding box coordinates, during this process we can discard bounding boxes with low confidences if we feel that an image won't contain more than say, 5 tactile tiles; Note that we emphasized on a low localization error from \\(\\M_1\\) and therefore expect that at the very least the bounding boxes of the tactile tiles are accurate as drawing bounding boxes manually is more time consuming than labelling whether a given bounding box is of class 0 or 1; We then run through manually over the predicted \\(1500\\) images to see if the predicted bounding boxes by \\(\\M_1\\) makes sense, and also to correct them if need be. Establish Metrics After understanding the problem better, we should probably define a metric to optimize. We adopt the commonly used metrics for object detection. IOU The concept of Intersection over Union (IOU) , also known as the Jaccard Index is used to decide whether the predicted bounding box is giving us a good outcome. It is defined by the formula: \\( \\(J(A,B) = \\dfrac{|(A \\cap B)|}{|(A \\cup B)|}\\) \\) where A and B are the area of the ground truth bounding box and predicted ground truth bounding box respectively as shown in figure below. The Typical Brothers (TP, FP, TN, FN) Object Detection at its core is a complicated algorithm comprising of both regression and classification. We first define the 4 good brothers that we see in a typical classification problem . Note in usual classification problem, we will have a probabality logit at the end of a softmax/sigmoid layer, which aids us in determining whether a classification belongs to TP, FP, TN or FN. In object detection, we use the IOU to determine if a given a bounding box prediction belongs to the four brothers. Let us define: \\(\\y\\) : 1 single ground truth bbox; \\(\\yhat\\) : 1 single predicted bbox; \\(\\iou(\\y, \\yhat)\\) : The IOU between them. \\(t\\) : The threshold for IOU to cross for it to be a positive, defaults to \\(0.5\\) . Then: TP: The model classified it as positive and it indeed is positive; \\(\\yhat\\) is TP iff \\(\\iou(\\y, \\yhat) > t\\) ; both the predictor and the ground truth are in agreement; FP: The model classified it as positive but it is negative; \\(\\yhat\\) is FP iff \\(\\iou(\\y, \\yhat) < t\\) ; the predictor raised a false alarm that there is an object but there actually isn't; FN: The model classified it as negative but it is positive; \\(\\yhat\\) is FN iff there exists a ground truth \\(\\y\\) that is not detected by our model; TN: This is usually ignored since it means that both the predictor and the annotator ground truth did not have a bounding box; it is often termed as correct rejections because there exist infinite places on an image where there are no bounding boxes from both the predictor and the annotator. The Iconic DUO (Precision and Recall) As with any other classification problem, we will see this iconic duo. Let us recap these 2 metrics in the settings of object detection. Precision Precision measures how many of the samples predicted as positive are actually positive. Mathematically, it is expressed as: \\[ \\text{Precision} = \\dfrac{\\text{TP}}{\\text{TP} + \\text{FP}}=P(Y=1 | \\hat{Y} = 1) \\] A Probablistic Interpretation Notice that the above definition has a probabilitic interpretation \\(P(Y = 1 | \\hat{Y} = 1)\\) , where \\(Y\\) and \\(\\hat{Y}\\) refers to the actual label and predicted labels respectively. We interpreted precision and recall not as ratios but as estimations of probabilities . Precision is then the estimated probability that a random point selected from the samples are positive. This might be a tough pill to swallow as someone who was never good in statistics but it is just conditional probability. If you try to think a bit further, you can form an intuition as follows: > If your classifier \\(h\\) is trained and the last layer is say, sigmoid, which in binary classification, calibrates the logits and turn them into probabilities. Then it can be interpretated that given a randomly chosen point \\(x \\in X_{train}\\) , what is the probability of this point \\(x\\) to be positive given that it is predicted as positive by the classifer? Informally, precision answers the question what proportion of positive predictions was actually correct ? In other words, out of all the positive predictions made by the model, how many of those positive predictions were actually positive when compared to the ground truth? In object detection, precision can be throught of as the fraction of correct object predictions among all objects detected by the model Recall Recall measures the following: out of all the actual positives (say, the real cancer patients), how many of them were identified correctly by the classifier? Mathematically, it is expressed as: \\[ \\text{Recall}= \\dfrac{\\text{TP}}{\\text{TP} + \\text{FN}}= P(\\hat{Y}=1 | Y = 1)=1-FNR \\] From the formula, we see the denominator to be defined as TP + FN, which is unsurprising as this gives you the actual number of positives. The dynamics is also similar to the one in precision. In object detection, recall can be thought of as the fraction of ground truth objects that are correctly detected by the model. Recall vs Precision The tug of war between this duo is still present, that is to say, as precision goes down, recall might go up and vice versa. Note that precision and recall are parametrized by the IOU threshold \\(t\\) , that means, for each threshold \\(t\\) , we can calculate the pair of precision and recall score. The Objectness Confidence Score Before we proceed to the behemoth Mean Average Precision , we should take a step back and recall that an usual object detection model is parametrized by: predicted bbox: [[...], [...]] 2 predictions; associated objectness confidence score: [0.2, 0.9] how confident that the predicted bbox really do contains an object; predicted labels: [1, 2] associated labels confidence score: [0.3, 0.8] how confident that the predicted label is correct, usually this is the probability logits. Before we even get to IOU calculation, we have another threshold to worry about, which is the objectness confidence score \\(\\tau\\) ; that is, if the predicted bboxes has confidence score below \\(\\tau\\) , then we immediately discard the predictions and do not even consider it a valid prediction. So if you are have a high \\(\\tau\\) threshold, then you may have a high precision at the cost of low recall. Let us breakdown why: If you raise \\(\\tau\\) , then we have a stringent requirement such that more objects might be missed by the model. A high FN ensues. If you decrease \\(\\tau\\) , then we may have many predictions, say a ground truth image has 2 gt bbox, but since we loosen \\(\\tau\\) , we have 10 predicted bboxes, this causes a lot of FPs. So a precision recall curve is a plot of precision vs recall at various thresholds \\(\\tau\\) . Note very carefully that the pr-curve is parametrized by \\(\\tau\\) and not \\(t\\) (the IOU threshold). Average Precision The big picture, precison-recall curve can be understood as y = f(x) where y is precision and recall x. More simply, it is just plotting pairs of precision-recall for each threshold \\(\\tau\\) and summing the area under the curve. At each confidence level (threshold), we ask what is the precision-recall score of the predictions of all bounding boxes at a specific IOU while discarding off those below the threshold, then average them over all thresholds? That's MAP Model Architecture RCNN Basic Architecture Perform selective search to extract multiple high-quality region proposals on the input image. These proposed regions are usually selected at multiple scales with different shapes and sizes. Each region proposal will be labeled with a class and a ground-truth bounding box. I believe out of the 2000 proposals, some of them won't even enclose any ground truth and so they should be discarded . Choose a pretrained CNN and truncate it before the output layer. Resize each region proposal to the input size required by the network, and output the extracted features for the region proposal through forward propagation. Take the extracted features and labeled class of each region proposal as an example. Train multiple support vector machines to classify objects, where each support vector machine individually determines whether the example contains a specific class. Take the extracted features and labeled bounding box of each region proposal as an example. Train a linear regression model to predict the ground-truth bounding box. Own words: Propose say 2000 regions (anchor boxes) then for each of the 2000 regions, we discard the ones with no IOU with ground truth; Run a CNN on each the remaining region proposals and take the output and: Feed into SVMs to classify the region; A linear regressor to regress the bboxes; Pros and Cons Cons : - Too slow as if 2000 proposals need to run 2000 CNNs on it. Fast-RCNN To resolve the 2000 proposals = 2000 CNNs issue, one can envision that we just use 1 CNN for the image and using this 1 CNN we \"inference on the 2000 proposals\". Basic Architecture Compared with the R-CNN, in the fast R-CNN the input of the CNN for feature extraction is the entire image, rather than individual region proposals. Moreover, this CNN is trainable. Given an input image, let the shape of the CNN output be \\(1 \\times c \\times h_1 \\times w_1\\) . Suppose that selective search generates \\(n\\) region proposals. These region proposals (of different shapes) mark regions of interest (of different shapes) on the CNN output. Then these regions of interest further extract features of the same shape (say height \\(h_2\\) and width \\(w_2\\) are specified) in order to be easily concatenated. To achieve this, the fast R-CNN introduces the region of interest (RoI) pooling layer: the CNN output and region proposals are input into this layer, outputting concatenated features of shape \\(n \\times c \\times h_2 \\times w_2\\) that are further extracted for all the region proposals. Using a fully-connected layer, transform the concatenated features into an output of shape \\(n \\times d\\) , where \\(d\\) depends on the model design. Predict the class and bounding box for each of the \\(n\\) region proposals. More concretely, in class and bounding box prediction, transform the fully-connected layer output into an output of shape \\(n \\times q\\) ( \\(q\\) is the number of classes) and an output of shape \\(n \\times 4\\) , respectively. The class prediction uses softmax regression. Own words: Decide on a CNN and truncate head Here say the CNN output feature map is shape (1, 128, 7, 7) with 128 filters of 7 by 7 Use proposal method to get say n=200 regions , these regions can be mapped to the CNN output in previous step. Each region can be shaped differently, so we use ROI pooling to shape all n of them into same width and height say 3 by 3. So now our shape is \\((200, 128, 3, 3)\\) where 200 is stacked region proposals. Then use the traditional FC with 2 heads to predict Pros and Cons Pros : - Solved 2000 CNN issue. Cons : - Selective search still slow. Faster-RCNN Basically solved selective search with RPN. Basic Architecture Use a \\(3\\times 3\\) convolutional layer with padding of 1 to transform the CNN output to a new output with \\(c\\) channels. In this way, each unit along the spatial dimensions of the CNN-extracted feature maps gets a new feature vector of length \\(c\\) . Centered on each pixel of the feature maps, generate multiple anchor boxes of different scales and aspect ratios and label them. Using the length- \\(c\\) feature vector at the center of each anchor box, predict the binary class (background or objects) and bounding box for this anchor box. Consider those predicted bounding boxes whose predicted classes are objects. Remove overlapped results using non-maximum suppression. The remaining predicted bounding boxes for objects are the region proposals required by the region of interest pooling layer. RPN like this, basically take feature map and connect to 2k and 4k mappings and compare with gt anchor boxes, can be learned so it is powerful and can predict better proposals. in_channels = 512 # depends on the output feature map. in vgg 16 it is equal to 512 mid_channels = 512 n_anchor = 9 # Number of anchors at each location conv1 = nn . Conv2d ( in_channels , mid_channels , 3 , 1 , 1 ) . to ( device ) conv1 . weight . data . normal_ ( 0 , 0.01 ) conv1 . bias . data . zero_ () reg_layer = nn . Conv2d ( mid_channels , n_anchor * 4 , 1 , 1 , 0 ) . to ( device ) reg_layer . weight . data . normal_ ( 0 , 0.01 ) reg_layer . bias . data . zero_ () cls_layer = nn . Conv2d ( mid_channels , n_anchor * 2 , 1 , 1 , 0 ) . to ( device ) # I will be going to use softmax here. you can equally use sigmoid if u replace 2 with 1. cls_layer . weight . data . normal_ ( 0 , 0.01 ) cls_layer . bias . data . zero_ (); It is worth noting that, as part of the faster R-CNN model, the region proposal network is jointly trained with the rest of the model. In other words, the objective function of the faster R-CNN includes not only the class and bounding box prediction in object detection, but also the binary class and bounding box prediction of anchor boxes in the region proposal network. As a result of the end-to-end training, the region proposal network learns how to generate high-quality region proposals, so as to stay accurate in object detection with a reduced number of region proposals that are learned from data. Pros and Cons Still considered 2-stage, accurate but slow. Yolo Pros and Cons VS faster rcnn Cons - Yolo v1 7 by 7 grid, each grid 2 bbox so total 98 bounding boxes. - In each grid cell one image is detected even though 2 bounding boxes proposed. - This means if objects in an image lie closely in a grid cell then will have issue detecting all of them. - It doesn\u2019t generalize well when objects in the image show rare aspects of ratio. Faster RCNN on the other hand, do detect small objects well since it has nine anchors in a single grid, however it fails to do real-time detection with its two step architecture cause too slow. Yolo V3 While YOLOv2 uses the DarkNet-19 as the model architecture, YOLOv3 uses a much more complex DarkNet-53 as the model backbone\u2014 a 106 layer neural network complete with residual blocks and upsampling networks. YOLOv3\u2019s architectural novelty allows it to predict at 3 different scales, with the feature maps being extracted at layers 82, 94, and 106 for these predictions.. By detecting features at 3 different scales, YOLOv3 makes up for the shortcomings of YOLOv2 and YOLO, particularly in the detection of smaller objects. With the architecture allowing the concatenation of the upsampled layer outputs with the features from previous layers, the fine-grained features that have been extracted are preserved thus making the detection of smaller objects easier. YOLOv3 only predicts 3 bounding boxes per cell (compared to 5 in YOLOv2) but it makes three predictions at different scales, totaling up to 9 anchor boxes. Screenshots Augmentations Augmentation techniques. This helps artificially expanding the dataset, intuition is in each training iteration (batch), the model sees a slightly different version of the original image, and thus helps to generalize. - Normal flips and rotational geometrical transformation, that is a given since photos can be upside down, left-right flipped. - Hue, Saturation and Color to match dusk, noon and dawn. - Random Eraser, similar to dropout, we mask certain parts of the image to force the model to learn with less information, a regularization to overfitting and memorization.","title":"LTA"},{"location":"reighns_ml_journey/projects/LTA_road_cracks_detection/notebooks/walkthrough/#data-collection-and-label","text":"","title":"Data Collection and Label"},{"location":"reighns_ml_journey/projects/LTA_road_cracks_detection/notebooks/walkthrough/#original-dataset","text":"500 images of size \\(4032 \\times 3024\\) provided by LTA: 150 positive class 1 where 1 means defective; 350 negative class 0 where 0 means non-defective; Class labels are subjected to changes depending on model used (i.e. F-RCNN treats background as 0 while Yolo does not care) There is some slight class imbalance: We checked if we can garner more data; We were given the green light; We collected about 1500 more photos.","title":"Original Dataset"},{"location":"reighns_ml_journey/projects/LTA_road_cracks_detection/notebooks/walkthrough/#labelling","text":"The total number of images is \\(2000\\) ; We need to label all of them as labels were not provided; Shortage of manpower prompted us to think of ways to reduce manual labour.","title":"Labelling"},{"location":"reighns_ml_journey/projects/LTA_road_cracks_detection/notebooks/walkthrough/#semi-supervised-learning","text":"Semi-supervised learning is an approach to machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training. Semi-supervised learning falls between unsupervised learning (with no labeled training data) and supervised learning (with only labeled training data). It is a special instance of weak supervision. High-level overview We manually labour \\(500\\) images of 50-50 ratio with class balanced; We train on the \\(500\\) images using an Object Detection Model \\(\\M_1\\) until convergence (i.e. an acceptable result say MAP > 0.8, but more importantly, we do not care the class accuracy, rather, we want the localization error to be low); We use \\(\\M_1\\) and perform inference on the remaining \\(1500\\) images so that \\(\\M_1\\) can return us each image's bounding box coordinates, during this process we can discard bounding boxes with low confidences if we feel that an image won't contain more than say, 5 tactile tiles; Note that we emphasized on a low localization error from \\(\\M_1\\) and therefore expect that at the very least the bounding boxes of the tactile tiles are accurate as drawing bounding boxes manually is more time consuming than labelling whether a given bounding box is of class 0 or 1; We then run through manually over the predicted \\(1500\\) images to see if the predicted bounding boxes by \\(\\M_1\\) makes sense, and also to correct them if need be.","title":"Semi-Supervised Learning"},{"location":"reighns_ml_journey/projects/LTA_road_cracks_detection/notebooks/walkthrough/#establish-metrics","text":"After understanding the problem better, we should probably define a metric to optimize. We adopt the commonly used metrics for object detection.","title":"Establish Metrics"},{"location":"reighns_ml_journey/projects/LTA_road_cracks_detection/notebooks/walkthrough/#iou","text":"The concept of Intersection over Union (IOU) , also known as the Jaccard Index is used to decide whether the predicted bounding box is giving us a good outcome. It is defined by the formula: \\( \\(J(A,B) = \\dfrac{|(A \\cap B)|}{|(A \\cup B)|}\\) \\) where A and B are the area of the ground truth bounding box and predicted ground truth bounding box respectively as shown in figure below.","title":"IOU"},{"location":"reighns_ml_journey/projects/LTA_road_cracks_detection/notebooks/walkthrough/#the-typical-brothers-tp-fp-tn-fn","text":"Object Detection at its core is a complicated algorithm comprising of both regression and classification. We first define the 4 good brothers that we see in a typical classification problem . Note in usual classification problem, we will have a probabality logit at the end of a softmax/sigmoid layer, which aids us in determining whether a classification belongs to TP, FP, TN or FN. In object detection, we use the IOU to determine if a given a bounding box prediction belongs to the four brothers. Let us define: \\(\\y\\) : 1 single ground truth bbox; \\(\\yhat\\) : 1 single predicted bbox; \\(\\iou(\\y, \\yhat)\\) : The IOU between them. \\(t\\) : The threshold for IOU to cross for it to be a positive, defaults to \\(0.5\\) . Then: TP: The model classified it as positive and it indeed is positive; \\(\\yhat\\) is TP iff \\(\\iou(\\y, \\yhat) > t\\) ; both the predictor and the ground truth are in agreement; FP: The model classified it as positive but it is negative; \\(\\yhat\\) is FP iff \\(\\iou(\\y, \\yhat) < t\\) ; the predictor raised a false alarm that there is an object but there actually isn't; FN: The model classified it as negative but it is positive; \\(\\yhat\\) is FN iff there exists a ground truth \\(\\y\\) that is not detected by our model; TN: This is usually ignored since it means that both the predictor and the annotator ground truth did not have a bounding box; it is often termed as correct rejections because there exist infinite places on an image where there are no bounding boxes from both the predictor and the annotator.","title":"The Typical Brothers (TP, FP, TN, FN)"},{"location":"reighns_ml_journey/projects/LTA_road_cracks_detection/notebooks/walkthrough/#the-iconic-duo-precision-and-recall","text":"As with any other classification problem, we will see this iconic duo. Let us recap these 2 metrics in the settings of object detection.","title":"The Iconic DUO (Precision and Recall)"},{"location":"reighns_ml_journey/projects/LTA_road_cracks_detection/notebooks/walkthrough/#precision","text":"Precision measures how many of the samples predicted as positive are actually positive. Mathematically, it is expressed as: \\[ \\text{Precision} = \\dfrac{\\text{TP}}{\\text{TP} + \\text{FP}}=P(Y=1 | \\hat{Y} = 1) \\] A Probablistic Interpretation Notice that the above definition has a probabilitic interpretation \\(P(Y = 1 | \\hat{Y} = 1)\\) , where \\(Y\\) and \\(\\hat{Y}\\) refers to the actual label and predicted labels respectively. We interpreted precision and recall not as ratios but as estimations of probabilities . Precision is then the estimated probability that a random point selected from the samples are positive. This might be a tough pill to swallow as someone who was never good in statistics but it is just conditional probability. If you try to think a bit further, you can form an intuition as follows: > If your classifier \\(h\\) is trained and the last layer is say, sigmoid, which in binary classification, calibrates the logits and turn them into probabilities. Then it can be interpretated that given a randomly chosen point \\(x \\in X_{train}\\) , what is the probability of this point \\(x\\) to be positive given that it is predicted as positive by the classifer? Informally, precision answers the question what proportion of positive predictions was actually correct ? In other words, out of all the positive predictions made by the model, how many of those positive predictions were actually positive when compared to the ground truth? In object detection, precision can be throught of as the fraction of correct object predictions among all objects detected by the model","title":"Precision"},{"location":"reighns_ml_journey/projects/LTA_road_cracks_detection/notebooks/walkthrough/#recall","text":"Recall measures the following: out of all the actual positives (say, the real cancer patients), how many of them were identified correctly by the classifier? Mathematically, it is expressed as: \\[ \\text{Recall}= \\dfrac{\\text{TP}}{\\text{TP} + \\text{FN}}= P(\\hat{Y}=1 | Y = 1)=1-FNR \\] From the formula, we see the denominator to be defined as TP + FN, which is unsurprising as this gives you the actual number of positives. The dynamics is also similar to the one in precision. In object detection, recall can be thought of as the fraction of ground truth objects that are correctly detected by the model.","title":"Recall"},{"location":"reighns_ml_journey/projects/LTA_road_cracks_detection/notebooks/walkthrough/#recall-vs-precision","text":"The tug of war between this duo is still present, that is to say, as precision goes down, recall might go up and vice versa. Note that precision and recall are parametrized by the IOU threshold \\(t\\) , that means, for each threshold \\(t\\) , we can calculate the pair of precision and recall score.","title":"Recall vs Precision"},{"location":"reighns_ml_journey/projects/LTA_road_cracks_detection/notebooks/walkthrough/#the-objectness-confidence-score","text":"Before we proceed to the behemoth Mean Average Precision , we should take a step back and recall that an usual object detection model is parametrized by: predicted bbox: [[...], [...]] 2 predictions; associated objectness confidence score: [0.2, 0.9] how confident that the predicted bbox really do contains an object; predicted labels: [1, 2] associated labels confidence score: [0.3, 0.8] how confident that the predicted label is correct, usually this is the probability logits. Before we even get to IOU calculation, we have another threshold to worry about, which is the objectness confidence score \\(\\tau\\) ; that is, if the predicted bboxes has confidence score below \\(\\tau\\) , then we immediately discard the predictions and do not even consider it a valid prediction. So if you are have a high \\(\\tau\\) threshold, then you may have a high precision at the cost of low recall. Let us breakdown why: If you raise \\(\\tau\\) , then we have a stringent requirement such that more objects might be missed by the model. A high FN ensues. If you decrease \\(\\tau\\) , then we may have many predictions, say a ground truth image has 2 gt bbox, but since we loosen \\(\\tau\\) , we have 10 predicted bboxes, this causes a lot of FPs. So a precision recall curve is a plot of precision vs recall at various thresholds \\(\\tau\\) . Note very carefully that the pr-curve is parametrized by \\(\\tau\\) and not \\(t\\) (the IOU threshold).","title":"The Objectness Confidence Score"},{"location":"reighns_ml_journey/projects/LTA_road_cracks_detection/notebooks/walkthrough/#average-precision","text":"The big picture, precison-recall curve can be understood as y = f(x) where y is precision and recall x. More simply, it is just plotting pairs of precision-recall for each threshold \\(\\tau\\) and summing the area under the curve. At each confidence level (threshold), we ask what is the precision-recall score of the predictions of all bounding boxes at a specific IOU while discarding off those below the threshold, then average them over all thresholds? That's MAP","title":"Average Precision"},{"location":"reighns_ml_journey/projects/LTA_road_cracks_detection/notebooks/walkthrough/#model-architecture","text":"","title":"Model Architecture"},{"location":"reighns_ml_journey/projects/LTA_road_cracks_detection/notebooks/walkthrough/#rcnn","text":"","title":"RCNN"},{"location":"reighns_ml_journey/projects/LTA_road_cracks_detection/notebooks/walkthrough/#basic-architecture","text":"Perform selective search to extract multiple high-quality region proposals on the input image. These proposed regions are usually selected at multiple scales with different shapes and sizes. Each region proposal will be labeled with a class and a ground-truth bounding box. I believe out of the 2000 proposals, some of them won't even enclose any ground truth and so they should be discarded . Choose a pretrained CNN and truncate it before the output layer. Resize each region proposal to the input size required by the network, and output the extracted features for the region proposal through forward propagation. Take the extracted features and labeled class of each region proposal as an example. Train multiple support vector machines to classify objects, where each support vector machine individually determines whether the example contains a specific class. Take the extracted features and labeled bounding box of each region proposal as an example. Train a linear regression model to predict the ground-truth bounding box. Own words: Propose say 2000 regions (anchor boxes) then for each of the 2000 regions, we discard the ones with no IOU with ground truth; Run a CNN on each the remaining region proposals and take the output and: Feed into SVMs to classify the region; A linear regressor to regress the bboxes;","title":"Basic Architecture"},{"location":"reighns_ml_journey/projects/LTA_road_cracks_detection/notebooks/walkthrough/#pros-and-cons","text":"Cons : - Too slow as if 2000 proposals need to run 2000 CNNs on it.","title":"Pros and Cons"},{"location":"reighns_ml_journey/projects/LTA_road_cracks_detection/notebooks/walkthrough/#fast-rcnn","text":"To resolve the 2000 proposals = 2000 CNNs issue, one can envision that we just use 1 CNN for the image and using this 1 CNN we \"inference on the 2000 proposals\".","title":"Fast-RCNN"},{"location":"reighns_ml_journey/projects/LTA_road_cracks_detection/notebooks/walkthrough/#basic-architecture_1","text":"Compared with the R-CNN, in the fast R-CNN the input of the CNN for feature extraction is the entire image, rather than individual region proposals. Moreover, this CNN is trainable. Given an input image, let the shape of the CNN output be \\(1 \\times c \\times h_1 \\times w_1\\) . Suppose that selective search generates \\(n\\) region proposals. These region proposals (of different shapes) mark regions of interest (of different shapes) on the CNN output. Then these regions of interest further extract features of the same shape (say height \\(h_2\\) and width \\(w_2\\) are specified) in order to be easily concatenated. To achieve this, the fast R-CNN introduces the region of interest (RoI) pooling layer: the CNN output and region proposals are input into this layer, outputting concatenated features of shape \\(n \\times c \\times h_2 \\times w_2\\) that are further extracted for all the region proposals. Using a fully-connected layer, transform the concatenated features into an output of shape \\(n \\times d\\) , where \\(d\\) depends on the model design. Predict the class and bounding box for each of the \\(n\\) region proposals. More concretely, in class and bounding box prediction, transform the fully-connected layer output into an output of shape \\(n \\times q\\) ( \\(q\\) is the number of classes) and an output of shape \\(n \\times 4\\) , respectively. The class prediction uses softmax regression. Own words: Decide on a CNN and truncate head Here say the CNN output feature map is shape (1, 128, 7, 7) with 128 filters of 7 by 7 Use proposal method to get say n=200 regions , these regions can be mapped to the CNN output in previous step. Each region can be shaped differently, so we use ROI pooling to shape all n of them into same width and height say 3 by 3. So now our shape is \\((200, 128, 3, 3)\\) where 200 is stacked region proposals. Then use the traditional FC with 2 heads to predict","title":"Basic Architecture"},{"location":"reighns_ml_journey/projects/LTA_road_cracks_detection/notebooks/walkthrough/#pros-and-cons_1","text":"Pros : - Solved 2000 CNN issue. Cons : - Selective search still slow.","title":"Pros and Cons"},{"location":"reighns_ml_journey/projects/LTA_road_cracks_detection/notebooks/walkthrough/#faster-rcnn","text":"Basically solved selective search with RPN.","title":"Faster-RCNN"},{"location":"reighns_ml_journey/projects/LTA_road_cracks_detection/notebooks/walkthrough/#basic-architecture_2","text":"Use a \\(3\\times 3\\) convolutional layer with padding of 1 to transform the CNN output to a new output with \\(c\\) channels. In this way, each unit along the spatial dimensions of the CNN-extracted feature maps gets a new feature vector of length \\(c\\) . Centered on each pixel of the feature maps, generate multiple anchor boxes of different scales and aspect ratios and label them. Using the length- \\(c\\) feature vector at the center of each anchor box, predict the binary class (background or objects) and bounding box for this anchor box. Consider those predicted bounding boxes whose predicted classes are objects. Remove overlapped results using non-maximum suppression. The remaining predicted bounding boxes for objects are the region proposals required by the region of interest pooling layer. RPN like this, basically take feature map and connect to 2k and 4k mappings and compare with gt anchor boxes, can be learned so it is powerful and can predict better proposals. in_channels = 512 # depends on the output feature map. in vgg 16 it is equal to 512 mid_channels = 512 n_anchor = 9 # Number of anchors at each location conv1 = nn . Conv2d ( in_channels , mid_channels , 3 , 1 , 1 ) . to ( device ) conv1 . weight . data . normal_ ( 0 , 0.01 ) conv1 . bias . data . zero_ () reg_layer = nn . Conv2d ( mid_channels , n_anchor * 4 , 1 , 1 , 0 ) . to ( device ) reg_layer . weight . data . normal_ ( 0 , 0.01 ) reg_layer . bias . data . zero_ () cls_layer = nn . Conv2d ( mid_channels , n_anchor * 2 , 1 , 1 , 0 ) . to ( device ) # I will be going to use softmax here. you can equally use sigmoid if u replace 2 with 1. cls_layer . weight . data . normal_ ( 0 , 0.01 ) cls_layer . bias . data . zero_ (); It is worth noting that, as part of the faster R-CNN model, the region proposal network is jointly trained with the rest of the model. In other words, the objective function of the faster R-CNN includes not only the class and bounding box prediction in object detection, but also the binary class and bounding box prediction of anchor boxes in the region proposal network. As a result of the end-to-end training, the region proposal network learns how to generate high-quality region proposals, so as to stay accurate in object detection with a reduced number of region proposals that are learned from data.","title":"Basic Architecture"},{"location":"reighns_ml_journey/projects/LTA_road_cracks_detection/notebooks/walkthrough/#pros-and-cons_2","text":"Still considered 2-stage, accurate but slow.","title":"Pros and Cons"},{"location":"reighns_ml_journey/projects/LTA_road_cracks_detection/notebooks/walkthrough/#yolo","text":"","title":"Yolo"},{"location":"reighns_ml_journey/projects/LTA_road_cracks_detection/notebooks/walkthrough/#pros-and-cons-vs-faster-rcnn","text":"Cons - Yolo v1 7 by 7 grid, each grid 2 bbox so total 98 bounding boxes. - In each grid cell one image is detected even though 2 bounding boxes proposed. - This means if objects in an image lie closely in a grid cell then will have issue detecting all of them. - It doesn\u2019t generalize well when objects in the image show rare aspects of ratio. Faster RCNN on the other hand, do detect small objects well since it has nine anchors in a single grid, however it fails to do real-time detection with its two step architecture cause too slow.","title":"Pros and Cons VS faster rcnn"},{"location":"reighns_ml_journey/projects/LTA_road_cracks_detection/notebooks/walkthrough/#yolo-v3","text":"While YOLOv2 uses the DarkNet-19 as the model architecture, YOLOv3 uses a much more complex DarkNet-53 as the model backbone\u2014 a 106 layer neural network complete with residual blocks and upsampling networks. YOLOv3\u2019s architectural novelty allows it to predict at 3 different scales, with the feature maps being extracted at layers 82, 94, and 106 for these predictions.. By detecting features at 3 different scales, YOLOv3 makes up for the shortcomings of YOLOv2 and YOLO, particularly in the detection of smaller objects. With the architecture allowing the concatenation of the upsampled layer outputs with the features from previous layers, the fine-grained features that have been extracted are preserved thus making the detection of smaller objects easier. YOLOv3 only predicts 3 bounding boxes per cell (compared to 5 in YOLOv2) but it makes three predictions at different scales, totaling up to 9 anchor boxes.","title":"Yolo V3"},{"location":"reighns_ml_journey/projects/LTA_road_cracks_detection/notebooks/walkthrough/#screenshots","text":"","title":"Screenshots"},{"location":"reighns_ml_journey/projects/LTA_road_cracks_detection/notebooks/walkthrough/#augmentations","text":"Augmentation techniques. This helps artificially expanding the dataset, intuition is in each training iteration (batch), the model sees a slightly different version of the original image, and thus helps to generalize. - Normal flips and rotational geometrical transformation, that is a given since photos can be upside down, left-right flipped. - Hue, Saturation and Color to match dusk, noon and dawn. - Random Eraser, similar to dropout, we mask certain parts of the image to force the model to learn with less information, a regularization to overfitting and memorization.","title":"Augmentations"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/","text":"Seeding and Device import os import random import numpy as np import torch def seed_all ( seed : int = 1930 ): \"\"\"Seed all random number generators.\"\"\" print ( \"Using Seed Number {} \" . format ( seed )) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) # set PYTHONHASHSEED env var at fixed value torch . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) torch . cuda . manual_seed ( seed ) # pytorch (both CPU and CUDA) np . random . seed ( seed ) # for numpy pseudo-random generator random . seed ( seed ) # set fixed value for python built-in pseudo-random generator torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = False torch . backends . cudnn . enabled = False def seed_worker ( _worker_id ): \"\"\"Seed a worker with the given ID.\"\"\" worker_seed = torch . initial_seed () % 2 ** 32 np . random . seed ( worker_seed ) random . seed ( worker_seed ) _ = seed_all ( 1992 ) device = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) Using Seed Number 1992 Introduction Extracted from Kaggle Serious complications can occur as a result of malpositioned lines and tubes in patients. Doctors and nurses frequently use checklists for placement of lifesaving equipment to ensure they follow protocol in managing patients. Yet, these steps can be time consuming and are still prone to human error, especially in stressful situations when hospitals are at capacity. Hospital patients can have catheters and lines inserted during the course of their admission and serious complications can arise if they are positioned incorrectly. Nasogastric tube malpositioning into the airways has been reported in up to 3% of cases, with up to 40% of these cases demonstrating complications [1-3]. Airway tube malposition in adult patients intubated outside the operating room is seen in up to 25% of cases [4,5]. The likelihood of complication is directly related to both the experience level and specialty of the proceduralist. Early recognition of malpositioned tubes is the key to preventing risky complications (even death), even more so now that millions of COVID-19 patients are in need of these tubes and lines. The gold standard for the confirmation of line and tube positions are chest radiographs. However, a physician or radiologist must manually check these chest x-rays to verify that the lines and tubes are in the optimal position. Not only does this leave room for human error, but delays are also common as radiologists can be busy reporting other scans. Deep learning algorithms may be able to automatically detect malpositioned catheters and lines. Once alerted, clinicians can reposition or remove them to avoid life-threatening complications. The Royal Australian and New Zealand College of Radiologists (RANZCR) is a not-for-profit professional organisation for clinical radiologists and radiation oncologists in Australia, New Zealand, and Singapore. The group is one of many medical organisations around the world (including the NHS) that recognizes malpositioned tubes and lines as preventable. RANZCR is helping design safety systems where such errors will be caught. In this competition, you\u2019ll detect the presence and position of catheters and lines on chest x-rays. Use machine learning to train and test your model on 40,000 images to categorize a tube that is poorly placed. The dataset has been labelled with a set of definitions to ensure consistency with labelling. The normal category includes lines that were appropriately positioned and did not require repositioning. The borderline category includes lines that would ideally require some repositioning but would in most cases still function adequately in their current position. The abnormal category included lines that required immediate repositioning. If successful, your efforts may help clinicians save lives. Earlier detection of malpositioned catheters and lines is even more important as COVID-19 cases continue to surge. Many hospitals are at capacity and more patients are in need of these tubes and lines. Quick feedback on catheter and line placement could help clinicians better treat these patients. Beyond COVID-19, detection of line and tube position will ALWAYS be a requirement in many ill hospital patients. Dataset: Understanding our Data We will go through the data that we were given. Data Catalog/Description In this competition, you'll detect the presence and position of catheters and lines on chest x-rays. Use machine learning to train and test your model on \\(40,000\\) images to categorize a tube that is poorly placed. Train Set: \\(30083\\) images Public Test Set: \\(3582\\) images Private Test Set: ~ \\(14000\\) images train.csv : containing image IDs, labels and patient IDs . sample_submission.csv : a sample submission file in the correct format test/ : test images train/ : training images In particular the train csv has these columns: StudyInstanceUID - unique ID for each image ETT - Abnormal - endotracheal tube placement abnormal ETT - Borderline - endotracheal tube placement borderline abnormal ETT - Normal - endotracheal tube placement normal NGT - Abnormal - nasogastric tube placement abnormal NGT - Borderline - nasogastric tube placement borderline abnormal NGT - Incompletely Imaged - nasogastric tube placement inconclusive due to imaging NGT - Normal - nasogastric tube placement borderline normal CVC - Abnormal - central venous catheter placement abnormal CVC - Borderline - central venous catheter placement borderline abnormal CVC - Normal - central venous catheter placement normal Swan Ganz Catheter Present PatientID - unique ID for each patient in the dataset Objective: Multi-Label Binary Classification In general, a patient's single chest X-ray could present multiple medical conditions. Fig 1: Patient ID 323464123; By Hongnan G. Patient 323464123's first row corresponds to an unique image defined by the study instance UID. Noticed the 1's under the columns ETT-Normal , NGT - Incompletely Images , CVC-Borderline and CVC-Normal and 0's elsewhere. Unlike multi-class classification, where classes are mutually exclusive , multi-label is not. For example, a patient's X-ray scan of the lungs can show up pneumonia and covid-19 (both are conditions), as a result the class labels are not mutually exclusive (unlike multi-class). The same logic is applied in this setting, where the tube can be labelled differently. Metrics: Establish Metrics Macro-Averaged AUROC Then Macro-Average AUROC is calculated for each individual class, and then averaged over the total number of classes. \\[\\text{Macro-Average AUROC} = \\frac{1}{\\text{num_class}}\\sum_{i=1}^{\\text{num_class}}R_{i}\\] where \\(R_i\\) is the AUROC score for each individual class. A small example clears up the air: Consider 3 classes of apple, banana and carrot as a multi-label problem with class index \\([0, 1, 2]\\) . y_true : is a one-hot encoded matrix of 4 rows and 3 columns. The rows means 4 samples, and columns mean the class where 1 means positive and 0 negative. It is not a surprise that each row can hold multiple 1's since it is a multi-label problem. [0, 1, 1] just means that the \"image data\" contains both banana and carrot; y_pred : the predicted matrix, and is the same shape as y_true . We now treat the problem running multiple binary ROC computation: Calculate the ROC score between column \\(i\\) of y_true and y_pred respectively and call them \\(R_i\\) ; Sum \\(R_i\\) and divide by the number of classes and get the Macro-Averaged AUROC. import numpy as np from sklearn.metrics import roc_auc_score y_true = np . asarray ([[ 0 , 1 , 1 ], [ 0 , 0 , 1 ], [ 1 , 1 , 0 ], [ 1 , 1 , 1 ]]) y_pred = np . asarray ([[ 0 , 1 , 0 ], [ 1 , 0 , 0 ], [ 1 , 1 , 0 ], [ 1 , 0 , 1 ]]) macro_auroc = np . mean ([ roc_auc_score ( y_true [:, i ], y_pred [:, i ]) for i in range ( 3 )]) macro_auroc 0.75 Tip For a more wholesome treatment of metrics, see my Melanoma write-up and blog . Validation and Resampling: Cross-Validation How should we split out data into folds? We should examine the data for a few factors: Is the data \\(\\mathcal{X}\\) imbalanced? Is the data \\(\\mathcal{X}\\) generated in a i.i.d. manner, more specifically, if I split \\(\\mathcal{X}\\) to \\(\\mathcal{X}_{train}\\) and \\(\\mathcal{X}_{val}\\) , can we ensure that \\(\\mathcal{X}_{val}\\) has no dependency on \\(\\mathcal{X}_{train}\\) ? We came to the conclusion: Yes, there is quite some imbalanced distribution, in particular, CVC - Normal , ETT - Normal and CVC - Borderline are significantly more than the rest of the classes. Therefore, a stratified cross validation is reasonable. Stratified KFold ensures that relative class frequencies is approximately preserved in each train and validation fold. More concretely, we will not experience the scenario where \\(X_{train}\\) has \\(m^{+}\\) and \\(m^{-}\\) positive and negative samples, but \\(X_{val}\\) has only \\(p^{+}\\) positive samples only and 0 negative samples, simply due to the scarcity of negative samples In medical imaging, it is a well known fact that most of the data contains patient level repeatedly. To put it bluntly, if I have 100 samples, and according to PatientID , we see that the id 123456 (John Doe) appeared 20 times, this is normal as a patient can undergo multiple settings of say, X-rays. If we allow John Doe's data to appear in both train and validation set, then this poses a problem of information leakage, in which the data is no longer i.i.d. . One can think of each patient has an \"unique, underlying features\" which are highly correlated across their different samples. As a result, it is paramount to ensure that amongst this 3255 unique patients, we need to ensure that each unique patients' images DO NOT appear in the validation fold. That is to say, if patient John Doe has 100 X-ray images, but during our 5-fold splits, he has 70 images in Fold 1-4, while 30 images are in Fold 5, then if we were to train on Fold 1-4 and validate on Fold 5, there may be potential leakage and the model will predict with confidence for John Doe's images. This is under the assumption that John Doe's data does not fulfill the i.i.d process. StratifiedGroupKFold With the above consideration, we will use StratifiedGroupKFold where \\(K = 5\\) splits. There wasn't this splitting function in scikit-learn at the time of competition and as a result, we used a custom written (by someone else) RepeatedStratifiedGroupKFold function and just set n_splits = 1 to get StratifiedGroupKFold (yes we cannot afford to do repeated sample, so setting the split to be 1 will collapse the repeated function to just the normal stratified group kfold). However, as of 2022, this function is readily available in the Scikit-Learn library. To recap, we applied stratified logic such that each train and validation set has an equal weightage of positive and negative samples. We also grouped the patients in the process such that patient \\(i\\) will not appear in both training and validation set. Data leakage can cause you to have blind confidence on your model. We are also guilty of committing one since we trained our models with the NiH pretrained weights, without taking into consideration if the weights overlap with the training and validation folds information. In other words, we did not check properly if the weights trained on the NiH dataset has information in our RANZCR dataset. Take note this is different from training altogether on the NiH dataset, we are merely using the weights instead of the imagenet weights, which brings to the next point. Referring to figure 1, patient 323464123 has 6 images uniquely recorded on different visits. We should absolutely put them all in either train or validation set, as if say 2 images are in train and 4 in validation, there might be data leakage. Cross-Validation Workflow To recap, we have the following: Training Set ( \\(X_{\\text{train}}\\) ) : This will be further split into K validation sets during our cross-validation. This set is used to fit a particular hypothesis \\(h \\in \\mathcal{H}\\) . Validation Set ( \\(X_{\\text{val}}\\) ) : This is split from our \\(X_{\\text{train}}\\) during cross-validation. This set is used for model selection (i.e. find best hyperparameters, attempt to produce a best hypothesis \\(g \\in \\mathcal{H}\\) ). Test Set ( \\(X_{\\text{test}}\\) ) : This is an unseen test set, and we will only use it after we finish tuning our model/hypothesis. Suppose we have a final best model \\(g\\) , we will use \\(g\\) to predict on the test set to get an estimate of the generalization error (also called out-of-sample error). Pipeline. Courtesy of scikit-learn on a typical Cross-Validation workflow. Transfer Learning: The core of Deep Learning As we all know, if we train on imagenet weights, we may take quite a while to converge, even if we finetune it. The intuition is simple, imagenet were trained on many common items in life, and none of them resemble closely to the image structures of X-rays . Therefore, we have a few options. Freeze earlier layers but unfreeze the later Conv Layers: this is intuitive as earlier layers detect shapes and colors, all low level details that is very useful even for such dissimilar tasks , and unfreeze the later conv layers which is what we call the \"abstract feature layers\", where it is more important for the model to learn from scratch. Fine-Tuning; Feature Extraction; Fine-Tuning Instead of random initialization, we initialize the network with a pretrained network, like the one that is trained on imagenet 1000 dataset. This is what we will be doing and we managed to find a set of pretrained weights trained specifically on this dataset as a starting point. The weights can be found here . We used a few models and found out that resnet200d has the best results on this set of training images. The reason we used this is mostly empirical, but using gradcam we can see how the model sees the images. Feature Extraction ConvNet as fixed feature extractor: Here, we will freeze the weights for all of the network except that of the final fully connected layer. This last fully connected layer is replaced with a new one with random weights and only this layer is trained. This is less relevant to us as we aren't using it for feature extractions. Therefore, the model may have a hard time detecting abstract features such shapes and details from the X-rays. We can of course unfreeze all the layers and retrain them from scratch, using various backbones, Preprocessing Most preprocessing techniques we do in an image recognition competition is mostly as follows: Mean and Standard Deviation Perform mean and std for the dataset given to us. Note that this step may make sense on paper, but empirically, using imagenet's default mean std will always work as well, if not better. Nevertheless, here are the stats: Imagenet: mean = [0.485, 0.456, 0.406] and; std = [0.229, 0.224, 0.225] RANZCR: mean = [0.4887381077884414, ...] and; std = [0.23064819430546407, ...] Channel Distribution This is usually done to check for \"surprises\". More specifically, I remember vividly when participating in Singapore Airline challenge where the classifier recognize weird objects as luggages. After plotting the pixel histogram, we observed that the luggages colors are all of a non-normal distribution, in fact, it is quite scattered. Then it dawned upon us that the classifier is learning the \"color\" too much, instead of the shape of the luggage. When we grayed out the images, the classifier starts to ignore the noise in the colors, and instead focus on other features like shapes. So this removes signal to noise in a way - using the objection detection example such as detecting a strawberry in a tree full of green leaves, then color is important, but if we detect leaves in a tree full of green leaves, we do not wish to incorporate color here as anything green might suggest that it is a leaf. We found, and as mentioned also by Rueben Schmidt in this post , there are some images that have black borders around them. I experimented by removed them during both the training process. There was no significant increase on the LB score, even if there was, it is in the 3-4th decimal places, but I noticed my local cv increased, so I think that some noise are removed locally, but not reflected in the test set. Therefore, during inference, I also removed the black borders, which should be the correct approach (learning from mistakes!). In conclusion, there is a small boost in score, if I keep this consistent in both training and inference, I reckon that no surprise factor would pop out. Here is the code: image = cv2 . imread ( image_path , cv2 . IMREAD_GRAYSCALE ) mask = image > 0 image = image [ np . ix_ ( mask . any ( 1 ), mask . any ( 0 ))] image = cv2 . cvtColor ( image , cv2 . COLOR_GRAY2RGB ) Notice that the code removes any pixel that is > 0, where black pixel is 0. On hindsight for the Singapore Airline project, I now know there is GradCam , where we can see how the model is learning, as it will highlight the areas on which the model is focusing on in an image. Convert Gray to RGB We know that X-rays are Grayscale images so converting a grayscale image to RGB is just setting R=G=B=Grayscale pixel for all channels. import cv2 image = cv2 . imread ( \"../images/1.2.826.0.1.3680043.8.498.10000428974990117276582711948006105617.png\" , cv2 . IMREAD_GRAYSCALE ) print ( image . shape ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) print ( image . shape ) (256, 256) (256, 256, 3) Augmentations We know that augmentation is central in an image competition, as essentially we are adding more data into the training process, effectively reducing overfitting and improve generalization . Heavy augmentations are used during Train-Time-Augmentation. But during Test-Time-Augmentation, we used the same set of training augmentations to inference with \\(100\\%\\) probability. Train-Time Augmentation The typicals! train_augmentations = [ albumentations . RandomResizedCrop ( height = config . image_size , width = config . image_size ), albumentations . HorizontalFlip ( p = 0.5 ), albumentations . ShiftScaleRotate ( p = 0.5 ), albumentations . HueSaturationValue ( hue_shift_limit = 0.2 , sat_shift_limit = 0.2 , val_shift_limit = 0.2 , p = 0.5 ), albumentations . RandomBrightnessContrast ( brightness_limit = ( - 0.1 , 0.1 ), contrast_limit = ( - 0.1 , 0.1 ), p = 0.5 ), albumentations . CoarseDropout ( p = 0.5 ), albumentations . Normalize ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ], max_pixel_value = 255.0 , p = 1.0 , ), ToTensorV2 ( p = 1.0 ), ] Test-Time Augmentation The exact same set of augmentations were used in inference. Not all TTAs provided a increase in score. Model Architectures, Training Parameters & Tricks Model Architecture Overview A brief overview of our model architecture is shown in fig 2.1 and 2.2: Fig 2.1; Single-Head Approach courtesy of Tawara Fig 2.2; Multi-Head Approach courtesy of Tawara Backbone We used both resnet200d and seresnet152d but will focus more on the first model. We used resnet200d , a ResNet variant model as our main backbone. ResNet-D is a modification on the ResNet architecture that utilises an average pooling tweak for downsampling. The motivation is that in the unmodified ResNet, the 1\u00d71 convolution for the downsampling block ignores 3/4 of input feature maps, so this is modified so no information will be ignored. Info So 1x1 convolutional reduces the feature maps depth but not the width or height while pooling reduces the width or height but not the depth. I think the results are better with the latter. In our case, we did the following: Create the model with: model = timm.create_model('resnet200d', pretrained=True, num_classes=1000) ; Load the pretrained weights from NiH trained ; Reset Classifier Head with Global Average Pooling: self.model.reset_classifier(num_classes=0, global_pool=\"avg\") ; Attach our own Classifier Head with 11 classes. import torch import timm from torchinfo import summary model = timm . create_model ( 'resnet18' , pretrained = True , num_classes = 1000 ) batch_size = 2 image_shape = ( 3 , 224 , 224 ) input_image_tensor = torch . rand ( size = ( batch_size , * image_shape )) # print(summary(model, (2, 3, 224, 224))) model.reset_classifier(num_classes=0, global_pool=\"\") means we do not want global average pooling and thus the shape at the last conv layer (penultimate layer) is \\((2, 512, 7, 7)\\) model.reset_classifier(num_classes=0, global_pool=\"avg\") means we do want global average pooling and thus the shape at the last conv layer (penultimate layer) is \\((2, 512)\\) whereby for each and every of the 512 feature maps \\(f_i\\) , we average \\(f_i\\) across all pixels (i.e. if \\(f_i\\) is 3 by 3 then average means add all \\(3 \\times 3 = 9\\) pixels and average) and concat to become one \\(512\\) vector. model = timm . create_model ( 'resnet18' , pretrained = True , num_classes = 1000 ) o = model ( input_image_tensor ) print ( f 'Original shape: { o . shape } ' ) model . reset_classifier ( num_classes = 0 , global_pool = \"\" ) o = model ( input_image_tensor ) print ( f 'Unpooled shape: { o . shape } ' ) Original shape: torch.Size([2, 1000]) Unpooled shape: torch.Size([2, 512, 7, 7]) model = timm . create_model ( 'resnet18' , pretrained = True , num_classes = 1000 ) o = model ( input_image_tensor ) print ( f 'Original shape: { o . shape } ' ) model . reset_classifier ( num_classes = 0 , global_pool = \"avg\" ) o = model ( input_image_tensor ) print ( f 'Pooled shape: { o . shape } ' ) Original shape: torch.Size([2, 1000]) Pooled shape: torch.Size([2, 512]) Note: Empirically, we realized the ResNet200D works very well for this particular task. We all asked ourselves why, and it was also discussed by many, but we all agreed that through various experiments, this model seems to consistently outperform their other SOTA counterparts. However, the closest possible paper on Revisiting ResNets: Improved Training and Scaling Strategies . Of course, to add diversity to our final predictions, we trained one more SeResNet152d as well. In general, ensembling models with vastly different architectures may result in a more robust solution. As an example, you can think of each model as a \"average learner\", and if their structure is different, it may very well so learn information that the other model might miss, hence ensembling them will average out such differences. Later on I will touch upon an ensembling technique called Forward Ensembling/Selection in this task, it has since worked well for other similar competitions. Classifier Heads - Multi-Head Approach We will focus a bit more on the multi-heads we used. Reading on self attention in X-ray What is a multi-headed model? And what exactly is a 'head' in a model? Intuition The usage of multi-heads is not uncommon, let us detail a simple example in Object Detection. The image below shows the general architecture: In object detection, we want to predict two things, the image class label and its bounding box coordinates ; The backbone network (\"convolution and pooling\") is responsible for extracting a feature map from the image that contains higher level summarized information. Each head uses this feature map as input to predict its desired outcome. The main intuition why feature maps of the last few layers (last layer usually) are important is one needs to recognize the earlier conv layer's feature maps find simple features like shapes, sizes, edges from an image, while the deep conv layers will be of more abstract features in an image. As a result, we really just want the abstract feature maps as they are more class specific to the image instead of the earlier layers which gives generic shapes . Let us say you used a ResNet as the backbone, then: Remove the classifier head, or rather just take the backbone which is all layers up to the last conv layer: say at the last layer. the output feature maps has a shape of (512, 7, 7); These 512 feature maps of 7 by 7 are high level information of the image encoded; We then connect a classification head to the backbone to predict the image's class given the true class labels; We then connect a regression head to the backbone to predict the image's bounding box coordinates given the true bounding box coordinates; We can create two heads, one responsible for classification for the class label and the other regression to work on the localization of the bounding boxes; Thus here we have 2 heads, one for classifying what class the image object is, the other is to localize where the image object is. The loss that you optimize for during training is usually a weighted sum of the individual losses for each prediction head. Multi-Head for Multi-Label Since we have 11 targets in this competition and they can be divided into 4 distinct groups: ETT , NGT , CVC and Swan . We envision that different groups have different areas in images to focus on. One possible way to leverage this idea is a multi-head approach we talked about. Multipe groups can share one single CNN backbone but have independent classifier heads. Fig 2.1; Single-Head Approach courtesy of Tawara Fig 2.2; Multi-Head Approach courtesy of Tawara With this in mind, let us move on: Multi-Label This is a multi-label classification problem. The section on the activation functions fully explained the single head version of using sigmoid layer. In fact, it is not uncommon to train N number of heads on a N-class Multi-Label problem. One thing to note is that if your classification head is Linear layer only (with BCE loss), then the back gradient propagation is the same whether you train one head, or multiple heads. However, we have non-linear layers in the head, including the SpatialAttentionBlock ! At the time of writing, I won't say I fully grasp of all the inner workings of an Attention Module across various use cases, but an analogy to aid my understanding is as follows: Having taken Learning From Data from Professor Yaser, the inner joke is about the Hypothesis Space. Let me elaborate, given a resnet200D as our hypothesis space \\(\\mathcal{H}\\) , we aim to find a \\(h \\in \\mathcal{H}\\) that best represents our true function \\(f\\) . Now suppose our learning algorithm \\(\\mathcal{A}\\) does a good job in helping us to find such a optimal \\(h\\) , it may take time, maybe say 100 epochs before finding it. Now if I break down the problem into 4 parts, each corresponding to a group, and we \"aid\" the learning algorithm by giving more attention to 4 focused areas, then we might find both a good \\(h\\) that estimate the \\(f\\) well, and may even be faster! If the above is too meh for understanding, imagine you are taking an exam in Machine Learning, as we all know, this field is a rabbit hole with never ending topics, let us say that there are 20 topics for you to study for the exam, you are dilligent and does that. But you have limited time and you decided to devote equal time to each topic, the consequence is you may not perform well for the exam due to limited understanding of each topic. Now, if I were to tell you that, hey, out of the 20 topics, can you study these 4 topics, as I think they have a higher chance of coming out, you will likely do better in the exam given that you devoted much more time on those \"focused (attention!)\" topics. The following code explains this methodology with reference to the above images. We first note to the readers that typically, if we use a single head approach, where if we were given a problem set \\(\\mathcal{D} = X \\times y\\) , a hypothesis space \\(\\mathcal{H}\\) we learn from a learning algorithm \\(\\mathcal{A}\\) , producing a final hypothesis \\(g\\) (or h, depends on your notation), that predicts as such \\(g(X_{val}) = y_{\\text{val_pred}}\\) , where each element in \\(y_{\\text{val_pred}}\\) corresponds to the class. Think of the basic MNIST example, our prediction vector's first element corresponds to the probability of it being an 0, and so on and so forth (assuming we use soft labels here). The change here is after the feature extraction layer (i.e. the feature logits after backbone), instead of just connecting it to a linear head for classification, we instead split the 11 outputs to 4 distinct groups. Each group will go through the head independent of the others, and this may prompt the model to put more attention on the independent groups. Finally, we torch.cat(..,axis=1) the outputs after they gone through their respective heads to recover the 11 outputs. model = CustomModel ( config , pretrained = True , load_weight = True , load_url = False , out_dim_heads = [ 3 , 4 , 3 , 1 ], ) # Multi Head for i , out_dim in enumerate ( self . out_dim_heads ): layer_name = f \"head_ { i } \" layer = torch . nn . Sequential ( SpatialAttentionBlock ( in_features , [ 64 , 32 , 16 , 1 ]), torch . nn . AdaptiveAvgPool2d ( output_size = 1 ), torch . nn . Flatten ( start_dim = 1 ), torch . nn . Linear ( in_features , in_features ), self . activation , torch . nn . Dropout ( 0.3 ), torch . nn . Linear ( in_features , out_dim ), ) setattr ( self , layer_name , layer ) def forward ( self , input_neurons ): \"\"\"Define the computation performed at every call.\"\"\" if self . use_custom_layers is False : output_predictions = self . model ( input_neurons ) else : if len ( self . out_dim_heads ) > 1 : output_logits_backbone = self . architecture [ \"backbone\" ]( input_neurons ) multi_outputs = [ getattr ( self , f \"head_ { i } \" )( output_logits_backbone ) for i in range ( self . num_heads ) ] output_predictions = torch . cat ( multi_outputs , axis = 1 ) We built-upon fellow Kaggler Tawara\u2019s Multi-head model for our best scoring models. In particular, we experimented with the activation functions and dropout rates. We found models with Swish activation in the multi-head component of the network to perform > best in our experiments. Our best scoring single model is a multi-head model with a resnet200d backbone. In particular, one single fold of resnet200d gives a private score of 0.970. Another very interesting approach is 3-4 stage training. We did not have time to experiment with the 3-4 stage training as we joined the competition late. Model Architectures: layer = torch.nn.Sequential( SpatialAttentionBlock(in_features, [64, 32, 16, 1]), torch.nn.AdaptiveAvgPool2d(output_size=1), torch.nn.Flatten(start_dim=1), torch.nn.Linear(in_features, in_features), self.activation, torch.nn.Dropout(0.3), torch.nn.Linear(in_features, out_dim), ) - Backbone : ResNet200D and SeResNet152d - Classifier Head: Separated and Independent Spatial-Attention Module and the typical Multi-Layer Perceptron for Target Group (ETT(3), NGT(4), CVC(3), and Swan(1)). - Spatial-Attention Module: SpatialAttentionBlock(in_features, [64, 32, 16, 1]) - MLP: : Linear -> Swish -> Dropout -> Linear ; It is worth noting after the Linear layer, there is a Sigmoid layer in this particular setup as we are using BCEWITHLOGITSLOSS from PyTorch for numerical stability. - Activation: One thing to note is we used Swish in our Classifier Head. Swish is a smooth and non-monotonic function, the latter contrasts when compared to many other activations. I will explain a bit in the next section. Activation Functions As we all know, activation functions are used to transform a neurons' linearity to non-linearity and decide whether to \"fire\" a neuron or not. We chose Swish as our main activation function in the classifier head layers. Swish When we design or choose an activation function, we need to ensure the follows: (Smoothness) Differentiable and Continuous: For example, the sigmoid function is continuous and hence differentiable. If the property is not fulfilled, we might face issues as backpropagation may not be performed properly since we cannot differentiate it.If you notice, the heaviside function is not. We cant perform GD using the HF as we cannot compute gradients but for the logistic function we can. The gradient of sigmoid function g is g(1-g) conveniently Monotonic: This helps the model to converge faster. But spoiler alert, Swish is not monotonic. The properties of Swish are as follows: Bounded below: It is claimed in the paper it serves as a strong regularization. Smoothness: More smooth than ReLU which allows the model to optimize better, the error landscape, when smoothed, is easier to traverse in order to find a minima. An intuitive idea is the hill again, imagine you traverse down Bukit Timah Hill, vs traversing down Mount Himalaya LOL!!! Let us see how swish looks like when plotted. import math import matplotlib.pyplot as plt import numpy as np def swish ( x ): sigmoid = 1 / ( 1 + np . exp ( - x )) swish = x * sigmoid return swish epsilon = 1e-20 x = np . linspace ( - 10 , 10 , 10 ) z = swish ( x ) print ( f \"x= { x } \" ) print ( f \" \\n z=swish(x)= { z } \" ) print ( f \" \\n min z = { min ( z ) } \" ) x=[-10. -7.77777778 -5.55555556 -3.33333333 -1.11111111 1.11111111 3.33333333 5.55555556 7.77777778 10. ] z=swish(x)=[-4.53978687e-04 -3.25707421e-03 -2.13946242e-02 -1.14817319e-01 -2.75182001e-01 8.35929110e-01 3.21851601e+00 5.53416093e+00 7.77452070e+00 9.99954602e+00] min z = -0.27518200126563513 plt . plot ( x , z ) plt . xlabel ( \"x\" ) plt . ylabel ( \"Swish(X)\" ) plt . show (); Model Architecture: Final Activation Layer Sigmoid vs Softmax I've noticed people often get directed to this question when searching whether to use sigmoid vs softmax in neural networks. If you are one of those people building a neural network classifier, here is how to decide whether to apply sigmoid or softmax to the raw output values from your network: If you have a multi-label classification problem = there is more than one \"right answer\" = the outputs are NOT mutually exclusive, then use a sigmoid function on each raw output independently. The sigmoid will allow you to have high probability for all of your classes, some of them, or none of them. Example: classifying diseases in a chest x-ray image. The image might contain pneumonia, emphysema, and/or cancer, or none of those findings. If you have a multi-class classification problem = there is only one \"right answer\" = the outputs are mutually exclusive, then use a softmax function. The softmax will enforce that the sum of the probabilities of your output classes are equal to one, so in order to increase the probability of a particular class, your model must correspondingly decrease the probability of at least one of the other classes. Example: classifying images from the MNIST data set of handwritten digits. A single picture of a digit has only one true identity - the picture cannot be a 7 and an 8 at the same time. In the below code we understand that our model's forward() call gives us a output output_logits of shape (8, 11) if the batch size is 8, and the 11 represents each logit for each of the class. If we apply Softmax to this function on dimension=1 , it simply means we are applying the function each row, from row 1 to 8. Take row 1 for example, the softmax function will squash all the 11 values into a 0-1 range, you can say this is a probability calibration, and the output_predictions is also of shape (8, 11) but all sums up to 1. If we apply Sigmoid to this function on dimension=1 , although PyTorch does not specifiy this because it automatically assumes we are applying sigmoid elementwise, that is to say you cannot simply pass an array of 11 elements to sigmoid function and but we are applying the sigmoid function each row as well. There is a lot of nuance and intricacies here. We take the first row as an example, the first element corresponds to the class ETT-Abnormal , when we apply sigmoid to this element 0.0762, we get 0.5190, and for the second element class ETT-Borderline , we have 0.0877 and when we apply sigmoid, we get 0.5219, so on and so forth for the first row. You should by now observe that they do not sum to 1. This is because each time sigmoid is applied, it is in a one-vs-all scenario. Meaning to say, the 0.519 for ETT-Abnormal means that ETT-Abnormal is treated as the positive class, and the remaining 10 classes are treated as negative class 0. In other words, with 11 elements and sigmoid, we are essentially performing 11 binary classification on the said 11 classes. So 0.519 actually means that the probability of it being class 1 ( ETT-Abnormal ) is 0.519, and the probability of it being NOT class 1 (ALL other classes) is 0.481. The same logic applies to each of the element in the first row. One thing worth noting is that the predictions for row 1 is not mutually exclusive , meaning that from the 11 classes, we can have say, ETT-Abnormal, NGH-Abnormal, CVC-Abnormal to all have say probability score of 0.9, meaning to say, it is highly likely to be all 3 conditions! This is okay and common in X-Ray imaging. In the table dataframe below, I put them into a dataframe for easy visualization. classes = [ \"ETT - Abnormal\" , \"ETT - Borderline\" , \"ETT - Normal\" , \"NGT - Abnormal\" , \"NGT - Borderline\" , \"NGT - Incompletely Imaged\" , \"NGT - Normal\" , \"CVC - Abnormal\" , \"CVC - Borderline\" , \"CVC - Normal\" , \"Swan Ganz Catheter Present\" , ] import torch import pandas as pd output_logits = torch . tensor ([ [ 0.0762 , 0.0877 , 0.1205 , - 0.0615 , - 0.0054 , 0.0661 , 0.1567 , - 0.0978 , 0.0248 , - 0.0350 , 0.0084 ], [ - 0.0196 , - 0.0729 , 0.0534 , 0.0307 , - 0.0428 , - 0.0016 , 0.0013 , - 0.0247 , - 0.0094 , - 0.0424 , 0.0192 ], [ - 0.0125 , - 0.0310 , 0.0118 , - 0.1301 , 0.0418 , 0.0229 , 0.0139 , - 0.0526 , 0.0870 , - 0.0681 , - 0.0068 ], [ - 0.0259 , - 0.0544 , - 0.0262 , 0.0018 , 0.0161 , - 0.0369 , - 0.0370 , - 0.0157 , 0.0036 , - 0.0592 , 0.0107 ], [ - 0.0366 , - 0.0695 , 0.0740 , - 0.0353 , - 0.0363 , - 0.0019 , 0.0085 , - 0.0144 , 0.0129 , - 0.0470 , 0.0043 ], [ - 0.0445 , - 0.0822 , 0.0487 , - 0.0851 , 0.0269 , - 0.0809 , - 0.0434 , 0.0110 , - 0.0631 , - 0.0733 , - 0.0188 ], [ - 0.0304 , 0.0012 , 0.0233 , - 0.0121 , - 0.0406 , - 0.0459 , - 0.0363 , 0.0089 , - 0.0009 , - 0.0797 , - 0.0017 ], [ - 0.0415 , 0.0787 , 0.0283 , - 0.0617 , - 0.0526 , - 0.0016 , - 0.0409 , - 0.0481 , 0.0583 , - 0.0810 , - 0.0050 ]], dtype = torch . float64 , device = device ) sigmoid = torch . nn . Sigmoid () softmax = torch . nn . Softmax ( dim = 1 ) output_predictions_sigmoid = sigmoid ( output_logits ) output_predictions_softmax = softmax ( output_logits ) print ( output_predictions_sigmoid ) print ( output_predictions_softmax ) tensor([[0.5190, 0.5219, 0.5301, 0.4846, 0.4987, 0.5165, 0.5391, 0.4756, 0.5062, 0.4913, 0.5021], [0.4951, 0.4818, 0.5133, 0.5077, 0.4893, 0.4996, 0.5003, 0.4938, 0.4977, 0.4894, 0.5048], [0.4969, 0.4923, 0.5029, 0.4675, 0.5104, 0.5057, 0.5035, 0.4869, 0.5217, 0.4830, 0.4983], [0.4935, 0.4864, 0.4935, 0.5004, 0.5040, 0.4908, 0.4908, 0.4961, 0.5009, 0.4852, 0.5027], [0.4909, 0.4826, 0.5185, 0.4912, 0.4909, 0.4995, 0.5021, 0.4964, 0.5032, 0.4883, 0.5011], [0.4889, 0.4795, 0.5122, 0.4787, 0.5067, 0.4798, 0.4892, 0.5027, 0.4842, 0.4817, 0.4953], [0.4924, 0.5003, 0.5058, 0.4970, 0.4899, 0.4885, 0.4909, 0.5022, 0.4998, 0.4801, 0.4996], [0.4896, 0.5197, 0.5071, 0.4846, 0.4869, 0.4996, 0.4898, 0.4880, 0.5146, 0.4798, 0.4988]], dtype=torch.float64) tensor([[0.0948, 0.0959, 0.0991, 0.0826, 0.0874, 0.0939, 0.1028, 0.0797, 0.0901, 0.0849, 0.0886], [0.0900, 0.0853, 0.0968, 0.0946, 0.0879, 0.0916, 0.0919, 0.0895, 0.0909, 0.0879, 0.0935], [0.0907, 0.0890, 0.0929, 0.0806, 0.0957, 0.0939, 0.0931, 0.0871, 0.1001, 0.0858, 0.0912], [0.0904, 0.0878, 0.0903, 0.0929, 0.0942, 0.0894, 0.0894, 0.0913, 0.0931, 0.0874, 0.0937], [0.0887, 0.0858, 0.0991, 0.0888, 0.0887, 0.0918, 0.0928, 0.0907, 0.0932, 0.0878, 0.0924], [0.0901, 0.0868, 0.0989, 0.0865, 0.0968, 0.0869, 0.0902, 0.0953, 0.0885, 0.0876, 0.0925], [0.0899, 0.0928, 0.0948, 0.0915, 0.0890, 0.0885, 0.0894, 0.0935, 0.0926, 0.0856, 0.0925], [0.0884, 0.0997, 0.0948, 0.0867, 0.0875, 0.0920, 0.0885, 0.0879, 0.0977, 0.0850, 0.0917]], dtype=torch.float64) df = pd . DataFrame ( data = output_predictions_sigmoid . detach () . cpu () . numpy (), columns = classes ) display ( df ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ETT - Abnormal ETT - Borderline ETT - Normal NGT - Abnormal NGT - Borderline NGT - Incompletely Imaged NGT - Normal CVC - Abnormal CVC - Borderline CVC - Normal Swan Ganz Catheter Present 0 0.519041 0.521911 0.530089 0.484630 0.498650 0.516519 0.539095 0.475569 0.506200 0.491251 0.502100 1 0.495100 0.481783 0.513347 0.507674 0.489302 0.499600 0.500325 0.493825 0.497650 0.489402 0.504800 2 0.496875 0.492251 0.502950 0.467521 0.510448 0.505725 0.503475 0.486853 0.521736 0.482982 0.498300 3 0.493525 0.486403 0.493450 0.500450 0.504025 0.490776 0.490751 0.496075 0.500900 0.485204 0.502675 4 0.490851 0.482632 0.518492 0.491176 0.490926 0.499525 0.502125 0.496400 0.503225 0.488252 0.501075 5 0.488877 0.479462 0.512173 0.478738 0.506725 0.479786 0.489152 0.502750 0.484230 0.481683 0.495300 6 0.492401 0.500300 0.505825 0.496975 0.489851 0.488527 0.490926 0.502225 0.499775 0.480086 0.499575 7 0.489626 0.519665 0.507075 0.484580 0.486853 0.499600 0.489776 0.487977 0.514571 0.479761 0.498750 Batch Size and Tricks Due to hardware limitation, we can barely fit in anything more than a batch_size of 8. Quoting from here : It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize [...] large-batch methods tend to converge to sharp minimizers of the training and testing functions\u2014and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. The above shows that large batch size may fit the model too well, as the model will learn features of the dataset in less iterations, and may memorize this particular dataset's features, leading to overfitting and poor generalization. However, too small a batch size causes our convergence to go too slow, empirically, we take 32 or 64 as the ideal batch size in this competition. We used both torch.amp and gradient accumulation to be able to fit more batch sizes. We did not freeze the batch_norm layers, which still yielded great results. What we should have done is to experiment more on how to freeze the batch norm layers properly, as I believe that it may help. In the end, we used a batch size of 8 and fit 4 iterations using gradient accumulation and trained a total number of 20 epochs to get a local CV score of roughly 0.969. Optimizer, Scheduler and Loss Scheduler The configuration can be seen here. But note that we incorporated GradualWarmUpScheduler along with CosineAnnealingLR , we also experimented with CosineAnnealingWarmRestarts , the results are similar. From the paper Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour , we learnt about the warmup technique. Although the context of the paper was training under large batch size, we find it helpful even in small batches for the training to converge. The basic algorithm is as follows: Set a base_lr or initial lr for what you want in a model, say 1e-4. If we set our warmup epoch to be 10, then we will start from with 1e-4/10 in the first epoch, and take equal steps each time to converge to 1e-4 in the 10th epoch. After the 10th epoch, warmup ends, we start applying our scheduler's normal steps. However, I took quite some time to understand the idea of gradual warmup, I made my understanding here . We should try OneCyclePolicy as detailed by fastai. num_epochs = 20 CosineAnnealingLR : T_max : num_epochs - 1 eta_min : 1.0e-07 last_epoch : - 1 verbose : true Notice in my configuration above, we set the parameter T_max to be the 19, which is like a one-shot training. import torch import matplotlib.pyplot as plt num_epochs = 20 model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 100 ) cosine_annealing_lr_scheduler_one_shot = torch . optim . lr_scheduler . CosineAnnealingLR ( optimizer , T_max = 19 , eta_min = 1e-7 ) cosine_annealing_lr_one_shot = [] for i in range ( num_epochs ): optimizer . step () cosine_annealing_lr_one_shot . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) cosine_annealing_lr_scheduler_one_shot . step () plt . plot ( cosine_annealing_lr_one_shot ); num_epochs = 20 model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 100 ) cosine_annealing_lr_scheduler_normal = torch . optim . lr_scheduler . CosineAnnealingLR ( optimizer , T_max = 2 , eta_min = 1e-7 ) cosine_annealing_lr_normal = [] for i in range ( num_epochs ): optimizer . step () cosine_annealing_lr_normal . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) cosine_annealing_lr_scheduler_normal . step () plt . plot ( cosine_annealing_lr_normal ) [<matplotlib.lines.Line2D at 0x260cde7e7c0>] Loss We should also experiment with Focal Loss but seeing negative results from fellow Kagglers, on top with limited resources, we did not try it. Ensembling Forward Ensembling We made use of the Forward Ensembling idea from Chris in SIIM-ISIC Melanoma Classification back in August 2020, I modified the code for this specific task. A simple description is as follows, modified from Chris, with more mathematical notations. We start off with a dataset \\(\\mathcal{D} = X \\times y\\) where it is sampled from the true population \\(\\mathcal{X} \\times \\mathcal{Y}\\) . We apply KFold (5 splits) to the dataset, as illustrated in the diagram. We can now train five different hypothesis \\(h_{F1}, h_{F2},...,h_{F5}\\) , where \\(h_{F1}\\) is trained on Fold 2 to Fold 5 and predict on Fold 1, \\(h_{F2}\\) is trained on Fold 1,3,4,5 and predict on Fold 2. The logic follows for all 5 hypothesis. Notice that in the five models, we are predicting on a unique validation fold, and as a result, after we trained all 5 folds, we will have the predictions made on the whole training set (F1-F5). This predictions is called the Out-of-Fold predictions. We then go a step further and calculate the AUC score with the OOF predictions with the ground truth to get the OOF AUC. We save it to a csv or dataframe called oof_1.csv , subsequent oof trained on different hypothesis space should be named oof_i.csv where \\(i \\in [2,3,...]\\) . After we trained all 5 folds, we will use \\(h_{1}\\) to predict on \\(X_{test}\\) and obtain predictions \\(Y_{\\text{h1 preds}}\\) , we then use \\(h_{2}\\) to predict on \\(X_{test}\\) and obtain predictions \\(Y_{\\text{h2 preds}}\\) , we do this for all five folds and finally \\(Y_{\\text{final preds}} = \\dfrac{1}{5}\\sum_{i=1}^{5}Y_{\\text{hi preds}}\\) . This is a typical pipeline in most machine learning problems. We save this final predictions as sub_1.csv , subsequence predictions trained on different hypothesis space should be named sub_i.csv where \\(i \\in [2,3,...]\\) . Now if we train another model, a completely different hypothesis space is used, to be more pedantic, we denote the previous model to be taken from the hypothesis space \\(\\mathcal{H}_{1}\\) , and now we move on to \\(\\mathcal{H}_{2}\\) . We repeat step 1-6 on this new model (Note that you are essentially training 10 \"models\" now since we are doing KFold twice, and oh, please set the seed of KFold to be the same, it should never be the case that both model comes from different splitting seed for apparent reasons). Here is the key (given the above setup with 2 different models trained on 5 folds): Normally, most people do a simple mean ensemble, that is \\(\\dfrac{Y_{\\text{final preds H1}} + Y_{\\text{final preds H2}}}{2}\\) . This works well most of the time as we trust both model holds equal importance in the final predictions. One issue may be that certain models should be weighted more than the rest, we should not simply take Leaderboard feedback score to judge the weight assignment. A general heuristic here is called Forward Selection. (Extract from Chris) Now say that you build 2 models (that means that you did 5 KFold twice). You now have oof_1.csv, oof_2.csv, sub_1.csv, and sub_2.csv. How do we blend the two models? We find the weight w such that w * oof_1.predictions + (1-w) * oof_2.predictions has the largest AUC. all = [] for w in [ 0.00 , 0.01 , 0.02 , ... , 0.98 , 0.99 , 1.00 ]: ensemble_pred = w * oof_1 . predictions + ( 1 - w ) * oof_2 . predictions ensemble_auc = roc_auc_score ( oof . target , ensemble_pred ) all . append ( ensemble_auc ) best_weight = np . argmax ( all ) / 100. Then we can assign the best weight like: final_ensemble_pred = best_weight * sub_1 . target + ( 1 - best_weight ) * sub_2 . target Coutersy of Chris In this competition, there are two approaches, either maximize the average of the macro AUC score of all the classes, or maximize each column/class separately. It turns out that maximizing the columns separately led to disastrous results (it could be my code and idea is wrong, as ROC is a ranking metric). Conclusion What we could have done better: Use more variety of classifier head like GeM . Use more variety of backbone . Use Neptune.ai to log our experiments as soon things start to get messy. Basically MLOps is important! Experiment on 3-4 stage training. Pseudo Labelling. Knowledge Distillation. Experiment more on maximizing AUC during ensembles. rank_pct etc. See novel three stage training: https://www.kaggle.com/yasufuminakama/ranzcr-resnet200d-3-stage-training-step1 References Multi-Head Deep Learning Model with Multi-Label Classification AUC Metric on Multi-Label Sigmoid and Softmax for Multi-Label Multi-Label Classification Tutorial Why we should use Multi-Head in Multi-Label Classification Follow Up 1 Follow Up 2 Follow Up 3 Sigmoid is Binary Cross Entropy Attention Blocks in Computer Vision Spatial Attention Blocks Spatial Attention Module Convolutional Block Attention Module [Dive Into Deep Learning - Chapter 10: Attention Mechanisms] Gradual Warmup: Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour","title":"RANZCR CLiP - Catheter and Line Position Challenge"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#seeding-and-device","text":"import os import random import numpy as np import torch def seed_all ( seed : int = 1930 ): \"\"\"Seed all random number generators.\"\"\" print ( \"Using Seed Number {} \" . format ( seed )) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) # set PYTHONHASHSEED env var at fixed value torch . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) torch . cuda . manual_seed ( seed ) # pytorch (both CPU and CUDA) np . random . seed ( seed ) # for numpy pseudo-random generator random . seed ( seed ) # set fixed value for python built-in pseudo-random generator torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = False torch . backends . cudnn . enabled = False def seed_worker ( _worker_id ): \"\"\"Seed a worker with the given ID.\"\"\" worker_seed = torch . initial_seed () % 2 ** 32 np . random . seed ( worker_seed ) random . seed ( worker_seed ) _ = seed_all ( 1992 ) device = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) Using Seed Number 1992","title":"Seeding and Device"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#introduction","text":"Extracted from Kaggle Serious complications can occur as a result of malpositioned lines and tubes in patients. Doctors and nurses frequently use checklists for placement of lifesaving equipment to ensure they follow protocol in managing patients. Yet, these steps can be time consuming and are still prone to human error, especially in stressful situations when hospitals are at capacity. Hospital patients can have catheters and lines inserted during the course of their admission and serious complications can arise if they are positioned incorrectly. Nasogastric tube malpositioning into the airways has been reported in up to 3% of cases, with up to 40% of these cases demonstrating complications [1-3]. Airway tube malposition in adult patients intubated outside the operating room is seen in up to 25% of cases [4,5]. The likelihood of complication is directly related to both the experience level and specialty of the proceduralist. Early recognition of malpositioned tubes is the key to preventing risky complications (even death), even more so now that millions of COVID-19 patients are in need of these tubes and lines. The gold standard for the confirmation of line and tube positions are chest radiographs. However, a physician or radiologist must manually check these chest x-rays to verify that the lines and tubes are in the optimal position. Not only does this leave room for human error, but delays are also common as radiologists can be busy reporting other scans. Deep learning algorithms may be able to automatically detect malpositioned catheters and lines. Once alerted, clinicians can reposition or remove them to avoid life-threatening complications. The Royal Australian and New Zealand College of Radiologists (RANZCR) is a not-for-profit professional organisation for clinical radiologists and radiation oncologists in Australia, New Zealand, and Singapore. The group is one of many medical organisations around the world (including the NHS) that recognizes malpositioned tubes and lines as preventable. RANZCR is helping design safety systems where such errors will be caught. In this competition, you\u2019ll detect the presence and position of catheters and lines on chest x-rays. Use machine learning to train and test your model on 40,000 images to categorize a tube that is poorly placed. The dataset has been labelled with a set of definitions to ensure consistency with labelling. The normal category includes lines that were appropriately positioned and did not require repositioning. The borderline category includes lines that would ideally require some repositioning but would in most cases still function adequately in their current position. The abnormal category included lines that required immediate repositioning. If successful, your efforts may help clinicians save lives. Earlier detection of malpositioned catheters and lines is even more important as COVID-19 cases continue to surge. Many hospitals are at capacity and more patients are in need of these tubes and lines. Quick feedback on catheter and line placement could help clinicians better treat these patients. Beyond COVID-19, detection of line and tube position will ALWAYS be a requirement in many ill hospital patients.","title":"Introduction"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#dataset-understanding-our-data","text":"We will go through the data that we were given.","title":"Dataset: Understanding our Data"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#data-catalogdescription","text":"In this competition, you'll detect the presence and position of catheters and lines on chest x-rays. Use machine learning to train and test your model on \\(40,000\\) images to categorize a tube that is poorly placed. Train Set: \\(30083\\) images Public Test Set: \\(3582\\) images Private Test Set: ~ \\(14000\\) images train.csv : containing image IDs, labels and patient IDs . sample_submission.csv : a sample submission file in the correct format test/ : test images train/ : training images In particular the train csv has these columns: StudyInstanceUID - unique ID for each image ETT - Abnormal - endotracheal tube placement abnormal ETT - Borderline - endotracheal tube placement borderline abnormal ETT - Normal - endotracheal tube placement normal NGT - Abnormal - nasogastric tube placement abnormal NGT - Borderline - nasogastric tube placement borderline abnormal NGT - Incompletely Imaged - nasogastric tube placement inconclusive due to imaging NGT - Normal - nasogastric tube placement borderline normal CVC - Abnormal - central venous catheter placement abnormal CVC - Borderline - central venous catheter placement borderline abnormal CVC - Normal - central venous catheter placement normal Swan Ganz Catheter Present PatientID - unique ID for each patient in the dataset","title":"Data Catalog/Description"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#objective-multi-label-binary-classification","text":"In general, a patient's single chest X-ray could present multiple medical conditions. Fig 1: Patient ID 323464123; By Hongnan G. Patient 323464123's first row corresponds to an unique image defined by the study instance UID. Noticed the 1's under the columns ETT-Normal , NGT - Incompletely Images , CVC-Borderline and CVC-Normal and 0's elsewhere. Unlike multi-class classification, where classes are mutually exclusive , multi-label is not. For example, a patient's X-ray scan of the lungs can show up pneumonia and covid-19 (both are conditions), as a result the class labels are not mutually exclusive (unlike multi-class). The same logic is applied in this setting, where the tube can be labelled differently.","title":"Objective: Multi-Label Binary Classification"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#metrics-establish-metrics","text":"","title":"Metrics: Establish Metrics"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#macro-averaged-auroc","text":"Then Macro-Average AUROC is calculated for each individual class, and then averaged over the total number of classes. \\[\\text{Macro-Average AUROC} = \\frac{1}{\\text{num_class}}\\sum_{i=1}^{\\text{num_class}}R_{i}\\] where \\(R_i\\) is the AUROC score for each individual class. A small example clears up the air: Consider 3 classes of apple, banana and carrot as a multi-label problem with class index \\([0, 1, 2]\\) . y_true : is a one-hot encoded matrix of 4 rows and 3 columns. The rows means 4 samples, and columns mean the class where 1 means positive and 0 negative. It is not a surprise that each row can hold multiple 1's since it is a multi-label problem. [0, 1, 1] just means that the \"image data\" contains both banana and carrot; y_pred : the predicted matrix, and is the same shape as y_true . We now treat the problem running multiple binary ROC computation: Calculate the ROC score between column \\(i\\) of y_true and y_pred respectively and call them \\(R_i\\) ; Sum \\(R_i\\) and divide by the number of classes and get the Macro-Averaged AUROC. import numpy as np from sklearn.metrics import roc_auc_score y_true = np . asarray ([[ 0 , 1 , 1 ], [ 0 , 0 , 1 ], [ 1 , 1 , 0 ], [ 1 , 1 , 1 ]]) y_pred = np . asarray ([[ 0 , 1 , 0 ], [ 1 , 0 , 0 ], [ 1 , 1 , 0 ], [ 1 , 0 , 1 ]]) macro_auroc = np . mean ([ roc_auc_score ( y_true [:, i ], y_pred [:, i ]) for i in range ( 3 )]) macro_auroc 0.75 Tip For a more wholesome treatment of metrics, see my Melanoma write-up and blog .","title":"Macro-Averaged AUROC"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#validation-and-resampling-cross-validation","text":"","title":"Validation and Resampling: Cross-Validation"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#how-should-we-split-out-data-into-folds","text":"We should examine the data for a few factors: Is the data \\(\\mathcal{X}\\) imbalanced? Is the data \\(\\mathcal{X}\\) generated in a i.i.d. manner, more specifically, if I split \\(\\mathcal{X}\\) to \\(\\mathcal{X}_{train}\\) and \\(\\mathcal{X}_{val}\\) , can we ensure that \\(\\mathcal{X}_{val}\\) has no dependency on \\(\\mathcal{X}_{train}\\) ? We came to the conclusion: Yes, there is quite some imbalanced distribution, in particular, CVC - Normal , ETT - Normal and CVC - Borderline are significantly more than the rest of the classes. Therefore, a stratified cross validation is reasonable. Stratified KFold ensures that relative class frequencies is approximately preserved in each train and validation fold. More concretely, we will not experience the scenario where \\(X_{train}\\) has \\(m^{+}\\) and \\(m^{-}\\) positive and negative samples, but \\(X_{val}\\) has only \\(p^{+}\\) positive samples only and 0 negative samples, simply due to the scarcity of negative samples In medical imaging, it is a well known fact that most of the data contains patient level repeatedly. To put it bluntly, if I have 100 samples, and according to PatientID , we see that the id 123456 (John Doe) appeared 20 times, this is normal as a patient can undergo multiple settings of say, X-rays. If we allow John Doe's data to appear in both train and validation set, then this poses a problem of information leakage, in which the data is no longer i.i.d. . One can think of each patient has an \"unique, underlying features\" which are highly correlated across their different samples. As a result, it is paramount to ensure that amongst this 3255 unique patients, we need to ensure that each unique patients' images DO NOT appear in the validation fold. That is to say, if patient John Doe has 100 X-ray images, but during our 5-fold splits, he has 70 images in Fold 1-4, while 30 images are in Fold 5, then if we were to train on Fold 1-4 and validate on Fold 5, there may be potential leakage and the model will predict with confidence for John Doe's images. This is under the assumption that John Doe's data does not fulfill the i.i.d process.","title":"How should we split out data into folds?"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#stratifiedgroupkfold","text":"With the above consideration, we will use StratifiedGroupKFold where \\(K = 5\\) splits. There wasn't this splitting function in scikit-learn at the time of competition and as a result, we used a custom written (by someone else) RepeatedStratifiedGroupKFold function and just set n_splits = 1 to get StratifiedGroupKFold (yes we cannot afford to do repeated sample, so setting the split to be 1 will collapse the repeated function to just the normal stratified group kfold). However, as of 2022, this function is readily available in the Scikit-Learn library. To recap, we applied stratified logic such that each train and validation set has an equal weightage of positive and negative samples. We also grouped the patients in the process such that patient \\(i\\) will not appear in both training and validation set. Data leakage can cause you to have blind confidence on your model. We are also guilty of committing one since we trained our models with the NiH pretrained weights, without taking into consideration if the weights overlap with the training and validation folds information. In other words, we did not check properly if the weights trained on the NiH dataset has information in our RANZCR dataset. Take note this is different from training altogether on the NiH dataset, we are merely using the weights instead of the imagenet weights, which brings to the next point. Referring to figure 1, patient 323464123 has 6 images uniquely recorded on different visits. We should absolutely put them all in either train or validation set, as if say 2 images are in train and 4 in validation, there might be data leakage.","title":"StratifiedGroupKFold"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#cross-validation-workflow","text":"To recap, we have the following: Training Set ( \\(X_{\\text{train}}\\) ) : This will be further split into K validation sets during our cross-validation. This set is used to fit a particular hypothesis \\(h \\in \\mathcal{H}\\) . Validation Set ( \\(X_{\\text{val}}\\) ) : This is split from our \\(X_{\\text{train}}\\) during cross-validation. This set is used for model selection (i.e. find best hyperparameters, attempt to produce a best hypothesis \\(g \\in \\mathcal{H}\\) ). Test Set ( \\(X_{\\text{test}}\\) ) : This is an unseen test set, and we will only use it after we finish tuning our model/hypothesis. Suppose we have a final best model \\(g\\) , we will use \\(g\\) to predict on the test set to get an estimate of the generalization error (also called out-of-sample error). Pipeline. Courtesy of scikit-learn on a typical Cross-Validation workflow.","title":"Cross-Validation Workflow"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#transfer-learning-the-core-of-deep-learning","text":"As we all know, if we train on imagenet weights, we may take quite a while to converge, even if we finetune it. The intuition is simple, imagenet were trained on many common items in life, and none of them resemble closely to the image structures of X-rays . Therefore, we have a few options. Freeze earlier layers but unfreeze the later Conv Layers: this is intuitive as earlier layers detect shapes and colors, all low level details that is very useful even for such dissimilar tasks , and unfreeze the later conv layers which is what we call the \"abstract feature layers\", where it is more important for the model to learn from scratch. Fine-Tuning; Feature Extraction;","title":"Transfer Learning: The core of Deep Learning"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#fine-tuning","text":"Instead of random initialization, we initialize the network with a pretrained network, like the one that is trained on imagenet 1000 dataset. This is what we will be doing and we managed to find a set of pretrained weights trained specifically on this dataset as a starting point. The weights can be found here . We used a few models and found out that resnet200d has the best results on this set of training images. The reason we used this is mostly empirical, but using gradcam we can see how the model sees the images.","title":"Fine-Tuning"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#feature-extraction","text":"ConvNet as fixed feature extractor: Here, we will freeze the weights for all of the network except that of the final fully connected layer. This last fully connected layer is replaced with a new one with random weights and only this layer is trained. This is less relevant to us as we aren't using it for feature extractions. Therefore, the model may have a hard time detecting abstract features such shapes and details from the X-rays. We can of course unfreeze all the layers and retrain them from scratch, using various backbones,","title":"Feature Extraction"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#preprocessing","text":"Most preprocessing techniques we do in an image recognition competition is mostly as follows:","title":"Preprocessing"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#mean-and-standard-deviation","text":"Perform mean and std for the dataset given to us. Note that this step may make sense on paper, but empirically, using imagenet's default mean std will always work as well, if not better. Nevertheless, here are the stats: Imagenet: mean = [0.485, 0.456, 0.406] and; std = [0.229, 0.224, 0.225] RANZCR: mean = [0.4887381077884414, ...] and; std = [0.23064819430546407, ...]","title":"Mean and Standard Deviation"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#channel-distribution","text":"This is usually done to check for \"surprises\". More specifically, I remember vividly when participating in Singapore Airline challenge where the classifier recognize weird objects as luggages. After plotting the pixel histogram, we observed that the luggages colors are all of a non-normal distribution, in fact, it is quite scattered. Then it dawned upon us that the classifier is learning the \"color\" too much, instead of the shape of the luggage. When we grayed out the images, the classifier starts to ignore the noise in the colors, and instead focus on other features like shapes. So this removes signal to noise in a way - using the objection detection example such as detecting a strawberry in a tree full of green leaves, then color is important, but if we detect leaves in a tree full of green leaves, we do not wish to incorporate color here as anything green might suggest that it is a leaf. We found, and as mentioned also by Rueben Schmidt in this post , there are some images that have black borders around them. I experimented by removed them during both the training process. There was no significant increase on the LB score, even if there was, it is in the 3-4th decimal places, but I noticed my local cv increased, so I think that some noise are removed locally, but not reflected in the test set. Therefore, during inference, I also removed the black borders, which should be the correct approach (learning from mistakes!). In conclusion, there is a small boost in score, if I keep this consistent in both training and inference, I reckon that no surprise factor would pop out. Here is the code: image = cv2 . imread ( image_path , cv2 . IMREAD_GRAYSCALE ) mask = image > 0 image = image [ np . ix_ ( mask . any ( 1 ), mask . any ( 0 ))] image = cv2 . cvtColor ( image , cv2 . COLOR_GRAY2RGB ) Notice that the code removes any pixel that is > 0, where black pixel is 0. On hindsight for the Singapore Airline project, I now know there is GradCam , where we can see how the model is learning, as it will highlight the areas on which the model is focusing on in an image.","title":"Channel Distribution"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#convert-gray-to-rgb","text":"We know that X-rays are Grayscale images so converting a grayscale image to RGB is just setting R=G=B=Grayscale pixel for all channels. import cv2 image = cv2 . imread ( \"../images/1.2.826.0.1.3680043.8.498.10000428974990117276582711948006105617.png\" , cv2 . IMREAD_GRAYSCALE ) print ( image . shape ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) print ( image . shape ) (256, 256) (256, 256, 3)","title":"Convert Gray to RGB"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#augmentations","text":"We know that augmentation is central in an image competition, as essentially we are adding more data into the training process, effectively reducing overfitting and improve generalization . Heavy augmentations are used during Train-Time-Augmentation. But during Test-Time-Augmentation, we used the same set of training augmentations to inference with \\(100\\%\\) probability.","title":"Augmentations"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#train-time-augmentation","text":"The typicals! train_augmentations = [ albumentations . RandomResizedCrop ( height = config . image_size , width = config . image_size ), albumentations . HorizontalFlip ( p = 0.5 ), albumentations . ShiftScaleRotate ( p = 0.5 ), albumentations . HueSaturationValue ( hue_shift_limit = 0.2 , sat_shift_limit = 0.2 , val_shift_limit = 0.2 , p = 0.5 ), albumentations . RandomBrightnessContrast ( brightness_limit = ( - 0.1 , 0.1 ), contrast_limit = ( - 0.1 , 0.1 ), p = 0.5 ), albumentations . CoarseDropout ( p = 0.5 ), albumentations . Normalize ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ], max_pixel_value = 255.0 , p = 1.0 , ), ToTensorV2 ( p = 1.0 ), ]","title":"Train-Time Augmentation"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#test-time-augmentation","text":"The exact same set of augmentations were used in inference. Not all TTAs provided a increase in score.","title":"Test-Time Augmentation"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#model-architectures-training-parameters-tricks","text":"","title":"Model Architectures, Training Parameters &amp; Tricks"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#model-architecture","text":"","title":"Model Architecture"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#overview","text":"A brief overview of our model architecture is shown in fig 2.1 and 2.2: Fig 2.1; Single-Head Approach courtesy of Tawara Fig 2.2; Multi-Head Approach courtesy of Tawara","title":"Overview"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#backbone","text":"We used both resnet200d and seresnet152d but will focus more on the first model. We used resnet200d , a ResNet variant model as our main backbone. ResNet-D is a modification on the ResNet architecture that utilises an average pooling tweak for downsampling. The motivation is that in the unmodified ResNet, the 1\u00d71 convolution for the downsampling block ignores 3/4 of input feature maps, so this is modified so no information will be ignored. Info So 1x1 convolutional reduces the feature maps depth but not the width or height while pooling reduces the width or height but not the depth. I think the results are better with the latter. In our case, we did the following: Create the model with: model = timm.create_model('resnet200d', pretrained=True, num_classes=1000) ; Load the pretrained weights from NiH trained ; Reset Classifier Head with Global Average Pooling: self.model.reset_classifier(num_classes=0, global_pool=\"avg\") ; Attach our own Classifier Head with 11 classes. import torch import timm from torchinfo import summary model = timm . create_model ( 'resnet18' , pretrained = True , num_classes = 1000 ) batch_size = 2 image_shape = ( 3 , 224 , 224 ) input_image_tensor = torch . rand ( size = ( batch_size , * image_shape )) # print(summary(model, (2, 3, 224, 224))) model.reset_classifier(num_classes=0, global_pool=\"\") means we do not want global average pooling and thus the shape at the last conv layer (penultimate layer) is \\((2, 512, 7, 7)\\) model.reset_classifier(num_classes=0, global_pool=\"avg\") means we do want global average pooling and thus the shape at the last conv layer (penultimate layer) is \\((2, 512)\\) whereby for each and every of the 512 feature maps \\(f_i\\) , we average \\(f_i\\) across all pixels (i.e. if \\(f_i\\) is 3 by 3 then average means add all \\(3 \\times 3 = 9\\) pixels and average) and concat to become one \\(512\\) vector. model = timm . create_model ( 'resnet18' , pretrained = True , num_classes = 1000 ) o = model ( input_image_tensor ) print ( f 'Original shape: { o . shape } ' ) model . reset_classifier ( num_classes = 0 , global_pool = \"\" ) o = model ( input_image_tensor ) print ( f 'Unpooled shape: { o . shape } ' ) Original shape: torch.Size([2, 1000]) Unpooled shape: torch.Size([2, 512, 7, 7]) model = timm . create_model ( 'resnet18' , pretrained = True , num_classes = 1000 ) o = model ( input_image_tensor ) print ( f 'Original shape: { o . shape } ' ) model . reset_classifier ( num_classes = 0 , global_pool = \"avg\" ) o = model ( input_image_tensor ) print ( f 'Pooled shape: { o . shape } ' ) Original shape: torch.Size([2, 1000]) Pooled shape: torch.Size([2, 512]) Note: Empirically, we realized the ResNet200D works very well for this particular task. We all asked ourselves why, and it was also discussed by many, but we all agreed that through various experiments, this model seems to consistently outperform their other SOTA counterparts. However, the closest possible paper on Revisiting ResNets: Improved Training and Scaling Strategies . Of course, to add diversity to our final predictions, we trained one more SeResNet152d as well. In general, ensembling models with vastly different architectures may result in a more robust solution. As an example, you can think of each model as a \"average learner\", and if their structure is different, it may very well so learn information that the other model might miss, hence ensembling them will average out such differences. Later on I will touch upon an ensembling technique called Forward Ensembling/Selection in this task, it has since worked well for other similar competitions.","title":"Backbone"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#classifier-heads-multi-head-approach","text":"We will focus a bit more on the multi-heads we used. Reading on self attention in X-ray What is a multi-headed model? And what exactly is a 'head' in a model?","title":"Classifier Heads - Multi-Head Approach"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#intuition","text":"The usage of multi-heads is not uncommon, let us detail a simple example in Object Detection. The image below shows the general architecture: In object detection, we want to predict two things, the image class label and its bounding box coordinates ; The backbone network (\"convolution and pooling\") is responsible for extracting a feature map from the image that contains higher level summarized information. Each head uses this feature map as input to predict its desired outcome. The main intuition why feature maps of the last few layers (last layer usually) are important is one needs to recognize the earlier conv layer's feature maps find simple features like shapes, sizes, edges from an image, while the deep conv layers will be of more abstract features in an image. As a result, we really just want the abstract feature maps as they are more class specific to the image instead of the earlier layers which gives generic shapes . Let us say you used a ResNet as the backbone, then: Remove the classifier head, or rather just take the backbone which is all layers up to the last conv layer: say at the last layer. the output feature maps has a shape of (512, 7, 7); These 512 feature maps of 7 by 7 are high level information of the image encoded; We then connect a classification head to the backbone to predict the image's class given the true class labels; We then connect a regression head to the backbone to predict the image's bounding box coordinates given the true bounding box coordinates; We can create two heads, one responsible for classification for the class label and the other regression to work on the localization of the bounding boxes; Thus here we have 2 heads, one for classifying what class the image object is, the other is to localize where the image object is. The loss that you optimize for during training is usually a weighted sum of the individual losses for each prediction head.","title":"Intuition"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#multi-head-for-multi-label","text":"Since we have 11 targets in this competition and they can be divided into 4 distinct groups: ETT , NGT , CVC and Swan . We envision that different groups have different areas in images to focus on. One possible way to leverage this idea is a multi-head approach we talked about. Multipe groups can share one single CNN backbone but have independent classifier heads. Fig 2.1; Single-Head Approach courtesy of Tawara Fig 2.2; Multi-Head Approach courtesy of Tawara With this in mind, let us move on: Multi-Label This is a multi-label classification problem. The section on the activation functions fully explained the single head version of using sigmoid layer. In fact, it is not uncommon to train N number of heads on a N-class Multi-Label problem. One thing to note is that if your classification head is Linear layer only (with BCE loss), then the back gradient propagation is the same whether you train one head, or multiple heads. However, we have non-linear layers in the head, including the SpatialAttentionBlock ! At the time of writing, I won't say I fully grasp of all the inner workings of an Attention Module across various use cases, but an analogy to aid my understanding is as follows: Having taken Learning From Data from Professor Yaser, the inner joke is about the Hypothesis Space. Let me elaborate, given a resnet200D as our hypothesis space \\(\\mathcal{H}\\) , we aim to find a \\(h \\in \\mathcal{H}\\) that best represents our true function \\(f\\) . Now suppose our learning algorithm \\(\\mathcal{A}\\) does a good job in helping us to find such a optimal \\(h\\) , it may take time, maybe say 100 epochs before finding it. Now if I break down the problem into 4 parts, each corresponding to a group, and we \"aid\" the learning algorithm by giving more attention to 4 focused areas, then we might find both a good \\(h\\) that estimate the \\(f\\) well, and may even be faster! If the above is too meh for understanding, imagine you are taking an exam in Machine Learning, as we all know, this field is a rabbit hole with never ending topics, let us say that there are 20 topics for you to study for the exam, you are dilligent and does that. But you have limited time and you decided to devote equal time to each topic, the consequence is you may not perform well for the exam due to limited understanding of each topic. Now, if I were to tell you that, hey, out of the 20 topics, can you study these 4 topics, as I think they have a higher chance of coming out, you will likely do better in the exam given that you devoted much more time on those \"focused (attention!)\" topics. The following code explains this methodology with reference to the above images. We first note to the readers that typically, if we use a single head approach, where if we were given a problem set \\(\\mathcal{D} = X \\times y\\) , a hypothesis space \\(\\mathcal{H}\\) we learn from a learning algorithm \\(\\mathcal{A}\\) , producing a final hypothesis \\(g\\) (or h, depends on your notation), that predicts as such \\(g(X_{val}) = y_{\\text{val_pred}}\\) , where each element in \\(y_{\\text{val_pred}}\\) corresponds to the class. Think of the basic MNIST example, our prediction vector's first element corresponds to the probability of it being an 0, and so on and so forth (assuming we use soft labels here). The change here is after the feature extraction layer (i.e. the feature logits after backbone), instead of just connecting it to a linear head for classification, we instead split the 11 outputs to 4 distinct groups. Each group will go through the head independent of the others, and this may prompt the model to put more attention on the independent groups. Finally, we torch.cat(..,axis=1) the outputs after they gone through their respective heads to recover the 11 outputs. model = CustomModel ( config , pretrained = True , load_weight = True , load_url = False , out_dim_heads = [ 3 , 4 , 3 , 1 ], ) # Multi Head for i , out_dim in enumerate ( self . out_dim_heads ): layer_name = f \"head_ { i } \" layer = torch . nn . Sequential ( SpatialAttentionBlock ( in_features , [ 64 , 32 , 16 , 1 ]), torch . nn . AdaptiveAvgPool2d ( output_size = 1 ), torch . nn . Flatten ( start_dim = 1 ), torch . nn . Linear ( in_features , in_features ), self . activation , torch . nn . Dropout ( 0.3 ), torch . nn . Linear ( in_features , out_dim ), ) setattr ( self , layer_name , layer ) def forward ( self , input_neurons ): \"\"\"Define the computation performed at every call.\"\"\" if self . use_custom_layers is False : output_predictions = self . model ( input_neurons ) else : if len ( self . out_dim_heads ) > 1 : output_logits_backbone = self . architecture [ \"backbone\" ]( input_neurons ) multi_outputs = [ getattr ( self , f \"head_ { i } \" )( output_logits_backbone ) for i in range ( self . num_heads ) ] output_predictions = torch . cat ( multi_outputs , axis = 1 ) We built-upon fellow Kaggler Tawara\u2019s Multi-head model for our best scoring models. In particular, we experimented with the activation functions and dropout rates. We found models with Swish activation in the multi-head component of the network to perform > best in our experiments. Our best scoring single model is a multi-head model with a resnet200d backbone. In particular, one single fold of resnet200d gives a private score of 0.970. Another very interesting approach is 3-4 stage training. We did not have time to experiment with the 3-4 stage training as we joined the competition late. Model Architectures: layer = torch.nn.Sequential( SpatialAttentionBlock(in_features, [64, 32, 16, 1]), torch.nn.AdaptiveAvgPool2d(output_size=1), torch.nn.Flatten(start_dim=1), torch.nn.Linear(in_features, in_features), self.activation, torch.nn.Dropout(0.3), torch.nn.Linear(in_features, out_dim), ) - Backbone : ResNet200D and SeResNet152d - Classifier Head: Separated and Independent Spatial-Attention Module and the typical Multi-Layer Perceptron for Target Group (ETT(3), NGT(4), CVC(3), and Swan(1)). - Spatial-Attention Module: SpatialAttentionBlock(in_features, [64, 32, 16, 1]) - MLP: : Linear -> Swish -> Dropout -> Linear ; It is worth noting after the Linear layer, there is a Sigmoid layer in this particular setup as we are using BCEWITHLOGITSLOSS from PyTorch for numerical stability. - Activation: One thing to note is we used Swish in our Classifier Head. Swish is a smooth and non-monotonic function, the latter contrasts when compared to many other activations. I will explain a bit in the next section.","title":"Multi-Head for Multi-Label"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#activation-functions","text":"As we all know, activation functions are used to transform a neurons' linearity to non-linearity and decide whether to \"fire\" a neuron or not. We chose Swish as our main activation function in the classifier head layers.","title":"Activation Functions"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#swish","text":"When we design or choose an activation function, we need to ensure the follows: (Smoothness) Differentiable and Continuous: For example, the sigmoid function is continuous and hence differentiable. If the property is not fulfilled, we might face issues as backpropagation may not be performed properly since we cannot differentiate it.If you notice, the heaviside function is not. We cant perform GD using the HF as we cannot compute gradients but for the logistic function we can. The gradient of sigmoid function g is g(1-g) conveniently Monotonic: This helps the model to converge faster. But spoiler alert, Swish is not monotonic. The properties of Swish are as follows: Bounded below: It is claimed in the paper it serves as a strong regularization. Smoothness: More smooth than ReLU which allows the model to optimize better, the error landscape, when smoothed, is easier to traverse in order to find a minima. An intuitive idea is the hill again, imagine you traverse down Bukit Timah Hill, vs traversing down Mount Himalaya LOL!!! Let us see how swish looks like when plotted. import math import matplotlib.pyplot as plt import numpy as np def swish ( x ): sigmoid = 1 / ( 1 + np . exp ( - x )) swish = x * sigmoid return swish epsilon = 1e-20 x = np . linspace ( - 10 , 10 , 10 ) z = swish ( x ) print ( f \"x= { x } \" ) print ( f \" \\n z=swish(x)= { z } \" ) print ( f \" \\n min z = { min ( z ) } \" ) x=[-10. -7.77777778 -5.55555556 -3.33333333 -1.11111111 1.11111111 3.33333333 5.55555556 7.77777778 10. ] z=swish(x)=[-4.53978687e-04 -3.25707421e-03 -2.13946242e-02 -1.14817319e-01 -2.75182001e-01 8.35929110e-01 3.21851601e+00 5.53416093e+00 7.77452070e+00 9.99954602e+00] min z = -0.27518200126563513 plt . plot ( x , z ) plt . xlabel ( \"x\" ) plt . ylabel ( \"Swish(X)\" ) plt . show ();","title":"Swish"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#model-architecture-final-activation-layer","text":"Sigmoid vs Softmax I've noticed people often get directed to this question when searching whether to use sigmoid vs softmax in neural networks. If you are one of those people building a neural network classifier, here is how to decide whether to apply sigmoid or softmax to the raw output values from your network: If you have a multi-label classification problem = there is more than one \"right answer\" = the outputs are NOT mutually exclusive, then use a sigmoid function on each raw output independently. The sigmoid will allow you to have high probability for all of your classes, some of them, or none of them. Example: classifying diseases in a chest x-ray image. The image might contain pneumonia, emphysema, and/or cancer, or none of those findings. If you have a multi-class classification problem = there is only one \"right answer\" = the outputs are mutually exclusive, then use a softmax function. The softmax will enforce that the sum of the probabilities of your output classes are equal to one, so in order to increase the probability of a particular class, your model must correspondingly decrease the probability of at least one of the other classes. Example: classifying images from the MNIST data set of handwritten digits. A single picture of a digit has only one true identity - the picture cannot be a 7 and an 8 at the same time. In the below code we understand that our model's forward() call gives us a output output_logits of shape (8, 11) if the batch size is 8, and the 11 represents each logit for each of the class. If we apply Softmax to this function on dimension=1 , it simply means we are applying the function each row, from row 1 to 8. Take row 1 for example, the softmax function will squash all the 11 values into a 0-1 range, you can say this is a probability calibration, and the output_predictions is also of shape (8, 11) but all sums up to 1. If we apply Sigmoid to this function on dimension=1 , although PyTorch does not specifiy this because it automatically assumes we are applying sigmoid elementwise, that is to say you cannot simply pass an array of 11 elements to sigmoid function and but we are applying the sigmoid function each row as well. There is a lot of nuance and intricacies here. We take the first row as an example, the first element corresponds to the class ETT-Abnormal , when we apply sigmoid to this element 0.0762, we get 0.5190, and for the second element class ETT-Borderline , we have 0.0877 and when we apply sigmoid, we get 0.5219, so on and so forth for the first row. You should by now observe that they do not sum to 1. This is because each time sigmoid is applied, it is in a one-vs-all scenario. Meaning to say, the 0.519 for ETT-Abnormal means that ETT-Abnormal is treated as the positive class, and the remaining 10 classes are treated as negative class 0. In other words, with 11 elements and sigmoid, we are essentially performing 11 binary classification on the said 11 classes. So 0.519 actually means that the probability of it being class 1 ( ETT-Abnormal ) is 0.519, and the probability of it being NOT class 1 (ALL other classes) is 0.481. The same logic applies to each of the element in the first row. One thing worth noting is that the predictions for row 1 is not mutually exclusive , meaning that from the 11 classes, we can have say, ETT-Abnormal, NGH-Abnormal, CVC-Abnormal to all have say probability score of 0.9, meaning to say, it is highly likely to be all 3 conditions! This is okay and common in X-Ray imaging. In the table dataframe below, I put them into a dataframe for easy visualization. classes = [ \"ETT - Abnormal\" , \"ETT - Borderline\" , \"ETT - Normal\" , \"NGT - Abnormal\" , \"NGT - Borderline\" , \"NGT - Incompletely Imaged\" , \"NGT - Normal\" , \"CVC - Abnormal\" , \"CVC - Borderline\" , \"CVC - Normal\" , \"Swan Ganz Catheter Present\" , ] import torch import pandas as pd output_logits = torch . tensor ([ [ 0.0762 , 0.0877 , 0.1205 , - 0.0615 , - 0.0054 , 0.0661 , 0.1567 , - 0.0978 , 0.0248 , - 0.0350 , 0.0084 ], [ - 0.0196 , - 0.0729 , 0.0534 , 0.0307 , - 0.0428 , - 0.0016 , 0.0013 , - 0.0247 , - 0.0094 , - 0.0424 , 0.0192 ], [ - 0.0125 , - 0.0310 , 0.0118 , - 0.1301 , 0.0418 , 0.0229 , 0.0139 , - 0.0526 , 0.0870 , - 0.0681 , - 0.0068 ], [ - 0.0259 , - 0.0544 , - 0.0262 , 0.0018 , 0.0161 , - 0.0369 , - 0.0370 , - 0.0157 , 0.0036 , - 0.0592 , 0.0107 ], [ - 0.0366 , - 0.0695 , 0.0740 , - 0.0353 , - 0.0363 , - 0.0019 , 0.0085 , - 0.0144 , 0.0129 , - 0.0470 , 0.0043 ], [ - 0.0445 , - 0.0822 , 0.0487 , - 0.0851 , 0.0269 , - 0.0809 , - 0.0434 , 0.0110 , - 0.0631 , - 0.0733 , - 0.0188 ], [ - 0.0304 , 0.0012 , 0.0233 , - 0.0121 , - 0.0406 , - 0.0459 , - 0.0363 , 0.0089 , - 0.0009 , - 0.0797 , - 0.0017 ], [ - 0.0415 , 0.0787 , 0.0283 , - 0.0617 , - 0.0526 , - 0.0016 , - 0.0409 , - 0.0481 , 0.0583 , - 0.0810 , - 0.0050 ]], dtype = torch . float64 , device = device ) sigmoid = torch . nn . Sigmoid () softmax = torch . nn . Softmax ( dim = 1 ) output_predictions_sigmoid = sigmoid ( output_logits ) output_predictions_softmax = softmax ( output_logits ) print ( output_predictions_sigmoid ) print ( output_predictions_softmax ) tensor([[0.5190, 0.5219, 0.5301, 0.4846, 0.4987, 0.5165, 0.5391, 0.4756, 0.5062, 0.4913, 0.5021], [0.4951, 0.4818, 0.5133, 0.5077, 0.4893, 0.4996, 0.5003, 0.4938, 0.4977, 0.4894, 0.5048], [0.4969, 0.4923, 0.5029, 0.4675, 0.5104, 0.5057, 0.5035, 0.4869, 0.5217, 0.4830, 0.4983], [0.4935, 0.4864, 0.4935, 0.5004, 0.5040, 0.4908, 0.4908, 0.4961, 0.5009, 0.4852, 0.5027], [0.4909, 0.4826, 0.5185, 0.4912, 0.4909, 0.4995, 0.5021, 0.4964, 0.5032, 0.4883, 0.5011], [0.4889, 0.4795, 0.5122, 0.4787, 0.5067, 0.4798, 0.4892, 0.5027, 0.4842, 0.4817, 0.4953], [0.4924, 0.5003, 0.5058, 0.4970, 0.4899, 0.4885, 0.4909, 0.5022, 0.4998, 0.4801, 0.4996], [0.4896, 0.5197, 0.5071, 0.4846, 0.4869, 0.4996, 0.4898, 0.4880, 0.5146, 0.4798, 0.4988]], dtype=torch.float64) tensor([[0.0948, 0.0959, 0.0991, 0.0826, 0.0874, 0.0939, 0.1028, 0.0797, 0.0901, 0.0849, 0.0886], [0.0900, 0.0853, 0.0968, 0.0946, 0.0879, 0.0916, 0.0919, 0.0895, 0.0909, 0.0879, 0.0935], [0.0907, 0.0890, 0.0929, 0.0806, 0.0957, 0.0939, 0.0931, 0.0871, 0.1001, 0.0858, 0.0912], [0.0904, 0.0878, 0.0903, 0.0929, 0.0942, 0.0894, 0.0894, 0.0913, 0.0931, 0.0874, 0.0937], [0.0887, 0.0858, 0.0991, 0.0888, 0.0887, 0.0918, 0.0928, 0.0907, 0.0932, 0.0878, 0.0924], [0.0901, 0.0868, 0.0989, 0.0865, 0.0968, 0.0869, 0.0902, 0.0953, 0.0885, 0.0876, 0.0925], [0.0899, 0.0928, 0.0948, 0.0915, 0.0890, 0.0885, 0.0894, 0.0935, 0.0926, 0.0856, 0.0925], [0.0884, 0.0997, 0.0948, 0.0867, 0.0875, 0.0920, 0.0885, 0.0879, 0.0977, 0.0850, 0.0917]], dtype=torch.float64) df = pd . DataFrame ( data = output_predictions_sigmoid . detach () . cpu () . numpy (), columns = classes ) display ( df ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ETT - Abnormal ETT - Borderline ETT - Normal NGT - Abnormal NGT - Borderline NGT - Incompletely Imaged NGT - Normal CVC - Abnormal CVC - Borderline CVC - Normal Swan Ganz Catheter Present 0 0.519041 0.521911 0.530089 0.484630 0.498650 0.516519 0.539095 0.475569 0.506200 0.491251 0.502100 1 0.495100 0.481783 0.513347 0.507674 0.489302 0.499600 0.500325 0.493825 0.497650 0.489402 0.504800 2 0.496875 0.492251 0.502950 0.467521 0.510448 0.505725 0.503475 0.486853 0.521736 0.482982 0.498300 3 0.493525 0.486403 0.493450 0.500450 0.504025 0.490776 0.490751 0.496075 0.500900 0.485204 0.502675 4 0.490851 0.482632 0.518492 0.491176 0.490926 0.499525 0.502125 0.496400 0.503225 0.488252 0.501075 5 0.488877 0.479462 0.512173 0.478738 0.506725 0.479786 0.489152 0.502750 0.484230 0.481683 0.495300 6 0.492401 0.500300 0.505825 0.496975 0.489851 0.488527 0.490926 0.502225 0.499775 0.480086 0.499575 7 0.489626 0.519665 0.507075 0.484580 0.486853 0.499600 0.489776 0.487977 0.514571 0.479761 0.498750","title":"Model Architecture: Final Activation Layer"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#batch-size-and-tricks","text":"Due to hardware limitation, we can barely fit in anything more than a batch_size of 8. Quoting from here : It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize [...] large-batch methods tend to converge to sharp minimizers of the training and testing functions\u2014and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. The above shows that large batch size may fit the model too well, as the model will learn features of the dataset in less iterations, and may memorize this particular dataset's features, leading to overfitting and poor generalization. However, too small a batch size causes our convergence to go too slow, empirically, we take 32 or 64 as the ideal batch size in this competition. We used both torch.amp and gradient accumulation to be able to fit more batch sizes. We did not freeze the batch_norm layers, which still yielded great results. What we should have done is to experiment more on how to freeze the batch norm layers properly, as I believe that it may help. In the end, we used a batch size of 8 and fit 4 iterations using gradient accumulation and trained a total number of 20 epochs to get a local CV score of roughly 0.969.","title":"Batch Size and Tricks"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#optimizer-scheduler-and-loss","text":"","title":"Optimizer, Scheduler and Loss"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#scheduler","text":"The configuration can be seen here. But note that we incorporated GradualWarmUpScheduler along with CosineAnnealingLR , we also experimented with CosineAnnealingWarmRestarts , the results are similar. From the paper Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour , we learnt about the warmup technique. Although the context of the paper was training under large batch size, we find it helpful even in small batches for the training to converge. The basic algorithm is as follows: Set a base_lr or initial lr for what you want in a model, say 1e-4. If we set our warmup epoch to be 10, then we will start from with 1e-4/10 in the first epoch, and take equal steps each time to converge to 1e-4 in the 10th epoch. After the 10th epoch, warmup ends, we start applying our scheduler's normal steps. However, I took quite some time to understand the idea of gradual warmup, I made my understanding here . We should try OneCyclePolicy as detailed by fastai. num_epochs = 20 CosineAnnealingLR : T_max : num_epochs - 1 eta_min : 1.0e-07 last_epoch : - 1 verbose : true Notice in my configuration above, we set the parameter T_max to be the 19, which is like a one-shot training. import torch import matplotlib.pyplot as plt num_epochs = 20 model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 100 ) cosine_annealing_lr_scheduler_one_shot = torch . optim . lr_scheduler . CosineAnnealingLR ( optimizer , T_max = 19 , eta_min = 1e-7 ) cosine_annealing_lr_one_shot = [] for i in range ( num_epochs ): optimizer . step () cosine_annealing_lr_one_shot . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) cosine_annealing_lr_scheduler_one_shot . step () plt . plot ( cosine_annealing_lr_one_shot ); num_epochs = 20 model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 100 ) cosine_annealing_lr_scheduler_normal = torch . optim . lr_scheduler . CosineAnnealingLR ( optimizer , T_max = 2 , eta_min = 1e-7 ) cosine_annealing_lr_normal = [] for i in range ( num_epochs ): optimizer . step () cosine_annealing_lr_normal . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) cosine_annealing_lr_scheduler_normal . step () plt . plot ( cosine_annealing_lr_normal ) [<matplotlib.lines.Line2D at 0x260cde7e7c0>]","title":"Scheduler"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#loss","text":"We should also experiment with Focal Loss but seeing negative results from fellow Kagglers, on top with limited resources, we did not try it.","title":"Loss"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#ensembling","text":"","title":"Ensembling"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#forward-ensembling","text":"We made use of the Forward Ensembling idea from Chris in SIIM-ISIC Melanoma Classification back in August 2020, I modified the code for this specific task. A simple description is as follows, modified from Chris, with more mathematical notations. We start off with a dataset \\(\\mathcal{D} = X \\times y\\) where it is sampled from the true population \\(\\mathcal{X} \\times \\mathcal{Y}\\) . We apply KFold (5 splits) to the dataset, as illustrated in the diagram. We can now train five different hypothesis \\(h_{F1}, h_{F2},...,h_{F5}\\) , where \\(h_{F1}\\) is trained on Fold 2 to Fold 5 and predict on Fold 1, \\(h_{F2}\\) is trained on Fold 1,3,4,5 and predict on Fold 2. The logic follows for all 5 hypothesis. Notice that in the five models, we are predicting on a unique validation fold, and as a result, after we trained all 5 folds, we will have the predictions made on the whole training set (F1-F5). This predictions is called the Out-of-Fold predictions. We then go a step further and calculate the AUC score with the OOF predictions with the ground truth to get the OOF AUC. We save it to a csv or dataframe called oof_1.csv , subsequent oof trained on different hypothesis space should be named oof_i.csv where \\(i \\in [2,3,...]\\) . After we trained all 5 folds, we will use \\(h_{1}\\) to predict on \\(X_{test}\\) and obtain predictions \\(Y_{\\text{h1 preds}}\\) , we then use \\(h_{2}\\) to predict on \\(X_{test}\\) and obtain predictions \\(Y_{\\text{h2 preds}}\\) , we do this for all five folds and finally \\(Y_{\\text{final preds}} = \\dfrac{1}{5}\\sum_{i=1}^{5}Y_{\\text{hi preds}}\\) . This is a typical pipeline in most machine learning problems. We save this final predictions as sub_1.csv , subsequence predictions trained on different hypothesis space should be named sub_i.csv where \\(i \\in [2,3,...]\\) . Now if we train another model, a completely different hypothesis space is used, to be more pedantic, we denote the previous model to be taken from the hypothesis space \\(\\mathcal{H}_{1}\\) , and now we move on to \\(\\mathcal{H}_{2}\\) . We repeat step 1-6 on this new model (Note that you are essentially training 10 \"models\" now since we are doing KFold twice, and oh, please set the seed of KFold to be the same, it should never be the case that both model comes from different splitting seed for apparent reasons). Here is the key (given the above setup with 2 different models trained on 5 folds): Normally, most people do a simple mean ensemble, that is \\(\\dfrac{Y_{\\text{final preds H1}} + Y_{\\text{final preds H2}}}{2}\\) . This works well most of the time as we trust both model holds equal importance in the final predictions. One issue may be that certain models should be weighted more than the rest, we should not simply take Leaderboard feedback score to judge the weight assignment. A general heuristic here is called Forward Selection. (Extract from Chris) Now say that you build 2 models (that means that you did 5 KFold twice). You now have oof_1.csv, oof_2.csv, sub_1.csv, and sub_2.csv. How do we blend the two models? We find the weight w such that w * oof_1.predictions + (1-w) * oof_2.predictions has the largest AUC. all = [] for w in [ 0.00 , 0.01 , 0.02 , ... , 0.98 , 0.99 , 1.00 ]: ensemble_pred = w * oof_1 . predictions + ( 1 - w ) * oof_2 . predictions ensemble_auc = roc_auc_score ( oof . target , ensemble_pred ) all . append ( ensemble_auc ) best_weight = np . argmax ( all ) / 100. Then we can assign the best weight like: final_ensemble_pred = best_weight * sub_1 . target + ( 1 - best_weight ) * sub_2 . target Coutersy of Chris In this competition, there are two approaches, either maximize the average of the macro AUC score of all the classes, or maximize each column/class separately. It turns out that maximizing the columns separately led to disastrous results (it could be my code and idea is wrong, as ROC is a ranking metric).","title":"Forward Ensembling"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#conclusion","text":"What we could have done better: Use more variety of classifier head like GeM . Use more variety of backbone . Use Neptune.ai to log our experiments as soon things start to get messy. Basically MLOps is important! Experiment on 3-4 stage training. Pseudo Labelling. Knowledge Distillation. Experiment more on maximizing AUC during ensembles. rank_pct etc. See novel three stage training: https://www.kaggle.com/yasufuminakama/ranzcr-resnet200d-3-stage-training-step1","title":"Conclusion"},{"location":"reighns_ml_journey/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/notebooks/walkthrough/#references","text":"Multi-Head Deep Learning Model with Multi-Label Classification AUC Metric on Multi-Label Sigmoid and Softmax for Multi-Label Multi-Label Classification Tutorial Why we should use Multi-Head in Multi-Label Classification Follow Up 1 Follow Up 2 Follow Up 3 Sigmoid is Binary Cross Entropy Attention Blocks in Computer Vision Spatial Attention Blocks Spatial Attention Module Convolutional Block Attention Module [Dive Into Deep Learning - Chapter 10: Attention Mechanisms] Gradual Warmup: Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour","title":"References"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/model_summary/","text":"Model Architectures, Training Parameters No Meta Data Model Architecture For models that did not make use of meta data, we have the following architecture. No Meta Data Model Architecture. Meta Data Model Architecture For models that did made use of meta data, we have the following architecture. Meta Data Model Architecture. We concat the flattened feature maps with the meta features: Meta Features : [ 'sex' , 'age_approx' , 'site_head/neck' , 'site_lower extremity' , 'site_oral/genital' , 'site_palms/soles' , 'site_torso' , 'site_upper extremity' , 'site_nan' ] and the meta features has its own sequential layers as ANN: OrderedDict ( [ ( \"fc1\" , torch . nn . Linear ( self . num_meta_features , 512 ), ), ( \"bn1\" , torch . nn . BatchNorm1d ( 512 ), ), ( \"swish1\" , torch . nn . SiLU (), ), ( \"dropout1\" , torch . nn . Dropout ( p = 0.3 ), ), ( \"fc2\" , torch . nn . Linear ( 512 , 128 ), ), ( \"bn2\" , torch . nn . BatchNorm1d ( 128 ), ), ( \"swish2\" , torch . nn . SiLU (), ), ] ) For example: image shape: \\([32, 3, 256, 256]\\) meta_inputs shape: \\([32, 9]\\) we have 9 features. feature_logits shape: \\([32, 1280]\\) flattened feature maps at the last conv layer. meta_logits shape: \\([32, 128]\\) where we passed in a small sequential ANN for the meta data. concat_logits shape: \\([32, 1280 + 128]\\) if self . use_meta : # from cnn images feature_logits = self . extract_features ( image ) # from meta features meta_logits = self . meta_layer ( meta_inputs ) # concatenate concat_logits = torch . cat (( feature_logits , meta_logits ), dim = 1 ) # classifier head classifier_logits = self . architecture [ \"head\" ]( concat_logits ) Activation Functions As we all know, activation functions are used to transform a neurons' linearity to non-linearity and decide whether to \"fire\" a neuron or not. When we design or choose an activation function, we need to ensure the follows: (Smoothness) Differentiable and Continuous: For example, the sigmoid function is continuous and hence differentiable. If the property is not fulfilled, we might face issues as backpropagation may not be performed properly since we cannot differentiate it.If you notice, the heaviside function is not. We cant perform GD using the HF as we cannot compute gradients but for the logistic function we can. The gradient of sigmoid function g is g(1-g) conveniently Monotonic: This helps the model to converge faster. But spoiler alert, Swish is not monotonic. The properties of Swish are as follows: Bounded below: It is claimed in the paper it serves as a strong regularization. Smoothness: More smooth than ReLU which allows the model to optimize better, the error landscape, when smoothed, is easier to traverse in order to find a minima. An intuitive idea is the hill again, imagine you traverse down your neighbourhood hill, vs traversing down Mount Himalaya. # Import matplotlib, numpy and math import matplotlib.pyplot as plt import numpy as np import math def swish ( x ): sigmoid = 1 / ( 1 + np . exp ( - x )) swish = x * sigmoid return swish epsilon = 1e-20 x = np . linspace ( - 100 , 100 , 100 ) z = swish ( x ) print ( z ) print ( min ( z )) plt . plot ( x , z ) plt . xlabel ( \"x\" ) plt . ylabel ( \"Swish(X)\" ) plt . show () ! pip install torchinfo Collecting torchinfo Downloading torchinfo-1.6.3-py3-none-any.whl (20 kB) Installing collected packages: torchinfo Successfully installed torchinfo-1.6.3 WARNING: You are using pip version 21.3.1; however, version 22.0.3 is available. You should consider upgrading via the 'C:\\Users\\reighns\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command. import timm from dataclasses import asdict , dataclass , field from pathlib import Path from typing import Any , Dict , List , Union import torchinfo import torch # Utility functions. import gc import json import os import random from pathlib import Path , PurePath from typing import Dict , Union , List import numpy as np import torch def seed_all ( seed : int = 1992 ) -> None : \"\"\"Seed all random number generators.\"\"\" print ( f \"Using Seed Number { seed } \" ) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) # set PYTHONHASHSEED env var at fixed value torch . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) torch . cuda . manual_seed ( seed ) # pytorch (both CPU and CUDA) np . random . seed ( seed ) # for numpy pseudo-random generator # set fixed value for python built-in pseudo-random generator random . seed ( seed ) torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = False torch . backends . cudnn . enabled = False seed_all () Using Seed Number 1992 @dataclass class ModelParams : \"\"\"A class to track model parameters. model_name (str): name of the model. pretrained (bool): If True, use pretrained model. input_channels (int): RGB image - 3 channels or Grayscale 1 channel output_dimension (int): Final output neuron. It is the number of classes in classification. Caution: If you use sigmoid layer for Binary, then it is 1. classification_type (str): classification type. \"\"\" model_name : str = \"resnet50d\" # resnet50d resnext50_32x4d \"tf_efficientnet_b0_ns\" # Debug use tf_efficientnet_b0_ns else tf_efficientnet_b4_ns vgg16 pretrained : bool = True input_channels : int = 3 output_dimension : int = 2 classification_type : str = \"multiclass\" use_meta : bool = False def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert to dictionary.\"\"\" return asdict ( self ) MODEL_PARAMS = ModelParams () class CustomNeuralNet ( torch . nn . Module ): def __init__ ( self , model_name : str = MODEL_PARAMS . model_name , out_features : int = MODEL_PARAMS . output_dimension , in_channels : int = MODEL_PARAMS . input_channels , pretrained : bool = MODEL_PARAMS . pretrained , use_meta : bool = MODEL_PARAMS . use_meta , ): \"\"\"Construct a new model. Args: model_name ([type], str): The name of the model to use. Defaults to MODEL_PARAMS.model_name. out_features ([type], int): The number of output features, this is usually the number of classes, but if you use sigmoid, then the output is 1. Defaults to MODEL_PARAMS.output_dimension. in_channels ([type], int): The number of input channels; RGB = 3, Grayscale = 1. Defaults to MODEL_PARAMS.input_channels. pretrained ([type], bool): If True, use pretrained model. Defaults to MODEL_PARAMS.pretrained. \"\"\" super () . __init__ () self . in_channels = in_channels self . pretrained = pretrained self . use_meta = use_meta self . backbone = timm . create_model ( model_name , pretrained = self . pretrained , in_chans = self . in_channels ) # removes head from backbone: # TODO: Global pool = \"avg\" vs \"\" behaves differently in shape, caution! self . backbone . reset_classifier ( num_classes = 0 , global_pool = \"avg\" ) # get the last layer's number of features in backbone (feature map) self . in_features = self . backbone . num_features self . out_features = out_features # Custom Head self . single_head_fc = torch . nn . Sequential ( torch . nn . Linear ( self . in_features , self . out_features ), ) self . architecture : Dict [ str , Callable ] = { \"backbone\" : self . backbone , \"bottleneck\" : None , \"head\" : self . single_head_fc , } def extract_features ( self , image : torch . FloatTensor ) -> torch . FloatTensor : \"\"\"Extract the features mapping logits from the model. This is the output from the backbone of a CNN. Args: image (torch.FloatTensor): The input image. Returns: feature_logits (torch.FloatTensor): The features logits. \"\"\" # TODO: To rename feature_logits to image embeddings, also find out what is image embedding. feature_logits = self . architecture [ \"backbone\" ]( image ) print ( f \"feature logits shape = { feature_logits . shape } \" ) return feature_logits def forward ( self , image : torch . FloatTensor ) -> torch . FloatTensor : \"\"\"The forward call of the model. Args: image (torch.FloatTensor): The input image. Returns: classifier_logits (torch.FloatTensor): The output logits of the classifier head. \"\"\" feature_logits = self . extract_features ( image ) classifier_logits = self . architecture [ \"head\" ]( feature_logits ) print ( f \"classifier_logits shape = { classifier_logits . shape } \" ) return classifier_logits model = CustomNeuralNet () batch_size , channel , height , width = 8 , 3 , 256 , 256 X = torch . randn (( batch_size , channel , height , width )) y = model ( image = X ) feature logits shape = torch.Size([8, 2048]) classifier_logits shape = torch.Size([8, 2]) _ = torchinfo . summary ( model , ( batch_size , channel , height , width ), col_names = [ \"input_size\" , \"output_size\" , \"num_params\" , \"kernel_size\" , \"mult_adds\" , ], depth = 3 , verbose = 1 ) torch.Size([8, 2048]) torch.Size([8, 2]) ========================================================================================================================================================================== Layer (type:depth-idx) Input Shape Output Shape Param # Kernel Shape Mult-Adds ========================================================================================================================================================================== CustomNeuralNet -- -- -- -- -- \u251c\u2500ResNet: 1-1 [8, 3, 256, 256] [8, 2048] -- -- -- \u2502 \u2514\u2500Sequential: 2-1 [8, 3, 256, 256] [8, 64, 128, 128] -- -- -- \u2502 \u2502 \u2514\u2500Conv2d: 3-1 [8, 3, 256, 256] [8, 32, 128, 128] 864 [3, 32, 3, 3] 113,246,208 \u2502 \u2502 \u2514\u2500BatchNorm2d: 3-2 [8, 32, 128, 128] [8, 32, 128, 128] 64 [32] 512 \u2502 \u2502 \u2514\u2500ReLU: 3-3 [8, 32, 128, 128] [8, 32, 128, 128] -- -- -- \u2502 \u2502 \u2514\u2500Conv2d: 3-4 [8, 32, 128, 128] [8, 32, 128, 128] 9,216 [32, 32, 3, 3] 1,207,959,552 \u2502 \u2502 \u2514\u2500BatchNorm2d: 3-5 [8, 32, 128, 128] [8, 32, 128, 128] 64 [32] 512 \u2502 \u2502 \u2514\u2500ReLU: 3-6 [8, 32, 128, 128] [8, 32, 128, 128] -- -- -- \u2502 \u2502 \u2514\u2500Conv2d: 3-7 [8, 32, 128, 128] [8, 64, 128, 128] 18,432 [32, 64, 3, 3] 2,415,919,104 \u2502 \u2514\u2500BatchNorm2d: 2-2 [8, 64, 128, 128] [8, 64, 128, 128] 128 [64] 1,024 \u2502 \u2514\u2500ReLU: 2-3 [8, 64, 128, 128] [8, 64, 128, 128] -- -- -- \u2502 \u2514\u2500MaxPool2d: 2-4 [8, 64, 128, 128] [8, 64, 64, 64] -- -- -- \u2502 \u2514\u2500Sequential: 2-5 [8, 64, 64, 64] [8, 256, 64, 64] -- -- -- \u2502 \u2502 \u2514\u2500Bottleneck: 3-8 [8, 64, 64, 64] [8, 256, 64, 64] 75,008 -- 2,415,929,344 \u2502 \u2502 \u2514\u2500Bottleneck: 3-9 [8, 256, 64, 64] [8, 256, 64, 64] 70,400 -- 2,281,707,520 \u2502 \u2502 \u2514\u2500Bottleneck: 3-10 [8, 256, 64, 64] [8, 256, 64, 64] 70,400 -- 2,281,707,520 \u2502 \u2514\u2500Sequential: 2-6 [8, 256, 64, 64] [8, 512, 32, 32] -- -- -- \u2502 \u2502 \u2514\u2500Bottleneck: 3-11 [8, 256, 64, 64] [8, 512, 32, 32] 379,392 -- 3,892,334,592 \u2502 \u2502 \u2514\u2500Bottleneck: 3-12 [8, 512, 32, 32] [8, 512, 32, 32] 280,064 -- 2,281,713,664 \u2502 \u2502 \u2514\u2500Bottleneck: 3-13 [8, 512, 32, 32] [8, 512, 32, 32] 280,064 -- 2,281,713,664 \u2502 \u2502 \u2514\u2500Bottleneck: 3-14 [8, 512, 32, 32] [8, 512, 32, 32] 280,064 -- 2,281,713,664 \u2502 \u2514\u2500Sequential: 2-7 [8, 512, 32, 32] [8, 1024, 16, 16] -- -- -- \u2502 \u2502 \u2514\u2500Bottleneck: 3-15 [8, 512, 32, 32] [8, 1024, 16, 16] 1,512,448 -- 3,892,355,072 \u2502 \u2502 \u2514\u2500Bottleneck: 3-16 [8, 1024, 16, 16] [8, 1024, 16, 16] 1,117,184 -- 2,281,725,952 \u2502 \u2502 \u2514\u2500Bottleneck: 3-17 [8, 1024, 16, 16] [8, 1024, 16, 16] 1,117,184 -- 2,281,725,952 \u2502 \u2502 \u2514\u2500Bottleneck: 3-18 [8, 1024, 16, 16] [8, 1024, 16, 16] 1,117,184 -- 2,281,725,952 \u2502 \u2502 \u2514\u2500Bottleneck: 3-19 [8, 1024, 16, 16] [8, 1024, 16, 16] 1,117,184 -- 2,281,725,952 \u2502 \u2502 \u2514\u2500Bottleneck: 3-20 [8, 1024, 16, 16] [8, 1024, 16, 16] 1,117,184 -- 2,281,725,952 \u2502 \u2514\u2500Sequential: 2-8 [8, 1024, 16, 16] [8, 2048, 8, 8] -- -- -- \u2502 \u2502 \u2514\u2500Bottleneck: 3-21 [8, 1024, 16, 16] [8, 2048, 8, 8] 6,039,552 -- 3,892,396,032 \u2502 \u2502 \u2514\u2500Bottleneck: 3-22 [8, 2048, 8, 8] [8, 2048, 8, 8] 4,462,592 -- 2,281,750,528 \u2502 \u2502 \u2514\u2500Bottleneck: 3-23 [8, 2048, 8, 8] [8, 2048, 8, 8] 4,462,592 -- 2,281,750,528 \u2502 \u2514\u2500SelectAdaptivePool2d: 2-9 [8, 2048, 8, 8] [8, 2048] -- -- -- \u2502 \u2502 \u2514\u2500AdaptiveAvgPool2d: 3-24 [8, 2048, 8, 8] [8, 2048, 1, 1] -- -- -- \u2502 \u2502 \u2514\u2500Flatten: 3-25 [8, 2048, 1, 1] [8, 2048] -- -- -- \u2502 \u2514\u2500Identity: 2-10 [8, 2048] [8, 2048] -- -- -- \u251c\u2500Sequential: 1-2 [8, 2048] [8, 2] -- -- -- \u2502 \u2514\u2500Linear: 2-11 [8, 2048] [8, 2] 4,098 [2048, 2] 32,784 ========================================================================================================================================================================== Total params: 23,531,362 Trainable params: 23,531,362 Non-trainable params: 0 Total mult-adds (G): 45.21 ========================================================================================================================================================================== Input size (MB): 6.29 Forward/backward pass size (MB): 1992.29 Params size (MB): 94.13 Estimated Total Size (MB): 2092.71 ========================================================================================================================================================================== This model architechure means that if I pass in a batch of \\(8\\) images of size \\((3, 256, 256)\\) , the model statistics will tell us a lot of information. Let us give some examples with a naive ResNet50d . Input Shape: \\([8, 3, 256, 256]\\) passing through the first Sequential Layer's Conv2d (3-1) with kernel size of Kernel Shape: \\([3, 32, 3, 3]\\) which means \\([\\textbf{in_channels, out_channels, kernel_size, kernel_size}]\\) will yield an output shape of Output Shape: \\([8, 32, 128, 128]\\) indicating that the each input images are now transformed into 32 kernels of size 256 by 256. Params: The Params column calculates the number of parameters in this layer at 864 learnable parameters. Once we know how to interpret the table, we can also see that our CustomNeuralnet() has extract_features which outputs the input at the last convolutional layer, in this example, it is at SelectAdaptivePool2d: 2-9 where it first went through AdaptiveAvgPool2d: 3-24 to squash the feature maps to \\([8, 2048, 1, 1]\\) and subsequently a Flatten: 3-25 layer to flatten out the last 2 dimensions to become \\([8, 2048]\\) so we can pass on to the dense layers. We can verify this by X = torch . randn (( batch_size , channel , height , width )) y = model ( image = X ) yielding feature logits shape = torch . Size ([ 8 , 2048 ]) classifier_logits shape = torch . Size ([ 8 , 2 ]) where the latter is the final shape of the input after passing through all the dense layers at \\([8, 2]\\) , where one can envision it as 2 output neurons.","title":"Model summary"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/model_summary/#model-architectures-training-parameters","text":"","title":"Model Architectures, Training Parameters"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/model_summary/#no-meta-data-model-architecture","text":"For models that did not make use of meta data, we have the following architecture. No Meta Data Model Architecture.","title":"No Meta Data Model Architecture"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/model_summary/#meta-data-model-architecture","text":"For models that did made use of meta data, we have the following architecture. Meta Data Model Architecture. We concat the flattened feature maps with the meta features: Meta Features : [ 'sex' , 'age_approx' , 'site_head/neck' , 'site_lower extremity' , 'site_oral/genital' , 'site_palms/soles' , 'site_torso' , 'site_upper extremity' , 'site_nan' ] and the meta features has its own sequential layers as ANN: OrderedDict ( [ ( \"fc1\" , torch . nn . Linear ( self . num_meta_features , 512 ), ), ( \"bn1\" , torch . nn . BatchNorm1d ( 512 ), ), ( \"swish1\" , torch . nn . SiLU (), ), ( \"dropout1\" , torch . nn . Dropout ( p = 0.3 ), ), ( \"fc2\" , torch . nn . Linear ( 512 , 128 ), ), ( \"bn2\" , torch . nn . BatchNorm1d ( 128 ), ), ( \"swish2\" , torch . nn . SiLU (), ), ] ) For example: image shape: \\([32, 3, 256, 256]\\) meta_inputs shape: \\([32, 9]\\) we have 9 features. feature_logits shape: \\([32, 1280]\\) flattened feature maps at the last conv layer. meta_logits shape: \\([32, 128]\\) where we passed in a small sequential ANN for the meta data. concat_logits shape: \\([32, 1280 + 128]\\) if self . use_meta : # from cnn images feature_logits = self . extract_features ( image ) # from meta features meta_logits = self . meta_layer ( meta_inputs ) # concatenate concat_logits = torch . cat (( feature_logits , meta_logits ), dim = 1 ) # classifier head classifier_logits = self . architecture [ \"head\" ]( concat_logits )","title":"Meta Data Model Architecture"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/model_summary/#activation-functions","text":"As we all know, activation functions are used to transform a neurons' linearity to non-linearity and decide whether to \"fire\" a neuron or not. When we design or choose an activation function, we need to ensure the follows: (Smoothness) Differentiable and Continuous: For example, the sigmoid function is continuous and hence differentiable. If the property is not fulfilled, we might face issues as backpropagation may not be performed properly since we cannot differentiate it.If you notice, the heaviside function is not. We cant perform GD using the HF as we cannot compute gradients but for the logistic function we can. The gradient of sigmoid function g is g(1-g) conveniently Monotonic: This helps the model to converge faster. But spoiler alert, Swish is not monotonic. The properties of Swish are as follows: Bounded below: It is claimed in the paper it serves as a strong regularization. Smoothness: More smooth than ReLU which allows the model to optimize better, the error landscape, when smoothed, is easier to traverse in order to find a minima. An intuitive idea is the hill again, imagine you traverse down your neighbourhood hill, vs traversing down Mount Himalaya. # Import matplotlib, numpy and math import matplotlib.pyplot as plt import numpy as np import math def swish ( x ): sigmoid = 1 / ( 1 + np . exp ( - x )) swish = x * sigmoid return swish epsilon = 1e-20 x = np . linspace ( - 100 , 100 , 100 ) z = swish ( x ) print ( z ) print ( min ( z )) plt . plot ( x , z ) plt . xlabel ( \"x\" ) plt . ylabel ( \"Swish(X)\" ) plt . show () ! pip install torchinfo Collecting torchinfo Downloading torchinfo-1.6.3-py3-none-any.whl (20 kB) Installing collected packages: torchinfo Successfully installed torchinfo-1.6.3 WARNING: You are using pip version 21.3.1; however, version 22.0.3 is available. You should consider upgrading via the 'C:\\Users\\reighns\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command. import timm from dataclasses import asdict , dataclass , field from pathlib import Path from typing import Any , Dict , List , Union import torchinfo import torch # Utility functions. import gc import json import os import random from pathlib import Path , PurePath from typing import Dict , Union , List import numpy as np import torch def seed_all ( seed : int = 1992 ) -> None : \"\"\"Seed all random number generators.\"\"\" print ( f \"Using Seed Number { seed } \" ) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) # set PYTHONHASHSEED env var at fixed value torch . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) torch . cuda . manual_seed ( seed ) # pytorch (both CPU and CUDA) np . random . seed ( seed ) # for numpy pseudo-random generator # set fixed value for python built-in pseudo-random generator random . seed ( seed ) torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = False torch . backends . cudnn . enabled = False seed_all () Using Seed Number 1992 @dataclass class ModelParams : \"\"\"A class to track model parameters. model_name (str): name of the model. pretrained (bool): If True, use pretrained model. input_channels (int): RGB image - 3 channels or Grayscale 1 channel output_dimension (int): Final output neuron. It is the number of classes in classification. Caution: If you use sigmoid layer for Binary, then it is 1. classification_type (str): classification type. \"\"\" model_name : str = \"resnet50d\" # resnet50d resnext50_32x4d \"tf_efficientnet_b0_ns\" # Debug use tf_efficientnet_b0_ns else tf_efficientnet_b4_ns vgg16 pretrained : bool = True input_channels : int = 3 output_dimension : int = 2 classification_type : str = \"multiclass\" use_meta : bool = False def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert to dictionary.\"\"\" return asdict ( self ) MODEL_PARAMS = ModelParams () class CustomNeuralNet ( torch . nn . Module ): def __init__ ( self , model_name : str = MODEL_PARAMS . model_name , out_features : int = MODEL_PARAMS . output_dimension , in_channels : int = MODEL_PARAMS . input_channels , pretrained : bool = MODEL_PARAMS . pretrained , use_meta : bool = MODEL_PARAMS . use_meta , ): \"\"\"Construct a new model. Args: model_name ([type], str): The name of the model to use. Defaults to MODEL_PARAMS.model_name. out_features ([type], int): The number of output features, this is usually the number of classes, but if you use sigmoid, then the output is 1. Defaults to MODEL_PARAMS.output_dimension. in_channels ([type], int): The number of input channels; RGB = 3, Grayscale = 1. Defaults to MODEL_PARAMS.input_channels. pretrained ([type], bool): If True, use pretrained model. Defaults to MODEL_PARAMS.pretrained. \"\"\" super () . __init__ () self . in_channels = in_channels self . pretrained = pretrained self . use_meta = use_meta self . backbone = timm . create_model ( model_name , pretrained = self . pretrained , in_chans = self . in_channels ) # removes head from backbone: # TODO: Global pool = \"avg\" vs \"\" behaves differently in shape, caution! self . backbone . reset_classifier ( num_classes = 0 , global_pool = \"avg\" ) # get the last layer's number of features in backbone (feature map) self . in_features = self . backbone . num_features self . out_features = out_features # Custom Head self . single_head_fc = torch . nn . Sequential ( torch . nn . Linear ( self . in_features , self . out_features ), ) self . architecture : Dict [ str , Callable ] = { \"backbone\" : self . backbone , \"bottleneck\" : None , \"head\" : self . single_head_fc , } def extract_features ( self , image : torch . FloatTensor ) -> torch . FloatTensor : \"\"\"Extract the features mapping logits from the model. This is the output from the backbone of a CNN. Args: image (torch.FloatTensor): The input image. Returns: feature_logits (torch.FloatTensor): The features logits. \"\"\" # TODO: To rename feature_logits to image embeddings, also find out what is image embedding. feature_logits = self . architecture [ \"backbone\" ]( image ) print ( f \"feature logits shape = { feature_logits . shape } \" ) return feature_logits def forward ( self , image : torch . FloatTensor ) -> torch . FloatTensor : \"\"\"The forward call of the model. Args: image (torch.FloatTensor): The input image. Returns: classifier_logits (torch.FloatTensor): The output logits of the classifier head. \"\"\" feature_logits = self . extract_features ( image ) classifier_logits = self . architecture [ \"head\" ]( feature_logits ) print ( f \"classifier_logits shape = { classifier_logits . shape } \" ) return classifier_logits model = CustomNeuralNet () batch_size , channel , height , width = 8 , 3 , 256 , 256 X = torch . randn (( batch_size , channel , height , width )) y = model ( image = X ) feature logits shape = torch.Size([8, 2048]) classifier_logits shape = torch.Size([8, 2]) _ = torchinfo . summary ( model , ( batch_size , channel , height , width ), col_names = [ \"input_size\" , \"output_size\" , \"num_params\" , \"kernel_size\" , \"mult_adds\" , ], depth = 3 , verbose = 1 ) torch.Size([8, 2048]) torch.Size([8, 2]) ========================================================================================================================================================================== Layer (type:depth-idx) Input Shape Output Shape Param # Kernel Shape Mult-Adds ========================================================================================================================================================================== CustomNeuralNet -- -- -- -- -- \u251c\u2500ResNet: 1-1 [8, 3, 256, 256] [8, 2048] -- -- -- \u2502 \u2514\u2500Sequential: 2-1 [8, 3, 256, 256] [8, 64, 128, 128] -- -- -- \u2502 \u2502 \u2514\u2500Conv2d: 3-1 [8, 3, 256, 256] [8, 32, 128, 128] 864 [3, 32, 3, 3] 113,246,208 \u2502 \u2502 \u2514\u2500BatchNorm2d: 3-2 [8, 32, 128, 128] [8, 32, 128, 128] 64 [32] 512 \u2502 \u2502 \u2514\u2500ReLU: 3-3 [8, 32, 128, 128] [8, 32, 128, 128] -- -- -- \u2502 \u2502 \u2514\u2500Conv2d: 3-4 [8, 32, 128, 128] [8, 32, 128, 128] 9,216 [32, 32, 3, 3] 1,207,959,552 \u2502 \u2502 \u2514\u2500BatchNorm2d: 3-5 [8, 32, 128, 128] [8, 32, 128, 128] 64 [32] 512 \u2502 \u2502 \u2514\u2500ReLU: 3-6 [8, 32, 128, 128] [8, 32, 128, 128] -- -- -- \u2502 \u2502 \u2514\u2500Conv2d: 3-7 [8, 32, 128, 128] [8, 64, 128, 128] 18,432 [32, 64, 3, 3] 2,415,919,104 \u2502 \u2514\u2500BatchNorm2d: 2-2 [8, 64, 128, 128] [8, 64, 128, 128] 128 [64] 1,024 \u2502 \u2514\u2500ReLU: 2-3 [8, 64, 128, 128] [8, 64, 128, 128] -- -- -- \u2502 \u2514\u2500MaxPool2d: 2-4 [8, 64, 128, 128] [8, 64, 64, 64] -- -- -- \u2502 \u2514\u2500Sequential: 2-5 [8, 64, 64, 64] [8, 256, 64, 64] -- -- -- \u2502 \u2502 \u2514\u2500Bottleneck: 3-8 [8, 64, 64, 64] [8, 256, 64, 64] 75,008 -- 2,415,929,344 \u2502 \u2502 \u2514\u2500Bottleneck: 3-9 [8, 256, 64, 64] [8, 256, 64, 64] 70,400 -- 2,281,707,520 \u2502 \u2502 \u2514\u2500Bottleneck: 3-10 [8, 256, 64, 64] [8, 256, 64, 64] 70,400 -- 2,281,707,520 \u2502 \u2514\u2500Sequential: 2-6 [8, 256, 64, 64] [8, 512, 32, 32] -- -- -- \u2502 \u2502 \u2514\u2500Bottleneck: 3-11 [8, 256, 64, 64] [8, 512, 32, 32] 379,392 -- 3,892,334,592 \u2502 \u2502 \u2514\u2500Bottleneck: 3-12 [8, 512, 32, 32] [8, 512, 32, 32] 280,064 -- 2,281,713,664 \u2502 \u2502 \u2514\u2500Bottleneck: 3-13 [8, 512, 32, 32] [8, 512, 32, 32] 280,064 -- 2,281,713,664 \u2502 \u2502 \u2514\u2500Bottleneck: 3-14 [8, 512, 32, 32] [8, 512, 32, 32] 280,064 -- 2,281,713,664 \u2502 \u2514\u2500Sequential: 2-7 [8, 512, 32, 32] [8, 1024, 16, 16] -- -- -- \u2502 \u2502 \u2514\u2500Bottleneck: 3-15 [8, 512, 32, 32] [8, 1024, 16, 16] 1,512,448 -- 3,892,355,072 \u2502 \u2502 \u2514\u2500Bottleneck: 3-16 [8, 1024, 16, 16] [8, 1024, 16, 16] 1,117,184 -- 2,281,725,952 \u2502 \u2502 \u2514\u2500Bottleneck: 3-17 [8, 1024, 16, 16] [8, 1024, 16, 16] 1,117,184 -- 2,281,725,952 \u2502 \u2502 \u2514\u2500Bottleneck: 3-18 [8, 1024, 16, 16] [8, 1024, 16, 16] 1,117,184 -- 2,281,725,952 \u2502 \u2502 \u2514\u2500Bottleneck: 3-19 [8, 1024, 16, 16] [8, 1024, 16, 16] 1,117,184 -- 2,281,725,952 \u2502 \u2502 \u2514\u2500Bottleneck: 3-20 [8, 1024, 16, 16] [8, 1024, 16, 16] 1,117,184 -- 2,281,725,952 \u2502 \u2514\u2500Sequential: 2-8 [8, 1024, 16, 16] [8, 2048, 8, 8] -- -- -- \u2502 \u2502 \u2514\u2500Bottleneck: 3-21 [8, 1024, 16, 16] [8, 2048, 8, 8] 6,039,552 -- 3,892,396,032 \u2502 \u2502 \u2514\u2500Bottleneck: 3-22 [8, 2048, 8, 8] [8, 2048, 8, 8] 4,462,592 -- 2,281,750,528 \u2502 \u2502 \u2514\u2500Bottleneck: 3-23 [8, 2048, 8, 8] [8, 2048, 8, 8] 4,462,592 -- 2,281,750,528 \u2502 \u2514\u2500SelectAdaptivePool2d: 2-9 [8, 2048, 8, 8] [8, 2048] -- -- -- \u2502 \u2502 \u2514\u2500AdaptiveAvgPool2d: 3-24 [8, 2048, 8, 8] [8, 2048, 1, 1] -- -- -- \u2502 \u2502 \u2514\u2500Flatten: 3-25 [8, 2048, 1, 1] [8, 2048] -- -- -- \u2502 \u2514\u2500Identity: 2-10 [8, 2048] [8, 2048] -- -- -- \u251c\u2500Sequential: 1-2 [8, 2048] [8, 2] -- -- -- \u2502 \u2514\u2500Linear: 2-11 [8, 2048] [8, 2] 4,098 [2048, 2] 32,784 ========================================================================================================================================================================== Total params: 23,531,362 Trainable params: 23,531,362 Non-trainable params: 0 Total mult-adds (G): 45.21 ========================================================================================================================================================================== Input size (MB): 6.29 Forward/backward pass size (MB): 1992.29 Params size (MB): 94.13 Estimated Total Size (MB): 2092.71 ========================================================================================================================================================================== This model architechure means that if I pass in a batch of \\(8\\) images of size \\((3, 256, 256)\\) , the model statistics will tell us a lot of information. Let us give some examples with a naive ResNet50d . Input Shape: \\([8, 3, 256, 256]\\) passing through the first Sequential Layer's Conv2d (3-1) with kernel size of Kernel Shape: \\([3, 32, 3, 3]\\) which means \\([\\textbf{in_channels, out_channels, kernel_size, kernel_size}]\\) will yield an output shape of Output Shape: \\([8, 32, 128, 128]\\) indicating that the each input images are now transformed into 32 kernels of size 256 by 256. Params: The Params column calculates the number of parameters in this layer at 864 learnable parameters. Once we know how to interpret the table, we can also see that our CustomNeuralnet() has extract_features which outputs the input at the last convolutional layer, in this example, it is at SelectAdaptivePool2d: 2-9 where it first went through AdaptiveAvgPool2d: 3-24 to squash the feature maps to \\([8, 2048, 1, 1]\\) and subsequently a Flatten: 3-25 layer to flatten out the last 2 dimensions to become \\([8, 2048]\\) so we can pass on to the dense layers. We can verify this by X = torch . randn (( batch_size , channel , height , width )) y = model ( image = X ) yielding feature logits shape = torch . Size ([ 8 , 2048 ]) classifier_logits shape = torch . Size ([ 8 , 2 ]) where the latter is the final shape of the input after passing through all the dense layers at \\([8, 2]\\) , where one can envision it as 2 output neurons.","title":"Activation Functions"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/","text":"SIIM-ISIC Melanoma Classification This competition is hosted on Kaggle and the description and overview is stated below . Skin cancer is the most prevalent type of cancer. Melanoma, specifically, is responsible for 75% of skin cancer deaths, despite being the least common skin cancer. The American Cancer Society estimates over 100,000 new melanoma cases will be diagnosed in 2020. It's also expected that almost 7,000 people will die from the disease. As with other cancers, early and accurate detection\u2014potentially aided by data science\u2014can make treatment more effective. Currently, dermatologists evaluate every one of a patient's moles to identify outlier lesions or \u201cugly ducklings\u201d that are most likely to be melanoma. Existing AI approaches have not adequately considered this clinical frame of reference. Dermatologists could enhance their diagnostic accuracy if detection algorithms take into account \u201ccontextual\u201d images within the same patient to determine which images represent a melanoma. If successful, classifiers would be more accurate and could better support dermatological clinic work. As the leading healthcare organization for informatics in medical imaging, the Society for Imaging Informatics in Medicine (SIIM)'s mission is to advance medical imaging informatics through education, research, and innovation in a multi-disciplinary community. SIIM is joined by the International Skin Imaging Collaboration (ISIC), an international effort to improve melanoma diagnosis. The ISIC Archive contains the largest publicly available collection of quality-controlled dermoscopic images of skin lesions. In this competition, you\u2019ll identify melanoma in images of skin lesions. In particular, you\u2019ll use images within the same patient and determine which are likely to represent a melanoma. Using patient-level contextual information may help the development of image analysis tools, which could better support clinical dermatologists. Melanoma is a deadly disease, but if caught early, most melanomas can be cured with minor surgery. Image analysis tools that automate the diagnosis of melanoma will improve dermatologists' diagnostic accuracy. Better detection of melanoma has the opportunity to positively impact millions of people. Establish Metrics After understanding the problem better, we should probably define a metric to optimize. As usual, this step should be closely tied to business problem. We were already given a metric score by the competition host and let us understand it better. Recall that we wish to have a well-calibrated model, the intuition is that a high performance model may not output meaningful probabilities, even if they can have extremely good performance score. Consider a model that outputs logits of \\(0.51\\) when y_true is 1 and \\(0.49\\) otherwise, then a decision threshold of \\(0.5\\) guarantees an accuracy of \\(100\\%\\) , we have no complaints here if we have no issue with our threshold if our only goal is to have a high scoring model. However, if in medical case, where doctor wants to understand \"probablistically\" the survival of a patient, then we might want to turn into logits probs. But apparently the example here holds almost no meaning, when compared to a \"well calibrated model\", more concretely. y_true = [ 0 , 0 , 1 , 1 ] y_prob_uncalibrated = [ 0.49 , 0.49 , 0.51 , 0.51 ] y_prob_calibrated = [ 0.1 , 0.45 , 0.99 , 0.6 ] both models give \\(100\\%\\) accuracy, but the latter (assuming calibrated), can give us a laymen idea that ok this patient has 0.99 chance and the other patient 0.6 chance of surviving etc. Benefit Structure One can introduce a benefit structure with relevant cost-benefit assignment. TP: + 100 FN: -1000 FP: -10 TP+FP: -1 (screening for example) With each TP, we net a profit of 100, and with each FN, we lose -1000, FP loses -10 and whenever the patient get predicted to die (1), send for further screening -1. So towards the end, we can have: \\[ cost = 100*TP - 1000 * FN - 10 * FP - 1 * (TP+FP) \\] This structure helps us decide which metrics to choose. ROC Definition: The basic (non-probablistic intepretation) of ROC is graph that plots the True Positive Rate on the y-axis and False Positive Rate on the x-axis parametrized by a threshold vector t . We then look at the area under the ROC curve (AUROC) to get an overall performance measure. Note that TPR is recall . TPR (recall) = TP / (TP + FN) FPR = FP / (FP + TN) Threshold invariant The ROC looks at the performance of a model hypothesis at all thresholds. This is better than just optimizing recall which only looks at a fixed threshold. Scale Invariant Not necessarily a good thing in this context, as this makes ROC a semi-proper scoring metric, that is, it takes in non-calibrated scores and perform well. The below code shows that as long as the order is preserved, y2 and y4 make zero difference in the outcome. In this case, the doctor may not be able to have a \u201cconfidence\u201d level of how likely the patient is going to survive. y1 = [ 1 , 0 , 1 , 0 ] y2 = [ 0.52 , 0.51 , 0.52 , 0.51 ] y3 = [ 52 , 51 , 52 , 51 ] y4 = [ 0.99 , 0.51 , 0.98 , 0.51 ] uncalibrated_roc = roc ( y1 , y2 ) == roc ( y1 , y3 ) == roc ( y1 , y4 ) print ( f \" { uncalibrated_roc } \" ) -> 1.0 This brings us to the next point. More info in notebook. Brier Score Loss Brier Score computes the squared difference between the probability of a prediction and its actual outcome. Intuitively, this score punishes \u201cunconfident and neutral\u201d probability logits. If a model consistently spits out probability that is near 0.5, then this score will be large. Proper scoring Tells us if the scores output are well calibrated. If not well calibrated, prompt us to either use a different model that calibrated well, or to perform calibration on the model itself. Logistic regression produces natural well calibrated probabilities since it optimizes the log-loss (ce loss), in fact, I think MLE models should always produce well calibrated probabilities since behind the scene it is minimizing KL divergence between ground truth distribution P and estimated distribution Q. It follows that models like DT do not produce well calibrated probabilities. More info in notebook. What can we explore? Did not provide insight if Precision-recall curve and if it may be well posed for this problem than ROC since there is some class imbalance. Did not go into details on calibration methods, in fact, models like RF are not well calibrated by construction. https://scikit-learn.org/stable/modules/calibration.html Validation and Resampling Strategy How should we split out data into folds? We should examine the data for a few factors: Is the data \\(\\mathcal{X}\\) imbalanced? Is the data \\(\\mathcal{X}\\) generated in a i.i.d. manner, more specifically, if I split \\(\\mathcal{X}\\) to \\(\\mathcal{X}_{train}\\) and \\(\\mathcal{X}_{val}\\) , can we ensure that \\(\\mathcal{X}_{val}\\) has no dependency on \\(\\mathcal{X}_{train}\\) ? We came to the conclusion: Yes, the data is severely imbalanced in which there are only around \\(2\\%\\) of positive (malignant) samples. Therefore, a stratified cross validation is reasonable. StratifiedKFold ensures that relative class frequencies is approximately preserved in each train and validation fold. More concretely, we will not experience the scenario where \\(X_{train}\\) has \\(m^{+}\\) and \\(m^{-}\\) positive and negative samples, but \\(X_{val}\\) has only \\(p^{+}\\) positive samples only and 0 negative samples, simply due to the scarcity of negative samples. In medical imaging, it is a well known fact that most of the data contains patient level repeatedly. To put it bluntly, if I have 100 samples, and according to PatientID , we see that the id 123456 (John Doe) appeared 20 times, this is normal as a patient can undergo multiple settings of say, X-rays. If we allow John Doe's data to appear in both train and validation set, then this poses a problem of information leakage, in which the data is no longer i.i.d. . One can think of each patient has an \"unique, underlying features\" which are highly correlated across their different samples. As a result, it is paramount to ensure that amongst this 3255 unique patients, we need to ensure that each unique patients' images DO NOT appear in the validation fold. That is to say, if patient John Doe has 100 X-ray images, but during our 5-fold splits, he has 70 images in Fold 1-4, while 30 images are in Fold 5, then if we were to train on Fold 1-4 and validate on Fold 5, there may be potential leakage and the model will predict with confidence for John Doe's images. This is under the assumption that John Doe's data does not fulfill the i.i.d proces With the above consideration, we will use StratifiedGroupKFold where \\(K = 5\\) splits. There wasn't this splitting function in scikit-learn at the time of competition and as a result, we used a custom written (by someone else) RepeatedStratifiedGroupKFold function and just set n_splits = 1 to get StratifiedGroupKFold (yes we cannot afford to repeated sample, so setting the split to be 1 will collapse the repeated function to just the normal stratified group kfold). However, as of 2022, this function is readily available in the Scikit-Learn library. To recap, we applied stratified logic such that each train and validation set has an equal weightage of positive and negative samples. We also grouped the patients in the process such that patient \\(i\\) will not appear in both training and validation set. It is worth mentioning the famous Kaggler Chris Deotte went one step further to Triple Stratify the data where he balanced patient count distribution. One can read more here . Cross-Validation Workflow To recap, we have the following: Training Set ( \\(X_{\\text{train}}\\) ) : This will be further split into K validation sets during our cross-validation. This set is used to fit a particular hypothesis \\(h \\in \\mathcal{H}\\) . Validation Set ( \\(X_{\\text{val}}\\) ) : This is split from our \\(X_{\\text{train}}\\) during cross-validation. This set is used for model selection (i.e. find best hyperparameters, attempt to produce a best hypothesis \\(g \\in \\mathcal{H}\\) ). Test Set ( \\(X_{\\text{test}}\\) ) : This is an unseen test set, and we will only use it after we finish tuning our model/hypothesis. Suppose we have a final best model \\(g\\) , we will use \\(g\\) to predict on the test set to get an estimate of the generalization error (also called out-of-sample error). Pipeline. Courtesy of scikit-learn on a typical Cross-Validation workflow. Transfer Learning Traditionally, training on ImageNet weights is a good choice to start. In the event that our training set has a very different distribution of what's inside ImageNet , the model may take a while to converge, even if we finetune it. The intuition is simple, ImageNet was trained on many common items in life, and none of them resemble closely to the image structures of Melanoma Images . Consequently, the model may have a hard time detecting shapes and details from these medical images. We can of course unfreeze all the layers and retrain them from scratch, using various backbones, however, due to limited hardware, we decided it is best to first check if ImageNet yields good results, if not, we can explore weights that were originally trained on skin cancer images. The community used a few models and found out that the EfficientNet variants yielded the best results on this set of training images using ImageNet and hence we adopt the EfficientNet family moving forward. Examining the Grad-CAM of the models revealed that this family of models not only focus on the center nucleus of the skin image but also corners, perhaps they capture something other models don't? We will compare them briefly later. Fine-Tuning Instead of random initialization, we initialize the network with a pretrained network, like the one that is trained on imagenet 1000 dataset. This is what we will be doing. References below. Feature Extraction ConvNet as fixed feature extractor: Here, we will freeze the weights for all of the network except that of the final fully connected layer. This last fully connected layer is replaced with a new one with random weights and only this layer is trained. Preprocessing Most preprocessing techniques we do in an image recognition competition is mostly as follows: Mean and Standard Deviation Perform mean and std for the dataset given to us. Note that this step may make sense on paper, but empirically, using imagenet's default mean std will always work as well, if not better. You can read my blog post here \" Imagenet on RGB: mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225] Channel Distribution This is usually done to check for \"surprises\". More specifically, I remembered once that someone trained a CNN on the blood cells dataset (red, white blood cells etc), as a beginner who just came out from MNIST, he/she grayscaled the images and yielded poor results. This is because one distinct way for the model to differentiate these cells might be because of the colors of the cells. Let the Model tell you where went wrong! Alternatively, the issues are not obvious and we can use tools like Grad-CAM to see where our model is looking to deduce why the model is performing poorly. Augmentations We know that augmentation is central in an image competition, as essentially we are adding more data into the training process, effectively reducing overfitting. Heavy augmentations are used during Train-Time-Augmentation. But during Test-Time-Augmentation, we used the same set of training augmentations to inference with \\(100\\%\\) probability. Train-Time Augmentation Community power. We made use of some innovative augmentations: AdvancedHairAugmentation where hairs were randomly added to the image and Microscope where images were made to look as if they were taken from a microscope. Both of these augmentations provided a steady increase in CV and LB. albumentations . Compose ( [ AdvancedHairAugmentation ( hairs_folder = pipeline_config . transforms . hairs_folder ), albumentations . RandomResizedCrop ( height = pipeline_config . transforms . image_size , width = pipeline_config . transforms . image_size , scale = ( 0.8 , 1.0 ), ratio = ( 0.75 , 1.3333333333333333 ), p = 1.0 , ), albumentations . VerticalFlip ( p = 0.5 ), albumentations . HorizontalFlip ( p = 0.5 ), albumentations . Cutout ( max_h_size = int ( pipeline_config . transforms . image_size * 0.375 ), max_w_size = int ( pipeline_config . transforms . image_size * 0.375 ), num_holes = 1 , p = 0.3 , ), Microscope ( p = 0.5 ), albumentations . Normalize ( mean = pipeline_config . transforms . mean , std = pipeline_config . transforms . std , max_pixel_value = 255.0 , p = 1.0 , ), ToTensorV2 ( p = 1.0 ), ] ) Test-Time Augmentation The exact same set of augmentations were used in inference. Not all TTAs provided a increase in score. Optimizer, Scheduler and Loss Optimizer We used good old AdamW keeping in mind the rule of thumb that if batch size increase by a factor of 2, learning rate should increase by a factor of 2 as well. optimizer_name : str = \"AdamW\" optimizer_params : Dict [ str , Any ] = field ( default_factory = lambda : { \"lr\" : 1e-4 , \"betas\" : ( 0.9 , 0.999 ), \"amsgrad\" : False , \"weight_decay\" : 1e-6 , \"eps\" : 1e-08 , } ) Scheduler We used the following settings: scheduler_name : str = \"CosineAnnealingWarmRestarts\" # Debug if scheduler_name == \"CosineAnnealingWarmRestarts\" : scheduler_params : Dict [ str , Any ] = field ( default_factory = lambda : { \"T_0\" : 10 , \"T_mult\" : 1 , \"eta_min\" : 1e-6 , \"last_epoch\" : - 1 , } ) One should note that OneCycleLR is very popular and yields good results with shorter convergence time. Loss We used CrossEntropyLoss loss with default parameters. Read more in my blog post . train_criterion_name : str = \"CrossEntropyLoss\" train_criterion_params : Dict [ str , Any ] = field ( default_factory = lambda : { \"weight\" : None , \"size_average\" : None , \"ignore_index\" : - 100 , \"reduce\" : None , \"reduction\" : \"mean\" , \"label_smoothing\" : 0.0 , } ) Model Architectures, Training Parameters No Meta Data Model Architecture For models that did not make use of meta data, we have the following architecture. No Meta Data Model Architecture. Meta Data Model Architecture For models that did made use of meta data, we have the following architecture. Meta Data Model Architecture. We concat the flattened feature maps with the meta features: Meta Features : [ 'sex' , 'age_approx' , 'site_head/neck' , 'site_lower extremity' , 'site_oral/genital' , 'site_palms/soles' , 'site_torso' , 'site_upper extremity' , 'site_nan' ] and the meta features has its own sequential layers as ANN: OrderedDict ( [ ( \"fc1\" , torch . nn . Linear ( self . num_meta_features , 512 ), ), ( \"bn1\" , torch . nn . BatchNorm1d ( 512 ), ), ( \"swish1\" , torch . nn . SiLU (), ), ( \"dropout1\" , torch . nn . Dropout ( p = 0.3 ), ), ( \"fc2\" , torch . nn . Linear ( 512 , 128 ), ), ( \"bn2\" , torch . nn . BatchNorm1d ( 128 ), ), ( \"swish2\" , torch . nn . SiLU (), ), ] ) For example: image shape: \\([32, 3, 256, 256]\\) meta_inputs shape: \\([32, 9]\\) we have 9 features. feature_logits shape: \\([32, 1280]\\) flattened feature maps at the last conv layer. meta_logits shape: \\([32, 128]\\) where we passed in a small sequential ANN for the meta data. concat_logits shape: \\([32, 1280 + 128]\\) if self . use_meta : # from cnn images feature_logits = self . extract_features ( image ) # from meta features meta_logits = self . meta_layer ( meta_inputs ) # concatenate concat_logits = torch . cat (( feature_logits , meta_logits ), dim = 1 ) # classifier head classifier_logits = self . architecture [ \"head\" ]( concat_logits ) Activation Functions As we all know, activation functions are used to transform a neurons' linearity to non-linearity and decide whether to \"fire\" a neuron or not. When we design or choose an activation function, we need to ensure the follows: (Smoothness) Differentiable and Continuous: For example, the sigmoid function is continuous and hence differentiable. If the property is not fulfilled, we might face issues as backpropagation may not be performed properly since we cannot differentiate it.If you notice, the heaviside function is not. We cant perform GD using the HF as we cannot compute gradients but for the logistic function we can. The gradient of sigmoid function g is g(1-g) conveniently Monotonic: This helps the model to converge faster. But spoiler alert, Swish is not monotonic. The properties of Swish are as follows: Bounded below: It is claimed in the paper it serves as a strong regularization. Smoothness: More smooth than ReLU which allows the model to optimize better, the error landscape, when smoothed, is easier to traverse in order to find a minima. An intuitive idea is the hill again, imagine you traverse down your neighbourhood hill, vs traversing down Mount Himalaya. # Import matplotlib, numpy and math import matplotlib.pyplot as plt import numpy as np import math def swish ( x ): sigmoid = 1 / ( 1 + np . exp ( - x )) swish = x * sigmoid return swish epsilon = 1e-20 x = np . linspace ( - 100 , 100 , 100 ) z = swish ( x ) print ( z ) print ( min ( z )) plt . plot ( x , z ) plt . xlabel ( \"x\" ) plt . ylabel ( \"Swish(X)\" ) plt . show () Ensemble Theory Mean Blending This is just simple mean blending. Forward Ensembling We made use of the Forward Ensembling idea from Chris in SIIM-ISIC Melanoma Classification back in August 2020, I modified the code for this specific task. A simple description is as follows, modified from Chris, with more mathematical notations. We start off with a dataset \\(\\mathcal{D} = X \\times y\\) where it is sampled from the true population \\(\\mathcal{X} \\times \\mathcal{Y}\\) . We apply KFold (5 splits) to the dataset, as illustrated in the diagram. We can now train five different hypothesis \\(h_{F1}, h_{F2},...,h_{F5}\\) , where \\(h_{F1}\\) is trained on Fold 2 to Fold 5 and predict on Fold 1, \\(h_{F2}\\) is trained on Fold 1,3,4,5 and predict on Fold 2. The logic follows for all 5 hypothesis. Notice that in the five models, we are predicting on a unique validation fold, and as a result, after we trained all 5 folds, we will have the predictions made on the whole training set (F1-F5). This predictions is called the Out-of-Fold predictions. We then go a step further and calculate the AUC score with the OOF predictions with the ground truth to get the OOF AUC. We save it to a csv or dataframe called oof_1.csv , subsequent oof trained on different hypothesis space should be named oof_i.csv where \\(i \\in [2,3,...]\\) . After we trained all 5 folds, we will use \\(h_{1}\\) to predict on \\(X_{test}\\) and obtain predictions \\(Y_{\\text{h1 preds}}\\) , we then use \\(h_{2}\\) to predict on \\(X_{test}\\) and obtain predictions \\(Y_{\\text{h2 preds}}\\) , we do this for all five folds and finally \\(Y_{\\text{final preds}} = \\dfrac{1}{5}\\sum_{i=1}^{5}Y_{\\text{hi preds}}\\) . This is a typical pipeline in most machine learning problems. We save this final predictions as sub_1.csv , subsequence predictions trained on different hypothesis space should be named sub_i.csv where \\(i \\in [2,3,...]\\) . Now if we train another model, a completely different hypothesis space is used, to be more pedantic, we denote the previous model to be taken from the hypothesis space \\(\\mathcal{H}_{1}\\) , and now we move on to \\(\\mathcal{H}_{2}\\) . We repeat step 1-6 on this new model (Note that you are essentially training 10 \"models\" now since we are doing KFold twice, and oh, please set the seed of KFold to be the same, it should never be the case that both model comes from different splitting seed for apparent reasons). Here is the key (given the above setup with 2 different models trained on 5 folds): Normally, most people do a simple mean ensemble, that is \\(\\dfrac{Y_{\\text{final preds H1}} + Y_{\\text{final preds H2}}}{2}\\) . This works well most of the time as we trust both model holds equal importance in the final predictions. One issue may be that certain models should be weighted more than the rest, we should not simply take Leaderboard feedback score to judge the weight assignment. A general heuristic here is called Forward Selection. (Extracted from Chris) Now say that you build 2 models (that means that you did 5 KFold twice). You now have oof_1.csv, oof_2.csv, sub_1.csv, and sub_2.csv. How do we blend the two models? We find the weight w such that w * oof_1.predictions + (1-w) * oof_2.predictions has the largest AUC. all = [] for w in [ 0.00 , 0.01 , 0.02 , ... , 0.98 , 0.99 , 1.00 ]: ensemble_pred = w * oof_1 . predictions + ( 1 - w ) * oof_2 . predictions ensemble_auc = roc_auc_score ( oof . target , ensemble_pred ) all . append ( ensemble_auc ) best_weight = np . argmax ( all ) / 100. Then we can assign the best weight like: final_ensemble_pred = best_weight * sub_1 . target + ( 1 - best_weight ) * sub_2 . target Read more from my blog post in references below. ! pip install torchinfo Collecting torchinfo Downloading torchinfo-1.6.3-py3-none-any.whl (20 kB) Installing collected packages: torchinfo Successfully installed torchinfo-1.6.3 WARNING: You are using pip version 21.3.1; however, version 22.0.3 is available. You should consider upgrading via the 'C:\\Users\\reighns\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command. import timm from dataclasses import asdict , dataclass , field from pathlib import Path from typing import Any , Dict , List , Union import torchinfo import torch # Utility functions. import gc import json import os import random from pathlib import Path , PurePath from typing import Dict , Union , List import numpy as np import torch def seed_all ( seed : int = 1992 ) -> None : \"\"\"Seed all random number generators.\"\"\" print ( f \"Using Seed Number { seed } \" ) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) # set PYTHONHASHSEED env var at fixed value torch . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) torch . cuda . manual_seed ( seed ) # pytorch (both CPU and CUDA) np . random . seed ( seed ) # for numpy pseudo-random generator # set fixed value for python built-in pseudo-random generator random . seed ( seed ) torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = False torch . backends . cudnn . enabled = False seed_all () Using Seed Number 1992 @dataclass class ModelParams : \"\"\"A class to track model parameters. model_name (str): name of the model. pretrained (bool): If True, use pretrained model. input_channels (int): RGB image - 3 channels or Grayscale 1 channel output_dimension (int): Final output neuron. It is the number of classes in classification. Caution: If you use sigmoid layer for Binary, then it is 1. classification_type (str): classification type. \"\"\" model_name : str = \"resnet50d\" # resnet50d resnext50_32x4d \"tf_efficientnet_b0_ns\" # Debug use tf_efficientnet_b0_ns else tf_efficientnet_b4_ns vgg16 pretrained : bool = True input_channels : int = 3 output_dimension : int = 2 classification_type : str = \"multiclass\" use_meta : bool = False def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert to dictionary.\"\"\" return asdict ( self ) MODEL_PARAMS = ModelParams () class CustomNeuralNet ( torch . nn . Module ): def __init__ ( self , model_name : str = MODEL_PARAMS . model_name , out_features : int = MODEL_PARAMS . output_dimension , in_channels : int = MODEL_PARAMS . input_channels , pretrained : bool = MODEL_PARAMS . pretrained , use_meta : bool = MODEL_PARAMS . use_meta , ): \"\"\"Construct a new model. Args: model_name ([type], str): The name of the model to use. Defaults to MODEL_PARAMS.model_name. out_features ([type], int): The number of output features, this is usually the number of classes, but if you use sigmoid, then the output is 1. Defaults to MODEL_PARAMS.output_dimension. in_channels ([type], int): The number of input channels; RGB = 3, Grayscale = 1. Defaults to MODEL_PARAMS.input_channels. pretrained ([type], bool): If True, use pretrained model. Defaults to MODEL_PARAMS.pretrained. \"\"\" super () . __init__ () self . in_channels = in_channels self . pretrained = pretrained self . use_meta = use_meta self . backbone = timm . create_model ( model_name , pretrained = self . pretrained , in_chans = self . in_channels ) # removes head from backbone: # TODO: Global pool = \"avg\" vs \"\" behaves differently in shape, caution! self . backbone . reset_classifier ( num_classes = 0 , global_pool = \"avg\" ) # get the last layer's number of features in backbone (feature map) self . in_features = self . backbone . num_features self . out_features = out_features # Custom Head self . single_head_fc = torch . nn . Sequential ( torch . nn . Linear ( self . in_features , self . out_features ), ) self . architecture : Dict [ str , Callable ] = { \"backbone\" : self . backbone , \"bottleneck\" : None , \"head\" : self . single_head_fc , } def extract_features ( self , image : torch . FloatTensor ) -> torch . FloatTensor : \"\"\"Extract the features mapping logits from the model. This is the output from the backbone of a CNN. Args: image (torch.FloatTensor): The input image. Returns: feature_logits (torch.FloatTensor): The features logits. \"\"\" # TODO: To rename feature_logits to image embeddings, also find out what is image embedding. feature_logits = self . architecture [ \"backbone\" ]( image ) print ( f \"feature logits shape = { feature_logits . shape } \" ) return feature_logits def forward ( self , image : torch . FloatTensor ) -> torch . FloatTensor : \"\"\"The forward call of the model. Args: image (torch.FloatTensor): The input image. Returns: classifier_logits (torch.FloatTensor): The output logits of the classifier head. \"\"\" feature_logits = self . extract_features ( image ) classifier_logits = self . architecture [ \"head\" ]( feature_logits ) print ( f \"classifier_logits shape = { classifier_logits . shape } \" ) return classifier_logits model = CustomNeuralNet () batch_size , channel , height , width = 8 , 3 , 256 , 256 X = torch . randn (( batch_size , channel , height , width )) y = model ( image = X ) feature logits shape = torch.Size([8, 2048]) classifier_logits shape = torch.Size([8, 2]) _ = torchinfo . summary ( model , ( batch_size , channel , height , width ), col_names = [ \"input_size\" , \"output_size\" , \"num_params\" , \"kernel_size\" , \"mult_adds\" , ], depth = 3 , verbose = 1 ) torch.Size([8, 2048]) torch.Size([8, 2]) ========================================================================================================================================================================== Layer (type:depth-idx) Input Shape Output Shape Param # Kernel Shape Mult-Adds ========================================================================================================================================================================== CustomNeuralNet -- -- -- -- -- \u251c\u2500ResNet: 1-1 [8, 3, 256, 256] [8, 2048] -- -- -- \u2502 \u2514\u2500Sequential: 2-1 [8, 3, 256, 256] [8, 64, 128, 128] -- -- -- \u2502 \u2502 \u2514\u2500Conv2d: 3-1 [8, 3, 256, 256] [8, 32, 128, 128] 864 [3, 32, 3, 3] 113,246,208 \u2502 \u2502 \u2514\u2500BatchNorm2d: 3-2 [8, 32, 128, 128] [8, 32, 128, 128] 64 [32] 512 \u2502 \u2502 \u2514\u2500ReLU: 3-3 [8, 32, 128, 128] [8, 32, 128, 128] -- -- -- \u2502 \u2502 \u2514\u2500Conv2d: 3-4 [8, 32, 128, 128] [8, 32, 128, 128] 9,216 [32, 32, 3, 3] 1,207,959,552 \u2502 \u2502 \u2514\u2500BatchNorm2d: 3-5 [8, 32, 128, 128] [8, 32, 128, 128] 64 [32] 512 \u2502 \u2502 \u2514\u2500ReLU: 3-6 [8, 32, 128, 128] [8, 32, 128, 128] -- -- -- \u2502 \u2502 \u2514\u2500Conv2d: 3-7 [8, 32, 128, 128] [8, 64, 128, 128] 18,432 [32, 64, 3, 3] 2,415,919,104 \u2502 \u2514\u2500BatchNorm2d: 2-2 [8, 64, 128, 128] [8, 64, 128, 128] 128 [64] 1,024 \u2502 \u2514\u2500ReLU: 2-3 [8, 64, 128, 128] [8, 64, 128, 128] -- -- -- \u2502 \u2514\u2500MaxPool2d: 2-4 [8, 64, 128, 128] [8, 64, 64, 64] -- -- -- \u2502 \u2514\u2500Sequential: 2-5 [8, 64, 64, 64] [8, 256, 64, 64] -- -- -- \u2502 \u2502 \u2514\u2500Bottleneck: 3-8 [8, 64, 64, 64] [8, 256, 64, 64] 75,008 -- 2,415,929,344 \u2502 \u2502 \u2514\u2500Bottleneck: 3-9 [8, 256, 64, 64] [8, 256, 64, 64] 70,400 -- 2,281,707,520 \u2502 \u2502 \u2514\u2500Bottleneck: 3-10 [8, 256, 64, 64] [8, 256, 64, 64] 70,400 -- 2,281,707,520 \u2502 \u2514\u2500Sequential: 2-6 [8, 256, 64, 64] [8, 512, 32, 32] -- -- -- \u2502 \u2502 \u2514\u2500Bottleneck: 3-11 [8, 256, 64, 64] [8, 512, 32, 32] 379,392 -- 3,892,334,592 \u2502 \u2502 \u2514\u2500Bottleneck: 3-12 [8, 512, 32, 32] [8, 512, 32, 32] 280,064 -- 2,281,713,664 \u2502 \u2502 \u2514\u2500Bottleneck: 3-13 [8, 512, 32, 32] [8, 512, 32, 32] 280,064 -- 2,281,713,664 \u2502 \u2502 \u2514\u2500Bottleneck: 3-14 [8, 512, 32, 32] [8, 512, 32, 32] 280,064 -- 2,281,713,664 \u2502 \u2514\u2500Sequential: 2-7 [8, 512, 32, 32] [8, 1024, 16, 16] -- -- -- \u2502 \u2502 \u2514\u2500Bottleneck: 3-15 [8, 512, 32, 32] [8, 1024, 16, 16] 1,512,448 -- 3,892,355,072 \u2502 \u2502 \u2514\u2500Bottleneck: 3-16 [8, 1024, 16, 16] [8, 1024, 16, 16] 1,117,184 -- 2,281,725,952 \u2502 \u2502 \u2514\u2500Bottleneck: 3-17 [8, 1024, 16, 16] [8, 1024, 16, 16] 1,117,184 -- 2,281,725,952 \u2502 \u2502 \u2514\u2500Bottleneck: 3-18 [8, 1024, 16, 16] [8, 1024, 16, 16] 1,117,184 -- 2,281,725,952 \u2502 \u2502 \u2514\u2500Bottleneck: 3-19 [8, 1024, 16, 16] [8, 1024, 16, 16] 1,117,184 -- 2,281,725,952 \u2502 \u2502 \u2514\u2500Bottleneck: 3-20 [8, 1024, 16, 16] [8, 1024, 16, 16] 1,117,184 -- 2,281,725,952 \u2502 \u2514\u2500Sequential: 2-8 [8, 1024, 16, 16] [8, 2048, 8, 8] -- -- -- \u2502 \u2502 \u2514\u2500Bottleneck: 3-21 [8, 1024, 16, 16] [8, 2048, 8, 8] 6,039,552 -- 3,892,396,032 \u2502 \u2502 \u2514\u2500Bottleneck: 3-22 [8, 2048, 8, 8] [8, 2048, 8, 8] 4,462,592 -- 2,281,750,528 \u2502 \u2502 \u2514\u2500Bottleneck: 3-23 [8, 2048, 8, 8] [8, 2048, 8, 8] 4,462,592 -- 2,281,750,528 \u2502 \u2514\u2500SelectAdaptivePool2d: 2-9 [8, 2048, 8, 8] [8, 2048] -- -- -- \u2502 \u2502 \u2514\u2500AdaptiveAvgPool2d: 3-24 [8, 2048, 8, 8] [8, 2048, 1, 1] -- -- -- \u2502 \u2502 \u2514\u2500Flatten: 3-25 [8, 2048, 1, 1] [8, 2048] -- -- -- \u2502 \u2514\u2500Identity: 2-10 [8, 2048] [8, 2048] -- -- -- \u251c\u2500Sequential: 1-2 [8, 2048] [8, 2] -- -- -- \u2502 \u2514\u2500Linear: 2-11 [8, 2048] [8, 2] 4,098 [2048, 2] 32,784 ========================================================================================================================================================================== Total params: 23,531,362 Trainable params: 23,531,362 Non-trainable params: 0 Total mult-adds (G): 45.21 ========================================================================================================================================================================== Input size (MB): 6.29 Forward/backward pass size (MB): 1992.29 Params size (MB): 94.13 Estimated Total Size (MB): 2092.71 ========================================================================================================================================================================== This model architechure means that if I pass in a batch of \\(8\\) images of size \\((3, 256, 256)\\) , the model statistics will tell us a lot of information. Let us give some examples with a naive ResNet50d . Input Shape: \\([8, 3, 256, 256]\\) passing through the first Sequential Layer's Conv2d (3-1) with kernel size of Kernel Shape: \\([3, 32, 3, 3]\\) which means \\([\\textbf{in_channels, out_channels, kernel_size, kernel_size}]\\) will yield an output shape of Output Shape: \\([8, 32, 128, 128]\\) indicating that the each input images are now transformed into 32 kernels of size 256 by 256. Params: The Params column calculates the number of parameters in this layer at 864 learnable parameters. Once we know how to interpret the table, we can also see that our CustomNeuralnet() has extract_features which outputs the input at the last convolutional layer, in this example, it is at SelectAdaptivePool2d: 2-9 where it first went through AdaptiveAvgPool2d: 3-24 to squash the feature maps to \\([8, 2048, 1, 1]\\) and subsequently a Flatten: 3-25 layer to flatten out the last 2 dimensions to become \\([8, 2048]\\) so we can pass on to the dense layers. We can verify this by X = torch . randn (( batch_size , channel , height , width )) y = model ( image = X ) yielding feature logits shape = torch . Size ([ 8 , 2048 ]) classifier_logits shape = torch . Size ([ 8 , 2 ]) where the latter is the final shape of the input after passing through all the dense layers at \\([8, 2]\\) , where one can envision it as 2 output neurons. Error Analysis using Grad-CAM There is some distinct difference when Grad-CAM is applied to different models, which can help us do error analysis. Grad-CAM of ResNet50d Grad-CAM of EfficietNet For more info on Grad-CAM , see my blog post. Next Steps MLOps (Weights & Biases for experiment tracking) Model Persistence Benefit Structure References Image Normalization Triple Stratified Leak-Free KFold CV Transfer Learning PyTorch Transfer Learning TensorFlow Cross-Entropy Loss Forward Ensemble Forward Ensemble Discussion Grad-CAM","title":"SIIM-ISIC Melanoma Classification"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#siim-isic-melanoma-classification","text":"This competition is hosted on Kaggle and the description and overview is stated below . Skin cancer is the most prevalent type of cancer. Melanoma, specifically, is responsible for 75% of skin cancer deaths, despite being the least common skin cancer. The American Cancer Society estimates over 100,000 new melanoma cases will be diagnosed in 2020. It's also expected that almost 7,000 people will die from the disease. As with other cancers, early and accurate detection\u2014potentially aided by data science\u2014can make treatment more effective. Currently, dermatologists evaluate every one of a patient's moles to identify outlier lesions or \u201cugly ducklings\u201d that are most likely to be melanoma. Existing AI approaches have not adequately considered this clinical frame of reference. Dermatologists could enhance their diagnostic accuracy if detection algorithms take into account \u201ccontextual\u201d images within the same patient to determine which images represent a melanoma. If successful, classifiers would be more accurate and could better support dermatological clinic work. As the leading healthcare organization for informatics in medical imaging, the Society for Imaging Informatics in Medicine (SIIM)'s mission is to advance medical imaging informatics through education, research, and innovation in a multi-disciplinary community. SIIM is joined by the International Skin Imaging Collaboration (ISIC), an international effort to improve melanoma diagnosis. The ISIC Archive contains the largest publicly available collection of quality-controlled dermoscopic images of skin lesions. In this competition, you\u2019ll identify melanoma in images of skin lesions. In particular, you\u2019ll use images within the same patient and determine which are likely to represent a melanoma. Using patient-level contextual information may help the development of image analysis tools, which could better support clinical dermatologists. Melanoma is a deadly disease, but if caught early, most melanomas can be cured with minor surgery. Image analysis tools that automate the diagnosis of melanoma will improve dermatologists' diagnostic accuracy. Better detection of melanoma has the opportunity to positively impact millions of people.","title":"SIIM-ISIC Melanoma Classification"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#establish-metrics","text":"After understanding the problem better, we should probably define a metric to optimize. As usual, this step should be closely tied to business problem. We were already given a metric score by the competition host and let us understand it better. Recall that we wish to have a well-calibrated model, the intuition is that a high performance model may not output meaningful probabilities, even if they can have extremely good performance score. Consider a model that outputs logits of \\(0.51\\) when y_true is 1 and \\(0.49\\) otherwise, then a decision threshold of \\(0.5\\) guarantees an accuracy of \\(100\\%\\) , we have no complaints here if we have no issue with our threshold if our only goal is to have a high scoring model. However, if in medical case, where doctor wants to understand \"probablistically\" the survival of a patient, then we might want to turn into logits probs. But apparently the example here holds almost no meaning, when compared to a \"well calibrated model\", more concretely. y_true = [ 0 , 0 , 1 , 1 ] y_prob_uncalibrated = [ 0.49 , 0.49 , 0.51 , 0.51 ] y_prob_calibrated = [ 0.1 , 0.45 , 0.99 , 0.6 ] both models give \\(100\\%\\) accuracy, but the latter (assuming calibrated), can give us a laymen idea that ok this patient has 0.99 chance and the other patient 0.6 chance of surviving etc.","title":"Establish Metrics"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#benefit-structure","text":"One can introduce a benefit structure with relevant cost-benefit assignment. TP: + 100 FN: -1000 FP: -10 TP+FP: -1 (screening for example) With each TP, we net a profit of 100, and with each FN, we lose -1000, FP loses -10 and whenever the patient get predicted to die (1), send for further screening -1. So towards the end, we can have: \\[ cost = 100*TP - 1000 * FN - 10 * FP - 1 * (TP+FP) \\] This structure helps us decide which metrics to choose.","title":"Benefit Structure"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#roc","text":"Definition: The basic (non-probablistic intepretation) of ROC is graph that plots the True Positive Rate on the y-axis and False Positive Rate on the x-axis parametrized by a threshold vector t . We then look at the area under the ROC curve (AUROC) to get an overall performance measure. Note that TPR is recall . TPR (recall) = TP / (TP + FN) FPR = FP / (FP + TN) Threshold invariant The ROC looks at the performance of a model hypothesis at all thresholds. This is better than just optimizing recall which only looks at a fixed threshold. Scale Invariant Not necessarily a good thing in this context, as this makes ROC a semi-proper scoring metric, that is, it takes in non-calibrated scores and perform well. The below code shows that as long as the order is preserved, y2 and y4 make zero difference in the outcome. In this case, the doctor may not be able to have a \u201cconfidence\u201d level of how likely the patient is going to survive. y1 = [ 1 , 0 , 1 , 0 ] y2 = [ 0.52 , 0.51 , 0.52 , 0.51 ] y3 = [ 52 , 51 , 52 , 51 ] y4 = [ 0.99 , 0.51 , 0.98 , 0.51 ] uncalibrated_roc = roc ( y1 , y2 ) == roc ( y1 , y3 ) == roc ( y1 , y4 ) print ( f \" { uncalibrated_roc } \" ) -> 1.0 This brings us to the next point. More info in notebook.","title":"ROC"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#brier-score-loss","text":"Brier Score computes the squared difference between the probability of a prediction and its actual outcome. Intuitively, this score punishes \u201cunconfident and neutral\u201d probability logits. If a model consistently spits out probability that is near 0.5, then this score will be large. Proper scoring Tells us if the scores output are well calibrated. If not well calibrated, prompt us to either use a different model that calibrated well, or to perform calibration on the model itself. Logistic regression produces natural well calibrated probabilities since it optimizes the log-loss (ce loss), in fact, I think MLE models should always produce well calibrated probabilities since behind the scene it is minimizing KL divergence between ground truth distribution P and estimated distribution Q. It follows that models like DT do not produce well calibrated probabilities. More info in notebook.","title":"Brier Score Loss"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#what-can-we-explore","text":"Did not provide insight if Precision-recall curve and if it may be well posed for this problem than ROC since there is some class imbalance. Did not go into details on calibration methods, in fact, models like RF are not well calibrated by construction. https://scikit-learn.org/stable/modules/calibration.html","title":"What can we explore?"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#validation-and-resampling-strategy","text":"","title":"Validation and Resampling Strategy"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#how-should-we-split-out-data-into-folds","text":"We should examine the data for a few factors: Is the data \\(\\mathcal{X}\\) imbalanced? Is the data \\(\\mathcal{X}\\) generated in a i.i.d. manner, more specifically, if I split \\(\\mathcal{X}\\) to \\(\\mathcal{X}_{train}\\) and \\(\\mathcal{X}_{val}\\) , can we ensure that \\(\\mathcal{X}_{val}\\) has no dependency on \\(\\mathcal{X}_{train}\\) ? We came to the conclusion: Yes, the data is severely imbalanced in which there are only around \\(2\\%\\) of positive (malignant) samples. Therefore, a stratified cross validation is reasonable. StratifiedKFold ensures that relative class frequencies is approximately preserved in each train and validation fold. More concretely, we will not experience the scenario where \\(X_{train}\\) has \\(m^{+}\\) and \\(m^{-}\\) positive and negative samples, but \\(X_{val}\\) has only \\(p^{+}\\) positive samples only and 0 negative samples, simply due to the scarcity of negative samples. In medical imaging, it is a well known fact that most of the data contains patient level repeatedly. To put it bluntly, if I have 100 samples, and according to PatientID , we see that the id 123456 (John Doe) appeared 20 times, this is normal as a patient can undergo multiple settings of say, X-rays. If we allow John Doe's data to appear in both train and validation set, then this poses a problem of information leakage, in which the data is no longer i.i.d. . One can think of each patient has an \"unique, underlying features\" which are highly correlated across their different samples. As a result, it is paramount to ensure that amongst this 3255 unique patients, we need to ensure that each unique patients' images DO NOT appear in the validation fold. That is to say, if patient John Doe has 100 X-ray images, but during our 5-fold splits, he has 70 images in Fold 1-4, while 30 images are in Fold 5, then if we were to train on Fold 1-4 and validate on Fold 5, there may be potential leakage and the model will predict with confidence for John Doe's images. This is under the assumption that John Doe's data does not fulfill the i.i.d proces With the above consideration, we will use StratifiedGroupKFold where \\(K = 5\\) splits. There wasn't this splitting function in scikit-learn at the time of competition and as a result, we used a custom written (by someone else) RepeatedStratifiedGroupKFold function and just set n_splits = 1 to get StratifiedGroupKFold (yes we cannot afford to repeated sample, so setting the split to be 1 will collapse the repeated function to just the normal stratified group kfold). However, as of 2022, this function is readily available in the Scikit-Learn library. To recap, we applied stratified logic such that each train and validation set has an equal weightage of positive and negative samples. We also grouped the patients in the process such that patient \\(i\\) will not appear in both training and validation set. It is worth mentioning the famous Kaggler Chris Deotte went one step further to Triple Stratify the data where he balanced patient count distribution. One can read more here .","title":"How should we split out data into folds?"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#cross-validation-workflow","text":"To recap, we have the following: Training Set ( \\(X_{\\text{train}}\\) ) : This will be further split into K validation sets during our cross-validation. This set is used to fit a particular hypothesis \\(h \\in \\mathcal{H}\\) . Validation Set ( \\(X_{\\text{val}}\\) ) : This is split from our \\(X_{\\text{train}}\\) during cross-validation. This set is used for model selection (i.e. find best hyperparameters, attempt to produce a best hypothesis \\(g \\in \\mathcal{H}\\) ). Test Set ( \\(X_{\\text{test}}\\) ) : This is an unseen test set, and we will only use it after we finish tuning our model/hypothesis. Suppose we have a final best model \\(g\\) , we will use \\(g\\) to predict on the test set to get an estimate of the generalization error (also called out-of-sample error). Pipeline. Courtesy of scikit-learn on a typical Cross-Validation workflow.","title":"Cross-Validation Workflow"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#transfer-learning","text":"Traditionally, training on ImageNet weights is a good choice to start. In the event that our training set has a very different distribution of what's inside ImageNet , the model may take a while to converge, even if we finetune it. The intuition is simple, ImageNet was trained on many common items in life, and none of them resemble closely to the image structures of Melanoma Images . Consequently, the model may have a hard time detecting shapes and details from these medical images. We can of course unfreeze all the layers and retrain them from scratch, using various backbones, however, due to limited hardware, we decided it is best to first check if ImageNet yields good results, if not, we can explore weights that were originally trained on skin cancer images. The community used a few models and found out that the EfficientNet variants yielded the best results on this set of training images using ImageNet and hence we adopt the EfficientNet family moving forward. Examining the Grad-CAM of the models revealed that this family of models not only focus on the center nucleus of the skin image but also corners, perhaps they capture something other models don't? We will compare them briefly later.","title":"Transfer Learning"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#fine-tuning","text":"Instead of random initialization, we initialize the network with a pretrained network, like the one that is trained on imagenet 1000 dataset. This is what we will be doing. References below.","title":"Fine-Tuning"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#feature-extraction","text":"ConvNet as fixed feature extractor: Here, we will freeze the weights for all of the network except that of the final fully connected layer. This last fully connected layer is replaced with a new one with random weights and only this layer is trained.","title":"Feature Extraction"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#preprocessing","text":"Most preprocessing techniques we do in an image recognition competition is mostly as follows:","title":"Preprocessing"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#mean-and-standard-deviation","text":"Perform mean and std for the dataset given to us. Note that this step may make sense on paper, but empirically, using imagenet's default mean std will always work as well, if not better. You can read my blog post here \" Imagenet on RGB: mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]","title":"Mean and Standard Deviation"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#channel-distribution","text":"This is usually done to check for \"surprises\". More specifically, I remembered once that someone trained a CNN on the blood cells dataset (red, white blood cells etc), as a beginner who just came out from MNIST, he/she grayscaled the images and yielded poor results. This is because one distinct way for the model to differentiate these cells might be because of the colors of the cells.","title":"Channel Distribution"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#let-the-model-tell-you-where-went-wrong","text":"Alternatively, the issues are not obvious and we can use tools like Grad-CAM to see where our model is looking to deduce why the model is performing poorly.","title":"Let the Model tell you where went wrong!"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#augmentations","text":"We know that augmentation is central in an image competition, as essentially we are adding more data into the training process, effectively reducing overfitting. Heavy augmentations are used during Train-Time-Augmentation. But during Test-Time-Augmentation, we used the same set of training augmentations to inference with \\(100\\%\\) probability.","title":"Augmentations"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#train-time-augmentation","text":"Community power. We made use of some innovative augmentations: AdvancedHairAugmentation where hairs were randomly added to the image and Microscope where images were made to look as if they were taken from a microscope. Both of these augmentations provided a steady increase in CV and LB. albumentations . Compose ( [ AdvancedHairAugmentation ( hairs_folder = pipeline_config . transforms . hairs_folder ), albumentations . RandomResizedCrop ( height = pipeline_config . transforms . image_size , width = pipeline_config . transforms . image_size , scale = ( 0.8 , 1.0 ), ratio = ( 0.75 , 1.3333333333333333 ), p = 1.0 , ), albumentations . VerticalFlip ( p = 0.5 ), albumentations . HorizontalFlip ( p = 0.5 ), albumentations . Cutout ( max_h_size = int ( pipeline_config . transforms . image_size * 0.375 ), max_w_size = int ( pipeline_config . transforms . image_size * 0.375 ), num_holes = 1 , p = 0.3 , ), Microscope ( p = 0.5 ), albumentations . Normalize ( mean = pipeline_config . transforms . mean , std = pipeline_config . transforms . std , max_pixel_value = 255.0 , p = 1.0 , ), ToTensorV2 ( p = 1.0 ), ] )","title":"Train-Time Augmentation"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#test-time-augmentation","text":"The exact same set of augmentations were used in inference. Not all TTAs provided a increase in score.","title":"Test-Time Augmentation"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#optimizer-scheduler-and-loss","text":"","title":"Optimizer, Scheduler and Loss"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#optimizer","text":"We used good old AdamW keeping in mind the rule of thumb that if batch size increase by a factor of 2, learning rate should increase by a factor of 2 as well. optimizer_name : str = \"AdamW\" optimizer_params : Dict [ str , Any ] = field ( default_factory = lambda : { \"lr\" : 1e-4 , \"betas\" : ( 0.9 , 0.999 ), \"amsgrad\" : False , \"weight_decay\" : 1e-6 , \"eps\" : 1e-08 , } )","title":"Optimizer"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#scheduler","text":"We used the following settings: scheduler_name : str = \"CosineAnnealingWarmRestarts\" # Debug if scheduler_name == \"CosineAnnealingWarmRestarts\" : scheduler_params : Dict [ str , Any ] = field ( default_factory = lambda : { \"T_0\" : 10 , \"T_mult\" : 1 , \"eta_min\" : 1e-6 , \"last_epoch\" : - 1 , } ) One should note that OneCycleLR is very popular and yields good results with shorter convergence time.","title":"Scheduler"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#loss","text":"We used CrossEntropyLoss loss with default parameters. Read more in my blog post . train_criterion_name : str = \"CrossEntropyLoss\" train_criterion_params : Dict [ str , Any ] = field ( default_factory = lambda : { \"weight\" : None , \"size_average\" : None , \"ignore_index\" : - 100 , \"reduce\" : None , \"reduction\" : \"mean\" , \"label_smoothing\" : 0.0 , } )","title":"Loss"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#model-architectures-training-parameters","text":"","title":"Model Architectures, Training Parameters"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#no-meta-data-model-architecture","text":"For models that did not make use of meta data, we have the following architecture. No Meta Data Model Architecture.","title":"No Meta Data Model Architecture"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#meta-data-model-architecture","text":"For models that did made use of meta data, we have the following architecture. Meta Data Model Architecture. We concat the flattened feature maps with the meta features: Meta Features : [ 'sex' , 'age_approx' , 'site_head/neck' , 'site_lower extremity' , 'site_oral/genital' , 'site_palms/soles' , 'site_torso' , 'site_upper extremity' , 'site_nan' ] and the meta features has its own sequential layers as ANN: OrderedDict ( [ ( \"fc1\" , torch . nn . Linear ( self . num_meta_features , 512 ), ), ( \"bn1\" , torch . nn . BatchNorm1d ( 512 ), ), ( \"swish1\" , torch . nn . SiLU (), ), ( \"dropout1\" , torch . nn . Dropout ( p = 0.3 ), ), ( \"fc2\" , torch . nn . Linear ( 512 , 128 ), ), ( \"bn2\" , torch . nn . BatchNorm1d ( 128 ), ), ( \"swish2\" , torch . nn . SiLU (), ), ] ) For example: image shape: \\([32, 3, 256, 256]\\) meta_inputs shape: \\([32, 9]\\) we have 9 features. feature_logits shape: \\([32, 1280]\\) flattened feature maps at the last conv layer. meta_logits shape: \\([32, 128]\\) where we passed in a small sequential ANN for the meta data. concat_logits shape: \\([32, 1280 + 128]\\) if self . use_meta : # from cnn images feature_logits = self . extract_features ( image ) # from meta features meta_logits = self . meta_layer ( meta_inputs ) # concatenate concat_logits = torch . cat (( feature_logits , meta_logits ), dim = 1 ) # classifier head classifier_logits = self . architecture [ \"head\" ]( concat_logits )","title":"Meta Data Model Architecture"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#activation-functions","text":"As we all know, activation functions are used to transform a neurons' linearity to non-linearity and decide whether to \"fire\" a neuron or not. When we design or choose an activation function, we need to ensure the follows: (Smoothness) Differentiable and Continuous: For example, the sigmoid function is continuous and hence differentiable. If the property is not fulfilled, we might face issues as backpropagation may not be performed properly since we cannot differentiate it.If you notice, the heaviside function is not. We cant perform GD using the HF as we cannot compute gradients but for the logistic function we can. The gradient of sigmoid function g is g(1-g) conveniently Monotonic: This helps the model to converge faster. But spoiler alert, Swish is not monotonic. The properties of Swish are as follows: Bounded below: It is claimed in the paper it serves as a strong regularization. Smoothness: More smooth than ReLU which allows the model to optimize better, the error landscape, when smoothed, is easier to traverse in order to find a minima. An intuitive idea is the hill again, imagine you traverse down your neighbourhood hill, vs traversing down Mount Himalaya. # Import matplotlib, numpy and math import matplotlib.pyplot as plt import numpy as np import math def swish ( x ): sigmoid = 1 / ( 1 + np . exp ( - x )) swish = x * sigmoid return swish epsilon = 1e-20 x = np . linspace ( - 100 , 100 , 100 ) z = swish ( x ) print ( z ) print ( min ( z )) plt . plot ( x , z ) plt . xlabel ( \"x\" ) plt . ylabel ( \"Swish(X)\" ) plt . show ()","title":"Activation Functions"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#ensemble-theory","text":"","title":"Ensemble Theory"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#mean-blending","text":"This is just simple mean blending.","title":"Mean Blending"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#forward-ensembling","text":"We made use of the Forward Ensembling idea from Chris in SIIM-ISIC Melanoma Classification back in August 2020, I modified the code for this specific task. A simple description is as follows, modified from Chris, with more mathematical notations. We start off with a dataset \\(\\mathcal{D} = X \\times y\\) where it is sampled from the true population \\(\\mathcal{X} \\times \\mathcal{Y}\\) . We apply KFold (5 splits) to the dataset, as illustrated in the diagram. We can now train five different hypothesis \\(h_{F1}, h_{F2},...,h_{F5}\\) , where \\(h_{F1}\\) is trained on Fold 2 to Fold 5 and predict on Fold 1, \\(h_{F2}\\) is trained on Fold 1,3,4,5 and predict on Fold 2. The logic follows for all 5 hypothesis. Notice that in the five models, we are predicting on a unique validation fold, and as a result, after we trained all 5 folds, we will have the predictions made on the whole training set (F1-F5). This predictions is called the Out-of-Fold predictions. We then go a step further and calculate the AUC score with the OOF predictions with the ground truth to get the OOF AUC. We save it to a csv or dataframe called oof_1.csv , subsequent oof trained on different hypothesis space should be named oof_i.csv where \\(i \\in [2,3,...]\\) . After we trained all 5 folds, we will use \\(h_{1}\\) to predict on \\(X_{test}\\) and obtain predictions \\(Y_{\\text{h1 preds}}\\) , we then use \\(h_{2}\\) to predict on \\(X_{test}\\) and obtain predictions \\(Y_{\\text{h2 preds}}\\) , we do this for all five folds and finally \\(Y_{\\text{final preds}} = \\dfrac{1}{5}\\sum_{i=1}^{5}Y_{\\text{hi preds}}\\) . This is a typical pipeline in most machine learning problems. We save this final predictions as sub_1.csv , subsequence predictions trained on different hypothesis space should be named sub_i.csv where \\(i \\in [2,3,...]\\) . Now if we train another model, a completely different hypothesis space is used, to be more pedantic, we denote the previous model to be taken from the hypothesis space \\(\\mathcal{H}_{1}\\) , and now we move on to \\(\\mathcal{H}_{2}\\) . We repeat step 1-6 on this new model (Note that you are essentially training 10 \"models\" now since we are doing KFold twice, and oh, please set the seed of KFold to be the same, it should never be the case that both model comes from different splitting seed for apparent reasons). Here is the key (given the above setup with 2 different models trained on 5 folds): Normally, most people do a simple mean ensemble, that is \\(\\dfrac{Y_{\\text{final preds H1}} + Y_{\\text{final preds H2}}}{2}\\) . This works well most of the time as we trust both model holds equal importance in the final predictions. One issue may be that certain models should be weighted more than the rest, we should not simply take Leaderboard feedback score to judge the weight assignment. A general heuristic here is called Forward Selection. (Extracted from Chris) Now say that you build 2 models (that means that you did 5 KFold twice). You now have oof_1.csv, oof_2.csv, sub_1.csv, and sub_2.csv. How do we blend the two models? We find the weight w such that w * oof_1.predictions + (1-w) * oof_2.predictions has the largest AUC. all = [] for w in [ 0.00 , 0.01 , 0.02 , ... , 0.98 , 0.99 , 1.00 ]: ensemble_pred = w * oof_1 . predictions + ( 1 - w ) * oof_2 . predictions ensemble_auc = roc_auc_score ( oof . target , ensemble_pred ) all . append ( ensemble_auc ) best_weight = np . argmax ( all ) / 100. Then we can assign the best weight like: final_ensemble_pred = best_weight * sub_1 . target + ( 1 - best_weight ) * sub_2 . target Read more from my blog post in references below. ! pip install torchinfo Collecting torchinfo Downloading torchinfo-1.6.3-py3-none-any.whl (20 kB) Installing collected packages: torchinfo Successfully installed torchinfo-1.6.3 WARNING: You are using pip version 21.3.1; however, version 22.0.3 is available. You should consider upgrading via the 'C:\\Users\\reighns\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command. import timm from dataclasses import asdict , dataclass , field from pathlib import Path from typing import Any , Dict , List , Union import torchinfo import torch # Utility functions. import gc import json import os import random from pathlib import Path , PurePath from typing import Dict , Union , List import numpy as np import torch def seed_all ( seed : int = 1992 ) -> None : \"\"\"Seed all random number generators.\"\"\" print ( f \"Using Seed Number { seed } \" ) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) # set PYTHONHASHSEED env var at fixed value torch . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) torch . cuda . manual_seed ( seed ) # pytorch (both CPU and CUDA) np . random . seed ( seed ) # for numpy pseudo-random generator # set fixed value for python built-in pseudo-random generator random . seed ( seed ) torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = False torch . backends . cudnn . enabled = False seed_all () Using Seed Number 1992 @dataclass class ModelParams : \"\"\"A class to track model parameters. model_name (str): name of the model. pretrained (bool): If True, use pretrained model. input_channels (int): RGB image - 3 channels or Grayscale 1 channel output_dimension (int): Final output neuron. It is the number of classes in classification. Caution: If you use sigmoid layer for Binary, then it is 1. classification_type (str): classification type. \"\"\" model_name : str = \"resnet50d\" # resnet50d resnext50_32x4d \"tf_efficientnet_b0_ns\" # Debug use tf_efficientnet_b0_ns else tf_efficientnet_b4_ns vgg16 pretrained : bool = True input_channels : int = 3 output_dimension : int = 2 classification_type : str = \"multiclass\" use_meta : bool = False def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert to dictionary.\"\"\" return asdict ( self ) MODEL_PARAMS = ModelParams () class CustomNeuralNet ( torch . nn . Module ): def __init__ ( self , model_name : str = MODEL_PARAMS . model_name , out_features : int = MODEL_PARAMS . output_dimension , in_channels : int = MODEL_PARAMS . input_channels , pretrained : bool = MODEL_PARAMS . pretrained , use_meta : bool = MODEL_PARAMS . use_meta , ): \"\"\"Construct a new model. Args: model_name ([type], str): The name of the model to use. Defaults to MODEL_PARAMS.model_name. out_features ([type], int): The number of output features, this is usually the number of classes, but if you use sigmoid, then the output is 1. Defaults to MODEL_PARAMS.output_dimension. in_channels ([type], int): The number of input channels; RGB = 3, Grayscale = 1. Defaults to MODEL_PARAMS.input_channels. pretrained ([type], bool): If True, use pretrained model. Defaults to MODEL_PARAMS.pretrained. \"\"\" super () . __init__ () self . in_channels = in_channels self . pretrained = pretrained self . use_meta = use_meta self . backbone = timm . create_model ( model_name , pretrained = self . pretrained , in_chans = self . in_channels ) # removes head from backbone: # TODO: Global pool = \"avg\" vs \"\" behaves differently in shape, caution! self . backbone . reset_classifier ( num_classes = 0 , global_pool = \"avg\" ) # get the last layer's number of features in backbone (feature map) self . in_features = self . backbone . num_features self . out_features = out_features # Custom Head self . single_head_fc = torch . nn . Sequential ( torch . nn . Linear ( self . in_features , self . out_features ), ) self . architecture : Dict [ str , Callable ] = { \"backbone\" : self . backbone , \"bottleneck\" : None , \"head\" : self . single_head_fc , } def extract_features ( self , image : torch . FloatTensor ) -> torch . FloatTensor : \"\"\"Extract the features mapping logits from the model. This is the output from the backbone of a CNN. Args: image (torch.FloatTensor): The input image. Returns: feature_logits (torch.FloatTensor): The features logits. \"\"\" # TODO: To rename feature_logits to image embeddings, also find out what is image embedding. feature_logits = self . architecture [ \"backbone\" ]( image ) print ( f \"feature logits shape = { feature_logits . shape } \" ) return feature_logits def forward ( self , image : torch . FloatTensor ) -> torch . FloatTensor : \"\"\"The forward call of the model. Args: image (torch.FloatTensor): The input image. Returns: classifier_logits (torch.FloatTensor): The output logits of the classifier head. \"\"\" feature_logits = self . extract_features ( image ) classifier_logits = self . architecture [ \"head\" ]( feature_logits ) print ( f \"classifier_logits shape = { classifier_logits . shape } \" ) return classifier_logits model = CustomNeuralNet () batch_size , channel , height , width = 8 , 3 , 256 , 256 X = torch . randn (( batch_size , channel , height , width )) y = model ( image = X ) feature logits shape = torch.Size([8, 2048]) classifier_logits shape = torch.Size([8, 2]) _ = torchinfo . summary ( model , ( batch_size , channel , height , width ), col_names = [ \"input_size\" , \"output_size\" , \"num_params\" , \"kernel_size\" , \"mult_adds\" , ], depth = 3 , verbose = 1 ) torch.Size([8, 2048]) torch.Size([8, 2]) ========================================================================================================================================================================== Layer (type:depth-idx) Input Shape Output Shape Param # Kernel Shape Mult-Adds ========================================================================================================================================================================== CustomNeuralNet -- -- -- -- -- \u251c\u2500ResNet: 1-1 [8, 3, 256, 256] [8, 2048] -- -- -- \u2502 \u2514\u2500Sequential: 2-1 [8, 3, 256, 256] [8, 64, 128, 128] -- -- -- \u2502 \u2502 \u2514\u2500Conv2d: 3-1 [8, 3, 256, 256] [8, 32, 128, 128] 864 [3, 32, 3, 3] 113,246,208 \u2502 \u2502 \u2514\u2500BatchNorm2d: 3-2 [8, 32, 128, 128] [8, 32, 128, 128] 64 [32] 512 \u2502 \u2502 \u2514\u2500ReLU: 3-3 [8, 32, 128, 128] [8, 32, 128, 128] -- -- -- \u2502 \u2502 \u2514\u2500Conv2d: 3-4 [8, 32, 128, 128] [8, 32, 128, 128] 9,216 [32, 32, 3, 3] 1,207,959,552 \u2502 \u2502 \u2514\u2500BatchNorm2d: 3-5 [8, 32, 128, 128] [8, 32, 128, 128] 64 [32] 512 \u2502 \u2502 \u2514\u2500ReLU: 3-6 [8, 32, 128, 128] [8, 32, 128, 128] -- -- -- \u2502 \u2502 \u2514\u2500Conv2d: 3-7 [8, 32, 128, 128] [8, 64, 128, 128] 18,432 [32, 64, 3, 3] 2,415,919,104 \u2502 \u2514\u2500BatchNorm2d: 2-2 [8, 64, 128, 128] [8, 64, 128, 128] 128 [64] 1,024 \u2502 \u2514\u2500ReLU: 2-3 [8, 64, 128, 128] [8, 64, 128, 128] -- -- -- \u2502 \u2514\u2500MaxPool2d: 2-4 [8, 64, 128, 128] [8, 64, 64, 64] -- -- -- \u2502 \u2514\u2500Sequential: 2-5 [8, 64, 64, 64] [8, 256, 64, 64] -- -- -- \u2502 \u2502 \u2514\u2500Bottleneck: 3-8 [8, 64, 64, 64] [8, 256, 64, 64] 75,008 -- 2,415,929,344 \u2502 \u2502 \u2514\u2500Bottleneck: 3-9 [8, 256, 64, 64] [8, 256, 64, 64] 70,400 -- 2,281,707,520 \u2502 \u2502 \u2514\u2500Bottleneck: 3-10 [8, 256, 64, 64] [8, 256, 64, 64] 70,400 -- 2,281,707,520 \u2502 \u2514\u2500Sequential: 2-6 [8, 256, 64, 64] [8, 512, 32, 32] -- -- -- \u2502 \u2502 \u2514\u2500Bottleneck: 3-11 [8, 256, 64, 64] [8, 512, 32, 32] 379,392 -- 3,892,334,592 \u2502 \u2502 \u2514\u2500Bottleneck: 3-12 [8, 512, 32, 32] [8, 512, 32, 32] 280,064 -- 2,281,713,664 \u2502 \u2502 \u2514\u2500Bottleneck: 3-13 [8, 512, 32, 32] [8, 512, 32, 32] 280,064 -- 2,281,713,664 \u2502 \u2502 \u2514\u2500Bottleneck: 3-14 [8, 512, 32, 32] [8, 512, 32, 32] 280,064 -- 2,281,713,664 \u2502 \u2514\u2500Sequential: 2-7 [8, 512, 32, 32] [8, 1024, 16, 16] -- -- -- \u2502 \u2502 \u2514\u2500Bottleneck: 3-15 [8, 512, 32, 32] [8, 1024, 16, 16] 1,512,448 -- 3,892,355,072 \u2502 \u2502 \u2514\u2500Bottleneck: 3-16 [8, 1024, 16, 16] [8, 1024, 16, 16] 1,117,184 -- 2,281,725,952 \u2502 \u2502 \u2514\u2500Bottleneck: 3-17 [8, 1024, 16, 16] [8, 1024, 16, 16] 1,117,184 -- 2,281,725,952 \u2502 \u2502 \u2514\u2500Bottleneck: 3-18 [8, 1024, 16, 16] [8, 1024, 16, 16] 1,117,184 -- 2,281,725,952 \u2502 \u2502 \u2514\u2500Bottleneck: 3-19 [8, 1024, 16, 16] [8, 1024, 16, 16] 1,117,184 -- 2,281,725,952 \u2502 \u2502 \u2514\u2500Bottleneck: 3-20 [8, 1024, 16, 16] [8, 1024, 16, 16] 1,117,184 -- 2,281,725,952 \u2502 \u2514\u2500Sequential: 2-8 [8, 1024, 16, 16] [8, 2048, 8, 8] -- -- -- \u2502 \u2502 \u2514\u2500Bottleneck: 3-21 [8, 1024, 16, 16] [8, 2048, 8, 8] 6,039,552 -- 3,892,396,032 \u2502 \u2502 \u2514\u2500Bottleneck: 3-22 [8, 2048, 8, 8] [8, 2048, 8, 8] 4,462,592 -- 2,281,750,528 \u2502 \u2502 \u2514\u2500Bottleneck: 3-23 [8, 2048, 8, 8] [8, 2048, 8, 8] 4,462,592 -- 2,281,750,528 \u2502 \u2514\u2500SelectAdaptivePool2d: 2-9 [8, 2048, 8, 8] [8, 2048] -- -- -- \u2502 \u2502 \u2514\u2500AdaptiveAvgPool2d: 3-24 [8, 2048, 8, 8] [8, 2048, 1, 1] -- -- -- \u2502 \u2502 \u2514\u2500Flatten: 3-25 [8, 2048, 1, 1] [8, 2048] -- -- -- \u2502 \u2514\u2500Identity: 2-10 [8, 2048] [8, 2048] -- -- -- \u251c\u2500Sequential: 1-2 [8, 2048] [8, 2] -- -- -- \u2502 \u2514\u2500Linear: 2-11 [8, 2048] [8, 2] 4,098 [2048, 2] 32,784 ========================================================================================================================================================================== Total params: 23,531,362 Trainable params: 23,531,362 Non-trainable params: 0 Total mult-adds (G): 45.21 ========================================================================================================================================================================== Input size (MB): 6.29 Forward/backward pass size (MB): 1992.29 Params size (MB): 94.13 Estimated Total Size (MB): 2092.71 ========================================================================================================================================================================== This model architechure means that if I pass in a batch of \\(8\\) images of size \\((3, 256, 256)\\) , the model statistics will tell us a lot of information. Let us give some examples with a naive ResNet50d . Input Shape: \\([8, 3, 256, 256]\\) passing through the first Sequential Layer's Conv2d (3-1) with kernel size of Kernel Shape: \\([3, 32, 3, 3]\\) which means \\([\\textbf{in_channels, out_channels, kernel_size, kernel_size}]\\) will yield an output shape of Output Shape: \\([8, 32, 128, 128]\\) indicating that the each input images are now transformed into 32 kernels of size 256 by 256. Params: The Params column calculates the number of parameters in this layer at 864 learnable parameters. Once we know how to interpret the table, we can also see that our CustomNeuralnet() has extract_features which outputs the input at the last convolutional layer, in this example, it is at SelectAdaptivePool2d: 2-9 where it first went through AdaptiveAvgPool2d: 3-24 to squash the feature maps to \\([8, 2048, 1, 1]\\) and subsequently a Flatten: 3-25 layer to flatten out the last 2 dimensions to become \\([8, 2048]\\) so we can pass on to the dense layers. We can verify this by X = torch . randn (( batch_size , channel , height , width )) y = model ( image = X ) yielding feature logits shape = torch . Size ([ 8 , 2048 ]) classifier_logits shape = torch . Size ([ 8 , 2 ]) where the latter is the final shape of the input after passing through all the dense layers at \\([8, 2]\\) , where one can envision it as 2 output neurons.","title":"Forward Ensembling"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#error-analysis-using-grad-cam","text":"There is some distinct difference when Grad-CAM is applied to different models, which can help us do error analysis. Grad-CAM of ResNet50d Grad-CAM of EfficietNet For more info on Grad-CAM , see my blog post.","title":"Error Analysis using Grad-CAM"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#next-steps","text":"MLOps (Weights & Biases for experiment tracking) Model Persistence Benefit Structure","title":"Next Steps"},{"location":"reighns_ml_journey/projects/SIIM-ISIC%20Melanoma%20Classification/notebooks/walkthrough/#references","text":"Image Normalization Triple Stratified Leak-Free KFold CV Transfer Learning PyTorch Transfer Learning TensorFlow Cross-Entropy Loss Forward Ensemble Forward Ensemble Discussion Grad-CAM","title":"References"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Appendix/","text":"Notations Input Space: \\(\\mathcal{X}\\) The input space contains the set of all possible examples/instances in a population . This is generally unknown. Output Space: \\(\\mathcal{Y}\\) The output space is the set of all possible labels/targets that corresponds to each point in \\(\\mathcal{X}\\) . Distribution : \\(\\mathcal{P}\\) Reference: Learning From Data p43. The unknown distribution that generated our input space \\(\\mathcal{X}\\) . In general, instead of the mapping \\(\\mathrm{y} = f(\\mathrm{x})\\) , we can take the output \\(\\mathrm{y}\\) to be a random variable that is affected by, rather than determined by, the input \\(\\mathrm{x}\\) . Formally, we have a target distribution \\(\\mathcal{P}(\\mathrm{y} | \\mathrm{x})\\) instead of just \\(\\mathrm{y} = f(\\mathrm{x})\\) . Now we say that any point \\((\\mathrm{x}, \\mathrm{y})\\) in \\(\\mathcal{X}\\) is now generated by the joint distribution \\( \\(\\mathcal{P}(\\mathrm{x}, \\mathrm{y}) = \\mathcal{P}(\\mathrm{x})\\mathcal{P}(\\mathrm{y} | \\mathrm{x})\\) \\) Data: \\(\\mathcal{D}\\) This is the set of samples drawn from \\(\\mathcal{X} \\times \\mathcal{Y}\\) over a distribution \\(\\mathcal{P}\\) . The general notation is as follows: \\( \\(\\mathcal{D} = [(\\mathrm{x^{(1)}}, \\mathrm{y^{(1)}}), (\\mathrm{x^{(2)}}, \\mathrm{y^{(2)}}), ..., (\\mathrm{x^{(N)}}, \\mathrm{y^{(N)}}))]\\) \\) where \\(N\\) denotes the number of training samples, and each \\(\\mathrm{x}^{(i)} \\in \\mathbb{R}^{n}\\) with \\(n\\) features. In general, \\(\\mathrm{y}^{(i)} \\in \\mathbb{R}\\) and is a single label. We can split \\(\\mathcal{D}\\) into two sets respectively, where \\(\\mathrm{X}\\) consists of all the \\(\\mathrm{x}\\) , and \\(\\mathrm{Y}\\) consists of all the \\(\\mathrm{y}\\) . We will see this next. Design Matrix: \\(\\mathrm{X}\\) Let \\(\\mathrm{X}\\) be the design matrix of dimensions \\(m\u2005\\times\u2005(n\u2005+\u20051)\\) where \\(m\\) is the number of observations (training samples) and \\(n\\) independent feature/input variables. Note the inconsistency in the matrix size, I just want to point out that the second matrix, has a column of one in the first row because we usually have a bias term \\(\\mathrm{x_0}\\) , which we set to 1. \\[\\mathrm{X} = \\begin{bmatrix} (\\mathbf{x^{(1)}})^{T} \\\\ (\\mathbf{x^{(2)}})^{T} \\\\ \\vdots \\\\ (\\mathbf{x^{(m)}})^{T}\\end{bmatrix}_{m \\times n} = \\begin{bmatrix} 1 & x_1^{(1)} & x_2^{(1)} & \\cdots & x_n^{(1)} \\\\\\\\ 1 & x_1^{(2)} & x_2^{(2)} & \\cdots & x_n^{(2)} \\\\\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\\\\ 1 & x_1^{(m)} & x_2^{(m)} & \\cdots & x_n^{(m)} \\end{bmatrix}_{m \\times (n+1)} \\] Single Training Vector: \\(\\mathrm{x}\\) It is worth noting the \\(\\mathrm{x}^{(i)}\\) defined above is formally defined to be the \\(i\\) -th column of \\(\\mathrm{X}\\) , which is the \\(i\\) -th training sample, represented as a \\(n \\times 1\\) column vector . However, the way we define the Design Matrix is that each row of \\(\\mathrm{X}\\) is the transpose of \\(\\mathrm{x}^{(i)}\\) . Note \\(x^{(i)}_j\\) is the value of feature/attribute j in the ith training instance. \\[\\mathbf{x^{(i)}} = \\begin{bmatrix} x_1^{(i)} \\\\ x_2^{(i)} \\\\ \\vdots \\\\ x_n^{(i)} \\end{bmatrix}_{n \\times 1}\\] Target/Label: \\(\\mathrm{Y}\\) This is the target vector. By default, it is a column vector of size \\(m \\times 1\\) . \\[\\mathbf{y} = \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)} \\end{bmatrix}_{m \\times 1}\\] Hypothesis Set: \\(\\mathcal{H}\\) The set where it contains all possible functions to approximate our true function \\(f\\) . Note that the Hypothesis Set can be either continuous or discrete, means to say it can be either a finite or infinite set. But in reality, it is almost always infinite. Hypothesis: \\(\\mathcal{h}: \\mathrm{X} \\to \\mathrm{Y}\\) where \\(\\mathrm{x} \\mapsto \\mathrm{y}\\) Note that this \\(\\mathcal{h} \\in \\mathcal{H}\\) is the hypothesis function, The final best hypothesis function is called \\(g\\) , which approximates the true function \\(f\\) . Learning Algorithm: \\(\\mathcal{A}\\) What this does is from the set of Hypothesis \\(\\mathcal{H}\\) , the learning algorithm's role is to pick one \\(\\mathcal{h} \\in \\mathcal{H}\\) such that this \\(h\\) is the hypothesis function. More often, we also call our final hypothesis learned from \\(\\mathcal{A}\\) \\(g\\) . Hypothesis Subscript \\(\\mathcal{D}\\) : \\(h_{\\mathcal{D}}\\) This is no different from the previous hypothesis, instead the previous \\(h\\) is a shorthand for this notation. This means that the hypothesis we choose is dependent on the sample data given to us, that is to say, given a \\(\\mathcal{D}\\) , we will use \\(\\mathcal{A}\\) to learn a \\(h_{\\mathcal{D}}\\) from \\(\\mathcal{H}\\) . Generalization Error/Test Error/Out-of-Sample Error: \\(\\mathcal{E}_{\\text{out}}(h)\\) Reference from Foundations of Machine Learning . Given a hypothesis \\(h \\in \\mathcal{H}\\) , a true function \\(f \\in \\mathcal{C}\\) , and an underlying distribution \\(\\mathcal{P}\\) , the test/out-of-sample error of \\(h\\) is defined by \\( \\(\\begin{aligned}\\mathcal{E}_{\\text{out}}(h) = \\underset{x \\sim \\mathcal{P}}{\\mathrm{Pr}}[h(\\mathrm{x}) \\neq f(\\mathrm{x})]\\end{aligned}\\) \\) Note that the above equation is just the error rate between the hypothesis function \\(h\\) and the true function \\(f\\) and as a result, the test error of a hypothesis is not known because both the distribution \\(\\mathcal{P}\\) and the true function \\(f\\) are unknown. This brings us to the next best thing we can measure, the In-sample/Empirical/Training Error. More formally, in a regression setting where we Mean Squared Error, \\( \\(\\begin{aligned}\\mathcal{E}_{\\text{out}}(h) = \\mathbb{E}_{\\mathrm{x}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}) - f(\\mathrm{x}))^2 \\right] \\end{aligned}\\) \\) This is difficult and confusing to understand. To water down the formal definition, it is worth taking an example, in \\(\\mathcal{E}_{\\text{out}}(h)\\) we are only talking about the Expected Test Error over the Test Set and nothing else. Think of a test set with only one query point , we call it \\(\\mathrm{x}_{q}\\) , then the above equation is just \\( \\(\\begin{aligned}\\mathcal{E}_{\\text{out}}(h) = \\mathbb{E}_{\\mathrm{x}_{q}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}_{q}) - f(\\mathrm{x}_{q}))^2 \\right] \\end{aligned}\\) \\) over a single point over the distribution \\(\\mathrm{x}_{q}\\) . That is if \\(\\mathrm{x}_{q} = 3\\) and \\(h_{\\mathcal{D}}(\\mathrm{x}_{q}) = 2\\) and \\(f(\\mathrm{x}_{q}) = 5\\) , then \\((h_{\\mathcal{D}}(\\mathrm{x}_{q}) - f(\\mathrm{x}_{q}))^2 = 9\\) and it follows that \\( \\(\\mathcal{E}_{\\text{out}}(h) = \\mathbb{E}_{\\mathrm{x}_{q}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}_{q}) - f(\\mathrm{x}_{q}))^2 \\right] = \\mathbb{E}_{\\mathrm{x}_{q}}[9] = \\frac{9}{1} = 9\\) \\) Note that I purposely denoted the denominator to be 1 because we have only 1 test point, if we were to have 2 test point, say \\(\\mathrm{x} = [x_{p}, x_{q}] = [3, 6]\\) , then if \\(h_{\\mathcal{D}}(x_{p}) = 4\\) and \\(f(x_{p}) = 6\\) , then our \\((h_{\\mathcal{D}}(\\mathrm{x}_{p}) - f(\\mathrm{x}_{p}))^2 = 4\\) . Then our \\( \\(\\mathcal{E}_{\\text{out}}(h) = \\mathbb{E}_{\\mathrm{x}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}) - f(\\mathrm{x}))^2 \\right] = \\mathbb{E}_{\\mathrm{x}_{q}}[[9, 4]] = \\frac{1}{2} [9 + 4] = 6.5\\) \\) Note how I secretly removed the subscript in \\(\\mathrm{x}\\) , and how when there are two points, we are taking expectation over the 2 points. So if we have \\(m\\) test points, then the expectation is taken over all the test points. Till now, our hypothesis \\(h\\) is fixed over a particular sample set \\(\\mathcal{D}\\) . We will now move on to the next concept on Expected Generalization Error (adding a word Expected in front makes a lot of difference). Expected Generalization Error/Test Error/Out-of-Sample Error: \\(\\mathbb{E}_{\\mathcal{D}}[\\mathcal{E}_{\\text{out}}(h)]\\) For the previous generalization error, we are only talking a fixed hypothesis generated by one particular \\(\\mathcal{D}\\) . In order to remove this dependency, we can simply take the expectation of Generalization Error of \\(h\\) over a particular \\(\\mathcal{D}\\) by simply taking the expectation over all such \\(\\mathcal{D}_{i}\\) , \\(i = 1,2,3,...K\\) . Then the Expected Generalization Test Error is independent of any particular realization of \\(\\mathcal{D}\\) : \\[\\begin{aligned}\\mathbb{E}_{\\mathcal{D}}[\\mathcal{E}_{\\text{out}}(h)] = \\mathbb{E}_{\\mathcal{D}}[\\mathbb{E}_{\\mathrm{x}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}) - f(\\mathrm{x}))^2 \\right]] \\end{aligned}\\] In the following example, we can calculate the Expected Generalization Error, where we are using the Error to be Mean Squared Error, so in essence, we are finding the expected MSE. Empirical Error/Training Error/In-Sample Error: \\(\\mathcal{E}_{\\text{in}}(h)\\) Given a hypothesis \\(h \\in \\mathcal{H}\\) , a true function \\(f \\in \\mathcal{C}\\) , and an underlying distribution \\(\\mathcal{P}\\) , and a sample \\(\\mathrm{X}\\) drawn from \\(\\mathcal{X}\\) i.i.d with distribution \\(\\mathcal{P}\\) , the test/out-of-sample error of \\(h\\) is defined by \\( \\(\\begin{aligned}\\mathcal{E}_{\\text{in}}(h) = \\frac{1}{\\mathrm{m}}\\sum_{i=1}^{\\mathrm{m}}\\text{sign}[h(\\mathrm{x}^{(i)}) \\neq f(\\mathrm{x}^{(i)})]\\end{aligned}\\) \\) Here the sign function is mainly used for binary classification, where if \\(h\\) and \\(f\\) disagrees at any point \\(x^{(i)}\\) , then \\(\\text{sign}[h(\\mathrm{x}^{(i)}) \\neq f(\\mathrm{x}^{(i)})]\\) evaluates to 1. We take the sum of all disagreements and divide by the total number of samples. In short, that is just the misclassification/error rate. The empirical error of \\(h \\in \\mathcal{H}\\) is its average error over the sample \\(\\mathcal{X}\\) , in contrast, the generalization error is its expected error based on the distribution \\(\\mathcal{P}\\) . Take careful note here that \\(h(x^{(i)})\\) is the prediction made by our hypothesis (model), we can conventionally call it \\(\\hat{y}^{(i)}\\) whereby our \\(f(x^{(i)})\\) is our ground truth label \\(y^{(i)}\\) . I believe that this ground truth label is realized once we draw the sample from \\(\\mathcal{X}\\) even though we do not know what \\(f\\) is. An additional note here, is that the summand of the in-sample error function is not fixated to the sign function. In fact, I believe you can define any loss function to calculate the \"error\". As an example, if we are dealing with regression, then we can modify the summand to our favourite Mean Squared Error. \\[\\begin{aligned}\\mathcal{E}_{\\text{in}}(h) = \\frac{1}{\\mathrm{m}}\\sum_{i=1}^{\\mathrm{m}}[h(\\mathrm{x}^{(i)}) - f(\\mathrm{x}^{(i)})]^2\\end{aligned}\\] Bias - Variance Decomposition This is a decomposition of the Expected Generalization Error. Formal Proof please read Learning From Data. Unless otherwise stated, we consider only the univariate case where \\(\\mathrm{x}\\) is a single test point. \\[\\begin{align*} \\mathbb{E}_{\\mathcal{D}}[\\mathcal{E}_{\\text{out}}(h)] &= \\mathbb{E}_{\\mathcal{D}}[\\mathbb{E}_{\\mathrm{x}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}) - f(\\mathrm{x}))^2 \\right]] \\\\ &= \\big(\\;\\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;] - f(x)\\;\\big)^2 + \\mathbb{E}_{\\mathcal{D}}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;])^2\\;\\big] + \\mathbb{E}\\big[(y-f(x))^2\\big] \\\\ &= \\big(\\;\\bar{h}(\\mathrm{x}) - f(x)\\;\\big)^2 + \\mathbb{E}_\\mathcal{D}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\bar{h}(\\mathrm{x}) \\;])^2\\;\\big]+ \\mathbb{E}\\big[(y-f(x))^2\\big] \\end{align*} \\] Where $\\big(\\;\\mathbb{E} {\\mathcal{D}}[\\;h }(x)\\;] - f(x)\\;\\big)^2 $ is the Bias, \\(\\mathbb{E}_\\mathcal{D}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;])^2\\;\\big]\\) is the Variance and \\(\\mathbb{E}\\big[(y-f(x))^2\\big]\\) is the irreducible error \\(\\epsilon\\) . Bias: \\(\\big(\\;\\mathbb{E}_\\mathcal{D}[\\;h_{\\mathcal{D}}(x)\\;] - f(x)\\;\\big)^2\\) In other form, we can express Bias as \\( \\(\\big(\\;\\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;] - f(x)\\;\\big)^2 = \\big(\\;\\bar{h}(\\mathrm{x}) - f(x)\\;\\big)^2\\) \\) See simulation on Bias-Variance Tradeoff to understand. If our test point is \\(x_{q} = 0.9\\) , then our bias is as such: \\[ \\widehat{\\text{bias}} \\left(\\hat{f}(0.90) \\right) = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(\\hat{f}_k^{[i]}(0.90) \\right) - f(0.90) \\] Variance: \\(\\mathbb{E}_\\mathcal{D}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;])^2\\;\\big]\\) This is more confusing, but we first express Variance as: \\[\\mathbb{E}_\\mathcal{D}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;])^2\\;\\big] = \\mathbb{E}_\\mathcal{D}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\bar{h}(\\mathrm{x}) \\;])^2\\;\\big]\\] If our test point is \\(x_{q} = 0.9\\) , then our variance is as such: \\[ \\widehat{\\text{var}} \\left(\\hat{f}(0.90) \\right) = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(\\hat{f}_k^{[i]}(0.90) - \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}}\\hat{f}_k^{[i]}(0.90) \\right)^2 \\] Pseudo Code Cross-Validation Define \\(G\\) as the set of combination of hyperparamters. Define number of splits to be \\(K\\) . For each set of hyperparameter \\(z \\in Z\\) : for fold \\(j\\) in K: Set \\(F_{\\text{train}}=\\bigcup\\limits_{i\\neq k}^{K} F_{i}\\) Set \\(F_{\\text{val}} = F_{j}\\) as the validation set Perform Standard Scaling on \\(F_{\\text{train}}\\) and find the mean and std Perform VIF recursively on \\(F_{\\text{train}}\\) and find the selected features Transform \\(F_{\\text{val}}\\) using the mean and std found using \\(F_{\\text{train}}\\) Transform \\(F_{\\text{val}}\\) to have only the selected features from \\(F_{\\text{train}}\\) Train and fit on \\(F_{\\text{train}}\\) Evaluate the fitted parameters on \\(F_{\\text{val}}\\) to obtain \\(\\mathcal{M}\\) Logistic Regression Given target variable \\(Y \\in \\{0, 1\\}\\) and predictors \\(X\\) , denote \\(\\mathbb{P}(X) = P(Y = 1 | X)\\) to estimate the probability of \\(Y\\) is of positive (malignant) class. LR expresses \\(\\mathbb{P}\\) as a function the predictors \\(X\\) as \\(\\mathbb{P}(X) = \\sigma(\\hat{\\mathrm{\\beta}}^T X) = \\frac{1}{1 + \\exp(\\hat{\\mathrm{\\beta}}^T X)}\\) where \\(\\hat{\\beta}\\) is the estimated coefficients of the model. One thing worth mentioning is the logistic function \\(\\sigma(z) = \\frac{1}{1 + \\exp(-z)}\\) outputs values from 0 to 1 which is actually the functional form of our hypothesis, and therefore makes up the \\textbf{Hypothesis Space} \\(\\mathcal{H}\\) . We then uses a learning algorithm \\(\\mathcal{A}\\) , \\textbf{Maximum Likelihood Estimation (MLE)}, to estimate the coefficients of our predictors; however, since there is no closed form solution to MLE, the learning algorithm will use optimization techniques like \\textbf{Gradient Descent}\\footnote{We can use Gradient Descent if we instead minimze the negative loglikehood function which is the same as maximizing MLE} to find \\(\\hat{\\beta}\\) . Readings and References False-positive and false-negative cases of fine-needle aspiration cytology for palpable breast lesions What is a Dendrogram? Breast Biopsy - Mayo Clinic Data Centric - Andrew Ng When is Multicollinearity not an issue - Paul Allison Intuitive Explanation of Multicollinearity in Linear Regression - Stackoverflow Hypothesis Testing Across Models Hypothesis Test for Comparing ML Algorithms - Jason Brownlee Regression Modelling Strategies - Professor Frank Harrell Damage Caused by Classification Accuracy and Other Discontinuous Improper Accuracy Scoring Rules - Professor Frank Harrell On a reliable cross validation split 1 On a reliable cross validation split 2 Estimate Generalization Error Using Boxplot to compare Model's Performance Common Pitfalls - Scikit-Learn Common pitfalls in the interpretation of coefficients of linear models - Scikit-Learn Calibrated Classification - Jason Brownlee scikit learn calibration Are you sure your models return probabilities? cambridge's probability calibration calibration in ML Terms Brier Score and Model Calibration - Neptune AI Google's take on calibrated models IMPORTANT: WHAT IS CALIBRATION Hands on sklearn calibration Hands on sklearn calibration v2 Examples of scoring rules Logistic Regression is well calibrated","title":"Notations"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Appendix/#notations","text":"","title":"Notations"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Appendix/#input-space-mathcalx","text":"The input space contains the set of all possible examples/instances in a population . This is generally unknown.","title":"Input Space: \\(\\mathcal{X}\\)"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Appendix/#output-space-mathcaly","text":"The output space is the set of all possible labels/targets that corresponds to each point in \\(\\mathcal{X}\\) .","title":"Output Space: \\(\\mathcal{Y}\\)"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Appendix/#distribution-mathcalp","text":"Reference: Learning From Data p43. The unknown distribution that generated our input space \\(\\mathcal{X}\\) . In general, instead of the mapping \\(\\mathrm{y} = f(\\mathrm{x})\\) , we can take the output \\(\\mathrm{y}\\) to be a random variable that is affected by, rather than determined by, the input \\(\\mathrm{x}\\) . Formally, we have a target distribution \\(\\mathcal{P}(\\mathrm{y} | \\mathrm{x})\\) instead of just \\(\\mathrm{y} = f(\\mathrm{x})\\) . Now we say that any point \\((\\mathrm{x}, \\mathrm{y})\\) in \\(\\mathcal{X}\\) is now generated by the joint distribution \\( \\(\\mathcal{P}(\\mathrm{x}, \\mathrm{y}) = \\mathcal{P}(\\mathrm{x})\\mathcal{P}(\\mathrm{y} | \\mathrm{x})\\) \\)","title":"Distribution: \\(\\mathcal{P}\\)"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Appendix/#data-mathcald","text":"This is the set of samples drawn from \\(\\mathcal{X} \\times \\mathcal{Y}\\) over a distribution \\(\\mathcal{P}\\) . The general notation is as follows: \\( \\(\\mathcal{D} = [(\\mathrm{x^{(1)}}, \\mathrm{y^{(1)}}), (\\mathrm{x^{(2)}}, \\mathrm{y^{(2)}}), ..., (\\mathrm{x^{(N)}}, \\mathrm{y^{(N)}}))]\\) \\) where \\(N\\) denotes the number of training samples, and each \\(\\mathrm{x}^{(i)} \\in \\mathbb{R}^{n}\\) with \\(n\\) features. In general, \\(\\mathrm{y}^{(i)} \\in \\mathbb{R}\\) and is a single label. We can split \\(\\mathcal{D}\\) into two sets respectively, where \\(\\mathrm{X}\\) consists of all the \\(\\mathrm{x}\\) , and \\(\\mathrm{Y}\\) consists of all the \\(\\mathrm{y}\\) . We will see this next.","title":"Data: \\(\\mathcal{D}\\)"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Appendix/#design-matrix-mathrmx","text":"Let \\(\\mathrm{X}\\) be the design matrix of dimensions \\(m\u2005\\times\u2005(n\u2005+\u20051)\\) where \\(m\\) is the number of observations (training samples) and \\(n\\) independent feature/input variables. Note the inconsistency in the matrix size, I just want to point out that the second matrix, has a column of one in the first row because we usually have a bias term \\(\\mathrm{x_0}\\) , which we set to 1. \\[\\mathrm{X} = \\begin{bmatrix} (\\mathbf{x^{(1)}})^{T} \\\\ (\\mathbf{x^{(2)}})^{T} \\\\ \\vdots \\\\ (\\mathbf{x^{(m)}})^{T}\\end{bmatrix}_{m \\times n} = \\begin{bmatrix} 1 & x_1^{(1)} & x_2^{(1)} & \\cdots & x_n^{(1)} \\\\\\\\ 1 & x_1^{(2)} & x_2^{(2)} & \\cdots & x_n^{(2)} \\\\\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\\\\ 1 & x_1^{(m)} & x_2^{(m)} & \\cdots & x_n^{(m)} \\end{bmatrix}_{m \\times (n+1)} \\]","title":"Design Matrix: \\(\\mathrm{X}\\)"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Appendix/#single-training-vector-mathrmx","text":"It is worth noting the \\(\\mathrm{x}^{(i)}\\) defined above is formally defined to be the \\(i\\) -th column of \\(\\mathrm{X}\\) , which is the \\(i\\) -th training sample, represented as a \\(n \\times 1\\) column vector . However, the way we define the Design Matrix is that each row of \\(\\mathrm{X}\\) is the transpose of \\(\\mathrm{x}^{(i)}\\) . Note \\(x^{(i)}_j\\) is the value of feature/attribute j in the ith training instance. \\[\\mathbf{x^{(i)}} = \\begin{bmatrix} x_1^{(i)} \\\\ x_2^{(i)} \\\\ \\vdots \\\\ x_n^{(i)} \\end{bmatrix}_{n \\times 1}\\]","title":"Single Training Vector: \\(\\mathrm{x}\\)"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Appendix/#targetlabel-mathrmy","text":"This is the target vector. By default, it is a column vector of size \\(m \\times 1\\) . \\[\\mathbf{y} = \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)} \\end{bmatrix}_{m \\times 1}\\]","title":"Target/Label: \\(\\mathrm{Y}\\)"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Appendix/#hypothesis-set-mathcalh","text":"The set where it contains all possible functions to approximate our true function \\(f\\) . Note that the Hypothesis Set can be either continuous or discrete, means to say it can be either a finite or infinite set. But in reality, it is almost always infinite.","title":"Hypothesis Set: \\(\\mathcal{H}\\)"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Appendix/#hypothesis-mathcalh-mathrmx-to-mathrmy-where-mathrmx-mapsto-mathrmy","text":"Note that this \\(\\mathcal{h} \\in \\mathcal{H}\\) is the hypothesis function, The final best hypothesis function is called \\(g\\) , which approximates the true function \\(f\\) .","title":"Hypothesis: \\(\\mathcal{h}: \\mathrm{X} \\to \\mathrm{Y}\\) where \\(\\mathrm{x} \\mapsto \\mathrm{y}\\)"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Appendix/#learning-algorithm-mathcala","text":"What this does is from the set of Hypothesis \\(\\mathcal{H}\\) , the learning algorithm's role is to pick one \\(\\mathcal{h} \\in \\mathcal{H}\\) such that this \\(h\\) is the hypothesis function. More often, we also call our final hypothesis learned from \\(\\mathcal{A}\\) \\(g\\) .","title":"Learning Algorithm: \\(\\mathcal{A}\\)"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Appendix/#hypothesis-subscript-mathcald-h_mathcald","text":"This is no different from the previous hypothesis, instead the previous \\(h\\) is a shorthand for this notation. This means that the hypothesis we choose is dependent on the sample data given to us, that is to say, given a \\(\\mathcal{D}\\) , we will use \\(\\mathcal{A}\\) to learn a \\(h_{\\mathcal{D}}\\) from \\(\\mathcal{H}\\) .","title":"Hypothesis Subscript \\(\\mathcal{D}\\): \\(h_{\\mathcal{D}}\\)"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Appendix/#generalization-errortest-errorout-of-sample-error-mathcale_textouth","text":"Reference from Foundations of Machine Learning . Given a hypothesis \\(h \\in \\mathcal{H}\\) , a true function \\(f \\in \\mathcal{C}\\) , and an underlying distribution \\(\\mathcal{P}\\) , the test/out-of-sample error of \\(h\\) is defined by \\( \\(\\begin{aligned}\\mathcal{E}_{\\text{out}}(h) = \\underset{x \\sim \\mathcal{P}}{\\mathrm{Pr}}[h(\\mathrm{x}) \\neq f(\\mathrm{x})]\\end{aligned}\\) \\) Note that the above equation is just the error rate between the hypothesis function \\(h\\) and the true function \\(f\\) and as a result, the test error of a hypothesis is not known because both the distribution \\(\\mathcal{P}\\) and the true function \\(f\\) are unknown. This brings us to the next best thing we can measure, the In-sample/Empirical/Training Error. More formally, in a regression setting where we Mean Squared Error, \\( \\(\\begin{aligned}\\mathcal{E}_{\\text{out}}(h) = \\mathbb{E}_{\\mathrm{x}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}) - f(\\mathrm{x}))^2 \\right] \\end{aligned}\\) \\) This is difficult and confusing to understand. To water down the formal definition, it is worth taking an example, in \\(\\mathcal{E}_{\\text{out}}(h)\\) we are only talking about the Expected Test Error over the Test Set and nothing else. Think of a test set with only one query point , we call it \\(\\mathrm{x}_{q}\\) , then the above equation is just \\( \\(\\begin{aligned}\\mathcal{E}_{\\text{out}}(h) = \\mathbb{E}_{\\mathrm{x}_{q}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}_{q}) - f(\\mathrm{x}_{q}))^2 \\right] \\end{aligned}\\) \\) over a single point over the distribution \\(\\mathrm{x}_{q}\\) . That is if \\(\\mathrm{x}_{q} = 3\\) and \\(h_{\\mathcal{D}}(\\mathrm{x}_{q}) = 2\\) and \\(f(\\mathrm{x}_{q}) = 5\\) , then \\((h_{\\mathcal{D}}(\\mathrm{x}_{q}) - f(\\mathrm{x}_{q}))^2 = 9\\) and it follows that \\( \\(\\mathcal{E}_{\\text{out}}(h) = \\mathbb{E}_{\\mathrm{x}_{q}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}_{q}) - f(\\mathrm{x}_{q}))^2 \\right] = \\mathbb{E}_{\\mathrm{x}_{q}}[9] = \\frac{9}{1} = 9\\) \\) Note that I purposely denoted the denominator to be 1 because we have only 1 test point, if we were to have 2 test point, say \\(\\mathrm{x} = [x_{p}, x_{q}] = [3, 6]\\) , then if \\(h_{\\mathcal{D}}(x_{p}) = 4\\) and \\(f(x_{p}) = 6\\) , then our \\((h_{\\mathcal{D}}(\\mathrm{x}_{p}) - f(\\mathrm{x}_{p}))^2 = 4\\) . Then our \\( \\(\\mathcal{E}_{\\text{out}}(h) = \\mathbb{E}_{\\mathrm{x}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}) - f(\\mathrm{x}))^2 \\right] = \\mathbb{E}_{\\mathrm{x}_{q}}[[9, 4]] = \\frac{1}{2} [9 + 4] = 6.5\\) \\) Note how I secretly removed the subscript in \\(\\mathrm{x}\\) , and how when there are two points, we are taking expectation over the 2 points. So if we have \\(m\\) test points, then the expectation is taken over all the test points. Till now, our hypothesis \\(h\\) is fixed over a particular sample set \\(\\mathcal{D}\\) . We will now move on to the next concept on Expected Generalization Error (adding a word Expected in front makes a lot of difference).","title":"Generalization Error/Test Error/Out-of-Sample Error: \\(\\mathcal{E}_{\\text{out}}(h)\\)"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Appendix/#expected-generalization-errortest-errorout-of-sample-error-mathbbe_mathcaldmathcale_textouth","text":"For the previous generalization error, we are only talking a fixed hypothesis generated by one particular \\(\\mathcal{D}\\) . In order to remove this dependency, we can simply take the expectation of Generalization Error of \\(h\\) over a particular \\(\\mathcal{D}\\) by simply taking the expectation over all such \\(\\mathcal{D}_{i}\\) , \\(i = 1,2,3,...K\\) . Then the Expected Generalization Test Error is independent of any particular realization of \\(\\mathcal{D}\\) : \\[\\begin{aligned}\\mathbb{E}_{\\mathcal{D}}[\\mathcal{E}_{\\text{out}}(h)] = \\mathbb{E}_{\\mathcal{D}}[\\mathbb{E}_{\\mathrm{x}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}) - f(\\mathrm{x}))^2 \\right]] \\end{aligned}\\] In the following example, we can calculate the Expected Generalization Error, where we are using the Error to be Mean Squared Error, so in essence, we are finding the expected MSE.","title":"Expected Generalization Error/Test Error/Out-of-Sample Error: \\(\\mathbb{E}_{\\mathcal{D}}[\\mathcal{E}_{\\text{out}}(h)]\\)"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Appendix/#empirical-errortraining-errorin-sample-error-mathcale_textinh","text":"Given a hypothesis \\(h \\in \\mathcal{H}\\) , a true function \\(f \\in \\mathcal{C}\\) , and an underlying distribution \\(\\mathcal{P}\\) , and a sample \\(\\mathrm{X}\\) drawn from \\(\\mathcal{X}\\) i.i.d with distribution \\(\\mathcal{P}\\) , the test/out-of-sample error of \\(h\\) is defined by \\( \\(\\begin{aligned}\\mathcal{E}_{\\text{in}}(h) = \\frac{1}{\\mathrm{m}}\\sum_{i=1}^{\\mathrm{m}}\\text{sign}[h(\\mathrm{x}^{(i)}) \\neq f(\\mathrm{x}^{(i)})]\\end{aligned}\\) \\) Here the sign function is mainly used for binary classification, where if \\(h\\) and \\(f\\) disagrees at any point \\(x^{(i)}\\) , then \\(\\text{sign}[h(\\mathrm{x}^{(i)}) \\neq f(\\mathrm{x}^{(i)})]\\) evaluates to 1. We take the sum of all disagreements and divide by the total number of samples. In short, that is just the misclassification/error rate. The empirical error of \\(h \\in \\mathcal{H}\\) is its average error over the sample \\(\\mathcal{X}\\) , in contrast, the generalization error is its expected error based on the distribution \\(\\mathcal{P}\\) . Take careful note here that \\(h(x^{(i)})\\) is the prediction made by our hypothesis (model), we can conventionally call it \\(\\hat{y}^{(i)}\\) whereby our \\(f(x^{(i)})\\) is our ground truth label \\(y^{(i)}\\) . I believe that this ground truth label is realized once we draw the sample from \\(\\mathcal{X}\\) even though we do not know what \\(f\\) is. An additional note here, is that the summand of the in-sample error function is not fixated to the sign function. In fact, I believe you can define any loss function to calculate the \"error\". As an example, if we are dealing with regression, then we can modify the summand to our favourite Mean Squared Error. \\[\\begin{aligned}\\mathcal{E}_{\\text{in}}(h) = \\frac{1}{\\mathrm{m}}\\sum_{i=1}^{\\mathrm{m}}[h(\\mathrm{x}^{(i)}) - f(\\mathrm{x}^{(i)})]^2\\end{aligned}\\]","title":"Empirical Error/Training Error/In-Sample Error: \\(\\mathcal{E}_{\\text{in}}(h)\\)"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Appendix/#bias-variance-decomposition","text":"This is a decomposition of the Expected Generalization Error. Formal Proof please read Learning From Data. Unless otherwise stated, we consider only the univariate case where \\(\\mathrm{x}\\) is a single test point. \\[\\begin{align*} \\mathbb{E}_{\\mathcal{D}}[\\mathcal{E}_{\\text{out}}(h)] &= \\mathbb{E}_{\\mathcal{D}}[\\mathbb{E}_{\\mathrm{x}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}) - f(\\mathrm{x}))^2 \\right]] \\\\ &= \\big(\\;\\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;] - f(x)\\;\\big)^2 + \\mathbb{E}_{\\mathcal{D}}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;])^2\\;\\big] + \\mathbb{E}\\big[(y-f(x))^2\\big] \\\\ &= \\big(\\;\\bar{h}(\\mathrm{x}) - f(x)\\;\\big)^2 + \\mathbb{E}_\\mathcal{D}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\bar{h}(\\mathrm{x}) \\;])^2\\;\\big]+ \\mathbb{E}\\big[(y-f(x))^2\\big] \\end{align*} \\] Where $\\big(\\;\\mathbb{E} {\\mathcal{D}}[\\;h }(x)\\;] - f(x)\\;\\big)^2 $ is the Bias, \\(\\mathbb{E}_\\mathcal{D}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;])^2\\;\\big]\\) is the Variance and \\(\\mathbb{E}\\big[(y-f(x))^2\\big]\\) is the irreducible error \\(\\epsilon\\) .","title":"Bias - Variance Decomposition"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Appendix/#bias-bigmathbbe_mathcaldh_mathcaldx-fxbig2","text":"In other form, we can express Bias as \\( \\(\\big(\\;\\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;] - f(x)\\;\\big)^2 = \\big(\\;\\bar{h}(\\mathrm{x}) - f(x)\\;\\big)^2\\) \\) See simulation on Bias-Variance Tradeoff to understand. If our test point is \\(x_{q} = 0.9\\) , then our bias is as such: \\[ \\widehat{\\text{bias}} \\left(\\hat{f}(0.90) \\right) = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(\\hat{f}_k^{[i]}(0.90) \\right) - f(0.90) \\]","title":"Bias: \\(\\big(\\;\\mathbb{E}_\\mathcal{D}[\\;h_{\\mathcal{D}}(x)\\;] - f(x)\\;\\big)^2\\)"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Appendix/#variance-mathbbe_mathcaldbigh_mathcaldx-mathbbe_mathcaldh_mathcaldx2big","text":"This is more confusing, but we first express Variance as: \\[\\mathbb{E}_\\mathcal{D}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;])^2\\;\\big] = \\mathbb{E}_\\mathcal{D}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\bar{h}(\\mathrm{x}) \\;])^2\\;\\big]\\] If our test point is \\(x_{q} = 0.9\\) , then our variance is as such: \\[ \\widehat{\\text{var}} \\left(\\hat{f}(0.90) \\right) = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(\\hat{f}_k^{[i]}(0.90) - \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}}\\hat{f}_k^{[i]}(0.90) \\right)^2 \\]","title":"Variance: \\(\\mathbb{E}_\\mathcal{D}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;])^2\\;\\big]\\)"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Appendix/#pseudo-code","text":"","title":"Pseudo Code"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Appendix/#cross-validation","text":"Define \\(G\\) as the set of combination of hyperparamters. Define number of splits to be \\(K\\) . For each set of hyperparameter \\(z \\in Z\\) : for fold \\(j\\) in K: Set \\(F_{\\text{train}}=\\bigcup\\limits_{i\\neq k}^{K} F_{i}\\) Set \\(F_{\\text{val}} = F_{j}\\) as the validation set Perform Standard Scaling on \\(F_{\\text{train}}\\) and find the mean and std Perform VIF recursively on \\(F_{\\text{train}}\\) and find the selected features Transform \\(F_{\\text{val}}\\) using the mean and std found using \\(F_{\\text{train}}\\) Transform \\(F_{\\text{val}}\\) to have only the selected features from \\(F_{\\text{train}}\\) Train and fit on \\(F_{\\text{train}}\\) Evaluate the fitted parameters on \\(F_{\\text{val}}\\) to obtain \\(\\mathcal{M}\\)","title":"Cross-Validation"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Appendix/#logistic-regression","text":"Given target variable \\(Y \\in \\{0, 1\\}\\) and predictors \\(X\\) , denote \\(\\mathbb{P}(X) = P(Y = 1 | X)\\) to estimate the probability of \\(Y\\) is of positive (malignant) class. LR expresses \\(\\mathbb{P}\\) as a function the predictors \\(X\\) as \\(\\mathbb{P}(X) = \\sigma(\\hat{\\mathrm{\\beta}}^T X) = \\frac{1}{1 + \\exp(\\hat{\\mathrm{\\beta}}^T X)}\\) where \\(\\hat{\\beta}\\) is the estimated coefficients of the model. One thing worth mentioning is the logistic function \\(\\sigma(z) = \\frac{1}{1 + \\exp(-z)}\\) outputs values from 0 to 1 which is actually the functional form of our hypothesis, and therefore makes up the \\textbf{Hypothesis Space} \\(\\mathcal{H}\\) . We then uses a learning algorithm \\(\\mathcal{A}\\) , \\textbf{Maximum Likelihood Estimation (MLE)}, to estimate the coefficients of our predictors; however, since there is no closed form solution to MLE, the learning algorithm will use optimization techniques like \\textbf{Gradient Descent}\\footnote{We can use Gradient Descent if we instead minimze the negative loglikehood function which is the same as maximizing MLE} to find \\(\\hat{\\beta}\\) .","title":"Logistic Regression"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Appendix/#readings-and-references","text":"False-positive and false-negative cases of fine-needle aspiration cytology for palpable breast lesions What is a Dendrogram? Breast Biopsy - Mayo Clinic Data Centric - Andrew Ng When is Multicollinearity not an issue - Paul Allison Intuitive Explanation of Multicollinearity in Linear Regression - Stackoverflow Hypothesis Testing Across Models Hypothesis Test for Comparing ML Algorithms - Jason Brownlee Regression Modelling Strategies - Professor Frank Harrell Damage Caused by Classification Accuracy and Other Discontinuous Improper Accuracy Scoring Rules - Professor Frank Harrell On a reliable cross validation split 1 On a reliable cross validation split 2 Estimate Generalization Error Using Boxplot to compare Model's Performance Common Pitfalls - Scikit-Learn Common pitfalls in the interpretation of coefficients of linear models - Scikit-Learn Calibrated Classification - Jason Brownlee scikit learn calibration Are you sure your models return probabilities? cambridge's probability calibration calibration in ML Terms Brier Score and Model Calibration - Neptune AI Google's take on calibrated models IMPORTANT: WHAT IS CALIBRATION Hands on sklearn calibration Hands on sklearn calibration v2 Examples of scoring rules Logistic Regression is well calibrated","title":"Readings and References"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%200%20-%20Introduction%20and%20Problem%20Statement/","text":"Stage 0: Defining Problem and Assumptions by Hongnan Gao Introduction Note Breast cancer accounts for 30% of the cancers in females, making it the most common cancer in women. Breast cancer is also one of the most curable disease if detected early and there are many measures in place to help. Courtesy of Singapore Cancer Society In our context, we are presented with a dataset that are taken from a biopsy procedure called Fine Needle Aspiration (FNA) performed on the breast. The tissue taken from the biopsy will then be sent to a lab and be examined by a pathologist, a report will be written if cancerous cells are spotted or not and be sent to the specialist to further explain the results to the patient. However, there may be disagreements whereby the pathologist report shows there are no signs of cancerous cells, while radiologist may disagree as he/she might find suspicious lesions from the mammogram/CT/MRI scans. This can happen if the biopsy taken is only on the benign cells and if there is dispute, a more thorough of biopsy may be performed again. Although our aim in Machine Learning is to classify whether a tumor is benign or malignant, we should bear in mind that we are not trying to dispute the expertise of the doctors/pathologists/radiologists. Instead, we develop models to aid their understanding, and also to come up with a more systematic benchmark for one to refer to. More concretely, the dataset has features that are computed from a digitized image from FNA , and each observation describes statistics/characteristics of the cell nucleus. There are 10 base features, and 3 different measurements are taken for each feature, namely, the mean, standard error and the \"worst/largest\" . One thing to note is that worst means the mean of the three largest values . Attribute Information: ID number Diagnosis (M = malignant, B = benign) Ten real-valued features are computed for each cell nucleus: radius (mean of distances from center to points on the perimeter) texture (standard deviation of gray-scale values) perimeter area smoothness (local variation in radius lengths) compactness ( \\(\\text{perimeter}^2 / \\text{area} - 1.0\\) ) concavity (severity of concave portions of the contour) concave points (number of concave portions of the contour) symmetry fractal dimension (\"coastline approximation\" - 1) With these in mind, let us move on to defining the problem and state some initial assumptions. Problem Statement Informal Description To develop a Machine Learning Model that can classify whether a tumor is benign or malignant. We also note that we care more about whether a cancer patient is classified correctly. Formal Description Given a dataset \\(\\mathcal{D}\\) describing characteristics of a tumor, the task \\(\\mathcal{T}\\) is a binary classification problem where we aim to find an optimal hypothesis \\(g \\in \\mathcal{H}\\) using a learning algorithm \\(\\mathcal{A}\\) . The optimal hypothesis \\(g\\) should generalize well, that is to say, has a low expected generalization error \\(\\mathcal{E}\\) over a performance measure \\(\\mathrm{M}\\) . We will choose the performance measure in the later sections (not accuracy). Considerations Info Size of Dataset: The dataset is not too large, we need to be wary of an overly complex model which may easily overfit, but may not generalize well. Model Interpretation: There is a tradeoff between Model's complexity/flexibility and it's interpretability. If we need to explain our model to our business stakeholders, then it is a good idea to choose a model that can be interpreted well, models like Logistic Regression with Lasso may be a good choice as the model itself has better interpretation, and with lasso we can reduce the number of features. If we only care about our model's ability to predict, then interpretability may not be so important and we may choose a model that performs well, but the weights may be more difficult to understand. Time and Space Complexity: Practically speaking, we need to strike a balance between the speed of the training and the performance measure of the model. Data Centric vs Model Centric: From the one and only Andrew Ng 1 we understood that data plays a critical role in the Machine Learning world. https://analyticsindiamag.com/big-data-to-good-data-andrew-ng-urges-ml-community-to-be-more-data-centric-and-less-model-centric/ \u21a9","title":"Introduction"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%200%20-%20Introduction%20and%20Problem%20Statement/#introduction","text":"Note Breast cancer accounts for 30% of the cancers in females, making it the most common cancer in women. Breast cancer is also one of the most curable disease if detected early and there are many measures in place to help. Courtesy of Singapore Cancer Society In our context, we are presented with a dataset that are taken from a biopsy procedure called Fine Needle Aspiration (FNA) performed on the breast. The tissue taken from the biopsy will then be sent to a lab and be examined by a pathologist, a report will be written if cancerous cells are spotted or not and be sent to the specialist to further explain the results to the patient. However, there may be disagreements whereby the pathologist report shows there are no signs of cancerous cells, while radiologist may disagree as he/she might find suspicious lesions from the mammogram/CT/MRI scans. This can happen if the biopsy taken is only on the benign cells and if there is dispute, a more thorough of biopsy may be performed again. Although our aim in Machine Learning is to classify whether a tumor is benign or malignant, we should bear in mind that we are not trying to dispute the expertise of the doctors/pathologists/radiologists. Instead, we develop models to aid their understanding, and also to come up with a more systematic benchmark for one to refer to. More concretely, the dataset has features that are computed from a digitized image from FNA , and each observation describes statistics/characteristics of the cell nucleus. There are 10 base features, and 3 different measurements are taken for each feature, namely, the mean, standard error and the \"worst/largest\" . One thing to note is that worst means the mean of the three largest values . Attribute Information: ID number Diagnosis (M = malignant, B = benign) Ten real-valued features are computed for each cell nucleus: radius (mean of distances from center to points on the perimeter) texture (standard deviation of gray-scale values) perimeter area smoothness (local variation in radius lengths) compactness ( \\(\\text{perimeter}^2 / \\text{area} - 1.0\\) ) concavity (severity of concave portions of the contour) concave points (number of concave portions of the contour) symmetry fractal dimension (\"coastline approximation\" - 1) With these in mind, let us move on to defining the problem and state some initial assumptions.","title":"Introduction"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%200%20-%20Introduction%20and%20Problem%20Statement/#problem-statement","text":"Informal Description To develop a Machine Learning Model that can classify whether a tumor is benign or malignant. We also note that we care more about whether a cancer patient is classified correctly. Formal Description Given a dataset \\(\\mathcal{D}\\) describing characteristics of a tumor, the task \\(\\mathcal{T}\\) is a binary classification problem where we aim to find an optimal hypothesis \\(g \\in \\mathcal{H}\\) using a learning algorithm \\(\\mathcal{A}\\) . The optimal hypothesis \\(g\\) should generalize well, that is to say, has a low expected generalization error \\(\\mathcal{E}\\) over a performance measure \\(\\mathrm{M}\\) . We will choose the performance measure in the later sections (not accuracy).","title":"Problem Statement"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%200%20-%20Introduction%20and%20Problem%20Statement/#considerations","text":"Info Size of Dataset: The dataset is not too large, we need to be wary of an overly complex model which may easily overfit, but may not generalize well. Model Interpretation: There is a tradeoff between Model's complexity/flexibility and it's interpretability. If we need to explain our model to our business stakeholders, then it is a good idea to choose a model that can be interpreted well, models like Logistic Regression with Lasso may be a good choice as the model itself has better interpretation, and with lasso we can reduce the number of features. If we only care about our model's ability to predict, then interpretability may not be so important and we may choose a model that performs well, but the weights may be more difficult to understand. Time and Space Complexity: Practically speaking, we need to strike a balance between the speed of the training and the performance measure of the model. Data Centric vs Model Centric: From the one and only Andrew Ng 1 we understood that data plays a critical role in the Machine Learning world. https://analyticsindiamag.com/big-data-to-good-data-andrew-ng-urges-ml-community-to-be-more-data-centric-and-less-model-centric/ \u21a9","title":"Considerations"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%201%20-%20Preliminary%20Data%20Inspection%20and%20Cleaning/","text":"Stage 1: Preliminary Data Inspection and Clearning by Hongnan Gao Quick Navigation Dependencies and Configuration Stage 1: Preliminary Data Inspection and Cleaning Load the dataset A brief look at the dataset Drop, drop, drop the columns! Data Types Summary Statistics Missing Data Save Data Dependencies and Configuration import random from dataclasses import dataclass , field from typing import List , Dict import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns @dataclass class config : raw_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/raw/data.csv\" processed_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/processed.csv\" train_size : float = 0.9 seed : int = 1992 num_folds : int = 5 cv_schema : str = \"StratifiedKFold\" classification_type : str = \"binary\" target_col : List [ str ] = field ( default_factory = lambda : [ \"diagnosis\" ]) unwanted_cols : List [ str ] = field ( default_factory = lambda : [ \"id\" , \"Unnamed: 32\" ]) # Plotting colors : List [ str ] = field ( default_factory = lambda : [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ] ) cmap_reversed = plt . cm . get_cmap ( \"mako_r\" ) def to_dict ( self ) -> Dict : \"\"\"Convert the config object to a dictionary. Returns: Dict: The config object as a dictionary. \"\"\" return { \"raw_data\" : self . raw_data , \"processed_data\" : self . processed_data , \"train_size\" : self . train_size , \"seed\" : self . seed , \"num_folds\" : self . num_folds , \"cv_schema\" : self . cv_schema , \"classification_type\" : self . classification_type , \"target_col\" : self . target_col , \"unwanted_cols\" : self . unwanted_cols , \"colors\" : self . colors , \"cmap_reversed\" : self . cmap_reversed , } def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) # set config config = config () # set seeding for reproducibility _ = set_seeds ( seed = config . seed ) Stage 1: Preliminary Data Inspection and Cleaning Load the dataset df = pd . read_csv ( config . raw_data ) A brief look at the dataset Info We will query the first five rows of the dataframe to get a feel on the dataset we are working on. We also call df.info() to see the data types of the columns, and to briefly check if there is any missing values in our data (more on that later). # Column Non-Null Count Dtype --- ------ -------------- ----- 0 diagnosis 569 non - null int64 1 radius_mean 569 non - null float64 2 texture_mean 569 non - null float64 3 perimeter_mean 569 non - null float64 Importance of data types We must be sharp and ensure that each column is indeed stored in their respective data types! In the real world, we may often query \"dirty\" data from say, the database, where numeric data are represented in string. It is now our duty to ensure sanity checks are in place! display ( df . head ()) display ( df . info ( verbose = True )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave points_mean symmetry_mean fractal_dimension_mean radius_se texture_se perimeter_se area_se smoothness_se compactness_se concavity_se concave points_se symmetry_se fractal_dimension_se radius_worst texture_worst perimeter_worst area_worst smoothness_worst compactness_worst concavity_worst concave points_worst symmetry_worst fractal_dimension_worst Unnamed: 32 0 842302 M 17.99 10.38 122.80 1001.0 0.11840 0.27760 0.3001 0.14710 0.2419 0.07871 1.0950 0.9053 8.589 153.40 0.006399 0.04904 0.05373 0.01587 0.03003 0.006193 25.38 17.33 184.60 2019.0 0.1622 0.6656 0.7119 0.2654 0.4601 0.11890 NaN 1 842517 M 20.57 17.77 132.90 1326.0 0.08474 0.07864 0.0869 0.07017 0.1812 0.05667 0.5435 0.7339 3.398 74.08 0.005225 0.01308 0.01860 0.01340 0.01389 0.003532 24.99 23.41 158.80 1956.0 0.1238 0.1866 0.2416 0.1860 0.2750 0.08902 NaN 2 84300903 M 19.69 21.25 130.00 1203.0 0.10960 0.15990 0.1974 0.12790 0.2069 0.05999 0.7456 0.7869 4.585 94.03 0.006150 0.04006 0.03832 0.02058 0.02250 0.004571 23.57 25.53 152.50 1709.0 0.1444 0.4245 0.4504 0.2430 0.3613 0.08758 NaN 3 84348301 M 11.42 20.38 77.58 386.1 0.14250 0.28390 0.2414 0.10520 0.2597 0.09744 0.4956 1.1560 3.445 27.23 0.009110 0.07458 0.05661 0.01867 0.05963 0.009208 14.91 26.50 98.87 567.7 0.2098 0.8663 0.6869 0.2575 0.6638 0.17300 NaN 4 84358402 M 20.29 14.34 135.10 1297.0 0.10030 0.13280 0.1980 0.10430 0.1809 0.05883 0.7572 0.7813 5.438 94.44 0.011490 0.02461 0.05688 0.01885 0.01756 0.005115 22.54 16.67 152.20 1575.0 0.1374 0.2050 0.4000 0.1625 0.2364 0.07678 NaN <class 'pandas.core.frame.DataFrame'> RangeIndex: 569 entries, 0 to 568 Data columns (total 33 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 id 569 non-null int64 1 diagnosis 569 non-null object 2 radius_mean 569 non-null float64 3 texture_mean 569 non-null float64 4 perimeter_mean 569 non-null float64 5 area_mean 569 non-null float64 6 smoothness_mean 569 non-null float64 7 compactness_mean 569 non-null float64 8 concavity_mean 569 non-null float64 9 concave points_mean 569 non-null float64 10 symmetry_mean 569 non-null float64 11 fractal_dimension_mean 569 non-null float64 12 radius_se 569 non-null float64 13 texture_se 569 non-null float64 14 perimeter_se 569 non-null float64 15 area_se 569 non-null float64 16 smoothness_se 569 non-null float64 17 compactness_se 569 non-null float64 18 concavity_se 569 non-null float64 19 concave points_se 569 non-null float64 20 symmetry_se 569 non-null float64 21 fractal_dimension_se 569 non-null float64 22 radius_worst 569 non-null float64 23 texture_worst 569 non-null float64 24 perimeter_worst 569 non-null float64 25 area_worst 569 non-null float64 26 smoothness_worst 569 non-null float64 27 compactness_worst 569 non-null float64 28 concavity_worst 569 non-null float64 29 concave points_worst 569 non-null float64 30 symmetry_worst 569 non-null float64 31 fractal_dimension_worst 569 non-null float64 32 Unnamed: 32 0 non-null float64 dtypes: float64(31), int64(1), object(1) memory usage: 146.8+ KB None A brief overview tells us our data is alright! There is, however, a column which is unnamed and has no values. This can be of various data source issues, for now, we quickly check the definition given by the dataset from UCI's Breast Cancer Wisconsin (Diagnostic) Data Set and confirm that there should only be 32 columns. With this in mind, we can safely delete the column. We also note that from the above, that the id column is the identifier for each patient. We will also drop this column as it holds no predictive power. When can ID be important? We should try to question our move and justify it. In this dataset, we have to ensure that each ID is unique, if it is not, it may suggest that there are patient records with multiple observation, which is a violation of i.i.d assumption and we may take note when doing cross-validation, so as to avoid information leakage. Since the ID column is unique, we will delete it. We will keep this at the back of our mind in the event that we ever need them for feature engineering. print ( f \"The ID column is unique : { df [ 'id' ] . is_unique } \" ) The ID column is unique : True Drop, drop, drop the columns! Here we define a drop_columns function to drop the unwanted columns. def drop_columns ( df : pd . DataFrame , columns : List ) -> pd . DataFrame : \"\"\"Drop unwanted columns from dataframe. Args: df (pd.DataFrame): Dataframe to be cleaned columns (List): list of columns to be dropped Returns: df_copy (pd.DataFrame): Dataframe with unwanted columns dropped. \"\"\" df_copy = df . copy () df_copy = df_copy . drop ( columns = columns , axis = 1 , inplace = False ) return df_copy . reset_index ( drop = True ) df = drop_columns ( df , columns = config . unwanted_cols ) Data Types Let us split the data types into a few unbrellas: Info Categorical Variables: diagnosis: The target variable diagnosis, although represented as a string in the dataframe, should be categorical! This is because machines do not really like working with \"strings\" and prefer your type to be of \"numbers\". We will map them to 0 and 1, representing benign and malignant respectively. Since the target variable is just two unique values, we can use a simple map from pandas to do the job. class_dict = { \"B\" : 0 , \"M\" : 1 } df [ 'diagnosis' ] = df [ 'diagnosis' ] . map ( class_dict ) We will make sure that our mapping is accurate by asserting the following. assert df [ 'diagnosis' ] . value_counts () . to_dict ()[ 0 ] == 357 assert df [ 'diagnosis' ] . value_counts () . to_dict ()[ 1 ] == 212 Info Continuous Variables: A preliminary look seems to suggest all our predictors are continuous. Success From the brief overview, there does not seem to be any Ordinal or Nominal Predictors. This suggest that we may not need to perform encoding in our preprocessing. Summary Statistics We will use a simple, yet powerful function call to check on the summary statistics of our dataframe. We note to the readers that there are much more powerful libraries like pandas-profiling to give us an even more thorough summary, but for our purpose, we will use the good ol' df.describe() . display ( df . describe ( include = 'all' )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave points_mean symmetry_mean fractal_dimension_mean radius_se texture_se perimeter_se area_se smoothness_se compactness_se concavity_se concave points_se symmetry_se fractal_dimension_se radius_worst texture_worst perimeter_worst area_worst smoothness_worst compactness_worst concavity_worst concave points_worst symmetry_worst fractal_dimension_worst count 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 mean 0.372583 14.127292 19.289649 91.969033 654.889104 0.096360 0.104341 0.088799 0.048919 0.181162 0.062798 0.405172 1.216853 2.866059 40.337079 0.007041 0.025478 0.031894 0.011796 0.020542 0.003795 16.269190 25.677223 107.261213 880.583128 0.132369 0.254265 0.272188 0.114606 0.290076 0.083946 std 0.483918 3.524049 4.301036 24.298981 351.914129 0.014064 0.052813 0.079720 0.038803 0.027414 0.007060 0.277313 0.551648 2.021855 45.491006 0.003003 0.017908 0.030186 0.006170 0.008266 0.002646 4.833242 6.146258 33.602542 569.356993 0.022832 0.157336 0.208624 0.065732 0.061867 0.018061 min 0.000000 6.981000 9.710000 43.790000 143.500000 0.052630 0.019380 0.000000 0.000000 0.106000 0.049960 0.111500 0.360200 0.757000 6.802000 0.001713 0.002252 0.000000 0.000000 0.007882 0.000895 7.930000 12.020000 50.410000 185.200000 0.071170 0.027290 0.000000 0.000000 0.156500 0.055040 25% 0.000000 11.700000 16.170000 75.170000 420.300000 0.086370 0.064920 0.029560 0.020310 0.161900 0.057700 0.232400 0.833900 1.606000 17.850000 0.005169 0.013080 0.015090 0.007638 0.015160 0.002248 13.010000 21.080000 84.110000 515.300000 0.116600 0.147200 0.114500 0.064930 0.250400 0.071460 50% 0.000000 13.370000 18.840000 86.240000 551.100000 0.095870 0.092630 0.061540 0.033500 0.179200 0.061540 0.324200 1.108000 2.287000 24.530000 0.006380 0.020450 0.025890 0.010930 0.018730 0.003187 14.970000 25.410000 97.660000 686.500000 0.131300 0.211900 0.226700 0.099930 0.282200 0.080040 75% 1.000000 15.780000 21.800000 104.100000 782.700000 0.105300 0.130400 0.130700 0.074000 0.195700 0.066120 0.478900 1.474000 3.357000 45.190000 0.008146 0.032450 0.042050 0.014710 0.023480 0.004558 18.790000 29.720000 125.400000 1084.000000 0.146000 0.339100 0.382900 0.161400 0.317900 0.092080 max 1.000000 28.110000 39.280000 188.500000 2501.000000 0.163400 0.345400 0.426800 0.201200 0.304000 0.097440 2.873000 4.885000 21.980000 542.200000 0.031130 0.135400 0.396000 0.052790 0.078950 0.029840 36.040000 49.540000 251.200000 4254.000000 0.222600 1.058000 1.252000 0.291000 0.663800 0.207500 The table does give us a good overview: for example, a brief glance give me the following observations: The features do not seem to be of the same scale . This is going to be a problem as some models do not perform well if your features are not on the same scale. A prime example is a KNN model with Euclidean Distance as the distance metric, the difference in range of different features will be amplified with the squared term, and the feature with wider range will dominate the one with smaller range. From our dataset it we see that area_mean is very large and there is likely to be a squared term (possibly from radius_mean ), we can look into them later through EDA. Humans are more visual and that is why we still need EDA later to capture our attention on any anomaly from the dataset, and of course, if the dataset has many columns, then this summary statistics may even clog your progress if you were to read it line by line. Missing Data Missing Alert? Although from our analysis, we did not see any missing data, it is always good to remind ourselves to check it. A simple function that does the job is as follows. def report_missing ( df : pd . DataFrame , columns : List ) -> pd . DataFrame : \"\"\"A function to check for missing data. Args: df (pd.DataFrame): The DataFrame to check. columns (List): The columns to check. Returns: missing_data_df (pd.DataFrame): Returns a DataFrame that reports missing data. \"\"\" missing_dict = { \"missing num\" : [], \"missing percentage\" : []} for col in columns : num_missing = df [ col ] . isnull () . sum () percentage_missing = num_missing / len ( df ) missing_dict [ \"missing num\" ] . append ( num_missing ) missing_dict [ \"missing percentage\" ] . append ( percentage_missing ) missing_data_df = pd . DataFrame ( index = columns , data = missing_dict ) return missing_data_df missing_df = report_missing ( df , columns = df . columns ) display ( missing_df . head ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } missing num missing percentage diagnosis 0 0.0 radius_mean 0 0.0 texture_mean 0 0.0 perimeter_mean 0 0.0 area_mean 0 0.0 Save data We save the data to processed and we can call it later on in subsequent notebooks.","title":"Preliminary Data Inspection and Cleaning"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%201%20-%20Preliminary%20Data%20Inspection%20and%20Cleaning/#dependencies-and-configuration","text":"import random from dataclasses import dataclass , field from typing import List , Dict import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns @dataclass class config : raw_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/raw/data.csv\" processed_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/processed.csv\" train_size : float = 0.9 seed : int = 1992 num_folds : int = 5 cv_schema : str = \"StratifiedKFold\" classification_type : str = \"binary\" target_col : List [ str ] = field ( default_factory = lambda : [ \"diagnosis\" ]) unwanted_cols : List [ str ] = field ( default_factory = lambda : [ \"id\" , \"Unnamed: 32\" ]) # Plotting colors : List [ str ] = field ( default_factory = lambda : [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ] ) cmap_reversed = plt . cm . get_cmap ( \"mako_r\" ) def to_dict ( self ) -> Dict : \"\"\"Convert the config object to a dictionary. Returns: Dict: The config object as a dictionary. \"\"\" return { \"raw_data\" : self . raw_data , \"processed_data\" : self . processed_data , \"train_size\" : self . train_size , \"seed\" : self . seed , \"num_folds\" : self . num_folds , \"cv_schema\" : self . cv_schema , \"classification_type\" : self . classification_type , \"target_col\" : self . target_col , \"unwanted_cols\" : self . unwanted_cols , \"colors\" : self . colors , \"cmap_reversed\" : self . cmap_reversed , } def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) # set config config = config () # set seeding for reproducibility _ = set_seeds ( seed = config . seed )","title":"Dependencies and Configuration"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%201%20-%20Preliminary%20Data%20Inspection%20and%20Cleaning/#stage-1-preliminary-data-inspection-and-cleaning","text":"","title":"Stage 1: Preliminary Data Inspection and Cleaning"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%201%20-%20Preliminary%20Data%20Inspection%20and%20Cleaning/#load-the-dataset","text":"df = pd . read_csv ( config . raw_data )","title":"Load the dataset"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%201%20-%20Preliminary%20Data%20Inspection%20and%20Cleaning/#a-brief-look-at-the-dataset","text":"Info We will query the first five rows of the dataframe to get a feel on the dataset we are working on. We also call df.info() to see the data types of the columns, and to briefly check if there is any missing values in our data (more on that later). # Column Non-Null Count Dtype --- ------ -------------- ----- 0 diagnosis 569 non - null int64 1 radius_mean 569 non - null float64 2 texture_mean 569 non - null float64 3 perimeter_mean 569 non - null float64 Importance of data types We must be sharp and ensure that each column is indeed stored in their respective data types! In the real world, we may often query \"dirty\" data from say, the database, where numeric data are represented in string. It is now our duty to ensure sanity checks are in place! display ( df . head ()) display ( df . info ( verbose = True )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave points_mean symmetry_mean fractal_dimension_mean radius_se texture_se perimeter_se area_se smoothness_se compactness_se concavity_se concave points_se symmetry_se fractal_dimension_se radius_worst texture_worst perimeter_worst area_worst smoothness_worst compactness_worst concavity_worst concave points_worst symmetry_worst fractal_dimension_worst Unnamed: 32 0 842302 M 17.99 10.38 122.80 1001.0 0.11840 0.27760 0.3001 0.14710 0.2419 0.07871 1.0950 0.9053 8.589 153.40 0.006399 0.04904 0.05373 0.01587 0.03003 0.006193 25.38 17.33 184.60 2019.0 0.1622 0.6656 0.7119 0.2654 0.4601 0.11890 NaN 1 842517 M 20.57 17.77 132.90 1326.0 0.08474 0.07864 0.0869 0.07017 0.1812 0.05667 0.5435 0.7339 3.398 74.08 0.005225 0.01308 0.01860 0.01340 0.01389 0.003532 24.99 23.41 158.80 1956.0 0.1238 0.1866 0.2416 0.1860 0.2750 0.08902 NaN 2 84300903 M 19.69 21.25 130.00 1203.0 0.10960 0.15990 0.1974 0.12790 0.2069 0.05999 0.7456 0.7869 4.585 94.03 0.006150 0.04006 0.03832 0.02058 0.02250 0.004571 23.57 25.53 152.50 1709.0 0.1444 0.4245 0.4504 0.2430 0.3613 0.08758 NaN 3 84348301 M 11.42 20.38 77.58 386.1 0.14250 0.28390 0.2414 0.10520 0.2597 0.09744 0.4956 1.1560 3.445 27.23 0.009110 0.07458 0.05661 0.01867 0.05963 0.009208 14.91 26.50 98.87 567.7 0.2098 0.8663 0.6869 0.2575 0.6638 0.17300 NaN 4 84358402 M 20.29 14.34 135.10 1297.0 0.10030 0.13280 0.1980 0.10430 0.1809 0.05883 0.7572 0.7813 5.438 94.44 0.011490 0.02461 0.05688 0.01885 0.01756 0.005115 22.54 16.67 152.20 1575.0 0.1374 0.2050 0.4000 0.1625 0.2364 0.07678 NaN <class 'pandas.core.frame.DataFrame'> RangeIndex: 569 entries, 0 to 568 Data columns (total 33 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 id 569 non-null int64 1 diagnosis 569 non-null object 2 radius_mean 569 non-null float64 3 texture_mean 569 non-null float64 4 perimeter_mean 569 non-null float64 5 area_mean 569 non-null float64 6 smoothness_mean 569 non-null float64 7 compactness_mean 569 non-null float64 8 concavity_mean 569 non-null float64 9 concave points_mean 569 non-null float64 10 symmetry_mean 569 non-null float64 11 fractal_dimension_mean 569 non-null float64 12 radius_se 569 non-null float64 13 texture_se 569 non-null float64 14 perimeter_se 569 non-null float64 15 area_se 569 non-null float64 16 smoothness_se 569 non-null float64 17 compactness_se 569 non-null float64 18 concavity_se 569 non-null float64 19 concave points_se 569 non-null float64 20 symmetry_se 569 non-null float64 21 fractal_dimension_se 569 non-null float64 22 radius_worst 569 non-null float64 23 texture_worst 569 non-null float64 24 perimeter_worst 569 non-null float64 25 area_worst 569 non-null float64 26 smoothness_worst 569 non-null float64 27 compactness_worst 569 non-null float64 28 concavity_worst 569 non-null float64 29 concave points_worst 569 non-null float64 30 symmetry_worst 569 non-null float64 31 fractal_dimension_worst 569 non-null float64 32 Unnamed: 32 0 non-null float64 dtypes: float64(31), int64(1), object(1) memory usage: 146.8+ KB None A brief overview tells us our data is alright! There is, however, a column which is unnamed and has no values. This can be of various data source issues, for now, we quickly check the definition given by the dataset from UCI's Breast Cancer Wisconsin (Diagnostic) Data Set and confirm that there should only be 32 columns. With this in mind, we can safely delete the column. We also note that from the above, that the id column is the identifier for each patient. We will also drop this column as it holds no predictive power. When can ID be important? We should try to question our move and justify it. In this dataset, we have to ensure that each ID is unique, if it is not, it may suggest that there are patient records with multiple observation, which is a violation of i.i.d assumption and we may take note when doing cross-validation, so as to avoid information leakage. Since the ID column is unique, we will delete it. We will keep this at the back of our mind in the event that we ever need them for feature engineering. print ( f \"The ID column is unique : { df [ 'id' ] . is_unique } \" ) The ID column is unique : True","title":"A brief look at the dataset"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%201%20-%20Preliminary%20Data%20Inspection%20and%20Cleaning/#drop-drop-drop-the-columns","text":"Here we define a drop_columns function to drop the unwanted columns. def drop_columns ( df : pd . DataFrame , columns : List ) -> pd . DataFrame : \"\"\"Drop unwanted columns from dataframe. Args: df (pd.DataFrame): Dataframe to be cleaned columns (List): list of columns to be dropped Returns: df_copy (pd.DataFrame): Dataframe with unwanted columns dropped. \"\"\" df_copy = df . copy () df_copy = df_copy . drop ( columns = columns , axis = 1 , inplace = False ) return df_copy . reset_index ( drop = True ) df = drop_columns ( df , columns = config . unwanted_cols )","title":"Drop, drop, drop the columns!"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%201%20-%20Preliminary%20Data%20Inspection%20and%20Cleaning/#data-types","text":"Let us split the data types into a few unbrellas: Info Categorical Variables: diagnosis: The target variable diagnosis, although represented as a string in the dataframe, should be categorical! This is because machines do not really like working with \"strings\" and prefer your type to be of \"numbers\". We will map them to 0 and 1, representing benign and malignant respectively. Since the target variable is just two unique values, we can use a simple map from pandas to do the job. class_dict = { \"B\" : 0 , \"M\" : 1 } df [ 'diagnosis' ] = df [ 'diagnosis' ] . map ( class_dict ) We will make sure that our mapping is accurate by asserting the following. assert df [ 'diagnosis' ] . value_counts () . to_dict ()[ 0 ] == 357 assert df [ 'diagnosis' ] . value_counts () . to_dict ()[ 1 ] == 212 Info Continuous Variables: A preliminary look seems to suggest all our predictors are continuous. Success From the brief overview, there does not seem to be any Ordinal or Nominal Predictors. This suggest that we may not need to perform encoding in our preprocessing.","title":"Data Types"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%201%20-%20Preliminary%20Data%20Inspection%20and%20Cleaning/#summary-statistics","text":"We will use a simple, yet powerful function call to check on the summary statistics of our dataframe. We note to the readers that there are much more powerful libraries like pandas-profiling to give us an even more thorough summary, but for our purpose, we will use the good ol' df.describe() . display ( df . describe ( include = 'all' )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave points_mean symmetry_mean fractal_dimension_mean radius_se texture_se perimeter_se area_se smoothness_se compactness_se concavity_se concave points_se symmetry_se fractal_dimension_se radius_worst texture_worst perimeter_worst area_worst smoothness_worst compactness_worst concavity_worst concave points_worst symmetry_worst fractal_dimension_worst count 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 mean 0.372583 14.127292 19.289649 91.969033 654.889104 0.096360 0.104341 0.088799 0.048919 0.181162 0.062798 0.405172 1.216853 2.866059 40.337079 0.007041 0.025478 0.031894 0.011796 0.020542 0.003795 16.269190 25.677223 107.261213 880.583128 0.132369 0.254265 0.272188 0.114606 0.290076 0.083946 std 0.483918 3.524049 4.301036 24.298981 351.914129 0.014064 0.052813 0.079720 0.038803 0.027414 0.007060 0.277313 0.551648 2.021855 45.491006 0.003003 0.017908 0.030186 0.006170 0.008266 0.002646 4.833242 6.146258 33.602542 569.356993 0.022832 0.157336 0.208624 0.065732 0.061867 0.018061 min 0.000000 6.981000 9.710000 43.790000 143.500000 0.052630 0.019380 0.000000 0.000000 0.106000 0.049960 0.111500 0.360200 0.757000 6.802000 0.001713 0.002252 0.000000 0.000000 0.007882 0.000895 7.930000 12.020000 50.410000 185.200000 0.071170 0.027290 0.000000 0.000000 0.156500 0.055040 25% 0.000000 11.700000 16.170000 75.170000 420.300000 0.086370 0.064920 0.029560 0.020310 0.161900 0.057700 0.232400 0.833900 1.606000 17.850000 0.005169 0.013080 0.015090 0.007638 0.015160 0.002248 13.010000 21.080000 84.110000 515.300000 0.116600 0.147200 0.114500 0.064930 0.250400 0.071460 50% 0.000000 13.370000 18.840000 86.240000 551.100000 0.095870 0.092630 0.061540 0.033500 0.179200 0.061540 0.324200 1.108000 2.287000 24.530000 0.006380 0.020450 0.025890 0.010930 0.018730 0.003187 14.970000 25.410000 97.660000 686.500000 0.131300 0.211900 0.226700 0.099930 0.282200 0.080040 75% 1.000000 15.780000 21.800000 104.100000 782.700000 0.105300 0.130400 0.130700 0.074000 0.195700 0.066120 0.478900 1.474000 3.357000 45.190000 0.008146 0.032450 0.042050 0.014710 0.023480 0.004558 18.790000 29.720000 125.400000 1084.000000 0.146000 0.339100 0.382900 0.161400 0.317900 0.092080 max 1.000000 28.110000 39.280000 188.500000 2501.000000 0.163400 0.345400 0.426800 0.201200 0.304000 0.097440 2.873000 4.885000 21.980000 542.200000 0.031130 0.135400 0.396000 0.052790 0.078950 0.029840 36.040000 49.540000 251.200000 4254.000000 0.222600 1.058000 1.252000 0.291000 0.663800 0.207500 The table does give us a good overview: for example, a brief glance give me the following observations: The features do not seem to be of the same scale . This is going to be a problem as some models do not perform well if your features are not on the same scale. A prime example is a KNN model with Euclidean Distance as the distance metric, the difference in range of different features will be amplified with the squared term, and the feature with wider range will dominate the one with smaller range. From our dataset it we see that area_mean is very large and there is likely to be a squared term (possibly from radius_mean ), we can look into them later through EDA. Humans are more visual and that is why we still need EDA later to capture our attention on any anomaly from the dataset, and of course, if the dataset has many columns, then this summary statistics may even clog your progress if you were to read it line by line.","title":"Summary Statistics"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%201%20-%20Preliminary%20Data%20Inspection%20and%20Cleaning/#missing-data","text":"Missing Alert? Although from our analysis, we did not see any missing data, it is always good to remind ourselves to check it. A simple function that does the job is as follows. def report_missing ( df : pd . DataFrame , columns : List ) -> pd . DataFrame : \"\"\"A function to check for missing data. Args: df (pd.DataFrame): The DataFrame to check. columns (List): The columns to check. Returns: missing_data_df (pd.DataFrame): Returns a DataFrame that reports missing data. \"\"\" missing_dict = { \"missing num\" : [], \"missing percentage\" : []} for col in columns : num_missing = df [ col ] . isnull () . sum () percentage_missing = num_missing / len ( df ) missing_dict [ \"missing num\" ] . append ( num_missing ) missing_dict [ \"missing percentage\" ] . append ( percentage_missing ) missing_data_df = pd . DataFrame ( index = columns , data = missing_dict ) return missing_data_df missing_df = report_missing ( df , columns = df . columns ) display ( missing_df . head ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } missing num missing percentage diagnosis 0 0.0 radius_mean 0 0.0 texture_mean 0 0.0 perimeter_mean 0 0.0 area_mean 0 0.0","title":"Missing Data"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%201%20-%20Preliminary%20Data%20Inspection%20and%20Cleaning/#save-data","text":"We save the data to processed and we can call it later on in subsequent notebooks.","title":"Save data"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%202%20-%20Preliminary%20EDA/","text":"Stage 2: Preliminary EDA by Hongnan Gao Quick Navigation Dependencies and Configuration Stage 2: EDA: Preliminary Stage Distribution of Target and Predictor Target Distribution Univariate Distribution of Predictors Histogram and KDE Distribution Box Plots Correlation Plots Heatmap Cluster Plot Bivariate Analysis PCE and TSNE Save the Data Dependencies and Configuration import random from dataclasses import dataclass , field from typing import Dict , List import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from scipy.cluster import hierarchy from scipy.spatial.distance import squareform from scipy.stats import pearsonr , spearmanr from sklearn import decomposition , manifold , preprocessing @dataclass class config : raw_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/raw/data.csv\" processed_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/processed.csv\" train_size : float = 0.9 seed : int = 1992 num_folds : int = 5 cv_schema : str = \"StratifiedKFold\" classification_type : str = \"binary\" target_col : List [ str ] = field ( default_factory = lambda : [ \"diagnosis\" ]) unwanted_cols : List [ str ] = field ( default_factory = lambda : [ \"id\" , \"Unnamed: 32\" ]) # Plotting colors : List [ str ] = field ( default_factory = lambda : [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ] ) cmap_reversed = plt . cm . get_cmap ( \"mako_r\" ) def to_dict ( self ) -> Dict : \"\"\"Convert the config object to a dictionary. Returns: Dict: The config object as a dictionary. \"\"\" return { \"raw_data\" : self . raw_data , \"processed_data\" : self . processed_data , \"train_size\" : self . train_size , \"seed\" : self . seed , \"num_folds\" : self . num_folds , \"cv_schema\" : self . cv_schema , \"classification_type\" : self . classification_type , \"target_col\" : self . target_col , \"unwanted_cols\" : self . unwanted_cols , \"colors\" : self . colors , \"cmap_reversed\" : self . cmap_reversed , } def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) # set config config = config () # set seeding for reproducibility _ = set_seeds ( seed = config . seed ) # read data df = pd . read_csv ( config . processed_data ) Stage 2: EDA (Preliminary Stage) Terminology Alert I call this stage the preliminary EDA for the following reasons (but not limited to!) - EDA is an iterative process! Data is never clean! So we scrub -> send in for modelling -> scrub -> get new data -> scrub scrub scrub again! - Typically, I do a basic EDA analysis after Stage 1. As an example, in a classical regression setting, I would check for some assumptions like normality checks. And if some variables are of a \"bad distribution\", we can perform a transformation on it first, and plot them again in the next round of EDA to check if we corrected the issues. The below image summarizes the process. Courtesy of Farcaster In summary, we will do the following EDA here: Target and Predictors Distribution. Univariate Analysis. Bivariate Analysis. Multivariate Analysis. Some fancy looking Dimensionality Reduction Techniques to see how our features can be visualized in lower dimensions. Distribution of Target and Predictor Target Let us first visualize our distribution of target variable. Since the target is discrete, we will just plot a simple count plot to check. Class Imbalance For Classification problems, we often plot the target count to get a good gauge on how imbalanced the dataset is. From there, we can think of whether the class imbalance is going to be a factor in our modelling process. df [ 'diagnosis' ] . value_counts ( normalize = True ) * 100 Malignant : 37.3 % Benign : 62.7 % There is to certain degree a class imbalance issue, although this is not what I have envisioned in the first place. After all, it is just natural that there should be more negative (benign) than positive (malignant) cases, given that in the general population, there are much more people without cancer than those who do. Nevertheless, despite the slight imbalance issue, we should still take them into account and perform stratification during cross-validation later. def plot_target_distribution ( df : pd . DataFrame , target : str , colors : List [ str ]) -> None : \"\"\"Plot Target Distribution with percentage labels. Args: df (pd.DataFrame): DataFrame to plot. target (str): Target column name. colors (List[str]): List of colors to use for plotting. \"\"\" plt . rcParams [ \"figure.dpi\" ] = 100 plt . rcParams [ \"savefig.dpi\" ] = 300 x_axis = df [ \"diagnosis\" ] . value_counts () . index y_axis = df [ \"diagnosis\" ] . value_counts () figure , target_bar = plt . subplots ( figsize = ( 6 , 4 )) bar = sns . barplot ( x = x_axis , y = y_axis , ax = target_bar , palette = { 1 : colors [ 0 ], 0 : colors [ 3 ]}) target_bar . set_xticklabels ([ \"Benign\" , \"Malignant\" ]) target_bar . set_ylabel ( \"Frequency Count\" ) target_bar . legend ([ \"Benign\" , \"Malignant\" ], loc = \"upper right\" ) target_bar . set_title ( \"Count of Target (Diagnosis)\" , fontsize = 16 ) figure . text ( x = 0.27 , y = 0.8 , s = \" {:.1f} %\" . format ( df [ \"diagnosis\" ] . value_counts ( normalize = True )[ 0 ] * 100 ), ** { \"weight\" : \"bold\" , \"color\" : \"black\" }, ) figure . text ( x = 0.66 , y = 0.5 , s = \" {:.1f} %\" . format ( df [ \"diagnosis\" ] . value_counts ( normalize = True )[ 1 ] * 100 ), ** { \"weight\" : \"bold\" , \"color\" : \"black\" }, ) plt . show () _ = plot_target_distribution ( df = df , target = config . target_col , colors = config . colors ) Univariate Distributions of Predictors Univariate Plot on Continuous Variables The predictors are all continuous variables. We can do a univariate plot to show the histogram/kde distribution of these features, we color the hue so that the distribution is parametrized by the target. In addition, we will also perform a box plot to check for potential outliers. predictor_cols = df . columns . to_list ()[ 1 :] Histogram and KDE distribution We can plot a histogram with KDE for each of the feature, parametrized by the target. The aim of this visual is to briefly see how skewed the features are, whether the features are gaussian, and also the distribution of each feature with respect to the target. def plot_univariate ( df : pd . DataFrame , predictor : str , colors : List [ str ]) -> None : \"\"\"Take in continuous predictors and plot univariate distribution. Note in this setting, we have kde=True. Args: df (pd.DataFrame): Dataframe. predictor (str): Predictor name. \"\"\" univariate_params = { \"nrows\" : 10 , \"ncols\" : 3 , \"figsize\" : ( 12 , 24 ), \"dpi\" : 80 } fig , axs = plt . subplots ( ** univariate_params ) for i , col in enumerate ( predictor ): sns . histplot ( data = df , x = col , kde = True , hue = \"diagnosis\" , ax = axs [ i % univariate_params [ \"nrows\" ]][ i // univariate_params [ \"nrows\" ]], legend = False , palette = { 1 : colors [ 0 ], 0 : colors [ 3 ]}, ) plt . subplots_adjust ( hspace = 2 ) fig . suptitle ( \"Breast Cancer Predictors Univariate Distribution\" , y = 1.01 , fontsize = \"x-large\" ) fig . legend ( df [ \"diagnosis\" ] . unique ()) fig . tight_layout () plt . show () _ = plot_univariate ( df = df , predictor = predictor_cols , colors = config . colors ) Univariate Insights From the plots above, we can form very basic hypothesis on the features, we select a few features to explain: - radius_mean: seems to have a clear boundary to discriminate between benign and malignant classes; in general, the bigger the radius mean, the higher the likelihood of the tumor being malignant; - smoothness_mean: judging the smoothness mean (jaggedness) alone , we can see there is quite a fair bit of overlaps in between the class distributions, the boundary in between both classes are not so clear when compared to radius_mean . This suggests that the feature alone may not distinguish whether a patient's tumor is malignant or not. However, seemingly \"non-informative\" features may become extremely useful when coupled with other features. We can see most graphs are skewed towards the right. Box Plots Although a good alternative to box plots is the violin plot, but we do have the the distribution of KDE earlier on, so we can just zoom in at the box plots to check for outliers. There are some outliers present in the features. Outliers Alert! Outliers are tricky, without domain knowledge, it is sometimes hard to tell whether or not an outlier should be removed. A rule of thumb is that if you are sure the outliers are caused by a labelling or human error, then you can remove them. Otherwise, we may need to investigate further to check if these outliers should be retained during modelling. def plot_univariate_boxplot ( df : pd . DataFrame , predictor : str ) -> None : \"\"\"Take in continuous predictors and plot univariate boxplot distribution. Note in this setting, we have kde=True. Args: df (pd.DataFrame): DataFrame. predictor (str): Predictor name. \"\"\" univariate_params = { \"nrows\" : 10 , \"ncols\" : 3 , \"figsize\" : ( 12 , 24 ), \"dpi\" : 80 } fig , axs = plt . subplots ( ** univariate_params ) for i , col in enumerate ( predictor ): sns . boxplot ( data = df , x = col , hue = \"diagnosis\" , ax = axs [ i % univariate_params [ \"nrows\" ]][ i // univariate_params [ \"nrows\" ]], ) plt . subplots_adjust ( hspace = 2 ) fig . suptitle ( \"Breast Cancer Predictors Boxplot Distribution\" , y = 1.01 , fontsize = \"x-large\" ) fig . legend ( df [ \"diagnosis\" ] . unique ()) fig . tight_layout () plt . show () predictor_cols = df . columns . to_list ()[ 1 :] _ = plot_univariate_boxplot ( df = df , predictor = predictor_cols ) Correlation Plots Through the definitions given on the features of the dataset, we know that there are some features that are correlated to each other. For example, we can even make some hypothesis before we plot. Hypothesis Radius Mean, Area Mean and Perimeter Mean are correlated. This makes sense as if a cell nucleus is approximately circle, then the radius \\(r\\) is related linearly with perimeter \\(2\\pi r\\) and quadratically related with area \\(\\pi r ^2\\) . Heatmap We can plot a simple correlation heatmap to visualize the \"hot spots\" in which the correlation value is high. def plot_heatmap ( df : pd . DataFrame , predictors : List [ str ], cmap : str ) -> pd . DataFrame : \"\"\"This function takes in a dataframe and a list of predictors, and output the correlation matrix, as well as a plot of heatmap. 1. Note that annot_kws attempts to make the size of the font visible and contained in the heatmap. 2. Note that the CMAP is reversed and darker color indicates higher correlation as I find this more intuitive. Args: df (pd.DataFrame): The dataframe to be plotted. predictors (List[str]): The list of predictors to be plotted. Returns: pd.DataFrame: [description] \"\"\" corr = df [ predictors ] . corr () annot_kws = { \"size\" : 35 / np . sqrt ( len ( corr ))} fig , _ = plt . subplots ( figsize = ( 16 , 12 )) sns . heatmap ( corr , annot = True , cmap = cmap , annot_kws = annot_kws ) return corr corr_matrix = plot_heatmap ( df = df , predictors = predictor_cols , cmap = config . cmap_reversed ) From the correlation plot above, we discovered quite a lot of features being correlated, indicating multi-collinearity. We can further strengthen our stance by using a clusterplot to check. Cluster Plot We can use a Hierarchical Clustering to visualize the correlated clusters, the Cluster Map outputs a Dendrogram and in our Seaborn plot, we used Ward's Linkage as our method and the distance metric is Euclidean's Distance. Hence, in the diagram below, the dendrogram implies A and B are more \"correlated\" than A and C. Courtesy of What is a Dendrogram? corr = df [ predictor_cols ] . corr () g = sns . clustermap ( corr , method = \"ward\" , metric = 'euclidean' , cmap = config . cmap_reversed ); Bivariate Analysis Clutter Alert! Now we have 30 predictors, if we plot all of them in the pairplots below, we will roughly have \\({30 \\choose 2} = 435\\) such figures, which is not really pleasing to look at. We can zoom in on a few predictors and see if we can find more insights. A good way is to be selective when you do pair plots. From the correlation plot above, we can further choose a set of correlated features and plot them. For our purpose, we will plot 10 features only, all of them are related to the mean of the features. The plot will have its diagonal conveniently displaying its univariate histogram and kde distribution, while the off-diagonal will show bivariate scatter plots. mean_cols = predictor_cols [: 10 ] def corrfunc ( x : np . ndarray , y : np . ndarray , ax = None , ** kws ) -> None : \"\"\"Plot the correlation coefficient in the top left hand corner of a plot. Args: x (np.ndarray): x-axis data. y (np.ndarray): y-axis data. ax ([type], optional): Defaults to None. Axes to plot on. \"\"\" r , _ = pearsonr ( x , y ) ax = ax or plt . gca () ax . annotate ( f \" { r : .1f } \" , xy = ( 0.7 , 0.15 ), xycoords = ax . transAxes ) pp_mean = sns . pairplot ( df , hue = \"diagnosis\" , vars = mean_cols , palette = { 1 : config . colors [ 0 ], 0 : config . colors [ 3 ]}, diag_kind = \"auto\" , corner = True , ) pp_mean = pp_mean . map_offdiag ( sns . scatterplot ) pp_mean = pp_mean . map_diag ( sns . histplot , kde = True ) pp_mean = pp_mean . map_lower ( corrfunc ) From the pairplot, we notice interesting things like the three \"good brothers\", radius, area and perimeter, whom are highly positively correlated. PCA, TSNE We can also plot PCA and TSNE to get a feel of how \"separable\" the data points are in 2 dimensional space. Standardization Alert A disclaimer/assumption here is that we are standardizing the full training set for the purpose of visualizations, and both PCA and TSNE benefit from data on the same scale. Note that preprocessing techniques such as standardization should not be applied to the test/hold-out set, as this will cause data leakage. def plot_dimensional_reduction ( df : pd . DataFrame , predictor_cols : List [ str ], colors : List [ str ] ): \"\"\"Plots PCA and TSNE for visualization of higher dimension to lower dimension. Args: df (pd.DataFrame): Dataframe to be plotted. predictor_cols (List[str]): List of predictor columns. colors (List[str]): List of colors for plotting. \"\"\" X_standardized = preprocessing . StandardScaler () . fit_transform ( df [ predictor_cols ]) # Binary classification: we can set n components to 2 to better visualize all features in 2 dimensions pca = decomposition . PCA ( n_components = 2 ) pca_2d = pca . fit_transform ( X_standardized ) tsne = manifold . TSNE ( n_components = 2 , verbose = 1 , perplexity = 40 , n_iter = 1500 ) tsne_2d = tsne . fit_transform ( X_standardized ) # Plot the TSNE and PCA visuals side-by-side plt . figure ( figsize = ( 16 , 11 )) plt . subplot ( 121 ) plt . scatter ( pca_2d [:, 0 ], pca_2d [:, 1 ], c = df [ \"diagnosis\" ], edgecolor = \"None\" , alpha = 0.35 ) plt . colorbar () plt . title ( \"PCA Scatter Plot\" ) plt . subplot ( 122 ) plt . scatter ( tsne_2d [:, 0 ], tsne_2d [:, 1 ], c = df [ \"diagnosis\" ], edgecolor = \"None\" , alpha = 0.35 , ) plt . colorbar () plt . title ( \"TSNE Scatter Plot\" ) plt . show () _ = plot_dimensional_reduction ( df = df , predictor_cols = predictor_cols , colors = [ config . colors [ 0 ], config . colors [ 1 ]]) [t-SNE] Computing 121 nearest neighbors... [t-SNE] Indexed 569 samples in 0.003s... [t-SNE] Computed neighbors for 569 samples in 0.052s... [t-SNE] Computed conditional probabilities for sample 569 / 569 [t-SNE] Mean sigma: 1.522404 [t-SNE] KL divergence after 250 iterations with early exaggeration: 63.866753 [t-SNE] KL divergence after 1450 iterations: 0.847863 Welp, the purpose of this plot is to show how well separated the data points are in 2d-space (binary classification). It does seem that TSNE can distinguish the clusters clearer than PCA. This may suggest that your data points are non-linear, which is one assumption that PCA takes. We are not restricted to only linear models, so this is fine!","title":"EDA"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%202%20-%20Preliminary%20EDA/#dependencies-and-configuration","text":"import random from dataclasses import dataclass , field from typing import Dict , List import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from scipy.cluster import hierarchy from scipy.spatial.distance import squareform from scipy.stats import pearsonr , spearmanr from sklearn import decomposition , manifold , preprocessing @dataclass class config : raw_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/raw/data.csv\" processed_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/processed.csv\" train_size : float = 0.9 seed : int = 1992 num_folds : int = 5 cv_schema : str = \"StratifiedKFold\" classification_type : str = \"binary\" target_col : List [ str ] = field ( default_factory = lambda : [ \"diagnosis\" ]) unwanted_cols : List [ str ] = field ( default_factory = lambda : [ \"id\" , \"Unnamed: 32\" ]) # Plotting colors : List [ str ] = field ( default_factory = lambda : [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ] ) cmap_reversed = plt . cm . get_cmap ( \"mako_r\" ) def to_dict ( self ) -> Dict : \"\"\"Convert the config object to a dictionary. Returns: Dict: The config object as a dictionary. \"\"\" return { \"raw_data\" : self . raw_data , \"processed_data\" : self . processed_data , \"train_size\" : self . train_size , \"seed\" : self . seed , \"num_folds\" : self . num_folds , \"cv_schema\" : self . cv_schema , \"classification_type\" : self . classification_type , \"target_col\" : self . target_col , \"unwanted_cols\" : self . unwanted_cols , \"colors\" : self . colors , \"cmap_reversed\" : self . cmap_reversed , } def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) # set config config = config () # set seeding for reproducibility _ = set_seeds ( seed = config . seed ) # read data df = pd . read_csv ( config . processed_data )","title":"Dependencies and Configuration"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%202%20-%20Preliminary%20EDA/#stage-2-eda-preliminary-stage","text":"Terminology Alert I call this stage the preliminary EDA for the following reasons (but not limited to!) - EDA is an iterative process! Data is never clean! So we scrub -> send in for modelling -> scrub -> get new data -> scrub scrub scrub again! - Typically, I do a basic EDA analysis after Stage 1. As an example, in a classical regression setting, I would check for some assumptions like normality checks. And if some variables are of a \"bad distribution\", we can perform a transformation on it first, and plot them again in the next round of EDA to check if we corrected the issues. The below image summarizes the process. Courtesy of Farcaster In summary, we will do the following EDA here: Target and Predictors Distribution. Univariate Analysis. Bivariate Analysis. Multivariate Analysis. Some fancy looking Dimensionality Reduction Techniques to see how our features can be visualized in lower dimensions.","title":"Stage 2: EDA (Preliminary Stage)"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%202%20-%20Preliminary%20EDA/#distribution-of-target-and-predictor","text":"","title":"Distribution of Target and Predictor"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%202%20-%20Preliminary%20EDA/#target","text":"Let us first visualize our distribution of target variable. Since the target is discrete, we will just plot a simple count plot to check. Class Imbalance For Classification problems, we often plot the target count to get a good gauge on how imbalanced the dataset is. From there, we can think of whether the class imbalance is going to be a factor in our modelling process. df [ 'diagnosis' ] . value_counts ( normalize = True ) * 100 Malignant : 37.3 % Benign : 62.7 % There is to certain degree a class imbalance issue, although this is not what I have envisioned in the first place. After all, it is just natural that there should be more negative (benign) than positive (malignant) cases, given that in the general population, there are much more people without cancer than those who do. Nevertheless, despite the slight imbalance issue, we should still take them into account and perform stratification during cross-validation later. def plot_target_distribution ( df : pd . DataFrame , target : str , colors : List [ str ]) -> None : \"\"\"Plot Target Distribution with percentage labels. Args: df (pd.DataFrame): DataFrame to plot. target (str): Target column name. colors (List[str]): List of colors to use for plotting. \"\"\" plt . rcParams [ \"figure.dpi\" ] = 100 plt . rcParams [ \"savefig.dpi\" ] = 300 x_axis = df [ \"diagnosis\" ] . value_counts () . index y_axis = df [ \"diagnosis\" ] . value_counts () figure , target_bar = plt . subplots ( figsize = ( 6 , 4 )) bar = sns . barplot ( x = x_axis , y = y_axis , ax = target_bar , palette = { 1 : colors [ 0 ], 0 : colors [ 3 ]}) target_bar . set_xticklabels ([ \"Benign\" , \"Malignant\" ]) target_bar . set_ylabel ( \"Frequency Count\" ) target_bar . legend ([ \"Benign\" , \"Malignant\" ], loc = \"upper right\" ) target_bar . set_title ( \"Count of Target (Diagnosis)\" , fontsize = 16 ) figure . text ( x = 0.27 , y = 0.8 , s = \" {:.1f} %\" . format ( df [ \"diagnosis\" ] . value_counts ( normalize = True )[ 0 ] * 100 ), ** { \"weight\" : \"bold\" , \"color\" : \"black\" }, ) figure . text ( x = 0.66 , y = 0.5 , s = \" {:.1f} %\" . format ( df [ \"diagnosis\" ] . value_counts ( normalize = True )[ 1 ] * 100 ), ** { \"weight\" : \"bold\" , \"color\" : \"black\" }, ) plt . show () _ = plot_target_distribution ( df = df , target = config . target_col , colors = config . colors )","title":"Target"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%202%20-%20Preliminary%20EDA/#univariate-distributions-of-predictors","text":"Univariate Plot on Continuous Variables The predictors are all continuous variables. We can do a univariate plot to show the histogram/kde distribution of these features, we color the hue so that the distribution is parametrized by the target. In addition, we will also perform a box plot to check for potential outliers. predictor_cols = df . columns . to_list ()[ 1 :]","title":"Univariate Distributions of Predictors"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%202%20-%20Preliminary%20EDA/#histogram-and-kde-distribution","text":"We can plot a histogram with KDE for each of the feature, parametrized by the target. The aim of this visual is to briefly see how skewed the features are, whether the features are gaussian, and also the distribution of each feature with respect to the target. def plot_univariate ( df : pd . DataFrame , predictor : str , colors : List [ str ]) -> None : \"\"\"Take in continuous predictors and plot univariate distribution. Note in this setting, we have kde=True. Args: df (pd.DataFrame): Dataframe. predictor (str): Predictor name. \"\"\" univariate_params = { \"nrows\" : 10 , \"ncols\" : 3 , \"figsize\" : ( 12 , 24 ), \"dpi\" : 80 } fig , axs = plt . subplots ( ** univariate_params ) for i , col in enumerate ( predictor ): sns . histplot ( data = df , x = col , kde = True , hue = \"diagnosis\" , ax = axs [ i % univariate_params [ \"nrows\" ]][ i // univariate_params [ \"nrows\" ]], legend = False , palette = { 1 : colors [ 0 ], 0 : colors [ 3 ]}, ) plt . subplots_adjust ( hspace = 2 ) fig . suptitle ( \"Breast Cancer Predictors Univariate Distribution\" , y = 1.01 , fontsize = \"x-large\" ) fig . legend ( df [ \"diagnosis\" ] . unique ()) fig . tight_layout () plt . show () _ = plot_univariate ( df = df , predictor = predictor_cols , colors = config . colors ) Univariate Insights From the plots above, we can form very basic hypothesis on the features, we select a few features to explain: - radius_mean: seems to have a clear boundary to discriminate between benign and malignant classes; in general, the bigger the radius mean, the higher the likelihood of the tumor being malignant; - smoothness_mean: judging the smoothness mean (jaggedness) alone , we can see there is quite a fair bit of overlaps in between the class distributions, the boundary in between both classes are not so clear when compared to radius_mean . This suggests that the feature alone may not distinguish whether a patient's tumor is malignant or not. However, seemingly \"non-informative\" features may become extremely useful when coupled with other features. We can see most graphs are skewed towards the right.","title":"Histogram and KDE distribution"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%202%20-%20Preliminary%20EDA/#box-plots","text":"Although a good alternative to box plots is the violin plot, but we do have the the distribution of KDE earlier on, so we can just zoom in at the box plots to check for outliers. There are some outliers present in the features. Outliers Alert! Outliers are tricky, without domain knowledge, it is sometimes hard to tell whether or not an outlier should be removed. A rule of thumb is that if you are sure the outliers are caused by a labelling or human error, then you can remove them. Otherwise, we may need to investigate further to check if these outliers should be retained during modelling. def plot_univariate_boxplot ( df : pd . DataFrame , predictor : str ) -> None : \"\"\"Take in continuous predictors and plot univariate boxplot distribution. Note in this setting, we have kde=True. Args: df (pd.DataFrame): DataFrame. predictor (str): Predictor name. \"\"\" univariate_params = { \"nrows\" : 10 , \"ncols\" : 3 , \"figsize\" : ( 12 , 24 ), \"dpi\" : 80 } fig , axs = plt . subplots ( ** univariate_params ) for i , col in enumerate ( predictor ): sns . boxplot ( data = df , x = col , hue = \"diagnosis\" , ax = axs [ i % univariate_params [ \"nrows\" ]][ i // univariate_params [ \"nrows\" ]], ) plt . subplots_adjust ( hspace = 2 ) fig . suptitle ( \"Breast Cancer Predictors Boxplot Distribution\" , y = 1.01 , fontsize = \"x-large\" ) fig . legend ( df [ \"diagnosis\" ] . unique ()) fig . tight_layout () plt . show () predictor_cols = df . columns . to_list ()[ 1 :] _ = plot_univariate_boxplot ( df = df , predictor = predictor_cols )","title":"Box Plots"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%202%20-%20Preliminary%20EDA/#correlation-plots","text":"Through the definitions given on the features of the dataset, we know that there are some features that are correlated to each other. For example, we can even make some hypothesis before we plot. Hypothesis Radius Mean, Area Mean and Perimeter Mean are correlated. This makes sense as if a cell nucleus is approximately circle, then the radius \\(r\\) is related linearly with perimeter \\(2\\pi r\\) and quadratically related with area \\(\\pi r ^2\\) .","title":"Correlation Plots"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%202%20-%20Preliminary%20EDA/#heatmap","text":"We can plot a simple correlation heatmap to visualize the \"hot spots\" in which the correlation value is high. def plot_heatmap ( df : pd . DataFrame , predictors : List [ str ], cmap : str ) -> pd . DataFrame : \"\"\"This function takes in a dataframe and a list of predictors, and output the correlation matrix, as well as a plot of heatmap. 1. Note that annot_kws attempts to make the size of the font visible and contained in the heatmap. 2. Note that the CMAP is reversed and darker color indicates higher correlation as I find this more intuitive. Args: df (pd.DataFrame): The dataframe to be plotted. predictors (List[str]): The list of predictors to be plotted. Returns: pd.DataFrame: [description] \"\"\" corr = df [ predictors ] . corr () annot_kws = { \"size\" : 35 / np . sqrt ( len ( corr ))} fig , _ = plt . subplots ( figsize = ( 16 , 12 )) sns . heatmap ( corr , annot = True , cmap = cmap , annot_kws = annot_kws ) return corr corr_matrix = plot_heatmap ( df = df , predictors = predictor_cols , cmap = config . cmap_reversed ) From the correlation plot above, we discovered quite a lot of features being correlated, indicating multi-collinearity. We can further strengthen our stance by using a clusterplot to check.","title":"Heatmap"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%202%20-%20Preliminary%20EDA/#cluster-plot","text":"We can use a Hierarchical Clustering to visualize the correlated clusters, the Cluster Map outputs a Dendrogram and in our Seaborn plot, we used Ward's Linkage as our method and the distance metric is Euclidean's Distance. Hence, in the diagram below, the dendrogram implies A and B are more \"correlated\" than A and C. Courtesy of What is a Dendrogram? corr = df [ predictor_cols ] . corr () g = sns . clustermap ( corr , method = \"ward\" , metric = 'euclidean' , cmap = config . cmap_reversed );","title":"Cluster Plot"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%202%20-%20Preliminary%20EDA/#bivariate-analysis","text":"Clutter Alert! Now we have 30 predictors, if we plot all of them in the pairplots below, we will roughly have \\({30 \\choose 2} = 435\\) such figures, which is not really pleasing to look at. We can zoom in on a few predictors and see if we can find more insights. A good way is to be selective when you do pair plots. From the correlation plot above, we can further choose a set of correlated features and plot them. For our purpose, we will plot 10 features only, all of them are related to the mean of the features. The plot will have its diagonal conveniently displaying its univariate histogram and kde distribution, while the off-diagonal will show bivariate scatter plots. mean_cols = predictor_cols [: 10 ] def corrfunc ( x : np . ndarray , y : np . ndarray , ax = None , ** kws ) -> None : \"\"\"Plot the correlation coefficient in the top left hand corner of a plot. Args: x (np.ndarray): x-axis data. y (np.ndarray): y-axis data. ax ([type], optional): Defaults to None. Axes to plot on. \"\"\" r , _ = pearsonr ( x , y ) ax = ax or plt . gca () ax . annotate ( f \" { r : .1f } \" , xy = ( 0.7 , 0.15 ), xycoords = ax . transAxes ) pp_mean = sns . pairplot ( df , hue = \"diagnosis\" , vars = mean_cols , palette = { 1 : config . colors [ 0 ], 0 : config . colors [ 3 ]}, diag_kind = \"auto\" , corner = True , ) pp_mean = pp_mean . map_offdiag ( sns . scatterplot ) pp_mean = pp_mean . map_diag ( sns . histplot , kde = True ) pp_mean = pp_mean . map_lower ( corrfunc ) From the pairplot, we notice interesting things like the three \"good brothers\", radius, area and perimeter, whom are highly positively correlated.","title":"Bivariate Analysis"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%202%20-%20Preliminary%20EDA/#pca-tsne","text":"We can also plot PCA and TSNE to get a feel of how \"separable\" the data points are in 2 dimensional space. Standardization Alert A disclaimer/assumption here is that we are standardizing the full training set for the purpose of visualizations, and both PCA and TSNE benefit from data on the same scale. Note that preprocessing techniques such as standardization should not be applied to the test/hold-out set, as this will cause data leakage. def plot_dimensional_reduction ( df : pd . DataFrame , predictor_cols : List [ str ], colors : List [ str ] ): \"\"\"Plots PCA and TSNE for visualization of higher dimension to lower dimension. Args: df (pd.DataFrame): Dataframe to be plotted. predictor_cols (List[str]): List of predictor columns. colors (List[str]): List of colors for plotting. \"\"\" X_standardized = preprocessing . StandardScaler () . fit_transform ( df [ predictor_cols ]) # Binary classification: we can set n components to 2 to better visualize all features in 2 dimensions pca = decomposition . PCA ( n_components = 2 ) pca_2d = pca . fit_transform ( X_standardized ) tsne = manifold . TSNE ( n_components = 2 , verbose = 1 , perplexity = 40 , n_iter = 1500 ) tsne_2d = tsne . fit_transform ( X_standardized ) # Plot the TSNE and PCA visuals side-by-side plt . figure ( figsize = ( 16 , 11 )) plt . subplot ( 121 ) plt . scatter ( pca_2d [:, 0 ], pca_2d [:, 1 ], c = df [ \"diagnosis\" ], edgecolor = \"None\" , alpha = 0.35 ) plt . colorbar () plt . title ( \"PCA Scatter Plot\" ) plt . subplot ( 122 ) plt . scatter ( tsne_2d [:, 0 ], tsne_2d [:, 1 ], c = df [ \"diagnosis\" ], edgecolor = \"None\" , alpha = 0.35 , ) plt . colorbar () plt . title ( \"TSNE Scatter Plot\" ) plt . show () _ = plot_dimensional_reduction ( df = df , predictor_cols = predictor_cols , colors = [ config . colors [ 0 ], config . colors [ 1 ]]) [t-SNE] Computing 121 nearest neighbors... [t-SNE] Indexed 569 samples in 0.003s... [t-SNE] Computed neighbors for 569 samples in 0.052s... [t-SNE] Computed conditional probabilities for sample 569 / 569 [t-SNE] Mean sigma: 1.522404 [t-SNE] KL divergence after 250 iterations with early exaggeration: 63.866753 [t-SNE] KL divergence after 1450 iterations: 0.847863 Welp, the purpose of this plot is to show how well separated the data points are in 2d-space (binary classification). It does seem that TSNE can distinguish the clusters clearer than PCA. This may suggest that your data points are non-linear, which is one assumption that PCA takes. We are not restricted to only linear models, so this is fine!","title":"PCA, TSNE"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%203%20-%20Feature%20Engineering/","text":"Stage 3: Feature Engineering by Hongnan Gao Quick Navigation Dependencies and Configuration Stage 3: Feature Engineering/Feature Selection Multicollinearity and Feature Selection Target Distribution Using Statsmodels Variance Inflation Factor Oh Dear, we have a Multicollinearity Problem Save the Data Dependencies and Configuration import random from collections import defaultdict from dataclasses import dataclass , field from typing import Dict , List , Union import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from sklearn import ( base , decomposition , linear_model , manifold , metrics , preprocessing ) from statsmodels.stats.outliers_influence import variance_inflation_factor /usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm @dataclass class config : raw_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/raw/data.csv\" processed_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/processed.csv\" train_size : float = 0.9 seed : int = 1992 num_folds : int = 5 cv_schema : str = \"StratifiedKFold\" classification_type : str = \"binary\" target_col : List [ str ] = field ( default_factory = lambda : [ \"diagnosis\" ]) unwanted_cols : List [ str ] = field ( default_factory = lambda : [ \"id\" , \"Unnamed: 32\" ]) # Plotting colors : List [ str ] = field ( default_factory = lambda : [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ] ) cmap_reversed = plt . cm . get_cmap ( \"mako_r\" ) def to_dict ( self ) -> Dict : \"\"\"Convert the config object to a dictionary. Returns: Dict: The config object as a dictionary. \"\"\" return { \"raw_data\" : self . raw_data , \"processed_data\" : self . processed_data , \"train_size\" : self . train_size , \"seed\" : self . seed , \"num_folds\" : self . num_folds , \"cv_schema\" : self . cv_schema , \"classification_type\" : self . classification_type , \"target_col\" : self . target_col , \"unwanted_cols\" : self . unwanted_cols , \"colors\" : self . colors , \"cmap_reversed\" : self . cmap_reversed , } def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) config = config () # set seeding for reproducibility _ = set_seeds ( seed = config . seed ) # read data df = pd . read_csv ( config . processed_data ) Stage 3: Feature Engineering/Feature Selection Foreward on Data Leakage We are fully aware that oftentimes practitioner may accidentally cause data leakage during preprocessing, for example, a subtle yet often mistake is to standardize the whole dataset prior to splitting, or performing feature selection prior to modelling using the information of our response/target variable. However, this does not mean we should not do any preprocessing before modelling, instead, in the case of removing multicollinearity features, we can still screen predictors for multicollinearity during EDA phase and have a good intuition on which predictors are highly correlated - subsequently, we will incorporate feature selection techniques in our modelling pipeline. Multicollinearity and Feature Selection Why Feature Selection? We need feature selection in certain problems for the following reasons: - Well, one would definitely have heard of the dreaded Curse of Dimensionality in the journey of learning Machine Learning where having too many predictor/features can lead to overfitting; on the other hand, too many dimensions can cause distance between observations to appear equidistance from one another. The equidistance phenomenon causes observations to become harder to cluster, thereby clogging the model's ability to cluster data points (imagine the horror if you use KNN on 1000 dimensions, all the points will be almost the same distance from each other, poor KNN will not know how to predict now). - In case you have access to Google's GPU clusters, you likely want to train your model faster. Reducing the number predictors can aid this process. - Reducing uninformative features may aid in model's performance, the idea is to remove unnecessary noise from the dataset. Multi-Collinearity Looking back at our dataset, it is clear to me that there are quite a number of features that are correlated with each other, causing multi-collinearity. Multi-Collinearity is an issue in the history of Linear Models, as quoted 1 Consider the simplest case where Y is regressed against X and Z and where X and Z are highly positively correlated. Then the effect of X on Y is hard to distinguish from the effect of Z on Y because any increase in X tends to be associated with an increase in Z. We also note that multi-collinearity is not that big of a problem for non-parametric models such as Decision Tree or Random Forests, however, I will attempt to show that it is still best to avoid in this problem setting. Feature Selection Methods There are many methods to perform feature selection. Scikit-Learn offers some of the following: - Univariate feature selection. - Recursive feature elimination. - Backward Elimination of features using Hypothesis Testing. We need to be careful when selecting features before cross-validation. It is therefore, recommended to include feature selection in cross-validation to avoid any \"bias\" introduced before model selection phase! I decided to use the good old Variance Inflation Factor (VIF) as a way to reduce multicollinearity. Unfortunately, there is no out-of-the-box function to integrate into the Pipeline of scikit-learn. Thus, I heavily modified an existing code in order achieve what I want below. Variance Inflation Factor A classical way to check for multicollinearity amongst predictors is to calculate the Variable Inflation Factor (VIF). It is simply done by regressing each predictor \\(\\mathrm{x}_i\\) against all other predictors \\(\\mathrm{x}_j, j \\neq i\\) . In other words, the VIF for a predictor variable \\(i\\) is given by: \\[\\text{VIF}_i = \\dfrac{1}{1 - R^{2}_{i}}\\] where \\(R^{2}_{i}\\) is, by definition, the proportion of the variation in the \"dependent variable\" \\(\\mathrm{x}_i\\) that is predictable from the indepedent predictors \\(\\mathrm{x}_j, j \\neq i\\) . Consequently, the higher the \\(R^2_i\\) of a predictor, the higher the VIF, and this indicates there is linear dependence among predictors. Using Statsmodels Variance Inflation Factor Note that we need to perform scaling first before fitting our ReduceVIF to get the exact same result as the previous version. In this version, I manually added a hard threshold for the number of features remaining to be 15. This hard coded number can be turned into a parameter (hyperparameter) in our pipeline. import numpy as np import pandas as pd from sklearn import base from statsmodels.regression.linear_model import OLS def variance_inflation_factor ( exog , idx_kept , vif_idx ): \"\"\"Compute VIF for one feature. Args: exog (np.ndarray): Observations idx_kept (List[int]): Indices of features to consider vif_idx (int): Index of feature for which to compute VIF Returns: float: VIF for the selected feature \"\"\" exog = np . asarray ( exog ) x_i = exog [:, vif_idx ] mask = [ col for col in idx_kept if col != vif_idx ] x_noti = exog [:, mask ] r_squared_i = OLS ( x_i , x_noti ) . fit () . rsquared vif = 1. / ( 1. - r_squared_i ) return vif class ReduceVIF ( base . BaseEstimator , base . TransformerMixin ): \"\"\"The base of the class structure is implemented in https://www.kaggle.com/ffisegydd/sklearn-multicollinearity-class; I heavily modified the class such that it can take in numpy arrays and correctly implemented the fit and transform method. \"\"\" def __init__ ( self , thresh = 10 , max_drop = 20 ): self . thresh = thresh self . max_drop = max_drop self . column_indices_kept_ = [] self . feature_names_kept_ = None def reset ( self ): \"\"\"Resets the state of predictor columns after each fold.\"\"\" self . column_indices_kept_ = [] self . feature_names_kept_ = None def fit ( self , X , y = None ): \"\"\"Fits the Recursive VIF on the training folds and save the selected feature names in self.feature_names Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" self . column_indices_kept_ , self . feature_names_kept_ = self . calculate_vif ( X ) return self def transform ( self , X , y = None ): \"\"\"Transforms the Validation Set according to the selected feature names. Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" return X [:, self . column_indices_kept_ ] def calculate_vif ( self , X : Union [ np . ndarray , pd . DataFrame ]): \"\"\"Implements a VIF function that recursively eliminates features. Args: X (Union[np.ndarray, pd.DataFrame]): [description] Returns: [type]: [description] \"\"\" feature_names = None column_indices_kept = list ( range ( X . shape [ 1 ])) if isinstance ( X , pd . DataFrame ): feature_names = X . columns dropped = True count = 0 while dropped and count <= self . max_drop : dropped = False max_vif , max_vif_col = None , None for col in column_indices_kept : vif = variance_inflation_factor ( X , column_indices_kept , col ) if max_vif is None or vif > max_vif : max_vif = vif max_vif_col = col if max_vif > self . thresh : print ( f \"Droppingggggg { max_vif_col } with vif= { max_vif } \" ) column_indices_kept . remove ( max_vif_col ) if feature_names is not None : feature_names . pop ( max_vif_col ) dropped = True count += 1 return column_indices_kept , feature_names We do a sanity check if this coincides with the previous defined class, and the results are the same. predictor_cols = df . columns [ 1 :] transformer = ReduceVIF () scaler = preprocessing . StandardScaler () X = scaler . fit_transform ( df [ predictor_cols ]) # Only use 10 columns for speed in this example X = transformer . fit_transform ( X ) print ( f \"Remaining Features: { transformer . column_indices_kept_ } \" ) Droppingggggg 0 with vif=3806.1152963979675 Droppingggggg 20 with vif=616.3508614719424 Droppingggggg 2 with vif=325.64131198187516 Droppingggggg 22 with vif=123.25781086343038 Droppingggggg 6 with vif=64.65479584770004 Droppingggggg 10 with vif=35.61751844352034 Droppingggggg 25 with vif=33.96063880508537 Droppingggggg 27 with vif=30.596655364834078 Droppingggggg 3 with vif=25.387829695531458 Droppingggggg 5 with vif=18.843208489973282 Droppingggggg 21 with vif=17.232376192128665 Droppingggggg 13 with vif=16.333806476471736 Droppingggggg 26 with vif=15.510661467365699 Remaining Features: [1, 4, 7, 8, 9, 11, 12, 14, 15, 16, 17, 18, 19, 23, 24, 28, 29] We have the remaining indices, and therefore simply use numpy to subset the column indices to get back the original column names that are kept. vif_df = pd . DataFrame ({ 'Predictors' : predictor_cols [ transformer . column_indices_kept_ ]}) display ( vif_df ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Predictors 0 texture_mean 1 smoothness_mean 2 concave points_mean 3 symmetry_mean 4 fractal_dimension_mean 5 texture_se 6 perimeter_se 7 smoothness_se 8 compactness_se 9 concavity_se 10 concave points_se 11 symmetry_se 12 fractal_dimension_se 13 area_worst 14 smoothness_worst 15 symmetry_worst 16 fractal_dimension_worst Oh Dear, we have a Multicollinearity Problem Using VIF in Modelling Pipeline At this step, we are just showing how we can remove multicollinear features using VIF; but we will not remove them at this point in time. We will incorporate this feature selection technique in our Cross-Validation pipeline in order to avoid data leakage. Save the Data In this phase, we did not make any changes to the predictor or target columns. So there is nothing to save. https://stats.stackexchange.com/questions/1149/is-there-an-intuitive-explanation-why-multicollinearity-is-a-problem-in-linear-r \u21a9","title":"Feature Engineering"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%203%20-%20Feature%20Engineering/#dependencies-and-configuration","text":"import random from collections import defaultdict from dataclasses import dataclass , field from typing import Dict , List , Union import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from sklearn import ( base , decomposition , linear_model , manifold , metrics , preprocessing ) from statsmodels.stats.outliers_influence import variance_inflation_factor /usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm @dataclass class config : raw_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/raw/data.csv\" processed_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/processed.csv\" train_size : float = 0.9 seed : int = 1992 num_folds : int = 5 cv_schema : str = \"StratifiedKFold\" classification_type : str = \"binary\" target_col : List [ str ] = field ( default_factory = lambda : [ \"diagnosis\" ]) unwanted_cols : List [ str ] = field ( default_factory = lambda : [ \"id\" , \"Unnamed: 32\" ]) # Plotting colors : List [ str ] = field ( default_factory = lambda : [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ] ) cmap_reversed = plt . cm . get_cmap ( \"mako_r\" ) def to_dict ( self ) -> Dict : \"\"\"Convert the config object to a dictionary. Returns: Dict: The config object as a dictionary. \"\"\" return { \"raw_data\" : self . raw_data , \"processed_data\" : self . processed_data , \"train_size\" : self . train_size , \"seed\" : self . seed , \"num_folds\" : self . num_folds , \"cv_schema\" : self . cv_schema , \"classification_type\" : self . classification_type , \"target_col\" : self . target_col , \"unwanted_cols\" : self . unwanted_cols , \"colors\" : self . colors , \"cmap_reversed\" : self . cmap_reversed , } def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) config = config () # set seeding for reproducibility _ = set_seeds ( seed = config . seed ) # read data df = pd . read_csv ( config . processed_data )","title":"Dependencies and Configuration"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%203%20-%20Feature%20Engineering/#stage-3-feature-engineeringfeature-selection","text":"Foreward on Data Leakage We are fully aware that oftentimes practitioner may accidentally cause data leakage during preprocessing, for example, a subtle yet often mistake is to standardize the whole dataset prior to splitting, or performing feature selection prior to modelling using the information of our response/target variable. However, this does not mean we should not do any preprocessing before modelling, instead, in the case of removing multicollinearity features, we can still screen predictors for multicollinearity during EDA phase and have a good intuition on which predictors are highly correlated - subsequently, we will incorporate feature selection techniques in our modelling pipeline.","title":"Stage 3: Feature Engineering/Feature Selection"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%203%20-%20Feature%20Engineering/#multicollinearity-and-feature-selection","text":"Why Feature Selection? We need feature selection in certain problems for the following reasons: - Well, one would definitely have heard of the dreaded Curse of Dimensionality in the journey of learning Machine Learning where having too many predictor/features can lead to overfitting; on the other hand, too many dimensions can cause distance between observations to appear equidistance from one another. The equidistance phenomenon causes observations to become harder to cluster, thereby clogging the model's ability to cluster data points (imagine the horror if you use KNN on 1000 dimensions, all the points will be almost the same distance from each other, poor KNN will not know how to predict now). - In case you have access to Google's GPU clusters, you likely want to train your model faster. Reducing the number predictors can aid this process. - Reducing uninformative features may aid in model's performance, the idea is to remove unnecessary noise from the dataset. Multi-Collinearity Looking back at our dataset, it is clear to me that there are quite a number of features that are correlated with each other, causing multi-collinearity. Multi-Collinearity is an issue in the history of Linear Models, as quoted 1 Consider the simplest case where Y is regressed against X and Z and where X and Z are highly positively correlated. Then the effect of X on Y is hard to distinguish from the effect of Z on Y because any increase in X tends to be associated with an increase in Z. We also note that multi-collinearity is not that big of a problem for non-parametric models such as Decision Tree or Random Forests, however, I will attempt to show that it is still best to avoid in this problem setting. Feature Selection Methods There are many methods to perform feature selection. Scikit-Learn offers some of the following: - Univariate feature selection. - Recursive feature elimination. - Backward Elimination of features using Hypothesis Testing. We need to be careful when selecting features before cross-validation. It is therefore, recommended to include feature selection in cross-validation to avoid any \"bias\" introduced before model selection phase! I decided to use the good old Variance Inflation Factor (VIF) as a way to reduce multicollinearity. Unfortunately, there is no out-of-the-box function to integrate into the Pipeline of scikit-learn. Thus, I heavily modified an existing code in order achieve what I want below.","title":"Multicollinearity and Feature Selection"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%203%20-%20Feature%20Engineering/#variance-inflation-factor","text":"A classical way to check for multicollinearity amongst predictors is to calculate the Variable Inflation Factor (VIF). It is simply done by regressing each predictor \\(\\mathrm{x}_i\\) against all other predictors \\(\\mathrm{x}_j, j \\neq i\\) . In other words, the VIF for a predictor variable \\(i\\) is given by: \\[\\text{VIF}_i = \\dfrac{1}{1 - R^{2}_{i}}\\] where \\(R^{2}_{i}\\) is, by definition, the proportion of the variation in the \"dependent variable\" \\(\\mathrm{x}_i\\) that is predictable from the indepedent predictors \\(\\mathrm{x}_j, j \\neq i\\) . Consequently, the higher the \\(R^2_i\\) of a predictor, the higher the VIF, and this indicates there is linear dependence among predictors.","title":"Variance Inflation Factor"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%203%20-%20Feature%20Engineering/#using-statsmodels-variance-inflation-factor","text":"Note that we need to perform scaling first before fitting our ReduceVIF to get the exact same result as the previous version. In this version, I manually added a hard threshold for the number of features remaining to be 15. This hard coded number can be turned into a parameter (hyperparameter) in our pipeline. import numpy as np import pandas as pd from sklearn import base from statsmodels.regression.linear_model import OLS def variance_inflation_factor ( exog , idx_kept , vif_idx ): \"\"\"Compute VIF for one feature. Args: exog (np.ndarray): Observations idx_kept (List[int]): Indices of features to consider vif_idx (int): Index of feature for which to compute VIF Returns: float: VIF for the selected feature \"\"\" exog = np . asarray ( exog ) x_i = exog [:, vif_idx ] mask = [ col for col in idx_kept if col != vif_idx ] x_noti = exog [:, mask ] r_squared_i = OLS ( x_i , x_noti ) . fit () . rsquared vif = 1. / ( 1. - r_squared_i ) return vif class ReduceVIF ( base . BaseEstimator , base . TransformerMixin ): \"\"\"The base of the class structure is implemented in https://www.kaggle.com/ffisegydd/sklearn-multicollinearity-class; I heavily modified the class such that it can take in numpy arrays and correctly implemented the fit and transform method. \"\"\" def __init__ ( self , thresh = 10 , max_drop = 20 ): self . thresh = thresh self . max_drop = max_drop self . column_indices_kept_ = [] self . feature_names_kept_ = None def reset ( self ): \"\"\"Resets the state of predictor columns after each fold.\"\"\" self . column_indices_kept_ = [] self . feature_names_kept_ = None def fit ( self , X , y = None ): \"\"\"Fits the Recursive VIF on the training folds and save the selected feature names in self.feature_names Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" self . column_indices_kept_ , self . feature_names_kept_ = self . calculate_vif ( X ) return self def transform ( self , X , y = None ): \"\"\"Transforms the Validation Set according to the selected feature names. Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" return X [:, self . column_indices_kept_ ] def calculate_vif ( self , X : Union [ np . ndarray , pd . DataFrame ]): \"\"\"Implements a VIF function that recursively eliminates features. Args: X (Union[np.ndarray, pd.DataFrame]): [description] Returns: [type]: [description] \"\"\" feature_names = None column_indices_kept = list ( range ( X . shape [ 1 ])) if isinstance ( X , pd . DataFrame ): feature_names = X . columns dropped = True count = 0 while dropped and count <= self . max_drop : dropped = False max_vif , max_vif_col = None , None for col in column_indices_kept : vif = variance_inflation_factor ( X , column_indices_kept , col ) if max_vif is None or vif > max_vif : max_vif = vif max_vif_col = col if max_vif > self . thresh : print ( f \"Droppingggggg { max_vif_col } with vif= { max_vif } \" ) column_indices_kept . remove ( max_vif_col ) if feature_names is not None : feature_names . pop ( max_vif_col ) dropped = True count += 1 return column_indices_kept , feature_names We do a sanity check if this coincides with the previous defined class, and the results are the same. predictor_cols = df . columns [ 1 :] transformer = ReduceVIF () scaler = preprocessing . StandardScaler () X = scaler . fit_transform ( df [ predictor_cols ]) # Only use 10 columns for speed in this example X = transformer . fit_transform ( X ) print ( f \"Remaining Features: { transformer . column_indices_kept_ } \" ) Droppingggggg 0 with vif=3806.1152963979675 Droppingggggg 20 with vif=616.3508614719424 Droppingggggg 2 with vif=325.64131198187516 Droppingggggg 22 with vif=123.25781086343038 Droppingggggg 6 with vif=64.65479584770004 Droppingggggg 10 with vif=35.61751844352034 Droppingggggg 25 with vif=33.96063880508537 Droppingggggg 27 with vif=30.596655364834078 Droppingggggg 3 with vif=25.387829695531458 Droppingggggg 5 with vif=18.843208489973282 Droppingggggg 21 with vif=17.232376192128665 Droppingggggg 13 with vif=16.333806476471736 Droppingggggg 26 with vif=15.510661467365699 Remaining Features: [1, 4, 7, 8, 9, 11, 12, 14, 15, 16, 17, 18, 19, 23, 24, 28, 29] We have the remaining indices, and therefore simply use numpy to subset the column indices to get back the original column names that are kept. vif_df = pd . DataFrame ({ 'Predictors' : predictor_cols [ transformer . column_indices_kept_ ]}) display ( vif_df ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Predictors 0 texture_mean 1 smoothness_mean 2 concave points_mean 3 symmetry_mean 4 fractal_dimension_mean 5 texture_se 6 perimeter_se 7 smoothness_se 8 compactness_se 9 concavity_se 10 concave points_se 11 symmetry_se 12 fractal_dimension_se 13 area_worst 14 smoothness_worst 15 symmetry_worst 16 fractal_dimension_worst","title":"Using Statsmodels Variance Inflation Factor"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%203%20-%20Feature%20Engineering/#oh-dear-we-have-a-multicollinearity-problem","text":"Using VIF in Modelling Pipeline At this step, we are just showing how we can remove multicollinear features using VIF; but we will not remove them at this point in time. We will incorporate this feature selection technique in our Cross-Validation pipeline in order to avoid data leakage.","title":"Oh Dear, we have a Multicollinearity Problem"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%203%20-%20Feature%20Engineering/#save-the-data","text":"In this phase, we did not make any changes to the predictor or target columns. So there is nothing to save. https://stats.stackexchange.com/questions/1149/is-there-an-intuitive-explanation-why-multicollinearity-is-a-problem-in-linear-r \u21a9","title":"Save the Data"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%204%20-%20Modelling%20%28Metric%20to%20Optimize%29/","text":"Stage 4: Modelling (Metrics) by Hongnan Gao Define Metrics Disclaimer: For a more detailed understanding of different metrics, do navigate to my self-made notes on metrics here . Choosing a metric to measure the classifier's (hypothesis) performance is important, as choosing the wrong one can lead to disastrous interpretations. One prime example is using the accuracy metric for imbalanced datasets; consider 1 mil data points, dichotomized by \\(99\\%\\) benign and \\(1\\%\\) malignant samples, even a baseline model zeroR model which predicts the majority class no matter the process will give a \\(99\\%\\) accuracy, completely missing out any positive samples, which unfortunately, is what we may be more interested in. Say No to Accuracy: Consider an imbalanced set, where the training data set has 100 patients (data points), and the ground truth is 90 patients are of class = 0, which means that these patients do not have cancer, whereas the remaining 10 patients are in class 1, where they do have cancer. This is an example of class imbalance where the ratio of class 1 to class 0 is 1:9. Consider a baseline (almost trivial) classifier : def zeroR ( patient_data ): training ... return benign where we predict the patient's class as the most frequent class. Meaning, the most frequent class in this question is the class = 0, where patients do not have cancer, so we just assign this class to everyone in this set. By doing this, we will inevitably achieve a in-sample accuracy rate of \\(\\frac{90}{100} = 90\\%\\) . But unfortunately, this supposedly high accuracy value is completely useless, because this classifier did not label any of the cancer patients correctly. The consequence can be serious, assuming the test set has the same distribution as our training set, where if we have a test set of 1000 patients, there are 900 negative and 100 positive. Our model just literally predict every one of them as benign, yielding a \\(90\\%\\) out-of-sample accuracy. What did we conclude? Well, for one, our accuracy can be 90% high and looks good to the laymen, but it failed to predict the most important class of people - yes, misclassifying true cancer patients as healthy people is very bad! For the reasons mentioned above, we will use metric that can help us reduce False Negatives, and at the same time, outputs meaningful predictions. In order to achieve for both, we will use Receiver operating characteristic (ROC) as the primary metric for the model to maximize (which is our \\(\\mathcal{M}\\) , and Brier Score , a proper scoring rule to measure the performance of our probabilistic predictions. We will go into some details in the next two subsections to justify our choice. Proper Scoring Rule The math behind the idea of Proper Scoring Rule is non-trivial. Here, we try to understand why a proper scoring rule is desired in the context of binary classification. Strictly Proper Scoring Rule: Brier Score Loss, for example, tells us that the best possible score, 0 (lowest loss), is obtained if and only if, the probability prediction we get for a sample, is the true probability itself. In other words, if a selected sample is of class 1, our prediction for this must be 1, with 100% probability, in order to get a score loss of 0. Proper Scoring Rule: Read [here](https://stats.stackexchange.com/questions/339919/what-does-it-mean-that-auc-is-a-semi-proper-scoring-rule) for this. Semi Proper Scoring Rule: AUROC, as mentioned, does not help out in telling whether a prediction by a classifier is close to the true probability or not. In our example, we even see that we can obtain a full score of 1, even if the probabilities all lie within 0.51 and 0.52. Improper Scoring Rule: Accuracy is a prime example, the accuracy score does not, whatsoever, tells us about how close our predicted probabilities are, to the true probability distribution of our samples. Receiver operating characteristic (ROC) Definition: The basic (non-probablistic intepretation) of ROC is graph that plots the True Positive Rate on the y-axis and False Positive Rate on the x-axis parametrized by a threshold vector $\\vec{t}$. We then look at the area under the ROC curve (AUROC) to get an overall performance measure. The choice of ROC over other metrics such as Accuracy is detailed initially. We also established we want to reduce False Negative (FN), since misclassifying a positive patient as benign is way more costly than the other way round. One can choose to minimize Recall in order to reduce FN, but this is less than ideal during training because it is a thresholded metric, and does not provide at which threshold the recall is at minimum. This leads us to choose ROC for the following two main reasons: Threshold Invariant By definition, ROC computes the pair \\(TPR \\times FPR\\) over all thresholds \\(t\\) , consequently, the AUROC is threshold invariant, allowing us to look at the model's performance over all thresholds. We note that ROC may not be that reliable in the case of very imbalanced datasets where majority is in the negative class, as \\(FPR = \\dfrac{FP}{FP+TN}\\) may seem deceptively low as denominator may be made small by the sheer amount of TN, in this case, we may also look at the Precision-Recall curve. Scale Invariant Technically, this is not the desired property that we need, as this means that the ROC is non-proper in scoring, it can take in non-calibrated scores and still perform relatively well. A classic example I always use is the following: y1 = [ 1 , 0 , 1 , 0 ] y2 = [ 0.52 , 0.51 , 0.52 , 0.51 ] y3 = [ 52 , 51 , 52 , 51 ] uncalibrated_roc = roc ( y1 , y2 ) == roc ( y1 , y3 ) print ( f \" { uncalibrated_roc } \" ) -> 1.0 The example tells us two things, as long as the ranking of predictions is preserved, the final AUROC score is the same, regardless of scale. We also notice that even though the model gives very unconfident predictions, the AUROC score is 1, which can be misleadingly over-optimistic. With that, we introduce Brier Score. Common Pitfalls Careful when using ROC function! We also note that when passing arguments to scikit-learn's roc_auc_score function, we should be careful not to pass y_score=model.predict(X) inside as we have to understand that we are passing in non-thresholded probabilities into y_score . If you pass the predicted values (full of 0 and 1s), then you are thresholding on 0 and 1 only, which is incorrect by definition. Brier Score Definition: Brier Score computes the squared difference between the probability of a prediction and its actual outcome. Brier Score is a strictly proper scoring rule while ROC is not ; the lower the Brier Score, the better the predictions are calibrated. We can first compute the AUROC score of the model, and compute Brier Score to give us how well calibrated (confident) the predictions are. Well Calibrated A intuitive way of understanding well calibrated probabilities is as follows, extracted from cambridge's probability calibration : In very simple terms, these are probabilities which can be interpreted as a confidence interval. Furthermore, a classifier is said to produce well calibrated probabilities if for the instances (data points) receiving probability 0.5, 50% of those instances belongs to the positive class. In my own words, if a classifier is well calibrated, say in our context where we predict binary target, and pretend that out of our test set, 100 of the samples have a probability of around 0.1, then this means 10% of these 100 samples actually belong to the positive class. The generic steps are as follows to calculate a calibrated plot: Sort all the samples by the classifier's predicted probabilities, in either ascending or descending order. Bin your diagram into N bins, usually we take 10, which means on the X-axis, note this does not mean we have 0-0.1, 0.1-0.2, ..., 0.9-1 as the 10 bins. What step 2 means is let's say you have 100 predictions, if you bin by 10 bins, and since the predictions are sorted , we can easily divide the 100 predictions into 10 intervals: for illustration, assume the 100 predictions are as follows, where we sort by ascending order and the prediction 0.1 has 10 of them, 0.2 have 10 of them, so on and so forth. y_pred = [ 0.1 , 0.1 , ..... , 0.2 , 0.2 , ... , 0.9 , 0.9 , ... , 1 , 1 , .. .1 ] Since we can divide the above into 10 bins, bin 1 will have 10 samples of predictions 0.1, bin 2 will have 10 samples of predictions 0.2, etc. We then take the mean of the predictions of each bin , that is for the first bin, we calculate \\(\\dfrac{1}{10}\\sum_{i=1}^{10}0.1 = 0.1\\) , and second bin, \\(\\dfrac{1}{10}\\sum_{i=1}^{10}0.2 = 0.2\\) . Note that this may not be such a nice number in reality, I made this example for the ease of illustration! Now, we have our X-axis from step 4, that is, we turned 10 bins, into 10 numbers, 0.1, 0.2, 0.3, ..., 1, and then we need to find the corresponding points for each of the 10 numbers! This is easy, for 0.1, the corresponding y-axis is just the fraction of positives , which means, out of the 10 samples in the first bin, how many of these 10 samples were actually positive? We do this for all 10 bins (points), and plot a line graph as seen in scikit-learn. Now this should be apparent now that a well calibrated model should lie close to the \\(y = x\\) line. That is, if the mean predicted probability is 0.1, then the y-axis should also be 0.1, meaning to say that out of all the samples that were predicted as 0.1, we should really only have about 10% of them being positive. The same logic applies to the rest! Brier Score Loss Brier Score Loss is a handy metric to measure whether a classifier is well calibrated, as quoted from scikit-learn : Brier Score Loss may be used to assess how well a classifier is calibrated. However, this metric should be used with care because a lower Brier score does not always mean a better calibrated model. This is because the Brier score metric is a combination of calibration loss and refinement loss. Calibration loss is defined as the mean squared deviation from empirical probabilities derived from the slope of ROC segments. Refinement loss can be defined as the expected optimal loss as measured by the area under the optimal cost curve. As refinement loss can change independently from calibration loss, a lower Brier score does not necessarily mean a better calibrated model. Common Pitfalls Class Imbalance: The good ol' class imbalance issue almost always pop up anywhere and everywhere. Intuitively, if we have a super rare positive/negative class, then if the model is very confident in its predictions for the majority class, but not so confident on the rare class, the overall Brier Score Loss may not be sufficient in discriminating the classifier's inability in correctly classifying the minority class.","title":"Modelling (Metrics)"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%204%20-%20Modelling%20%28Metric%20to%20Optimize%29/#define-metrics","text":"Disclaimer: For a more detailed understanding of different metrics, do navigate to my self-made notes on metrics here . Choosing a metric to measure the classifier's (hypothesis) performance is important, as choosing the wrong one can lead to disastrous interpretations. One prime example is using the accuracy metric for imbalanced datasets; consider 1 mil data points, dichotomized by \\(99\\%\\) benign and \\(1\\%\\) malignant samples, even a baseline model zeroR model which predicts the majority class no matter the process will give a \\(99\\%\\) accuracy, completely missing out any positive samples, which unfortunately, is what we may be more interested in. Say No to Accuracy: Consider an imbalanced set, where the training data set has 100 patients (data points), and the ground truth is 90 patients are of class = 0, which means that these patients do not have cancer, whereas the remaining 10 patients are in class 1, where they do have cancer. This is an example of class imbalance where the ratio of class 1 to class 0 is 1:9. Consider a baseline (almost trivial) classifier : def zeroR ( patient_data ): training ... return benign where we predict the patient's class as the most frequent class. Meaning, the most frequent class in this question is the class = 0, where patients do not have cancer, so we just assign this class to everyone in this set. By doing this, we will inevitably achieve a in-sample accuracy rate of \\(\\frac{90}{100} = 90\\%\\) . But unfortunately, this supposedly high accuracy value is completely useless, because this classifier did not label any of the cancer patients correctly. The consequence can be serious, assuming the test set has the same distribution as our training set, where if we have a test set of 1000 patients, there are 900 negative and 100 positive. Our model just literally predict every one of them as benign, yielding a \\(90\\%\\) out-of-sample accuracy. What did we conclude? Well, for one, our accuracy can be 90% high and looks good to the laymen, but it failed to predict the most important class of people - yes, misclassifying true cancer patients as healthy people is very bad! For the reasons mentioned above, we will use metric that can help us reduce False Negatives, and at the same time, outputs meaningful predictions. In order to achieve for both, we will use Receiver operating characteristic (ROC) as the primary metric for the model to maximize (which is our \\(\\mathcal{M}\\) , and Brier Score , a proper scoring rule to measure the performance of our probabilistic predictions. We will go into some details in the next two subsections to justify our choice.","title":"Define Metrics"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%204%20-%20Modelling%20%28Metric%20to%20Optimize%29/#proper-scoring-rule","text":"The math behind the idea of Proper Scoring Rule is non-trivial. Here, we try to understand why a proper scoring rule is desired in the context of binary classification. Strictly Proper Scoring Rule: Brier Score Loss, for example, tells us that the best possible score, 0 (lowest loss), is obtained if and only if, the probability prediction we get for a sample, is the true probability itself. In other words, if a selected sample is of class 1, our prediction for this must be 1, with 100% probability, in order to get a score loss of 0. Proper Scoring Rule: Read [here](https://stats.stackexchange.com/questions/339919/what-does-it-mean-that-auc-is-a-semi-proper-scoring-rule) for this. Semi Proper Scoring Rule: AUROC, as mentioned, does not help out in telling whether a prediction by a classifier is close to the true probability or not. In our example, we even see that we can obtain a full score of 1, even if the probabilities all lie within 0.51 and 0.52. Improper Scoring Rule: Accuracy is a prime example, the accuracy score does not, whatsoever, tells us about how close our predicted probabilities are, to the true probability distribution of our samples.","title":"Proper Scoring Rule"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%204%20-%20Modelling%20%28Metric%20to%20Optimize%29/#receiver-operating-characteristic-roc","text":"Definition: The basic (non-probablistic intepretation) of ROC is graph that plots the True Positive Rate on the y-axis and False Positive Rate on the x-axis parametrized by a threshold vector $\\vec{t}$. We then look at the area under the ROC curve (AUROC) to get an overall performance measure. The choice of ROC over other metrics such as Accuracy is detailed initially. We also established we want to reduce False Negative (FN), since misclassifying a positive patient as benign is way more costly than the other way round. One can choose to minimize Recall in order to reduce FN, but this is less than ideal during training because it is a thresholded metric, and does not provide at which threshold the recall is at minimum. This leads us to choose ROC for the following two main reasons:","title":"Receiver operating characteristic (ROC)"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%204%20-%20Modelling%20%28Metric%20to%20Optimize%29/#threshold-invariant","text":"By definition, ROC computes the pair \\(TPR \\times FPR\\) over all thresholds \\(t\\) , consequently, the AUROC is threshold invariant, allowing us to look at the model's performance over all thresholds. We note that ROC may not be that reliable in the case of very imbalanced datasets where majority is in the negative class, as \\(FPR = \\dfrac{FP}{FP+TN}\\) may seem deceptively low as denominator may be made small by the sheer amount of TN, in this case, we may also look at the Precision-Recall curve.","title":"Threshold Invariant"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%204%20-%20Modelling%20%28Metric%20to%20Optimize%29/#scale-invariant","text":"Technically, this is not the desired property that we need, as this means that the ROC is non-proper in scoring, it can take in non-calibrated scores and still perform relatively well. A classic example I always use is the following: y1 = [ 1 , 0 , 1 , 0 ] y2 = [ 0.52 , 0.51 , 0.52 , 0.51 ] y3 = [ 52 , 51 , 52 , 51 ] uncalibrated_roc = roc ( y1 , y2 ) == roc ( y1 , y3 ) print ( f \" { uncalibrated_roc } \" ) -> 1.0 The example tells us two things, as long as the ranking of predictions is preserved, the final AUROC score is the same, regardless of scale. We also notice that even though the model gives very unconfident predictions, the AUROC score is 1, which can be misleadingly over-optimistic. With that, we introduce Brier Score.","title":"Scale Invariant"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%204%20-%20Modelling%20%28Metric%20to%20Optimize%29/#common-pitfalls","text":"Careful when using ROC function! We also note that when passing arguments to scikit-learn's roc_auc_score function, we should be careful not to pass y_score=model.predict(X) inside as we have to understand that we are passing in non-thresholded probabilities into y_score . If you pass the predicted values (full of 0 and 1s), then you are thresholding on 0 and 1 only, which is incorrect by definition.","title":"Common Pitfalls"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%204%20-%20Modelling%20%28Metric%20to%20Optimize%29/#brier-score","text":"Definition: Brier Score computes the squared difference between the probability of a prediction and its actual outcome. Brier Score is a strictly proper scoring rule while ROC is not ; the lower the Brier Score, the better the predictions are calibrated. We can first compute the AUROC score of the model, and compute Brier Score to give us how well calibrated (confident) the predictions are.","title":"Brier Score"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%204%20-%20Modelling%20%28Metric%20to%20Optimize%29/#well-calibrated","text":"A intuitive way of understanding well calibrated probabilities is as follows, extracted from cambridge's probability calibration : In very simple terms, these are probabilities which can be interpreted as a confidence interval. Furthermore, a classifier is said to produce well calibrated probabilities if for the instances (data points) receiving probability 0.5, 50% of those instances belongs to the positive class. In my own words, if a classifier is well calibrated, say in our context where we predict binary target, and pretend that out of our test set, 100 of the samples have a probability of around 0.1, then this means 10% of these 100 samples actually belong to the positive class. The generic steps are as follows to calculate a calibrated plot: Sort all the samples by the classifier's predicted probabilities, in either ascending or descending order. Bin your diagram into N bins, usually we take 10, which means on the X-axis, note this does not mean we have 0-0.1, 0.1-0.2, ..., 0.9-1 as the 10 bins. What step 2 means is let's say you have 100 predictions, if you bin by 10 bins, and since the predictions are sorted , we can easily divide the 100 predictions into 10 intervals: for illustration, assume the 100 predictions are as follows, where we sort by ascending order and the prediction 0.1 has 10 of them, 0.2 have 10 of them, so on and so forth. y_pred = [ 0.1 , 0.1 , ..... , 0.2 , 0.2 , ... , 0.9 , 0.9 , ... , 1 , 1 , .. .1 ] Since we can divide the above into 10 bins, bin 1 will have 10 samples of predictions 0.1, bin 2 will have 10 samples of predictions 0.2, etc. We then take the mean of the predictions of each bin , that is for the first bin, we calculate \\(\\dfrac{1}{10}\\sum_{i=1}^{10}0.1 = 0.1\\) , and second bin, \\(\\dfrac{1}{10}\\sum_{i=1}^{10}0.2 = 0.2\\) . Note that this may not be such a nice number in reality, I made this example for the ease of illustration! Now, we have our X-axis from step 4, that is, we turned 10 bins, into 10 numbers, 0.1, 0.2, 0.3, ..., 1, and then we need to find the corresponding points for each of the 10 numbers! This is easy, for 0.1, the corresponding y-axis is just the fraction of positives , which means, out of the 10 samples in the first bin, how many of these 10 samples were actually positive? We do this for all 10 bins (points), and plot a line graph as seen in scikit-learn. Now this should be apparent now that a well calibrated model should lie close to the \\(y = x\\) line. That is, if the mean predicted probability is 0.1, then the y-axis should also be 0.1, meaning to say that out of all the samples that were predicted as 0.1, we should really only have about 10% of them being positive. The same logic applies to the rest!","title":"Well Calibrated"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%204%20-%20Modelling%20%28Metric%20to%20Optimize%29/#brier-score-loss","text":"Brier Score Loss is a handy metric to measure whether a classifier is well calibrated, as quoted from scikit-learn : Brier Score Loss may be used to assess how well a classifier is calibrated. However, this metric should be used with care because a lower Brier score does not always mean a better calibrated model. This is because the Brier score metric is a combination of calibration loss and refinement loss. Calibration loss is defined as the mean squared deviation from empirical probabilities derived from the slope of ROC segments. Refinement loss can be defined as the expected optimal loss as measured by the area under the optimal cost curve. As refinement loss can change independently from calibration loss, a lower Brier score does not necessarily mean a better calibrated model.","title":"Brier Score Loss"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%204%20-%20Modelling%20%28Metric%20to%20Optimize%29/#common-pitfalls_1","text":"Class Imbalance: The good ol' class imbalance issue almost always pop up anywhere and everywhere. Intuitively, if we have a super rare positive/negative class, then if the model is very confident in its predictions for the majority class, but not so confident on the rare class, the overall Brier Score Loss may not be sufficient in discriminating the classifier's inability in correctly classifying the minority class.","title":"Common Pitfalls"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%205%20-%20Modelling%20%28Cross-Validation%20Methodology%29/","text":"Stage 5: Modelling (Cross-Validation) by Hongnan Gao Dependencies and Configuration import logging import random from dataclasses import dataclass , field from time import time from typing import Any , Callable , Dict , List , Optional , Union import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from sklearn import model_selection @dataclass class config : raw_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/raw/data.csv\" processed_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/processed.csv\" train_size : float = 0.9 seed : int = 1992 num_folds : int = 5 cv_schema : str = \"StratifiedKFold\" classification_type : str = \"binary\" target_col : List [ str ] = field ( default_factory = lambda : [ \"diagnosis\" ]) unwanted_cols : List [ str ] = field ( default_factory = lambda : [ \"id\" , \"Unnamed: 32\" ]) # Plotting colors : List [ str ] = field ( default_factory = lambda : [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ]) cmap_reversed = plt . cm . get_cmap ( 'mako_r' ) def to_dict ( self ) -> Dict : \"\"\"Convert the config object to a dictionary. Returns: Dict: The config object as a dictionary. \"\"\" return { \"raw_data\" : self . raw_data , \"processed_data\" : self . processed_data , \"train_size\" : self . train_size , \"seed\" : self . seed , \"num_folds\" : self . num_folds , \"cv_schema\" : self . cv_schema , \"classification_type\" : self . classification_type , \"target_col\" : self . target_col , \"unwanted_cols\" : self . unwanted_cols , \"colors\" : self . colors , \"cmap_reversed\" : self . cmap_reversed } def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) def init_logger ( log_file : str = \"info.log\" ): \"\"\" Initialize logger. \"\"\" logger = logging . getLogger ( __name__ ) logger . setLevel ( logging . INFO ) stream_handler = logging . StreamHandler () stream_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) file_handler = logging . FileHandler ( filename = log_file ) file_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) logger . addHandler ( stream_handler ) logger . addHandler ( file_handler ) return logger config = config () logger = init_logger () # set seeding for reproducibility _ = set_seeds ( seed = config . seed ) # read data df = pd . read_csv ( config . processed_data ) Cross-Validation Strategy Generalization Ultimately, we are interested in the Generalization Error made by the model, that is, how well the model perform on unseen data that is not taken from our sample set \\(\\mathcal{D}\\) . In general, we use validation set for Model Selection and the test set for an estimate of generalization error on new data. - Refactored from Elements of Statistical Learning, Chapter 7.2 Step 1: Train-Test-Split Since this dataset is relatively small, we will not use the train-validation-test split and only split into train and test in a ratio of 9:1, whereby the split is stratified on our target, using stratify=y parameter in train_test_split() to ensure that our target has equal representation in both train and test. We note that this is a relatively small dataset and in practice, we need a large sample size to get a reliable/stable split, it is also recommended to retrain the whole dataset (without the \"unseen\" test set) after we have done the model selection process (eg. finding best hyperparameters). Step 2: Resampling Strategy Note that we will be performing StratifiedKFold as our resampling strategy. After our split in Step 1, we have a training set \\(X_{\\text{train}}\\) , we will then perform our resampling strategy on this \\(X_{\\text{train}}\\) . We will choose our choice of \\(K = 5\\) . The choice of \\(K\\) is somewhat arbitrary, and is derived empirically . Cross-Validation Workflow To recap, we have the following: Training Set ( \\(X_{\\text{train}}\\) ) : This will be further split into K validation sets during our cross-validation. This set is used to fit a particular hypothesis \\(h \\in \\mathcal{H}\\) . Validation Set ( \\(X_{\\text{val}}\\) ) : This is split from our \\(X_{\\text{train}}\\) during cross-validation. This set is used for model selection (i.e. find best hyperparameters, attempt to produce a best hypothesis \\(g \\in \\mathcal{H}\\) ). Test Set ( \\(X_{\\text{test}}\\) ) : This is an unseen test set, and we will only use it after we finish tuning our model/hypothesis. Suppose we have a final best model \\(g\\) , we will use \\(g\\) to predict on the test set to get an estimate of the generalization error (also called out-of-sample error). Courtesy of scikit-learn on a typical Cross-Validation workflow. # Make a copy of df and assign it to X X = df . copy () # Pop diagnosis, the target column from X and assign the target column data to y y = X . pop ( \"diagnosis\" ) # Assign predictors and target accordingly predictor_cols = X . columns . to_list () target_col = config . target_col # Split train - test X_train , X_test , y_train , y_test = model_selection . train_test_split ( X , y , train_size = config . train_size , shuffle = True , stratify = y , random_state = config . seed , ) We confirm that we have stratified properly. We do observe that the distribution of targets in both y_train and y_test are similar. # Log and send a class proportion plot to wandb for both train and test set logger . info ( f \"Y Train Distribution is : { y_train . value_counts ( normalize = True ) . to_dict () } \" ) logger . info ( f \"Y Test Distribution is : { y_test . value_counts ( normalize = True ) . to_dict () } \" ) # wandb.sklearn.plot_class_proportions(y_train, y_test, labels=[0, 1]) 2021-11-13,09:53:22 - Y Train Distribution is : {0: 0.626953125, 1: 0.373046875} 2021-11-13,09:53:22 - Y Test Distribution is : {0: 0.631578947368421, 1: 0.3684210526315789} def make_folds ( df : pd . DataFrame , num_folds : int , cv_schema : str , seed : int , predictor_col : List , target_col : List , ) -> pd . DataFrame : \"\"\"Split the given dataframe into training folds. Args: df (pd.DataFrame): The dataframe to be split. num_folds (int): The number of folds to be created. cv_schema (str): The type of cross validation to be used. seed (int): The seed number to be used. Returns: df_folds (pd.DataFrame): The dataframe containing the folds. \"\"\" if cv_schema == \"KFold\" : df_folds = df . copy () kf = model_selection . KFold ( n_splits = num_folds , shuffle = True , random_state = seed ) for fold , ( train_idx , val_idx ) in enumerate ( kf . split ( X = df_folds [ predictor_col ], y = df_folds [ target_col ]) ): df_folds . loc [ val_idx , \"fold\" ] = int ( fold + 1 ) df_folds [ \"fold\" ] = df_folds [ \"fold\" ] . astype ( int ) elif cv_schema == \"StratifiedKFold\" : df_folds = df . copy () skf = model_selection . StratifiedKFold ( n_splits = num_folds , shuffle = True , random_state = seed ) for fold , ( train_idx , val_idx ) in enumerate ( skf . split ( X = df_folds [ predictor_col ], y = df_folds [ target_col ]) ): df_folds . loc [ val_idx , \"fold\" ] = int ( fold + 1 ) df_folds [ \"fold\" ] = df_folds [ \"fold\" ] . astype ( int ) print ( df_folds . groupby ([ \"fold\" , \"diagnosis\" ]) . size ()) return df_folds # Concat X_train and y_train to apply make_folds on it and return a new dataframe df_folds with # an additional column fold to indicate each sample's fold X_y_train = pd . concat ([ X_train , y_train ], axis = 1 ) . reset_index ( drop = True ) df_folds = make_folds ( X_y_train , num_folds = config . num_folds , cv_schema = config . cv_schema , seed = config . seed , predictor_col = predictor_cols , target_col = config . target_col , ) # TODO: write directly to GCP df_folds . to_csv ( \"df_folds.csv\" , index = False ) fold diagnosis 1 0 64 1 39 2 0 65 1 38 3 0 64 1 38 4 0 64 1 38 5 0 64 1 38 dtype: int64 Looks good! All our five folds are stratified!","title":"Modelling (Cross-Validation Schema)"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%205%20-%20Modelling%20%28Cross-Validation%20Methodology%29/#dependencies-and-configuration","text":"import logging import random from dataclasses import dataclass , field from time import time from typing import Any , Callable , Dict , List , Optional , Union import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from sklearn import model_selection @dataclass class config : raw_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/raw/data.csv\" processed_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/processed.csv\" train_size : float = 0.9 seed : int = 1992 num_folds : int = 5 cv_schema : str = \"StratifiedKFold\" classification_type : str = \"binary\" target_col : List [ str ] = field ( default_factory = lambda : [ \"diagnosis\" ]) unwanted_cols : List [ str ] = field ( default_factory = lambda : [ \"id\" , \"Unnamed: 32\" ]) # Plotting colors : List [ str ] = field ( default_factory = lambda : [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ]) cmap_reversed = plt . cm . get_cmap ( 'mako_r' ) def to_dict ( self ) -> Dict : \"\"\"Convert the config object to a dictionary. Returns: Dict: The config object as a dictionary. \"\"\" return { \"raw_data\" : self . raw_data , \"processed_data\" : self . processed_data , \"train_size\" : self . train_size , \"seed\" : self . seed , \"num_folds\" : self . num_folds , \"cv_schema\" : self . cv_schema , \"classification_type\" : self . classification_type , \"target_col\" : self . target_col , \"unwanted_cols\" : self . unwanted_cols , \"colors\" : self . colors , \"cmap_reversed\" : self . cmap_reversed } def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) def init_logger ( log_file : str = \"info.log\" ): \"\"\" Initialize logger. \"\"\" logger = logging . getLogger ( __name__ ) logger . setLevel ( logging . INFO ) stream_handler = logging . StreamHandler () stream_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) file_handler = logging . FileHandler ( filename = log_file ) file_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) logger . addHandler ( stream_handler ) logger . addHandler ( file_handler ) return logger config = config () logger = init_logger () # set seeding for reproducibility _ = set_seeds ( seed = config . seed ) # read data df = pd . read_csv ( config . processed_data )","title":"Dependencies and Configuration"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%205%20-%20Modelling%20%28Cross-Validation%20Methodology%29/#cross-validation-strategy","text":"Generalization Ultimately, we are interested in the Generalization Error made by the model, that is, how well the model perform on unseen data that is not taken from our sample set \\(\\mathcal{D}\\) . In general, we use validation set for Model Selection and the test set for an estimate of generalization error on new data. - Refactored from Elements of Statistical Learning, Chapter 7.2","title":"Cross-Validation Strategy"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%205%20-%20Modelling%20%28Cross-Validation%20Methodology%29/#step-1-train-test-split","text":"Since this dataset is relatively small, we will not use the train-validation-test split and only split into train and test in a ratio of 9:1, whereby the split is stratified on our target, using stratify=y parameter in train_test_split() to ensure that our target has equal representation in both train and test. We note that this is a relatively small dataset and in practice, we need a large sample size to get a reliable/stable split, it is also recommended to retrain the whole dataset (without the \"unseen\" test set) after we have done the model selection process (eg. finding best hyperparameters).","title":"Step 1: Train-Test-Split"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%205%20-%20Modelling%20%28Cross-Validation%20Methodology%29/#step-2-resampling-strategy","text":"Note that we will be performing StratifiedKFold as our resampling strategy. After our split in Step 1, we have a training set \\(X_{\\text{train}}\\) , we will then perform our resampling strategy on this \\(X_{\\text{train}}\\) . We will choose our choice of \\(K = 5\\) . The choice of \\(K\\) is somewhat arbitrary, and is derived empirically .","title":"Step 2: Resampling Strategy"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%205%20-%20Modelling%20%28Cross-Validation%20Methodology%29/#cross-validation-workflow","text":"To recap, we have the following: Training Set ( \\(X_{\\text{train}}\\) ) : This will be further split into K validation sets during our cross-validation. This set is used to fit a particular hypothesis \\(h \\in \\mathcal{H}\\) . Validation Set ( \\(X_{\\text{val}}\\) ) : This is split from our \\(X_{\\text{train}}\\) during cross-validation. This set is used for model selection (i.e. find best hyperparameters, attempt to produce a best hypothesis \\(g \\in \\mathcal{H}\\) ). Test Set ( \\(X_{\\text{test}}\\) ) : This is an unseen test set, and we will only use it after we finish tuning our model/hypothesis. Suppose we have a final best model \\(g\\) , we will use \\(g\\) to predict on the test set to get an estimate of the generalization error (also called out-of-sample error). Courtesy of scikit-learn on a typical Cross-Validation workflow. # Make a copy of df and assign it to X X = df . copy () # Pop diagnosis, the target column from X and assign the target column data to y y = X . pop ( \"diagnosis\" ) # Assign predictors and target accordingly predictor_cols = X . columns . to_list () target_col = config . target_col # Split train - test X_train , X_test , y_train , y_test = model_selection . train_test_split ( X , y , train_size = config . train_size , shuffle = True , stratify = y , random_state = config . seed , ) We confirm that we have stratified properly. We do observe that the distribution of targets in both y_train and y_test are similar. # Log and send a class proportion plot to wandb for both train and test set logger . info ( f \"Y Train Distribution is : { y_train . value_counts ( normalize = True ) . to_dict () } \" ) logger . info ( f \"Y Test Distribution is : { y_test . value_counts ( normalize = True ) . to_dict () } \" ) # wandb.sklearn.plot_class_proportions(y_train, y_test, labels=[0, 1]) 2021-11-13,09:53:22 - Y Train Distribution is : {0: 0.626953125, 1: 0.373046875} 2021-11-13,09:53:22 - Y Test Distribution is : {0: 0.631578947368421, 1: 0.3684210526315789} def make_folds ( df : pd . DataFrame , num_folds : int , cv_schema : str , seed : int , predictor_col : List , target_col : List , ) -> pd . DataFrame : \"\"\"Split the given dataframe into training folds. Args: df (pd.DataFrame): The dataframe to be split. num_folds (int): The number of folds to be created. cv_schema (str): The type of cross validation to be used. seed (int): The seed number to be used. Returns: df_folds (pd.DataFrame): The dataframe containing the folds. \"\"\" if cv_schema == \"KFold\" : df_folds = df . copy () kf = model_selection . KFold ( n_splits = num_folds , shuffle = True , random_state = seed ) for fold , ( train_idx , val_idx ) in enumerate ( kf . split ( X = df_folds [ predictor_col ], y = df_folds [ target_col ]) ): df_folds . loc [ val_idx , \"fold\" ] = int ( fold + 1 ) df_folds [ \"fold\" ] = df_folds [ \"fold\" ] . astype ( int ) elif cv_schema == \"StratifiedKFold\" : df_folds = df . copy () skf = model_selection . StratifiedKFold ( n_splits = num_folds , shuffle = True , random_state = seed ) for fold , ( train_idx , val_idx ) in enumerate ( skf . split ( X = df_folds [ predictor_col ], y = df_folds [ target_col ]) ): df_folds . loc [ val_idx , \"fold\" ] = int ( fold + 1 ) df_folds [ \"fold\" ] = df_folds [ \"fold\" ] . astype ( int ) print ( df_folds . groupby ([ \"fold\" , \"diagnosis\" ]) . size ()) return df_folds # Concat X_train and y_train to apply make_folds on it and return a new dataframe df_folds with # an additional column fold to indicate each sample's fold X_y_train = pd . concat ([ X_train , y_train ], axis = 1 ) . reset_index ( drop = True ) df_folds = make_folds ( X_y_train , num_folds = config . num_folds , cv_schema = config . cv_schema , seed = config . seed , predictor_col = predictor_cols , target_col = config . target_col , ) # TODO: write directly to GCP df_folds . to_csv ( \"df_folds.csv\" , index = False ) fold diagnosis 1 0 64 1 39 2 0 65 1 38 3 0 64 1 38 4 0 64 1 38 5 0 64 1 38 dtype: int64 Looks good! All our five folds are stratified!","title":"Cross-Validation Workflow"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%206%20-%20Modelling%20%28Preprocessing%20and%20Spot%20Checking%29/","text":"Stage 6: Spot Checking Algorithms by Hongnan Gao Dependencies and Configuration # !pip install gcloud == 0.18.3 ! pip install - q mlxtend == 0.19.0 ! pip install - q statsmodels == 0.13.1 import copy import csv import logging import random from dataclasses import dataclass , field from functools import wraps from time import time from typing import Any , Callable , Dict , List , Optional , Union , Tuple import matplotlib.pyplot as plt import mlxtend import numpy as np import pandas as pd import seaborn as sns from mlxtend.evaluate import bias_variance_decomp , paired_ttest_5x2cv from scipy import stats from sklearn import ( base , decomposition , dummy , ensemble , feature_selection , linear_model , metrics , model_selection , neighbors , pipeline , preprocessing , svm , tree ) from statsmodels.regression.linear_model import OLS \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.3 MB 5.2 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.8 MB 1.3 MB/s \u001b[?25h @dataclass class config : raw_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/raw/data.csv\" processed_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/processed.csv\" df_folds : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/df_folds.csv\" train_size : float = 0.9 seed : int = 1992 num_folds : int = 5 cv_schema : str = \"StratifiedKFold\" classification_type : str = \"binary\" target_col : List [ str ] = field ( default_factory = lambda : [ \"diagnosis\" ]) unwanted_cols : List [ str ] = field ( default_factory = lambda : [ \"id\" , \"Unnamed: 32\" ]) # Plotting colors : List [ str ] = field ( default_factory = lambda : [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ]) cmap_reversed = plt . cm . get_cmap ( 'mako_r' ) def to_dict ( self ) -> Dict : \"\"\"Convert the config object to a dictionary. Returns: Dict: The config object as a dictionary. \"\"\" return { \"raw_data\" : self . raw_data , \"processed_data\" : self . processed_data , \"train_size\" : self . train_size , \"seed\" : self . seed , \"num_folds\" : self . num_folds , \"cv_schema\" : self . cv_schema , \"classification_type\" : self . classification_type , \"target_col\" : self . target_col , \"unwanted_cols\" : self . unwanted_cols , \"colors\" : self . colors , \"cmap_reversed\" : self . cmap_reversed } # spot_checking_boxplot = \"../data/images/spot_checking_boxplot.png\" # oof_confusion_matrix = \"../data/images/oof_confusion_matrix.png\" # final_train_confusion_matrix = \"../data/images/final_train_confusion_matrix.png\" # precision_recall_threshold_plot = \"../data/images/precision_recall_threshold_plot.png\" # roc_plot = \"../data/images/roc_plot.png\" # feature_importance = \"../data/images/feature_importance.png\" def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) def init_logger ( log_file : str = \"info.log\" ): \"\"\" Initialize logger. \"\"\" logger = logging . getLogger ( __name__ ) logger . setLevel ( logging . INFO ) stream_handler = logging . StreamHandler () stream_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) file_handler = logging . FileHandler ( filename = log_file ) file_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) logger . addHandler ( stream_handler ) logger . addHandler ( file_handler ) return logger # Utils functions that we need def variance_inflation_factor ( exog , idx_kept , vif_idx ): \"\"\"Compute VIF for one feature. Args: exog (np.ndarray): Observations idx_kept (List[int]): Indices of features to consider vif_idx (int): Index of feature for which to compute VIF Returns: float: VIF for the selected feature \"\"\" exog = np . asarray ( exog ) x_i = exog [:, vif_idx ] mask = [ col for col in idx_kept if col != vif_idx ] x_noti = exog [:, mask ] r_squared_i = OLS ( x_i , x_noti ) . fit () . rsquared vif = 1. / ( 1. - r_squared_i ) return vif class ReduceVIF ( base . BaseEstimator , base . TransformerMixin ): \"\"\"The base of the class structure is implemented in https://www.kaggle.com/ffisegydd/sklearn-multicollinearity-class; I heavily modified the class such that it can take in numpy arrays and correctly implemented the fit and transform method. \"\"\" def __init__ ( self , thresh = 10 , max_drop = 20 ): self . thresh = thresh self . max_drop = max_drop self . column_indices_kept_ = [] self . feature_names_kept_ = None def reset ( self ): \"\"\"Resets the state of predictor columns after each fold.\"\"\" self . column_indices_kept_ = [] self . feature_names_kept_ = None def fit ( self , X , y = None ): \"\"\"Fits the Recursive VIF on the training folds and save the selected feature names in self.feature_names Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" self . column_indices_kept_ , self . feature_names_kept_ = self . calculate_vif ( X ) return self def transform ( self , X , y = None ): \"\"\"Transforms the Validation Set according to the selected feature names. Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" return X [:, self . column_indices_kept_ ] def calculate_vif ( self , X : Union [ np . ndarray , pd . DataFrame ]): \"\"\"Implements a VIF function that recursively eliminates features. Args: X (Union[np.ndarray, pd.DataFrame]): [description] Returns: [type]: [description] \"\"\" feature_names = None column_indices_kept = list ( range ( X . shape [ 1 ])) if isinstance ( X , pd . DataFrame ): feature_names = X . columns dropped = True count = 0 while dropped and count <= self . max_drop : dropped = False max_vif , max_vif_col = None , None for col in column_indices_kept : vif = variance_inflation_factor ( X , column_indices_kept , col ) if max_vif is None or vif > max_vif : max_vif = vif max_vif_col = col if max_vif > self . thresh : # print(f\"Dropping {max_vif_col} with vif={max_vif}\") column_indices_kept . remove ( max_vif_col ) if feature_names is not None : feature_names . pop ( max_vif_col ) dropped = True count += 1 return column_indices_kept , feature_names config = config () logger = init_logger () # set seeding for reproducibility _ = set_seeds ( seed = config . seed ) # read data df_folds = pd . read_csv ( config . df_folds ) # Assign predictors and target accordingly predictor_cols = df_folds . columns . to_list ()[: - 2 ] target_col = config . target_col Spot Checking Algorithms Terminology Alert! This method is advocated by Jason Brownlee PhD and this serves as the first stage of my modelling process. We will rapidly test (spot check) different classifier algorithms, from DummyClassifier , to LinearModel to more sophisticated ensemble trees like RandomForest . I also note to the readers that we need to think of a few things when choosing the \"optimal\" machine learning algorithm: No Lunch Free Theorem intuitively says that no single optimization algorithm can work best in all situations. Therefore, spot checking can help us form a basis of which algorithm might work better in this particular scenario. Occam's Razor often appears in many Machine Learning textbook, and the narrative is that a simpler model more often times generalizes better than a complex model. This is not unfamiliar when we think of the bias-variance tradeoff, and that is why there is always a tradeoff that we must make. Say No to Data Leakage! Say No to Data Leakage: This has been emphasized throughout and we must be careful as we should never touch the test set when fitting the model. In fact, we should try our best to not contaminate our validation set as well. This means that preprocessing steps such as StandardScaling() should only be fitted on the training data, and then apply the same transformation (mean and std) on the test data. In other words, do not apply scaling on the whole dataset before splitting. However, it is also equally important to take note not to contaminate our validation set, which is often overlooked, resulting in over optimistic results from model selection phase, but perform badly on unseen test set. As a result, when we use a 5 fold cross validation, we should be careful during fitting that the preprocessing steps are only applied on the training folds, and not on all 5 folds. The same idea is also applied to our ReduceVIF() preprocessing step. We should also include this in our pipeline and not select the features outside the cross-validation loop. Quoting from scikit-learn : Data leakage occurs when information that would not be available at prediction time is used when building the model. This results in overly optimistic performance estimates, for example from cross-validation , and thus poorer performance when the model is used on actually novel data, for example during production. A common cause is not keeping the test and train data subsets separate. Test data should never be used to make choices about the model. The general rule is to never call fit on the test data. While this may sound obvious, this is easy to miss in some cases, for example when applying certain pre-processing steps. Although both train and test data subsets should receive the same preprocessing transformation (as described in the previous section), it is important that these transformations are only learnt from the training data. For example, if you have a normalization step where you divide by the average value, the average should be the average of the train subset, not the average of all the data. If the test subset is included in the average calculation, information from the test subset is influencing the model. How to avoid Data Leakage? We know the pitfalls of fitting on validation/test data, the natural question is how can we avoid it completely? You can code it up yourself, but as a starter, we can use scikit-learn's Pipeline object. My tips are as follows: Any preprocessing step must be done after splitting the whole dataset into train and test. If you are also using cross-validation, then we should only apply the preprocessing steps on the train set, and then use the metrics obtained from the train set to transform the validation set. You can see my pseudo-code below for a rough outline. The Pipeline object of Scikit-Learn can help prevent data leakage. Pseudo-Code of Cross-Validation and Pipeline The below outlines a pseudo code of the cross-validation scheme using Pipeline object. Note that I included the most outer loop, which is searching for hyperparameters. Define \\(G\\) as the set of combination of hyperparamters. Define number of splits to be \\(K\\) . For each set of hyperparameter \\(z \\in Z\\) : for fold \\(j\\) in K: Set \\(F_{\\text{train}}=\\bigcup\\limits_{i\\neq k}^{K} F_{i}\\) Set \\(F_{\\text{val}} = F_{j}\\) as the validation set Perform Standard Scaling on \\(F_{\\text{train}}\\) and find the mean and std Perform VIF recursively on \\(F_{\\text{train}}\\) and find the selected features Transform \\(F_{\\text{val}}\\) using the mean and std found using \\(F_{\\text{train}}\\) Transform \\(F_{\\text{val}}\\) to have only the selected features from \\(F_{\\text{train}}\\) Train and fit on \\(F_{\\text{train}}\\) Evaluate the fitted parameters on \\(F_{\\text{val}}\\) to obtain \\(\\mathcal{M}\\) Training Pipeline Make Pipeline # @ TODO: https://www.kaggle.com/kabure/predicting-house-prices-xgb-rf-bagging-reg-pipe # Different models can potentially have different pre-processing steps, consider putting steps as a # passable list. def make_pipeline ( model : Callable ) -> Callable : \"\"\"Create a feature preparation pipeline for a model. Args: model (Callable): The model to be used. Returns: _pipeline (Callable): pipeline object \"\"\" # Create a list of steps, note that some models may not need certain steps, and hence # may need an if-else here steps = list () # standardization steps . append (( \"standardize\" , preprocessing . StandardScaler ())) # reduce VIF steps . append (( \"remove_multicollinearity\" , ReduceVIF ( thresh = 10 ))) # the model to be appended at the last step steps . append (( \"model\" , model )) # create pipeline _pipeline = pipeline . Pipeline ( steps = steps ) return _pipeline classifiers = [ # baseline model dummy . DummyClassifier ( random_state = config . seed , strategy = \"stratified\" ), # linear model linear_model . LogisticRegression ( random_state = config . seed , solver = \"liblinear\" ), # nearest neighbours neighbors . KNeighborsClassifier ( n_neighbors = 8 ), # SVM svm . SVC ( probability = True , random_state = config . seed ), # tree tree . DecisionTreeClassifier ( random_state = config . seed ), # ensemble ensemble . RandomForestClassifier ( n_estimators = 10 , random_state = config . seed ), ] classifiers = [ make_pipeline ( model ) for model in classifiers ] Results Class The Results class will help us store model's results. Careful when using ROC function! We also note that when passing arguments to scikit-learn's roc_auc_score function, we should be careful not to pass y_score=model.predict(X) inside as we have to understand that we are passing in non-thresholded probabilities into y_score . If you pass the predicted values (full of 0 and 1s), then you are thresholding on 0 and 1 only, which is incorrect by definition. default_result_names = [ \"y_true\" , \"y_pred\" , \"y_prob\" ] default_logit_names = [ \"y_true\" , \"y_pred\" , \"y_prob\" ] default_score_names = [ \"accuracy_score\" , \"precision_recall_fscore_support\" , \"confusion_matrix\" , # \"average_precision_score\", \"multiclass_roc_auc_score\" , \"brier_score_loss\" , ] custom_score_names = [ \"multiclass_roc_auc_score\" , \"brier_score_loss\" ] use_preds = [ \"accuracy_score\" , \"precision_recall_fscore_support\" , \"confusion_matrix\" , ] use_probs = [ \"average_precision_score\" ] class Results : \"\"\"Stores results for model training in columnwise format.\"\"\" _result_dict : Dict logit_names : List [ str ] score_names : List [ str ] def __init__ ( self , logit_names : List [ str ] = default_logit_names , score_names : List [ str ] = default_score_names , existing_dict : Optional [ Dict ] = None , ): \"\"\"Construct a new results store.\"\"\" self . logit_names = logit_names self . score_names = score_names if existing_dict is not None : self . _result_dict = copy . deepcopy ( existing_dict ) return dict_keys = [ \"identifier\" , * logit_names , * score_names ] self . _result_dict = { key : [] for key in dict_keys } def add ( self , identifier : str , results : Dict , in_place = False ): \"\"\"Add a new results row.\"\"\" if not in_place : return Results ( self . logit_names , self . score_names , self . _result_dict ) . add ( identifier , results , in_place = True ) self . _result_dict [ \"identifier\" ] . append ( identifier ) for result_name in set ([ * results . keys (), * self . logit_names , * self . score_names ]): result_value = results . get ( result_name , np . nan ) self . _result_dict [ result_name ] . append ( result_value ) return self def get_result ( self , result_name : str ) -> Dict [ str , Any ]: \"\"\"Get a map of identifiers to result values for a result.\"\"\" return { identifier : result_value for identifier , result_value in zip ( self . _result_dict [ \"identifier\" ], self . _result_dict [ result_name ]) } def get_result_values ( self , result_name : str ) -> List [ Any ]: \"\"\"Get a list of values for a result.\"\"\" return self . _result_dict [ result_name ] def to_dataframe ( self ) -> pd . DataFrame : \"\"\"Get a Data Frame containing the results.\"\"\" return pd . DataFrame . from_dict ( self . _result_dict ) def to_dict ( self ) -> Dict : \"\"\"Get a dictionary containing the results. Returns: Dict[str, List[Any]]: Dictionary of result columns \"\"\" return self . _result_dict def multiclass_label_binarize ( y : np . ndarray , class_labels : List [ int ], pos_label = 1 , neg_label = 0 ): \"\"\"Binarize labels in one-vs-all fashion. # TODO: to replace with the above vstack method. Args: y (np.ndarray) Sequence of integer labels to encode class_labels (array-like) Labels for each class pos_label (int) Value for positive labels neg_label (int) Value for negative labels Returns: np.ndarray of shape (n_samples, n_classes) Encoded dataset \"\"\" if isinstance ( y , list ): y = np . asarray ( y ) columns = [ np . where ( y == label , pos_label , neg_label ) for label in class_labels ] return np . column_stack ( columns ) def multiclass_roc_auc_score ( y_true , y_score , classes = None ): \"\"\"Compute ROC-AUC score for each class in a multiclass dataset. Args: y_true (np.ndarray of shape (n_samples, n_classes)) True labels y_score (np.ndarray of shape (n_samples, n_classes)) Target scores classes (array-like of shape (n_classes,)) List of dataset classes. If `None`, the lexicographical order of the labels in `y_true` is used. Returns: array-like: ROC-AUC score for each class, in the same order as `classes` \"\"\" classes = ( np . unique ( y_true ) if classes is None else classes ) y_true_multiclass = multiclass_label_binarize ( y_true , class_labels = classes ) def oneclass_roc_auc_score ( class_id ): y_true_class = y_true_multiclass [:, class_id ] y_score_class = y_score [:, class_id ] fpr , tpr , _ = metrics . roc_curve ( y_true = y_true_class , y_score = y_score_class , pos_label = 1 ) return metrics . auc ( fpr , tpr ) return [ oneclass_roc_auc_score ( class_id ) for class_id in range ( len ( classes )) ] Utilities Some utility functions to prepare data and post-process data. def prepare_y ( y : np . ndarray ) -> np . ndarray : \"\"\"Prepare the target variable for the model. If Binary Classification, we need to ravel the array to 1d. Args: y (np.ndarray): Target variable. Returns: np.ndarray: Transformed Target variable. \"\"\" return y . ravel () if config . classification_type == \"binary\" else y def mean_score ( score_values ) -> Union [ float , np . ndarray ]: \"\"\"Compute the mean score.\"\"\" score_values = np . array ( score_values ) shape = score_values . shape if len ( shape ) == 1 : return score_values . mean () return score_values . mean ( axis = 0 ) def mean_cv_results ( model_results : Results ) -> Dict : \"\"\"Add mean cross-validation results. This method computes the mean value for all score types in the model_results, including for scores (e.g., confusion matrix) where the mean value may contain decimal places. \"\"\" cv_logits = { y_result : np . concatenate ( model_results . get_result_values ( y_result )) for y_result in model_results . logit_names } cv_scores = { score : mean_score ( model_results . get_result_values ( score ) ) for score in model_results . score_names } return { ** cv_logits , ** cv_scores , } def oof_cv_results ( model_results : Results ) -> Dict : \"\"\"Add OOF cross-validation results.\"\"\" cv_logits = { y_result : np . concatenate ( model_results . get_result_values ( y_result ) ) for y_result in model_results . logit_names } cv_scores = compute_metrics ( cv_logits ) return { ** cv_logits , ** cv_scores , } def add_cv_results ( model_results : Results ): \"\"\"Add cross-validation results. This method returns a copy of the given model results with summary columns for mean and CV cross-validation. \"\"\" mean_cv = mean_cv_results ( model_results ) oof_cv = oof_cv_results ( model_results ) return ( model_results . add ( \"mean_cv\" , mean_cv ) . add ( \"oof_cv\" , oof_cv ) ) def compute_metrics ( logits : Dict [ str , np . ndarray ]) -> Dict [ str , Any ]: \"\"\"Compute metrics from logits. use_probs: all metrics that use probabilities. use_preds: all metrics that use thresholded predictions. # TODO add this precision, recall, fbeta_score, _ = metrics.precision_recall_fscore_support( y_true=y_val, y_pred = y_val_pred, labels=np.unique(y_val), average=None ) \"\"\" y_true , y_pred , y_prob = ( logits [ \"y_true\" ], logits [ \"y_pred\" ], logits [ \"y_prob\" ], ) use_preds = [ \"accuracy_score\" , \"precision_recall_fscore_support\" , \"confusion_matrix\" , ] use_probs = [ \"average_precision_score\" ] default_metrics_dict : Dict [ str , float ] = {} custom_metrics_dict : Dict [ str , float ] = {} for metric_name in default_score_names : if hasattr ( metrics , metric_name ): # TODO: get metric score with default parameters, consider adding kwargs if you want to configure parameters if metric_name in use_preds : metric_score = getattr ( metrics , metric_name )( y_true , y_pred ) elif metric_name in use_probs : # logger.info(\"TODO: write custom scores for precision-recall as here is hardcoded\") pass # metric_score = getattr(metrics, metric_name)( # y_true, y_prob # ) else : # add custom metrics here multiclass_roc_auc = multiclass_roc_auc_score ( y_true , y_prob ) brier_score_loss = ( metrics . brier_score_loss ( y_true = y_true , y_prob = y_prob [:, 1 ]) if config . classification_type == \"binary\" else np . nan ) custom_metrics_dict [ \"multiclass_roc_auc_score\" ] = multiclass_roc_auc custom_metrics_dict [ \"brier_score_loss\" ] = brier_score_loss if metric_name not in default_metrics_dict : default_metrics_dict [ metric_name ] = metric_score metrics_dict = { ** default_metrics_dict , ** custom_metrics_dict } return metrics_dict def train_on_fold ( df_folds : pd . DataFrame , models : List [ Callable ], num_folds : int , predictor_col : List [ str ], target_col : List [ str ], ) -> Dict [ str , List ]: \"\"\"Take in a dataframe with fold number as column, and a models which holds a list of callable models, we will loop through and return a dictionary of cv results. Args: df_folds (pd.DataFrame): Dataframe with fold number as column. model (Callable): A callable model. num_folds (int): Number of folds. predictor_col (List[str]): List of predictor columns. target_col (List[str]): List of target columns. Returns: model_dict (Dict[str, Results]: Dictionary of model results with model name as key. \"\"\" y_true = df_folds [ target_col ] . values . flatten () # test_pred_arr: np.ndarray = np.zeros(len(X_test)) model_dict = {} for model in models : model_results = Results () if isinstance ( model , pipeline . Pipeline ): model_name = model [ \"model\" ] . __class__ . __name__ else : model_name = model . __class__ . __name__ # out-of-fold validation predictions oof_pred_arr : np . ndarray = np . zeros ( len ( df_folds )) for fold in range ( 1 , num_folds + 1 ): train_df = df_folds [ df_folds [ \"fold\" ] != fold ] . reset_index ( drop = True ) val_df = df_folds [ df_folds [ \"fold\" ] == fold ] . reset_index ( drop = True ) val_idx = df_folds [ df_folds [ \"fold\" ] == fold ] . index . values X_train , y_train = train_df [ predictor_col ] . values , prepare_y ( train_df [ target_col ] . values ) X_val , y_val = val_df [ predictor_col ] . values , prepare_y ( val_df [ target_col ] . values ) model . fit ( X_train , y_train ) y_val_pred = model . predict ( X_val ) y_val_prob = model . predict_proba ( X_val ) logits = { \"y_true\" : y_val , \"y_pred\" : y_val_pred , \"y_prob\" : y_val_prob , } metrics = compute_metrics ( logits ) model_results . add ( f \"fold { fold } \" , { ** logits , ** metrics }, in_place = True ) if model_name not in model_dict : model_dict [ model_name ] = model_results return model_dict # Returns a dict in the format of # {'LogisticRegression': <__main__.Results at 0x7f3bca575e90>} model_dict = train_on_fold ( df_folds , models = classifiers , num_folds = 5 , predictor_col = predictor_cols , target_col = config . target_col ) # Takes in a model_dict and add cv results to the dict model_dict_with_summary = { model : add_cv_results ( model_results ) for model , model_results in model_dict . items () } # Transforms model_dict_with_summary to a Dict of dataframes # model_results_df['LogisticRegression'] -> df model_results_df = { name : results . to_dataframe () . T for name , results in model_dict_with_summary . items () } results_df = pd . concat ( model_results_df , axis = 0 ) results_df . columns = [ 'fold 1' , 'fold 2' , 'fold 3' , 'fold 4' , 'fold 5' , 'mean_cv' , 'oof_cv' ] results_df . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv RandomForestClassifier accuracy_score 0.932039 0.961165 0.95098 0.960784 0.95098 0.95119 0.951172 precision_recall_fscore_support ([0.9672131147540983, 0.8809523809523809], [0.... ([0.9552238805970149, 0.9722222222222222], [0.... ([0.927536231884058, 1.0], [1.0, 0.86842105263... ([0.9545454545454546, 0.9722222222222222], [0.... ([0.9402985074626866, 0.9714285714285714], [0.... [[0.9489634378486625, 0.9593650793650793], [0.... ([0.9484848484848485, 0.9560439560439561], [0.... confusion_matrix [[59, 5], [2, 37]] [[64, 1], [3, 35]] [[64, 0], [5, 33]] [[63, 1], [3, 35]] [[63, 1], [4, 34]] [[62.6, 1.6], [3.4, 34.8]] [[313, 8], [17, 174]] multiclass_roc_auc_score [0.9709535256410255, 0.9709535256410255] [0.9896761133603239, 0.9896761133603239] [0.9967105263157895, 0.9967105263157895] [0.9930098684210527, 0.9930098684210527] [0.9802631578947368, 0.980263157894737] [0.9861226383265856, 0.9861226383265856] [0.9849374500497463, 0.9849374500497464] brier_score_loss 0.0594175 0.0415534 0.0376471 0.0347059 0.0413725 0.0429393 0.0429688 Comparison of Cross-Validated Models (CV + OOF) The point of the following comparison is to check how different models are performing across folds. More specifically, if we have 5 folds, we will have a metric score for each fold, subsequently, we can find the standard error of model's performance. We need to be aware of models that have high variance across folds in terms of the metrics performance. This can indicate that the model is highly unstable, and may be a sign of overfitting. def summarize_metrics ( model_dict : Dict [ str , Results ], metric_name : str = \"roc\" , pos_label : int = 1 ): \"\"\" Summarize metrics of each fold with its standard error. We also plot a boxplot to show the results. \"\"\" results = [] for model_name , model_results in model_dict . items (): result_dict = model_results . get_result ( result_name = metric_name ) tmp_score = [] for fold , metric in result_dict . items (): pos_class_score = metric [ pos_label ] results . append (( model_name , fold , pos_class_score )) tmp_score . append ( pos_class_score ) # append the Standard Error of K folds results . append ( ( model_name , \"SE\" , np . std ( tmp_score , ddof = 1 ) / len ( tmp_score ) ** 0.5 ) ) summary_df = pd . DataFrame ( results , columns = [ \"model\" , \"fold\" , metric_name ]) fig , ax = plt . subplots ( figsize = ( 15 , 8 )) _ = sns . boxplot ( x = \"model\" , y = metric_name , data = summary_df [ ( summary_df [ \"model\" ] != \"DummyClassifier\" ) & ( summary_df [ \"fold\" ] != \"SE\" ) ], ax = ax , ) # fig.savefig(config.spot_checking_boxplot, format='png', dpi=300) return summary_df summary_df = summarize_metrics ( model_dict = model_dict , metric_name = \"multiclass_roc_auc_score\" ) display ( summary_df . tail ( 12 )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } model fold multiclass_roc_auc_score 24 DecisionTreeClassifier fold 1 0.865585 25 DecisionTreeClassifier fold 2 0.924291 26 DecisionTreeClassifier fold 3 0.923931 27 DecisionTreeClassifier fold 4 0.923931 28 DecisionTreeClassifier fold 5 0.895148 29 DecisionTreeClassifier SE 0.011677 30 RandomForestClassifier fold 1 0.970954 31 RandomForestClassifier fold 2 0.989676 32 RandomForestClassifier fold 3 0.996711 33 RandomForestClassifier fold 4 0.993010 34 RandomForestClassifier fold 5 0.980263 35 RandomForestClassifier SE 0.004672 Out-of-Fold Confusion Matrix We do have information on the performance of each folds, we now look at the performance of all 5 folds together. Typicall there are two ways to do it, one is to simply take the average of the score of five folds, the other is to take a look at out of folds predictions. From the confusion matrix of the out of fold performance, Logistic Regression does seem to be a model we can explore on, although slightly lower in terms of overall AUROC score than SVC, it seems to have the quite low False Negatives amongst all. With further hyperparameter tuning and threshold optimization, we can make it better. model_names = [ model for model in model_dict . keys ()] def plot_binary_confusion_matrix ( results_df : pd . DataFrame , model_names : List [ str ] ) -> None : n_models = len ( model_names ) # if 7 models, then 3 rows, 2 columns, and 7 subplots # always fix column to be 3 n_cols = 3 n_rows = int ( np . ceil ( n_models / n_cols )) fig , ax = plt . subplots ( n_rows , n_cols , figsize = ( 10 , 10 )) for axes , algo in zip ( ax . ravel (), model_names ): # Unravel into tn, fp, fn and tp tn , fp , fn , tp = results_df . oof_cv [ algo ] . confusion_matrix . ravel () # reshape into tp, fp, fn, tn - this is personal preference reshaped_cm = np . asarray ([[ tp , fp ], [ fn , tn ]]) # Get positive ROC score - hardcoded here. positive_class_auroc = results_df . oof_cv [ algo ] . multiclass_roc_auc_score [ 1 ] # annotations labels = [ \"True Pos\" , \"False Pos\" , \"False Neg\" , \"True Neg\" ] counts = [ \" {0:0.0f} \" . format ( value ) for value in reshaped_cm . flatten ()] percentages = [ \" {0:.2%} \" . format ( value ) for value in reshaped_cm . flatten () / np . sum ( reshaped_cm ) ] # final annotations label = ( np . array ( [ f \" { v1 } \\n { v2 } \\n { v3 } \" for v1 , v2 , v3 in zip ( labels , counts , percentages )] ) ) . reshape ( 2 , 2 ) # heatmap sns . heatmap ( data = reshaped_cm , vmin = 0 , vmax = 330 , cmap = [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ], linewidth = 2 , linecolor = \"white\" , square = True , ax = axes , annot = label , fmt = \"\" , cbar = False , annot_kws = { \"size\" : 10 , \"color\" : \"black\" , \"weight\" : \"bold\" , \"alpha\" : 0.8 }, alpha = 1 , ) axes . text ( 0 , - 0 , \" {} \" . format ( algo ), { \"size\" : 12 , \"color\" : \"black\" , \"weight\" : \"bold\" } ) axes . scatter ( 1 , 1 , s = 3500 , c = \"white\" ) axes . text ( 0.72 , 1.0 , \"AUC: {} \" . format ( np . round ( positive_class_auroc , 3 )), { \"size\" : 10 , \"color\" : \"black\" , \"weight\" : \"bold\" }, ) ## ticks and labels axes . set_xticklabels ( \"\" ) axes . set_yticklabels ( \"\" ) ## titles and text fig . text ( 0 , 1.05 , \"Out Of Fold Confusion Matrix\" , { \"size\" : 22 , \"weight\" : \"bold\" }, alpha = 1 ) fig . text ( 0 , 1 , \"\"\"This Visualization show the results of various classifiers and there respective results.\"\"\" , { \"size\" : 14 , \"weight\" : \"normal\" }, alpha = 0.98 , ) fig . tight_layout ( pad = 2.5 , w_pad = 2.5 , h_pad = 2.5 ) # fig.savefig(config.oof_confusion_matrix, format='png', dpi=300) plot_binary_confusion_matrix ( results_df , model_names ) Hypothesis Testing Across Models I am slightly shocked at the performance of plain LogisticRegression, I decide to use an idea from Hypothesis Testing Across Models to check if the difference is really by chance or not. Note that I will be modifying his code as his code does not split using StratifiedKFold. The basic idea is to test if two model's difference in scores (in this case roc), is statistically significant or not. However, we note that this method may violate an assumption of Student's t test. Null Hypothesis \\(H_0\\) : The difference in the performance score of two classifiers is Statistically Significant. Alternate Hypothesis \\(H_1\\) : The difference in the performance score of two classifiers is not Statistically Significant. def paired_ttest_skfold_cv ( estimator1 : Callable , estimator2 : Callable , X : np . ndarray , y : np . ndarray , cv : int = 10 , scoring : str = None , shuffle : bool = False , random_seed : int = None , ) -> float : \"\"\"Modified from https://github.com/rasbt/mlxtend/blob/master/mlxtend/evaluate/ttest.py to accomodate StratifiedKFold. Args: estimator1 (Callable): [description] estimator2 (Callable): [description] X (np.ndarray): [description] y (np.ndarray): [description] cv (int, optional): [description]. Defaults to 10. scoring (str, optional): [description]. Defaults to None. shuffle (bool, optional): [description]. Defaults to False. random_seed (int, optional): [description]. Defaults to None. Raises: AttributeError: [description] Returns: float: [description] \"\"\" if not shuffle : skf = model_selection . StratifiedKFold ( n_splits = cv , shuffle = shuffle ) else : skf = model_selection . StratifiedKFold ( n_splits = cv , random_state = random_seed , shuffle = shuffle ) if scoring is None : if estimator1 . _estimator_type == \"classifier\" : scoring = \"accuracy\" elif estimator1 . _estimator_type == \"regressor\" : scoring = \"r2\" else : raise AttributeError ( \"Estimator must \" \"be a Classifier or Regressor.\" ) if isinstance ( scoring , str ): scorer = metrics . get_scorer ( scoring ) else : scorer = scoring score_diff = [] for train_index , test_index in skf . split ( X = X , y = y ): X_train , X_test = X [ train_index ], X [ test_index ] y_train , y_test = y [ train_index ], y [ test_index ] estimator1 . fit ( X_train , y_train ) estimator2 . fit ( X_train , y_train ) est1_score = scorer ( estimator1 , X_test , y_test ) est2_score = scorer ( estimator2 , X_test , y_test ) score_diff . append ( est1_score - est2_score ) avg_diff = np . mean ( score_diff ) numerator = avg_diff * np . sqrt ( cv ) denominator = np . sqrt ( sum ([( diff - avg_diff ) ** 2 for diff in score_diff ]) / ( cv - 1 ) ) t_stat = numerator / denominator pvalue = stats . t . sf ( np . abs ( t_stat ), cv - 1 ) * 2.0 return float ( t_stat ), float ( pvalue ) # check if difference between algorithms is real X_tmp = df_folds [ predictor_cols ] . values y_tmp = df_folds [ 'diagnosis' ] . values t , p = paired_ttest_skfold_cv ( estimator1 = classifiers [ 1 ], estimator2 = classifiers [ - 1 ], shuffle = True , cv = 5 , X = X_tmp , y = y_tmp , scoring = 'roc_auc' , random_seed = config . seed ) logger . info ( 'P-value: %.3f , t-Statistic: %.3f ' % ( p , t )) 2021-11-13,14:04:35 - P-value: 0.171, t-Statistic: 1.667 2021-11-13,14:04:35 - P-value: 0.171, t-Statistic: 1.667 Since \\(p\\) -value is quite high, and more the basic threshold of 0.05 or 0.1, we fail to reject the null hypothesis, and say that there is no significant difference between these two models.","title":"Modelling (Preprocessing and Spot Checking)"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%206%20-%20Modelling%20%28Preprocessing%20and%20Spot%20Checking%29/#dependencies-and-configuration","text":"# !pip install gcloud == 0.18.3 ! pip install - q mlxtend == 0.19.0 ! pip install - q statsmodels == 0.13.1 import copy import csv import logging import random from dataclasses import dataclass , field from functools import wraps from time import time from typing import Any , Callable , Dict , List , Optional , Union , Tuple import matplotlib.pyplot as plt import mlxtend import numpy as np import pandas as pd import seaborn as sns from mlxtend.evaluate import bias_variance_decomp , paired_ttest_5x2cv from scipy import stats from sklearn import ( base , decomposition , dummy , ensemble , feature_selection , linear_model , metrics , model_selection , neighbors , pipeline , preprocessing , svm , tree ) from statsmodels.regression.linear_model import OLS \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.3 MB 5.2 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.8 MB 1.3 MB/s \u001b[?25h @dataclass class config : raw_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/raw/data.csv\" processed_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/processed.csv\" df_folds : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/df_folds.csv\" train_size : float = 0.9 seed : int = 1992 num_folds : int = 5 cv_schema : str = \"StratifiedKFold\" classification_type : str = \"binary\" target_col : List [ str ] = field ( default_factory = lambda : [ \"diagnosis\" ]) unwanted_cols : List [ str ] = field ( default_factory = lambda : [ \"id\" , \"Unnamed: 32\" ]) # Plotting colors : List [ str ] = field ( default_factory = lambda : [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ]) cmap_reversed = plt . cm . get_cmap ( 'mako_r' ) def to_dict ( self ) -> Dict : \"\"\"Convert the config object to a dictionary. Returns: Dict: The config object as a dictionary. \"\"\" return { \"raw_data\" : self . raw_data , \"processed_data\" : self . processed_data , \"train_size\" : self . train_size , \"seed\" : self . seed , \"num_folds\" : self . num_folds , \"cv_schema\" : self . cv_schema , \"classification_type\" : self . classification_type , \"target_col\" : self . target_col , \"unwanted_cols\" : self . unwanted_cols , \"colors\" : self . colors , \"cmap_reversed\" : self . cmap_reversed } # spot_checking_boxplot = \"../data/images/spot_checking_boxplot.png\" # oof_confusion_matrix = \"../data/images/oof_confusion_matrix.png\" # final_train_confusion_matrix = \"../data/images/final_train_confusion_matrix.png\" # precision_recall_threshold_plot = \"../data/images/precision_recall_threshold_plot.png\" # roc_plot = \"../data/images/roc_plot.png\" # feature_importance = \"../data/images/feature_importance.png\" def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) def init_logger ( log_file : str = \"info.log\" ): \"\"\" Initialize logger. \"\"\" logger = logging . getLogger ( __name__ ) logger . setLevel ( logging . INFO ) stream_handler = logging . StreamHandler () stream_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) file_handler = logging . FileHandler ( filename = log_file ) file_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) logger . addHandler ( stream_handler ) logger . addHandler ( file_handler ) return logger # Utils functions that we need def variance_inflation_factor ( exog , idx_kept , vif_idx ): \"\"\"Compute VIF for one feature. Args: exog (np.ndarray): Observations idx_kept (List[int]): Indices of features to consider vif_idx (int): Index of feature for which to compute VIF Returns: float: VIF for the selected feature \"\"\" exog = np . asarray ( exog ) x_i = exog [:, vif_idx ] mask = [ col for col in idx_kept if col != vif_idx ] x_noti = exog [:, mask ] r_squared_i = OLS ( x_i , x_noti ) . fit () . rsquared vif = 1. / ( 1. - r_squared_i ) return vif class ReduceVIF ( base . BaseEstimator , base . TransformerMixin ): \"\"\"The base of the class structure is implemented in https://www.kaggle.com/ffisegydd/sklearn-multicollinearity-class; I heavily modified the class such that it can take in numpy arrays and correctly implemented the fit and transform method. \"\"\" def __init__ ( self , thresh = 10 , max_drop = 20 ): self . thresh = thresh self . max_drop = max_drop self . column_indices_kept_ = [] self . feature_names_kept_ = None def reset ( self ): \"\"\"Resets the state of predictor columns after each fold.\"\"\" self . column_indices_kept_ = [] self . feature_names_kept_ = None def fit ( self , X , y = None ): \"\"\"Fits the Recursive VIF on the training folds and save the selected feature names in self.feature_names Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" self . column_indices_kept_ , self . feature_names_kept_ = self . calculate_vif ( X ) return self def transform ( self , X , y = None ): \"\"\"Transforms the Validation Set according to the selected feature names. Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" return X [:, self . column_indices_kept_ ] def calculate_vif ( self , X : Union [ np . ndarray , pd . DataFrame ]): \"\"\"Implements a VIF function that recursively eliminates features. Args: X (Union[np.ndarray, pd.DataFrame]): [description] Returns: [type]: [description] \"\"\" feature_names = None column_indices_kept = list ( range ( X . shape [ 1 ])) if isinstance ( X , pd . DataFrame ): feature_names = X . columns dropped = True count = 0 while dropped and count <= self . max_drop : dropped = False max_vif , max_vif_col = None , None for col in column_indices_kept : vif = variance_inflation_factor ( X , column_indices_kept , col ) if max_vif is None or vif > max_vif : max_vif = vif max_vif_col = col if max_vif > self . thresh : # print(f\"Dropping {max_vif_col} with vif={max_vif}\") column_indices_kept . remove ( max_vif_col ) if feature_names is not None : feature_names . pop ( max_vif_col ) dropped = True count += 1 return column_indices_kept , feature_names config = config () logger = init_logger () # set seeding for reproducibility _ = set_seeds ( seed = config . seed ) # read data df_folds = pd . read_csv ( config . df_folds ) # Assign predictors and target accordingly predictor_cols = df_folds . columns . to_list ()[: - 2 ] target_col = config . target_col","title":"Dependencies and Configuration"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%206%20-%20Modelling%20%28Preprocessing%20and%20Spot%20Checking%29/#spot-checking-algorithms","text":"Terminology Alert! This method is advocated by Jason Brownlee PhD and this serves as the first stage of my modelling process. We will rapidly test (spot check) different classifier algorithms, from DummyClassifier , to LinearModel to more sophisticated ensemble trees like RandomForest . I also note to the readers that we need to think of a few things when choosing the \"optimal\" machine learning algorithm: No Lunch Free Theorem intuitively says that no single optimization algorithm can work best in all situations. Therefore, spot checking can help us form a basis of which algorithm might work better in this particular scenario. Occam's Razor often appears in many Machine Learning textbook, and the narrative is that a simpler model more often times generalizes better than a complex model. This is not unfamiliar when we think of the bias-variance tradeoff, and that is why there is always a tradeoff that we must make.","title":"Spot Checking Algorithms"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%206%20-%20Modelling%20%28Preprocessing%20and%20Spot%20Checking%29/#say-no-to-data-leakage","text":"Say No to Data Leakage: This has been emphasized throughout and we must be careful as we should never touch the test set when fitting the model. In fact, we should try our best to not contaminate our validation set as well. This means that preprocessing steps such as StandardScaling() should only be fitted on the training data, and then apply the same transformation (mean and std) on the test data. In other words, do not apply scaling on the whole dataset before splitting. However, it is also equally important to take note not to contaminate our validation set, which is often overlooked, resulting in over optimistic results from model selection phase, but perform badly on unseen test set. As a result, when we use a 5 fold cross validation, we should be careful during fitting that the preprocessing steps are only applied on the training folds, and not on all 5 folds. The same idea is also applied to our ReduceVIF() preprocessing step. We should also include this in our pipeline and not select the features outside the cross-validation loop. Quoting from scikit-learn : Data leakage occurs when information that would not be available at prediction time is used when building the model. This results in overly optimistic performance estimates, for example from cross-validation , and thus poorer performance when the model is used on actually novel data, for example during production. A common cause is not keeping the test and train data subsets separate. Test data should never be used to make choices about the model. The general rule is to never call fit on the test data. While this may sound obvious, this is easy to miss in some cases, for example when applying certain pre-processing steps. Although both train and test data subsets should receive the same preprocessing transformation (as described in the previous section), it is important that these transformations are only learnt from the training data. For example, if you have a normalization step where you divide by the average value, the average should be the average of the train subset, not the average of all the data. If the test subset is included in the average calculation, information from the test subset is influencing the model. How to avoid Data Leakage? We know the pitfalls of fitting on validation/test data, the natural question is how can we avoid it completely? You can code it up yourself, but as a starter, we can use scikit-learn's Pipeline object. My tips are as follows: Any preprocessing step must be done after splitting the whole dataset into train and test. If you are also using cross-validation, then we should only apply the preprocessing steps on the train set, and then use the metrics obtained from the train set to transform the validation set. You can see my pseudo-code below for a rough outline. The Pipeline object of Scikit-Learn can help prevent data leakage.","title":"Say No to Data Leakage!"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%206%20-%20Modelling%20%28Preprocessing%20and%20Spot%20Checking%29/#pseudo-code-of-cross-validation-and-pipeline","text":"The below outlines a pseudo code of the cross-validation scheme using Pipeline object. Note that I included the most outer loop, which is searching for hyperparameters. Define \\(G\\) as the set of combination of hyperparamters. Define number of splits to be \\(K\\) . For each set of hyperparameter \\(z \\in Z\\) : for fold \\(j\\) in K: Set \\(F_{\\text{train}}=\\bigcup\\limits_{i\\neq k}^{K} F_{i}\\) Set \\(F_{\\text{val}} = F_{j}\\) as the validation set Perform Standard Scaling on \\(F_{\\text{train}}\\) and find the mean and std Perform VIF recursively on \\(F_{\\text{train}}\\) and find the selected features Transform \\(F_{\\text{val}}\\) using the mean and std found using \\(F_{\\text{train}}\\) Transform \\(F_{\\text{val}}\\) to have only the selected features from \\(F_{\\text{train}}\\) Train and fit on \\(F_{\\text{train}}\\) Evaluate the fitted parameters on \\(F_{\\text{val}}\\) to obtain \\(\\mathcal{M}\\)","title":"Pseudo-Code of Cross-Validation and Pipeline"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%206%20-%20Modelling%20%28Preprocessing%20and%20Spot%20Checking%29/#training-pipeline","text":"","title":"Training Pipeline"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%206%20-%20Modelling%20%28Preprocessing%20and%20Spot%20Checking%29/#make-pipeline","text":"# @ TODO: https://www.kaggle.com/kabure/predicting-house-prices-xgb-rf-bagging-reg-pipe # Different models can potentially have different pre-processing steps, consider putting steps as a # passable list. def make_pipeline ( model : Callable ) -> Callable : \"\"\"Create a feature preparation pipeline for a model. Args: model (Callable): The model to be used. Returns: _pipeline (Callable): pipeline object \"\"\" # Create a list of steps, note that some models may not need certain steps, and hence # may need an if-else here steps = list () # standardization steps . append (( \"standardize\" , preprocessing . StandardScaler ())) # reduce VIF steps . append (( \"remove_multicollinearity\" , ReduceVIF ( thresh = 10 ))) # the model to be appended at the last step steps . append (( \"model\" , model )) # create pipeline _pipeline = pipeline . Pipeline ( steps = steps ) return _pipeline classifiers = [ # baseline model dummy . DummyClassifier ( random_state = config . seed , strategy = \"stratified\" ), # linear model linear_model . LogisticRegression ( random_state = config . seed , solver = \"liblinear\" ), # nearest neighbours neighbors . KNeighborsClassifier ( n_neighbors = 8 ), # SVM svm . SVC ( probability = True , random_state = config . seed ), # tree tree . DecisionTreeClassifier ( random_state = config . seed ), # ensemble ensemble . RandomForestClassifier ( n_estimators = 10 , random_state = config . seed ), ] classifiers = [ make_pipeline ( model ) for model in classifiers ]","title":"Make Pipeline"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%206%20-%20Modelling%20%28Preprocessing%20and%20Spot%20Checking%29/#results-class","text":"The Results class will help us store model's results. Careful when using ROC function! We also note that when passing arguments to scikit-learn's roc_auc_score function, we should be careful not to pass y_score=model.predict(X) inside as we have to understand that we are passing in non-thresholded probabilities into y_score . If you pass the predicted values (full of 0 and 1s), then you are thresholding on 0 and 1 only, which is incorrect by definition. default_result_names = [ \"y_true\" , \"y_pred\" , \"y_prob\" ] default_logit_names = [ \"y_true\" , \"y_pred\" , \"y_prob\" ] default_score_names = [ \"accuracy_score\" , \"precision_recall_fscore_support\" , \"confusion_matrix\" , # \"average_precision_score\", \"multiclass_roc_auc_score\" , \"brier_score_loss\" , ] custom_score_names = [ \"multiclass_roc_auc_score\" , \"brier_score_loss\" ] use_preds = [ \"accuracy_score\" , \"precision_recall_fscore_support\" , \"confusion_matrix\" , ] use_probs = [ \"average_precision_score\" ] class Results : \"\"\"Stores results for model training in columnwise format.\"\"\" _result_dict : Dict logit_names : List [ str ] score_names : List [ str ] def __init__ ( self , logit_names : List [ str ] = default_logit_names , score_names : List [ str ] = default_score_names , existing_dict : Optional [ Dict ] = None , ): \"\"\"Construct a new results store.\"\"\" self . logit_names = logit_names self . score_names = score_names if existing_dict is not None : self . _result_dict = copy . deepcopy ( existing_dict ) return dict_keys = [ \"identifier\" , * logit_names , * score_names ] self . _result_dict = { key : [] for key in dict_keys } def add ( self , identifier : str , results : Dict , in_place = False ): \"\"\"Add a new results row.\"\"\" if not in_place : return Results ( self . logit_names , self . score_names , self . _result_dict ) . add ( identifier , results , in_place = True ) self . _result_dict [ \"identifier\" ] . append ( identifier ) for result_name in set ([ * results . keys (), * self . logit_names , * self . score_names ]): result_value = results . get ( result_name , np . nan ) self . _result_dict [ result_name ] . append ( result_value ) return self def get_result ( self , result_name : str ) -> Dict [ str , Any ]: \"\"\"Get a map of identifiers to result values for a result.\"\"\" return { identifier : result_value for identifier , result_value in zip ( self . _result_dict [ \"identifier\" ], self . _result_dict [ result_name ]) } def get_result_values ( self , result_name : str ) -> List [ Any ]: \"\"\"Get a list of values for a result.\"\"\" return self . _result_dict [ result_name ] def to_dataframe ( self ) -> pd . DataFrame : \"\"\"Get a Data Frame containing the results.\"\"\" return pd . DataFrame . from_dict ( self . _result_dict ) def to_dict ( self ) -> Dict : \"\"\"Get a dictionary containing the results. Returns: Dict[str, List[Any]]: Dictionary of result columns \"\"\" return self . _result_dict def multiclass_label_binarize ( y : np . ndarray , class_labels : List [ int ], pos_label = 1 , neg_label = 0 ): \"\"\"Binarize labels in one-vs-all fashion. # TODO: to replace with the above vstack method. Args: y (np.ndarray) Sequence of integer labels to encode class_labels (array-like) Labels for each class pos_label (int) Value for positive labels neg_label (int) Value for negative labels Returns: np.ndarray of shape (n_samples, n_classes) Encoded dataset \"\"\" if isinstance ( y , list ): y = np . asarray ( y ) columns = [ np . where ( y == label , pos_label , neg_label ) for label in class_labels ] return np . column_stack ( columns ) def multiclass_roc_auc_score ( y_true , y_score , classes = None ): \"\"\"Compute ROC-AUC score for each class in a multiclass dataset. Args: y_true (np.ndarray of shape (n_samples, n_classes)) True labels y_score (np.ndarray of shape (n_samples, n_classes)) Target scores classes (array-like of shape (n_classes,)) List of dataset classes. If `None`, the lexicographical order of the labels in `y_true` is used. Returns: array-like: ROC-AUC score for each class, in the same order as `classes` \"\"\" classes = ( np . unique ( y_true ) if classes is None else classes ) y_true_multiclass = multiclass_label_binarize ( y_true , class_labels = classes ) def oneclass_roc_auc_score ( class_id ): y_true_class = y_true_multiclass [:, class_id ] y_score_class = y_score [:, class_id ] fpr , tpr , _ = metrics . roc_curve ( y_true = y_true_class , y_score = y_score_class , pos_label = 1 ) return metrics . auc ( fpr , tpr ) return [ oneclass_roc_auc_score ( class_id ) for class_id in range ( len ( classes )) ]","title":"Results Class"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%206%20-%20Modelling%20%28Preprocessing%20and%20Spot%20Checking%29/#utilities","text":"Some utility functions to prepare data and post-process data. def prepare_y ( y : np . ndarray ) -> np . ndarray : \"\"\"Prepare the target variable for the model. If Binary Classification, we need to ravel the array to 1d. Args: y (np.ndarray): Target variable. Returns: np.ndarray: Transformed Target variable. \"\"\" return y . ravel () if config . classification_type == \"binary\" else y def mean_score ( score_values ) -> Union [ float , np . ndarray ]: \"\"\"Compute the mean score.\"\"\" score_values = np . array ( score_values ) shape = score_values . shape if len ( shape ) == 1 : return score_values . mean () return score_values . mean ( axis = 0 ) def mean_cv_results ( model_results : Results ) -> Dict : \"\"\"Add mean cross-validation results. This method computes the mean value for all score types in the model_results, including for scores (e.g., confusion matrix) where the mean value may contain decimal places. \"\"\" cv_logits = { y_result : np . concatenate ( model_results . get_result_values ( y_result )) for y_result in model_results . logit_names } cv_scores = { score : mean_score ( model_results . get_result_values ( score ) ) for score in model_results . score_names } return { ** cv_logits , ** cv_scores , } def oof_cv_results ( model_results : Results ) -> Dict : \"\"\"Add OOF cross-validation results.\"\"\" cv_logits = { y_result : np . concatenate ( model_results . get_result_values ( y_result ) ) for y_result in model_results . logit_names } cv_scores = compute_metrics ( cv_logits ) return { ** cv_logits , ** cv_scores , } def add_cv_results ( model_results : Results ): \"\"\"Add cross-validation results. This method returns a copy of the given model results with summary columns for mean and CV cross-validation. \"\"\" mean_cv = mean_cv_results ( model_results ) oof_cv = oof_cv_results ( model_results ) return ( model_results . add ( \"mean_cv\" , mean_cv ) . add ( \"oof_cv\" , oof_cv ) ) def compute_metrics ( logits : Dict [ str , np . ndarray ]) -> Dict [ str , Any ]: \"\"\"Compute metrics from logits. use_probs: all metrics that use probabilities. use_preds: all metrics that use thresholded predictions. # TODO add this precision, recall, fbeta_score, _ = metrics.precision_recall_fscore_support( y_true=y_val, y_pred = y_val_pred, labels=np.unique(y_val), average=None ) \"\"\" y_true , y_pred , y_prob = ( logits [ \"y_true\" ], logits [ \"y_pred\" ], logits [ \"y_prob\" ], ) use_preds = [ \"accuracy_score\" , \"precision_recall_fscore_support\" , \"confusion_matrix\" , ] use_probs = [ \"average_precision_score\" ] default_metrics_dict : Dict [ str , float ] = {} custom_metrics_dict : Dict [ str , float ] = {} for metric_name in default_score_names : if hasattr ( metrics , metric_name ): # TODO: get metric score with default parameters, consider adding kwargs if you want to configure parameters if metric_name in use_preds : metric_score = getattr ( metrics , metric_name )( y_true , y_pred ) elif metric_name in use_probs : # logger.info(\"TODO: write custom scores for precision-recall as here is hardcoded\") pass # metric_score = getattr(metrics, metric_name)( # y_true, y_prob # ) else : # add custom metrics here multiclass_roc_auc = multiclass_roc_auc_score ( y_true , y_prob ) brier_score_loss = ( metrics . brier_score_loss ( y_true = y_true , y_prob = y_prob [:, 1 ]) if config . classification_type == \"binary\" else np . nan ) custom_metrics_dict [ \"multiclass_roc_auc_score\" ] = multiclass_roc_auc custom_metrics_dict [ \"brier_score_loss\" ] = brier_score_loss if metric_name not in default_metrics_dict : default_metrics_dict [ metric_name ] = metric_score metrics_dict = { ** default_metrics_dict , ** custom_metrics_dict } return metrics_dict def train_on_fold ( df_folds : pd . DataFrame , models : List [ Callable ], num_folds : int , predictor_col : List [ str ], target_col : List [ str ], ) -> Dict [ str , List ]: \"\"\"Take in a dataframe with fold number as column, and a models which holds a list of callable models, we will loop through and return a dictionary of cv results. Args: df_folds (pd.DataFrame): Dataframe with fold number as column. model (Callable): A callable model. num_folds (int): Number of folds. predictor_col (List[str]): List of predictor columns. target_col (List[str]): List of target columns. Returns: model_dict (Dict[str, Results]: Dictionary of model results with model name as key. \"\"\" y_true = df_folds [ target_col ] . values . flatten () # test_pred_arr: np.ndarray = np.zeros(len(X_test)) model_dict = {} for model in models : model_results = Results () if isinstance ( model , pipeline . Pipeline ): model_name = model [ \"model\" ] . __class__ . __name__ else : model_name = model . __class__ . __name__ # out-of-fold validation predictions oof_pred_arr : np . ndarray = np . zeros ( len ( df_folds )) for fold in range ( 1 , num_folds + 1 ): train_df = df_folds [ df_folds [ \"fold\" ] != fold ] . reset_index ( drop = True ) val_df = df_folds [ df_folds [ \"fold\" ] == fold ] . reset_index ( drop = True ) val_idx = df_folds [ df_folds [ \"fold\" ] == fold ] . index . values X_train , y_train = train_df [ predictor_col ] . values , prepare_y ( train_df [ target_col ] . values ) X_val , y_val = val_df [ predictor_col ] . values , prepare_y ( val_df [ target_col ] . values ) model . fit ( X_train , y_train ) y_val_pred = model . predict ( X_val ) y_val_prob = model . predict_proba ( X_val ) logits = { \"y_true\" : y_val , \"y_pred\" : y_val_pred , \"y_prob\" : y_val_prob , } metrics = compute_metrics ( logits ) model_results . add ( f \"fold { fold } \" , { ** logits , ** metrics }, in_place = True ) if model_name not in model_dict : model_dict [ model_name ] = model_results return model_dict # Returns a dict in the format of # {'LogisticRegression': <__main__.Results at 0x7f3bca575e90>} model_dict = train_on_fold ( df_folds , models = classifiers , num_folds = 5 , predictor_col = predictor_cols , target_col = config . target_col ) # Takes in a model_dict and add cv results to the dict model_dict_with_summary = { model : add_cv_results ( model_results ) for model , model_results in model_dict . items () } # Transforms model_dict_with_summary to a Dict of dataframes # model_results_df['LogisticRegression'] -> df model_results_df = { name : results . to_dataframe () . T for name , results in model_dict_with_summary . items () } results_df = pd . concat ( model_results_df , axis = 0 ) results_df . columns = [ 'fold 1' , 'fold 2' , 'fold 3' , 'fold 4' , 'fold 5' , 'mean_cv' , 'oof_cv' ] results_df . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv RandomForestClassifier accuracy_score 0.932039 0.961165 0.95098 0.960784 0.95098 0.95119 0.951172 precision_recall_fscore_support ([0.9672131147540983, 0.8809523809523809], [0.... ([0.9552238805970149, 0.9722222222222222], [0.... ([0.927536231884058, 1.0], [1.0, 0.86842105263... ([0.9545454545454546, 0.9722222222222222], [0.... ([0.9402985074626866, 0.9714285714285714], [0.... [[0.9489634378486625, 0.9593650793650793], [0.... ([0.9484848484848485, 0.9560439560439561], [0.... confusion_matrix [[59, 5], [2, 37]] [[64, 1], [3, 35]] [[64, 0], [5, 33]] [[63, 1], [3, 35]] [[63, 1], [4, 34]] [[62.6, 1.6], [3.4, 34.8]] [[313, 8], [17, 174]] multiclass_roc_auc_score [0.9709535256410255, 0.9709535256410255] [0.9896761133603239, 0.9896761133603239] [0.9967105263157895, 0.9967105263157895] [0.9930098684210527, 0.9930098684210527] [0.9802631578947368, 0.980263157894737] [0.9861226383265856, 0.9861226383265856] [0.9849374500497463, 0.9849374500497464] brier_score_loss 0.0594175 0.0415534 0.0376471 0.0347059 0.0413725 0.0429393 0.0429688","title":"Utilities"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%206%20-%20Modelling%20%28Preprocessing%20and%20Spot%20Checking%29/#comparison-of-cross-validated-models-cv-oof","text":"The point of the following comparison is to check how different models are performing across folds. More specifically, if we have 5 folds, we will have a metric score for each fold, subsequently, we can find the standard error of model's performance. We need to be aware of models that have high variance across folds in terms of the metrics performance. This can indicate that the model is highly unstable, and may be a sign of overfitting. def summarize_metrics ( model_dict : Dict [ str , Results ], metric_name : str = \"roc\" , pos_label : int = 1 ): \"\"\" Summarize metrics of each fold with its standard error. We also plot a boxplot to show the results. \"\"\" results = [] for model_name , model_results in model_dict . items (): result_dict = model_results . get_result ( result_name = metric_name ) tmp_score = [] for fold , metric in result_dict . items (): pos_class_score = metric [ pos_label ] results . append (( model_name , fold , pos_class_score )) tmp_score . append ( pos_class_score ) # append the Standard Error of K folds results . append ( ( model_name , \"SE\" , np . std ( tmp_score , ddof = 1 ) / len ( tmp_score ) ** 0.5 ) ) summary_df = pd . DataFrame ( results , columns = [ \"model\" , \"fold\" , metric_name ]) fig , ax = plt . subplots ( figsize = ( 15 , 8 )) _ = sns . boxplot ( x = \"model\" , y = metric_name , data = summary_df [ ( summary_df [ \"model\" ] != \"DummyClassifier\" ) & ( summary_df [ \"fold\" ] != \"SE\" ) ], ax = ax , ) # fig.savefig(config.spot_checking_boxplot, format='png', dpi=300) return summary_df summary_df = summarize_metrics ( model_dict = model_dict , metric_name = \"multiclass_roc_auc_score\" ) display ( summary_df . tail ( 12 )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } model fold multiclass_roc_auc_score 24 DecisionTreeClassifier fold 1 0.865585 25 DecisionTreeClassifier fold 2 0.924291 26 DecisionTreeClassifier fold 3 0.923931 27 DecisionTreeClassifier fold 4 0.923931 28 DecisionTreeClassifier fold 5 0.895148 29 DecisionTreeClassifier SE 0.011677 30 RandomForestClassifier fold 1 0.970954 31 RandomForestClassifier fold 2 0.989676 32 RandomForestClassifier fold 3 0.996711 33 RandomForestClassifier fold 4 0.993010 34 RandomForestClassifier fold 5 0.980263 35 RandomForestClassifier SE 0.004672","title":"Comparison of Cross-Validated Models (CV + OOF)"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%206%20-%20Modelling%20%28Preprocessing%20and%20Spot%20Checking%29/#out-of-fold-confusion-matrix","text":"We do have information on the performance of each folds, we now look at the performance of all 5 folds together. Typicall there are two ways to do it, one is to simply take the average of the score of five folds, the other is to take a look at out of folds predictions. From the confusion matrix of the out of fold performance, Logistic Regression does seem to be a model we can explore on, although slightly lower in terms of overall AUROC score than SVC, it seems to have the quite low False Negatives amongst all. With further hyperparameter tuning and threshold optimization, we can make it better. model_names = [ model for model in model_dict . keys ()] def plot_binary_confusion_matrix ( results_df : pd . DataFrame , model_names : List [ str ] ) -> None : n_models = len ( model_names ) # if 7 models, then 3 rows, 2 columns, and 7 subplots # always fix column to be 3 n_cols = 3 n_rows = int ( np . ceil ( n_models / n_cols )) fig , ax = plt . subplots ( n_rows , n_cols , figsize = ( 10 , 10 )) for axes , algo in zip ( ax . ravel (), model_names ): # Unravel into tn, fp, fn and tp tn , fp , fn , tp = results_df . oof_cv [ algo ] . confusion_matrix . ravel () # reshape into tp, fp, fn, tn - this is personal preference reshaped_cm = np . asarray ([[ tp , fp ], [ fn , tn ]]) # Get positive ROC score - hardcoded here. positive_class_auroc = results_df . oof_cv [ algo ] . multiclass_roc_auc_score [ 1 ] # annotations labels = [ \"True Pos\" , \"False Pos\" , \"False Neg\" , \"True Neg\" ] counts = [ \" {0:0.0f} \" . format ( value ) for value in reshaped_cm . flatten ()] percentages = [ \" {0:.2%} \" . format ( value ) for value in reshaped_cm . flatten () / np . sum ( reshaped_cm ) ] # final annotations label = ( np . array ( [ f \" { v1 } \\n { v2 } \\n { v3 } \" for v1 , v2 , v3 in zip ( labels , counts , percentages )] ) ) . reshape ( 2 , 2 ) # heatmap sns . heatmap ( data = reshaped_cm , vmin = 0 , vmax = 330 , cmap = [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ], linewidth = 2 , linecolor = \"white\" , square = True , ax = axes , annot = label , fmt = \"\" , cbar = False , annot_kws = { \"size\" : 10 , \"color\" : \"black\" , \"weight\" : \"bold\" , \"alpha\" : 0.8 }, alpha = 1 , ) axes . text ( 0 , - 0 , \" {} \" . format ( algo ), { \"size\" : 12 , \"color\" : \"black\" , \"weight\" : \"bold\" } ) axes . scatter ( 1 , 1 , s = 3500 , c = \"white\" ) axes . text ( 0.72 , 1.0 , \"AUC: {} \" . format ( np . round ( positive_class_auroc , 3 )), { \"size\" : 10 , \"color\" : \"black\" , \"weight\" : \"bold\" }, ) ## ticks and labels axes . set_xticklabels ( \"\" ) axes . set_yticklabels ( \"\" ) ## titles and text fig . text ( 0 , 1.05 , \"Out Of Fold Confusion Matrix\" , { \"size\" : 22 , \"weight\" : \"bold\" }, alpha = 1 ) fig . text ( 0 , 1 , \"\"\"This Visualization show the results of various classifiers and there respective results.\"\"\" , { \"size\" : 14 , \"weight\" : \"normal\" }, alpha = 0.98 , ) fig . tight_layout ( pad = 2.5 , w_pad = 2.5 , h_pad = 2.5 ) # fig.savefig(config.oof_confusion_matrix, format='png', dpi=300) plot_binary_confusion_matrix ( results_df , model_names )","title":"Out-of-Fold Confusion Matrix"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%206%20-%20Modelling%20%28Preprocessing%20and%20Spot%20Checking%29/#hypothesis-testing-across-models","text":"I am slightly shocked at the performance of plain LogisticRegression, I decide to use an idea from Hypothesis Testing Across Models to check if the difference is really by chance or not. Note that I will be modifying his code as his code does not split using StratifiedKFold. The basic idea is to test if two model's difference in scores (in this case roc), is statistically significant or not. However, we note that this method may violate an assumption of Student's t test. Null Hypothesis \\(H_0\\) : The difference in the performance score of two classifiers is Statistically Significant. Alternate Hypothesis \\(H_1\\) : The difference in the performance score of two classifiers is not Statistically Significant. def paired_ttest_skfold_cv ( estimator1 : Callable , estimator2 : Callable , X : np . ndarray , y : np . ndarray , cv : int = 10 , scoring : str = None , shuffle : bool = False , random_seed : int = None , ) -> float : \"\"\"Modified from https://github.com/rasbt/mlxtend/blob/master/mlxtend/evaluate/ttest.py to accomodate StratifiedKFold. Args: estimator1 (Callable): [description] estimator2 (Callable): [description] X (np.ndarray): [description] y (np.ndarray): [description] cv (int, optional): [description]. Defaults to 10. scoring (str, optional): [description]. Defaults to None. shuffle (bool, optional): [description]. Defaults to False. random_seed (int, optional): [description]. Defaults to None. Raises: AttributeError: [description] Returns: float: [description] \"\"\" if not shuffle : skf = model_selection . StratifiedKFold ( n_splits = cv , shuffle = shuffle ) else : skf = model_selection . StratifiedKFold ( n_splits = cv , random_state = random_seed , shuffle = shuffle ) if scoring is None : if estimator1 . _estimator_type == \"classifier\" : scoring = \"accuracy\" elif estimator1 . _estimator_type == \"regressor\" : scoring = \"r2\" else : raise AttributeError ( \"Estimator must \" \"be a Classifier or Regressor.\" ) if isinstance ( scoring , str ): scorer = metrics . get_scorer ( scoring ) else : scorer = scoring score_diff = [] for train_index , test_index in skf . split ( X = X , y = y ): X_train , X_test = X [ train_index ], X [ test_index ] y_train , y_test = y [ train_index ], y [ test_index ] estimator1 . fit ( X_train , y_train ) estimator2 . fit ( X_train , y_train ) est1_score = scorer ( estimator1 , X_test , y_test ) est2_score = scorer ( estimator2 , X_test , y_test ) score_diff . append ( est1_score - est2_score ) avg_diff = np . mean ( score_diff ) numerator = avg_diff * np . sqrt ( cv ) denominator = np . sqrt ( sum ([( diff - avg_diff ) ** 2 for diff in score_diff ]) / ( cv - 1 ) ) t_stat = numerator / denominator pvalue = stats . t . sf ( np . abs ( t_stat ), cv - 1 ) * 2.0 return float ( t_stat ), float ( pvalue ) # check if difference between algorithms is real X_tmp = df_folds [ predictor_cols ] . values y_tmp = df_folds [ 'diagnosis' ] . values t , p = paired_ttest_skfold_cv ( estimator1 = classifiers [ 1 ], estimator2 = classifiers [ - 1 ], shuffle = True , cv = 5 , X = X_tmp , y = y_tmp , scoring = 'roc_auc' , random_seed = config . seed ) logger . info ( 'P-value: %.3f , t-Statistic: %.3f ' % ( p , t )) 2021-11-13,14:04:35 - P-value: 0.171, t-Statistic: 1.667 2021-11-13,14:04:35 - P-value: 0.171, t-Statistic: 1.667 Since \\(p\\) -value is quite high, and more the basic threshold of 0.05 or 0.1, we fail to reject the null hypothesis, and say that there is no significant difference between these two models.","title":"Hypothesis Testing Across Models"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%207%20-%20Modelling%20%28Model%20Selection%20and%20Hyperparameter%20Tuning%29/","text":"Stage 7: Hyperparameter Tuning by Hongnan Gao Dependencies and Configuration %% capture ! pip install - q wandb # !pip install -q shap ! pip install - q mlxtend == 0.19.0 ! pip install - q statsmodels == 0.13.1 # !pip install gcloud == 0.18.3 import wandb wandb . login () \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect. True import copy import csv import logging import os import random from dataclasses import asdict , dataclass , field from functools import wraps from pathlib import Path from time import time from typing import Any , Callable , Dict , List , Optional , Tuple , Union import matplotlib.pyplot as plt import mlxtend import numpy as np import pandas as pd import seaborn as sns from joblib import dump , load from mlxtend.evaluate import bias_variance_decomp , paired_ttest_5x2cv from scipy import stats from sklearn import ( base , decomposition , dummy , ensemble , feature_selection , linear_model , metrics , model_selection , neighbors , pipeline , preprocessing , svm , tree ) from statsmodels.regression.linear_model import OLS Utils and Configurations @dataclass class config : raw_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/raw/data.csv\" processed_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/processed.csv\" df_folds : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/df_folds.csv\" train_size : float = 0.9 seed : int = 1992 num_folds : int = 5 cv_schema : str = \"StratifiedKFold\" classification_type : str = \"binary\" target_col : List [ str ] = field ( default_factory = lambda : [ \"diagnosis\" ]) unwanted_cols : List [ str ] = field ( default_factory = lambda : [ \"id\" , \"Unnamed: 32\" ]) # Plotting colors : List [ str ] = field ( default_factory = lambda : [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ]) cmap_reversed = plt . cm . get_cmap ( 'mako_r' ) def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert the config object to a dictionary. Returns: Dict: The config object as a dictionary. \"\"\" return asdict ( self ) # spot_checking_boxplot = \"../data/images/spot_checking_boxplot.png\" # oof_confusion_matrix = \"../data/images/oof_confusion_matrix.png\" # final_train_confusion_matrix = \"../data/images/final_train_confusion_matrix.png\" # precision_recall_threshold_plot = \"../data/images/precision_recall_threshold_plot.png\" # roc_plot = \"../data/images/roc_plot.png\" # feature_importance = \"../data/images/feature_importance.png\" def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) def init_logger ( log_file : str = \"info.log\" ): \"\"\" Initialize logger. \"\"\" logger = logging . getLogger ( __name__ ) logger . setLevel ( logging . INFO ) stream_handler = logging . StreamHandler () stream_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) file_handler = logging . FileHandler ( filename = log_file ) file_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) logger . addHandler ( stream_handler ) logger . addHandler ( file_handler ) return logger # Utils functions that we need def variance_inflation_factor ( exog , idx_kept , vif_idx ): \"\"\"Compute VIF for one feature. Args: exog (np.ndarray): Observations idx_kept (List[int]): Indices of features to consider vif_idx (int): Index of feature for which to compute VIF Returns: float: VIF for the selected feature \"\"\" exog = np . asarray ( exog ) x_i = exog [:, vif_idx ] mask = [ col for col in idx_kept if col != vif_idx ] x_noti = exog [:, mask ] r_squared_i = OLS ( x_i , x_noti ) . fit () . rsquared vif = 1. / ( 1. - r_squared_i ) return vif class ReduceVIF ( base . BaseEstimator , base . TransformerMixin ): \"\"\"The base of the class structure is implemented in https://www.kaggle.com/ffisegydd/sklearn-multicollinearity-class; I heavily modified the class such that it can take in numpy arrays and correctly implemented the fit and transform method. \"\"\" def __init__ ( self , thresh = 10 , max_drop = 20 ): self . thresh = thresh self . max_drop = max_drop self . column_indices_kept_ = [] self . feature_names_kept_ = None def reset ( self ): \"\"\"Resets the state of predictor columns after each fold.\"\"\" self . column_indices_kept_ = [] self . feature_names_kept_ = None def fit ( self , X , y = None ): \"\"\"Fits the Recursive VIF on the training folds and save the selected feature names in self.feature_names Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" self . column_indices_kept_ , self . feature_names_kept_ = self . calculate_vif ( X ) return self def transform ( self , X , y = None ): \"\"\"Transforms the Validation Set according to the selected feature names. Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" return X [:, self . column_indices_kept_ ] def calculate_vif ( self , X : Union [ np . ndarray , pd . DataFrame ]): \"\"\"Implements a VIF function that recursively eliminates features. Args: X (Union[np.ndarray, pd.DataFrame]): [description] Returns: [type]: [description] \"\"\" feature_names = None column_indices_kept = list ( range ( X . shape [ 1 ])) if isinstance ( X , pd . DataFrame ): feature_names = X . columns dropped = True count = 0 while dropped and count <= self . max_drop : dropped = False max_vif , max_vif_col = None , None for col in column_indices_kept : vif = variance_inflation_factor ( X , column_indices_kept , col ) if max_vif is None or vif > max_vif : max_vif = vif max_vif_col = col if max_vif > self . thresh : # print(f\"Dropping {max_vif_col} with vif={max_vif}\") column_indices_kept . remove ( max_vif_col ) if feature_names is not None : feature_names . pop ( max_vif_col ) dropped = True count += 1 return column_indices_kept , feature_names def prepare_y ( y : np . ndarray ) -> np . ndarray : \"\"\"Prepare the target variable for the model. If Binary Classification, we need to ravel the array to 1d. Args: y (np.ndarray): Target variable. Returns: np.ndarray: Transformed Target variable. \"\"\" return y . ravel () if config . classification_type == \"binary\" else y config = config () basic_config : Dict = config . to_dict () # train_config: Dict = Train().to_dict() global_config : Dict = dict ( basic = basic_config ) # We can log multiple dict under global_config - in wandb UI, it will show as basic. and train. to show which dict it is referring to. run = wandb . init ( project = \"bcw\" , name = \"classification\" , config = global_config ) # set logger logger = init_logger () # set seeding for reproducibility _ = set_seeds ( seed = config . seed ) # read data df_folds = pd . read_csv ( config . df_folds ) # Assign predictors and target accordingly predictor_cols = df_folds . columns . to_list ()[: - 2 ] target_col = config . target_col Model Selection: Hyperparameter Tuning with GridSearchCV Hyperparameter Tuning We have done a quick spot checking on algorithms and realized that LogisticRegression is doing well for this task. For this purpose, I will just perform hyperparameter tuning on this single algorithm. However, in practice and if resources are allowed, I will also tune other models such as RandomForest() , or gradient boosting algorithms such as XGBoost , as I believe they will perform no worse than our Logistic Regression model given the right hyperparameters. Grid Search is the Gwei? Meh! We will use an old-fashioned way to search for hyperparameters, which is brute force method. The time complexity of Grid Search is high and if you have many hyperparameters to tune, I recommend trying out Random Grid Search or libraries like Optuna that uses Bayesian Optimization. TODO Try to code up your own GridSearchCV to have maximum flexibility. Make Finetuning Pipeline The following make_finetuning_pipeline does exactly the same thing is as make_pipeline earlier. The only difference is we can pass in flexible list of steps to the pipeline from outside. def make_finetuning_pipeline ( model : Callable , steps : List [ Tuple [ str , Callable ]] ) -> pipeline . Pipeline : \"\"\"Return a pipeline that can be used for finetuning. Args: model (Callable): A model with default parameters. steps (List[Tuple[str, Callable]]): A list of preprocessing steps to pass in Pipeline object. Returns: Pipeline: Returns a pipeline that can be used for finetuning. \"\"\" return pipeline . Pipeline ([ * steps , ( \"model\" , model )]) # TODO: Make a class to hold pipelines? # class MakePipeline: # def __init__(self, estimator: Callable, steps: List[Callable]): # pass # def spot_checking_pipeline(): # pass # def fine_tuning_pipeline(): # pass finetuning_pipeline_steps = [ # standardization ( \"standardize\" , preprocessing . StandardScaler ()), # reduce VIF ( \"remove_multicollinearity\" , ReduceVIF ( thresh = 10 )) ] Search Space Run our hyperparameter search with cross-validation. For example, our param_grid has \\(2 \\times 10 = 20\\) combinations, and our cross validation has 5 folds, then there will be a total of 100 fits. Below details the pseudo code of what happens under the hood: Define \\(G\\) as the set of combination of hyperparamters. Define number of splits to be \\(K\\) . For each set of hyperparameter \\(z \\in Z\\) : for fold \\(j\\) in K: Set \\(F_{\\text{train}}=\\bigcup\\limits_{i\\neq k}^{K} F_{i}\\) Set \\(F_{\\text{val}} = F_{j}\\) as the validation set Perform Standard Scaling on \\(F_{\\text{train}}\\) and find the mean and std Perform VIF recursively on \\(F_{\\text{train}}\\) and find the selected features Transform \\(F_{\\text{val}}\\) using the mean and std found using \\(F_{\\text{train}}\\) Transform \\(F_{\\text{val}}\\) to have only the selected features from \\(F_{\\text{train}}\\) Train and fit on \\(F_{\\text{train}}\\) Evaluate the fitted parameters on \\(F_{\\text{val}}\\) to obtain \\(\\mathcal{M}\\) @dataclass class ModelForTuning : model : Callable param_grid : Dict Define our search space for the hyperparameters: logistic_r_param_grid = { model__penalty = [ \"l1\" , \"l2\" ], model__C = np . logspace ( - 4 , 4 , 10 )} We conveniently use dataclass to act as a medium so we can pass in model and param_grid independently for each model. We then collate them into a list of ModelForTuning object. models_list = [ ModelForTuning ( model = linear_model . LogisticRegression ( solver = \"saga\" , random_state = config . seed , max_iter = 10000 , n_jobs =- 1 , fit_intercept = True , ), param_grid = dict ( model__penalty = [ \"l1\" , \"l2\" ], model__C = np . logspace ( - 4 , 4 , 10 ), ), ), ModelForTuning ( model = tree . DecisionTreeClassifier ( random_state = config . seed ), param_grid = dict ( model__max_depth = [ 2 , 3 , 5 , 10 , 20 ], model__min_samples_leaf = [ 5 , 10 , 20 , 50 , 100 ], model__criterion = [ \"gini\" , \"entropy\" ], ), ), ModelForTuning ( model = ensemble . GradientBoostingClassifier ( n_estimators = 100 ), param_grid = dict ( model__max_depth = [ 3 , 6 ], model__learning_rate = [ 0.1 , 0.05 ], model__subsample = [ 1 , 0.5 , ], ), ), ] def optimize_models ( models_list : List [ ModelForTuning ], X_train : np . ndarray , y_train : np . ndarray , scorer : Union [ str , Callable ], steps : List [ Tuple [ str , Callable ]], ) -> List [ Callable ]: \"\"\"Optimize models in models_list using X_train and y_train. We are using GridSearchCV to find the best parameters for each model. Consider using Optuna for hyperparameter optimization (or wandb for hyperparameter optimization). Args: models_list (List[ModelForTuning]): List of models to optimize. X_train (np.ndarray): X_train data. y_train (np.ndarray): y_train data. Returns: grids (List[Callable]): List of optimized models. \"\"\" # @ TODO: make a scoring list to pass in so we can evaluate multiple metrics. grids = [ model_selection . GridSearchCV ( make_finetuning_pipeline ( model . model , steps ), param_grid = model . param_grid , cv = 5 , refit = True , verbose = 1 , scoring = scorer , n_jobs =- 1 , ) for model in models_list ] for grid in grids : grid . fit ( X_train , y_train ) return grids roc_auc_scorer = \"roc_auc_ovr\" # Unsure why this gives much lower score - to investigate # metrics.make_scorer(metrics.roc_auc_score, average=\"macro\", multi_class='ovr') X_train , y_train = df_folds [ predictor_cols ] . values , df_folds [ target_col ] . values y_train = prepare_y ( y_train ) grids = optimize_models ( models_list , X_train , y_train , scorer = roc_auc_scorer , steps = finetuning_pipeline_steps ) Fitting 5 folds for each of 20 candidates, totalling 100 fits Fitting 5 folds for each of 50 candidates, totalling 250 fits Fitting 5 folds for each of 8 candidates, totalling 40 fits # The above optimize code is equivalent to the below, for better readability # pipeline_logistic = make_finetuning_pipeline( # linear_model.LogisticRegression( # solver=\"saga\", random_state=config.seed, max_iter=10000, n_jobs=None, fit_intercept=True # ), steps=steps # ) # param_grid = dict( # model__penalty=[\"l1\", \"l2\"], # model__C=np.logspace(-4, 4, 10), # ) # grid = model_selection.GridSearchCV(pipeline_logistic, param_grid=param_grid, cv=5, refit=True, verbose=3, scoring = \"roc_auc\") # _ = grid.fit(X_train, y_train) We can save our results in a dataframe, we will also look at the top performing hyperparameter by querying the below: grid_cv_df = pd . DataFrame ( grid . cv_results_ ) grid_cv_df . loc [ grid_cv_df [ 'rank_test_score' ] == 1 ] # For example, we can see Logistic Regression's GridSearchCV # results like this. grid_cv_df = pd . DataFrame ( grids [ 0 ] . cv_results_ ) display ( grid_cv_df . loc [ grid_cv_df [ 'rank_test_score' ] == 1 ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean_fit_time std_fit_time mean_score_time std_score_time param_model__C param_model__penalty params split0_test_score split1_test_score split2_test_score split3_test_score split4_test_score mean_test_score std_test_score rank_test_score 8 0.931891 0.058794 0.002457 0.000387 0.359381 l1 {'model__C': 0.3593813663804626, 'model__penal... 0.997997 0.995547 0.997944 0.990132 0.995477 0.995419 0.002863 1 def return_grid_df ( grids : List [ model_selection . GridSearchCV ], ) -> Union [ pd . DataFrame , List [ model_selection . GridSearchCV ]]: \"\"\"Return a dataframe of the grids with shorted names. Args: grids (List[model_selection.GridSearchCV]): A list of GridSearchCV models that are tuned. Returns: grid_df, grids (Union[pd.DataFrame, List[model_selection.GridSearchCV]]): A dataframe of the grids with shorted names. \"\"\" def shorten_param ( param_name ): if \"__\" in param_name : return param_name . rsplit ( \"__\" , 1 )[ 1 ] return param_name grid_df = [] for grid in grids : model_name = grid . estimator [ \"model\" ] . __class__ . __name__ cv_results = pd . DataFrame ( grid . cv_results_ ) . sort_values ( \"mean_test_score\" , ascending = False ) # get the parameter names column_results = [ f \"param_ { name } \" for name in grid . param_grid . keys ()] column_results += [ \"mean_test_score\" , \"std_test_score\" , \"rank_test_score\" , ] cv_results = cv_results [ column_results ] cv_results = cv_results . rename ( shorten_param , axis = 1 ) cv_results [ \"model_name\" ] = model_name grid_df . append ( cv_results ) return grid_df , grids # grid_df and grids should necessarily be in the same sequence. # grid_df[0] == grids[0] in terms of model information, in this # case, the first index of both should be logistic regression. grid_df , grids = return_grid_df ( grids ) for model_df , grid in zip ( grid_df , grids ): best_hyperparams_df = model_df . iloc [[ 0 ]] model_name = best_hyperparams_df . model_name . unique ()[ 0 ] logger . info ( f \"Best hyperparameters found for { model_name } is as follows: \\n { grid . best_params_ } \" ) display ( best_hyperparams_df ) print () 2021-11-16,09:19:36 - Best hyperparameters found for LogisticRegression is as follows: {'model__C': 0.3593813663804626, 'model__penalty': 'l1'} 2021-11-16,09:19:36 - Best hyperparameters found for LogisticRegression is as follows: {'model__C': 0.3593813663804626, 'model__penalty': 'l1'} .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } penalty C mean_test_score std_test_score rank_test_score model_name 8 l1 0.359381 0.995419 0.002863 1 LogisticRegression 2021-11-16,09:19:36 - Best hyperparameters found for DecisionTreeClassifier is as follows: {'model__criterion': 'entropy', 'model__max_depth': 10, 'model__min_samples_leaf': 10} 2021-11-16,09:19:36 - Best hyperparameters found for DecisionTreeClassifier is as follows: {'model__criterion': 'entropy', 'model__max_depth': 10, 'model__min_samples_leaf': 10} .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } max_depth min_samples_leaf criterion mean_test_score std_test_score rank_test_score model_name 41 10 10 entropy 0.954515 0.015913 1 DecisionTreeClassifier 2021-11-16,09:19:37 - Best hyperparameters found for GradientBoostingClassifier is as follows: {'model__learning_rate': 0.1, 'model__max_depth': 3, 'model__subsample': 0.5} 2021-11-16,09:19:37 - Best hyperparameters found for GradientBoostingClassifier is as follows: {'model__learning_rate': 0.1, 'model__max_depth': 3, 'model__subsample': 0.5} .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } max_depth learning_rate subsample mean_test_score std_test_score rank_test_score model_name 1 3 0.1 0.5 0.991031 0.005869 1 GradientBoostingClassifier Success Our best performing set of hyperparameters for Logistic Regression {'model__C': 0.3593813663804626, 'model__penalty': 'l1'} gives rise to a mean cross validation score of \\(0.995419\\) , which is higher than the model with default hyperparameter scoring, \\(0.995\\) by a small margin. Not too surprising for Logistic Regression here since there aren't many things to tune, and should not see major improvements, but for Decesion Tree, it has increased from 0.907 to around 0.95, seeing quite a big jump with tuned params. DANGERRRRRRRRRRRRR I am being a bit hand wavy in terms of comparison here, I assumed THAT GridSearchCV used the exact same splitting strategy (yes it uses StratifiedKFold here) with the exact SEED/RANDOM_STATE , which I cannot promise as of now. Thus, a different splitting will, unfortunately, result in different results, although, I don't expect by a huge margin - so I think it is a no-go to compare like this. We can probably pass in a cv function into GridSearchCV to ensure seeding. This also highlights a problem that even K-fold splitting does not guarantee the reduction in variance. Room for Improvement Apart from the other methods to search for the optimal hyperparameters, we can also include preprocessing step as a tunable hyperparameter. More specifically, in our ReduceVIF() step, we hard coded two manual criterion in which the algorithm will stop; if the threshold reaches 10, or if the number of features removed hit 20; we can include them in the search space so we do not need to worry about how many features to remove! Model Persistence (Saving Models) Model Persistence We save our models using joblib and we can load it back any time. Note Save it to wandb or GCP storage to store models for better consistency. model_path = \"/content/\" def save_model ( grids : List [ Callable ], path : str ): \"\"\"Save a model to a file\"\"\" for grid in grids : model_name = grid . best_estimator_ [ \"model\" ] . __class__ . __name__ path_to_save = Path ( path , f \" { model_name } _grid.joblib\" ) # Dump to local path dump ( grid , Path ( path , path_to_save )) # Dump to wandb cloud # \"model.h5\" is saved in wandb.run.dir & will be uploaded at the end of training wandb . save ( os . path . join ( wandb . run . dir , path_to_save )) Save the model! Wandb We first see how we save and load using wandb. save_model ( grids , model_path ) \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\") \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\") \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\") logistic_path = \"LogisticRegression_grid.joblib\" # restore the model file \"model.h5\" from a specific run by user \"lavanyashukla\" # in project \"save_and_restore\" from run \"10pr4joa\" best_model = wandb . restore ( logistic_path ) # use the \"name\" attribute of the returned object # if your framework expects a filename, e.g. as in Keras # model.load_weights(best_model.name) Joblib We see how we use joblib to save and load. Load the model, and we can test it now if our loaded models is predicting correctly! logistic_grid = load ( \"/content/LogisticRegression_grid.joblib\" ) Great it seems to work! Sanity Check Note We just make sure our loaded weight from path is the same as the one we trained. We can easily compare predictions (or coefficients) by the following. load ( best_model . name ) . predict ( X_train ) . all () == logistic_grid . predict ( X_train ) . all () == grids [ 0 ] . predict ( X_train ) . all () True metrics . roc_auc_score ( y_train , logistic_grid . predict_proba ( X_train )[:, 1 ] ) == metrics . roc_auc_score ( y_train , grids [ 0 ] . predict_proba ( X_train )[:, 1 ] ) == metrics . roc_auc_score ( y_train , load ( best_model . name ) . predict_proba ( X_train )[:, 1 ] ) True Seems like the save and load method works perfectly. Warning Do not call this directly. grids [ 0 ] . best_estimator_ [ \"model\" ] . predict ( X_train ) This is because grids[0].best_estimator_[\"model\"] is only referring to the Logistic Regression Model WITHOUT the pipeline (preprocessing) steps. And hence will raise error if the preprocessing steps has feature selection. But the main idea is, be careful when using the above. # grids[0].best_estimator_[\"model\"].predict(X_train) Retrain using Hyperparameters Retraining Methods From the discussion 1 , my doubts are cleared. Quoting verbatim from the discussion, we have: K-folds cross validation was devised as a way to assess model performance using training data. A great paper on this from Sebastian Raschka is a must read https://arxiv.org/abs/1811.12808. You use K-folds cv to tune you model, then retrain on all training data with best hyperparamters found. However, once you have run K-fold cv, you get \\(K\\) trained models. Kagglers quickly found that ensembling these models was giving good results at zero computation cost, rather than having to retrain a model on full data. It soon became a very common practice. Takeway For small-medium datasets, after finding the best hyperparameters \\(G\\) , we use \\(G\\) in our model \\(h\\) to train on the whole dataset \\(\\mathcal{X}\\) again to get the fitted parameters of \\(h\\) . Then you use the newly gained fitted parameters to then evaluate on the Test Set . For large and computationally expensive datasets, when you finished your K-folds, say 5 folds, you get 5 \"different\" models, \\(h_{i}, i \\in {1, 2, 3, 4, 5}\\) , what you can do is to save the weights (or in normal ML, weights refer to the parameters gained), and evaluate on the test set for each of the five models, you then get 5 different test predictions, and a common practice is the do a simple mean of these 5 set of predictions. Retrain on K-Folds TODO: This should be easy for me as I dabbled more in Kaggle comp and are more familiar with this methodology. Retrain on the whole training set A common practice after the hyperparameter tuning phase is to retrain the model on the whole dataset \\(X_{\\text{train}}\\) where we will get the estimator's coefficients obtained from the retraining. This is actually already done as the scikit-learn's GridSearchCV has a parameter refit ; if we select it to be true, then after the model selection process is done (i.e. getting the best hyperparameters after cross validation with grid search), the grid search object will retrain on the whole \\(X_{\\text{train}}\\) with the best hyperparameters internally, and return us back an object in which we can call predict etc. Paranoia Alert However, to be extra careful, we can retrain manually using the best hyperparameters and check if scikit-learn is true to its documentation. We will just reconstruct the pipeline using the grid's best hyper parameters. We will then test if the retrained model's coefficients coincide with the grid's best estimator's coefficients. If there difference is 0, this means they are trained under the same circumstances and we can be sure that the refit parameter is behaving true to its words. grid_best_hyperparams = grid . best_params_ print ( grid_best_hyperparams ) -> { 'model__C' : 0.3593813663804626 , 'model__penalty' : 'l1' } retrain_logistic_pipeline = pipeline . Pipeline ( [ ( \"standardize\" , preprocessing . StandardScaler ()), ( \"remove_multicollinearity\" , ReduceVIF ( thresh = 10 )), ( \"model\" , linear_model . LogisticRegression ( C = 0.3593813663804626 , max_iter = 10000 , random_state = 1992 , solver = \"saga\" , penalty = \"l1\" , ), ), ] ) _ = retrain_logistic_pipeline . fit ( X_train , y_train ) logistic_grid = grids [ 0 ] coef_diff = ( retrain_logistic_pipeline [ \"model\" ] . coef_ - logistic_grid . best_estimator_ [ \"model\" ] . coef_ ) print ( \"...\" ) assert np . all ( coef_diff == 0 ) == True logger . info ( \"Retraining Assertion Passed!\" ) 2021-11-16,09:19:38 - Retraining Assertion Passed! 2021-11-16,09:19:38 - Retraining Assertion Passed! ... https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/275883 \u21a9","title":"Modelling (Model Selection)"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%207%20-%20Modelling%20%28Model%20Selection%20and%20Hyperparameter%20Tuning%29/#dependencies-and-configuration","text":"%% capture ! pip install - q wandb # !pip install -q shap ! pip install - q mlxtend == 0.19.0 ! pip install - q statsmodels == 0.13.1 # !pip install gcloud == 0.18.3 import wandb wandb . login () \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect. True import copy import csv import logging import os import random from dataclasses import asdict , dataclass , field from functools import wraps from pathlib import Path from time import time from typing import Any , Callable , Dict , List , Optional , Tuple , Union import matplotlib.pyplot as plt import mlxtend import numpy as np import pandas as pd import seaborn as sns from joblib import dump , load from mlxtend.evaluate import bias_variance_decomp , paired_ttest_5x2cv from scipy import stats from sklearn import ( base , decomposition , dummy , ensemble , feature_selection , linear_model , metrics , model_selection , neighbors , pipeline , preprocessing , svm , tree ) from statsmodels.regression.linear_model import OLS","title":"Dependencies and Configuration"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%207%20-%20Modelling%20%28Model%20Selection%20and%20Hyperparameter%20Tuning%29/#utils-and-configurations","text":"@dataclass class config : raw_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/raw/data.csv\" processed_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/processed.csv\" df_folds : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/df_folds.csv\" train_size : float = 0.9 seed : int = 1992 num_folds : int = 5 cv_schema : str = \"StratifiedKFold\" classification_type : str = \"binary\" target_col : List [ str ] = field ( default_factory = lambda : [ \"diagnosis\" ]) unwanted_cols : List [ str ] = field ( default_factory = lambda : [ \"id\" , \"Unnamed: 32\" ]) # Plotting colors : List [ str ] = field ( default_factory = lambda : [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ]) cmap_reversed = plt . cm . get_cmap ( 'mako_r' ) def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert the config object to a dictionary. Returns: Dict: The config object as a dictionary. \"\"\" return asdict ( self ) # spot_checking_boxplot = \"../data/images/spot_checking_boxplot.png\" # oof_confusion_matrix = \"../data/images/oof_confusion_matrix.png\" # final_train_confusion_matrix = \"../data/images/final_train_confusion_matrix.png\" # precision_recall_threshold_plot = \"../data/images/precision_recall_threshold_plot.png\" # roc_plot = \"../data/images/roc_plot.png\" # feature_importance = \"../data/images/feature_importance.png\" def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) def init_logger ( log_file : str = \"info.log\" ): \"\"\" Initialize logger. \"\"\" logger = logging . getLogger ( __name__ ) logger . setLevel ( logging . INFO ) stream_handler = logging . StreamHandler () stream_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) file_handler = logging . FileHandler ( filename = log_file ) file_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) logger . addHandler ( stream_handler ) logger . addHandler ( file_handler ) return logger # Utils functions that we need def variance_inflation_factor ( exog , idx_kept , vif_idx ): \"\"\"Compute VIF for one feature. Args: exog (np.ndarray): Observations idx_kept (List[int]): Indices of features to consider vif_idx (int): Index of feature for which to compute VIF Returns: float: VIF for the selected feature \"\"\" exog = np . asarray ( exog ) x_i = exog [:, vif_idx ] mask = [ col for col in idx_kept if col != vif_idx ] x_noti = exog [:, mask ] r_squared_i = OLS ( x_i , x_noti ) . fit () . rsquared vif = 1. / ( 1. - r_squared_i ) return vif class ReduceVIF ( base . BaseEstimator , base . TransformerMixin ): \"\"\"The base of the class structure is implemented in https://www.kaggle.com/ffisegydd/sklearn-multicollinearity-class; I heavily modified the class such that it can take in numpy arrays and correctly implemented the fit and transform method. \"\"\" def __init__ ( self , thresh = 10 , max_drop = 20 ): self . thresh = thresh self . max_drop = max_drop self . column_indices_kept_ = [] self . feature_names_kept_ = None def reset ( self ): \"\"\"Resets the state of predictor columns after each fold.\"\"\" self . column_indices_kept_ = [] self . feature_names_kept_ = None def fit ( self , X , y = None ): \"\"\"Fits the Recursive VIF on the training folds and save the selected feature names in self.feature_names Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" self . column_indices_kept_ , self . feature_names_kept_ = self . calculate_vif ( X ) return self def transform ( self , X , y = None ): \"\"\"Transforms the Validation Set according to the selected feature names. Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" return X [:, self . column_indices_kept_ ] def calculate_vif ( self , X : Union [ np . ndarray , pd . DataFrame ]): \"\"\"Implements a VIF function that recursively eliminates features. Args: X (Union[np.ndarray, pd.DataFrame]): [description] Returns: [type]: [description] \"\"\" feature_names = None column_indices_kept = list ( range ( X . shape [ 1 ])) if isinstance ( X , pd . DataFrame ): feature_names = X . columns dropped = True count = 0 while dropped and count <= self . max_drop : dropped = False max_vif , max_vif_col = None , None for col in column_indices_kept : vif = variance_inflation_factor ( X , column_indices_kept , col ) if max_vif is None or vif > max_vif : max_vif = vif max_vif_col = col if max_vif > self . thresh : # print(f\"Dropping {max_vif_col} with vif={max_vif}\") column_indices_kept . remove ( max_vif_col ) if feature_names is not None : feature_names . pop ( max_vif_col ) dropped = True count += 1 return column_indices_kept , feature_names def prepare_y ( y : np . ndarray ) -> np . ndarray : \"\"\"Prepare the target variable for the model. If Binary Classification, we need to ravel the array to 1d. Args: y (np.ndarray): Target variable. Returns: np.ndarray: Transformed Target variable. \"\"\" return y . ravel () if config . classification_type == \"binary\" else y config = config () basic_config : Dict = config . to_dict () # train_config: Dict = Train().to_dict() global_config : Dict = dict ( basic = basic_config ) # We can log multiple dict under global_config - in wandb UI, it will show as basic. and train. to show which dict it is referring to. run = wandb . init ( project = \"bcw\" , name = \"classification\" , config = global_config ) # set logger logger = init_logger () # set seeding for reproducibility _ = set_seeds ( seed = config . seed ) # read data df_folds = pd . read_csv ( config . df_folds ) # Assign predictors and target accordingly predictor_cols = df_folds . columns . to_list ()[: - 2 ] target_col = config . target_col","title":"Utils and Configurations"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%207%20-%20Modelling%20%28Model%20Selection%20and%20Hyperparameter%20Tuning%29/#model-selection-hyperparameter-tuning-with-gridsearchcv","text":"Hyperparameter Tuning We have done a quick spot checking on algorithms and realized that LogisticRegression is doing well for this task. For this purpose, I will just perform hyperparameter tuning on this single algorithm. However, in practice and if resources are allowed, I will also tune other models such as RandomForest() , or gradient boosting algorithms such as XGBoost , as I believe they will perform no worse than our Logistic Regression model given the right hyperparameters. Grid Search is the Gwei? Meh! We will use an old-fashioned way to search for hyperparameters, which is brute force method. The time complexity of Grid Search is high and if you have many hyperparameters to tune, I recommend trying out Random Grid Search or libraries like Optuna that uses Bayesian Optimization. TODO Try to code up your own GridSearchCV to have maximum flexibility.","title":"Model Selection: Hyperparameter Tuning with GridSearchCV"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%207%20-%20Modelling%20%28Model%20Selection%20and%20Hyperparameter%20Tuning%29/#make-finetuning-pipeline","text":"The following make_finetuning_pipeline does exactly the same thing is as make_pipeline earlier. The only difference is we can pass in flexible list of steps to the pipeline from outside. def make_finetuning_pipeline ( model : Callable , steps : List [ Tuple [ str , Callable ]] ) -> pipeline . Pipeline : \"\"\"Return a pipeline that can be used for finetuning. Args: model (Callable): A model with default parameters. steps (List[Tuple[str, Callable]]): A list of preprocessing steps to pass in Pipeline object. Returns: Pipeline: Returns a pipeline that can be used for finetuning. \"\"\" return pipeline . Pipeline ([ * steps , ( \"model\" , model )]) # TODO: Make a class to hold pipelines? # class MakePipeline: # def __init__(self, estimator: Callable, steps: List[Callable]): # pass # def spot_checking_pipeline(): # pass # def fine_tuning_pipeline(): # pass finetuning_pipeline_steps = [ # standardization ( \"standardize\" , preprocessing . StandardScaler ()), # reduce VIF ( \"remove_multicollinearity\" , ReduceVIF ( thresh = 10 )) ]","title":"Make Finetuning Pipeline"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%207%20-%20Modelling%20%28Model%20Selection%20and%20Hyperparameter%20Tuning%29/#search-space","text":"Run our hyperparameter search with cross-validation. For example, our param_grid has \\(2 \\times 10 = 20\\) combinations, and our cross validation has 5 folds, then there will be a total of 100 fits. Below details the pseudo code of what happens under the hood: Define \\(G\\) as the set of combination of hyperparamters. Define number of splits to be \\(K\\) . For each set of hyperparameter \\(z \\in Z\\) : for fold \\(j\\) in K: Set \\(F_{\\text{train}}=\\bigcup\\limits_{i\\neq k}^{K} F_{i}\\) Set \\(F_{\\text{val}} = F_{j}\\) as the validation set Perform Standard Scaling on \\(F_{\\text{train}}\\) and find the mean and std Perform VIF recursively on \\(F_{\\text{train}}\\) and find the selected features Transform \\(F_{\\text{val}}\\) using the mean and std found using \\(F_{\\text{train}}\\) Transform \\(F_{\\text{val}}\\) to have only the selected features from \\(F_{\\text{train}}\\) Train and fit on \\(F_{\\text{train}}\\) Evaluate the fitted parameters on \\(F_{\\text{val}}\\) to obtain \\(\\mathcal{M}\\) @dataclass class ModelForTuning : model : Callable param_grid : Dict Define our search space for the hyperparameters: logistic_r_param_grid = { model__penalty = [ \"l1\" , \"l2\" ], model__C = np . logspace ( - 4 , 4 , 10 )} We conveniently use dataclass to act as a medium so we can pass in model and param_grid independently for each model. We then collate them into a list of ModelForTuning object. models_list = [ ModelForTuning ( model = linear_model . LogisticRegression ( solver = \"saga\" , random_state = config . seed , max_iter = 10000 , n_jobs =- 1 , fit_intercept = True , ), param_grid = dict ( model__penalty = [ \"l1\" , \"l2\" ], model__C = np . logspace ( - 4 , 4 , 10 ), ), ), ModelForTuning ( model = tree . DecisionTreeClassifier ( random_state = config . seed ), param_grid = dict ( model__max_depth = [ 2 , 3 , 5 , 10 , 20 ], model__min_samples_leaf = [ 5 , 10 , 20 , 50 , 100 ], model__criterion = [ \"gini\" , \"entropy\" ], ), ), ModelForTuning ( model = ensemble . GradientBoostingClassifier ( n_estimators = 100 ), param_grid = dict ( model__max_depth = [ 3 , 6 ], model__learning_rate = [ 0.1 , 0.05 ], model__subsample = [ 1 , 0.5 , ], ), ), ] def optimize_models ( models_list : List [ ModelForTuning ], X_train : np . ndarray , y_train : np . ndarray , scorer : Union [ str , Callable ], steps : List [ Tuple [ str , Callable ]], ) -> List [ Callable ]: \"\"\"Optimize models in models_list using X_train and y_train. We are using GridSearchCV to find the best parameters for each model. Consider using Optuna for hyperparameter optimization (or wandb for hyperparameter optimization). Args: models_list (List[ModelForTuning]): List of models to optimize. X_train (np.ndarray): X_train data. y_train (np.ndarray): y_train data. Returns: grids (List[Callable]): List of optimized models. \"\"\" # @ TODO: make a scoring list to pass in so we can evaluate multiple metrics. grids = [ model_selection . GridSearchCV ( make_finetuning_pipeline ( model . model , steps ), param_grid = model . param_grid , cv = 5 , refit = True , verbose = 1 , scoring = scorer , n_jobs =- 1 , ) for model in models_list ] for grid in grids : grid . fit ( X_train , y_train ) return grids roc_auc_scorer = \"roc_auc_ovr\" # Unsure why this gives much lower score - to investigate # metrics.make_scorer(metrics.roc_auc_score, average=\"macro\", multi_class='ovr') X_train , y_train = df_folds [ predictor_cols ] . values , df_folds [ target_col ] . values y_train = prepare_y ( y_train ) grids = optimize_models ( models_list , X_train , y_train , scorer = roc_auc_scorer , steps = finetuning_pipeline_steps ) Fitting 5 folds for each of 20 candidates, totalling 100 fits Fitting 5 folds for each of 50 candidates, totalling 250 fits Fitting 5 folds for each of 8 candidates, totalling 40 fits # The above optimize code is equivalent to the below, for better readability # pipeline_logistic = make_finetuning_pipeline( # linear_model.LogisticRegression( # solver=\"saga\", random_state=config.seed, max_iter=10000, n_jobs=None, fit_intercept=True # ), steps=steps # ) # param_grid = dict( # model__penalty=[\"l1\", \"l2\"], # model__C=np.logspace(-4, 4, 10), # ) # grid = model_selection.GridSearchCV(pipeline_logistic, param_grid=param_grid, cv=5, refit=True, verbose=3, scoring = \"roc_auc\") # _ = grid.fit(X_train, y_train) We can save our results in a dataframe, we will also look at the top performing hyperparameter by querying the below: grid_cv_df = pd . DataFrame ( grid . cv_results_ ) grid_cv_df . loc [ grid_cv_df [ 'rank_test_score' ] == 1 ] # For example, we can see Logistic Regression's GridSearchCV # results like this. grid_cv_df = pd . DataFrame ( grids [ 0 ] . cv_results_ ) display ( grid_cv_df . loc [ grid_cv_df [ 'rank_test_score' ] == 1 ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean_fit_time std_fit_time mean_score_time std_score_time param_model__C param_model__penalty params split0_test_score split1_test_score split2_test_score split3_test_score split4_test_score mean_test_score std_test_score rank_test_score 8 0.931891 0.058794 0.002457 0.000387 0.359381 l1 {'model__C': 0.3593813663804626, 'model__penal... 0.997997 0.995547 0.997944 0.990132 0.995477 0.995419 0.002863 1 def return_grid_df ( grids : List [ model_selection . GridSearchCV ], ) -> Union [ pd . DataFrame , List [ model_selection . GridSearchCV ]]: \"\"\"Return a dataframe of the grids with shorted names. Args: grids (List[model_selection.GridSearchCV]): A list of GridSearchCV models that are tuned. Returns: grid_df, grids (Union[pd.DataFrame, List[model_selection.GridSearchCV]]): A dataframe of the grids with shorted names. \"\"\" def shorten_param ( param_name ): if \"__\" in param_name : return param_name . rsplit ( \"__\" , 1 )[ 1 ] return param_name grid_df = [] for grid in grids : model_name = grid . estimator [ \"model\" ] . __class__ . __name__ cv_results = pd . DataFrame ( grid . cv_results_ ) . sort_values ( \"mean_test_score\" , ascending = False ) # get the parameter names column_results = [ f \"param_ { name } \" for name in grid . param_grid . keys ()] column_results += [ \"mean_test_score\" , \"std_test_score\" , \"rank_test_score\" , ] cv_results = cv_results [ column_results ] cv_results = cv_results . rename ( shorten_param , axis = 1 ) cv_results [ \"model_name\" ] = model_name grid_df . append ( cv_results ) return grid_df , grids # grid_df and grids should necessarily be in the same sequence. # grid_df[0] == grids[0] in terms of model information, in this # case, the first index of both should be logistic regression. grid_df , grids = return_grid_df ( grids ) for model_df , grid in zip ( grid_df , grids ): best_hyperparams_df = model_df . iloc [[ 0 ]] model_name = best_hyperparams_df . model_name . unique ()[ 0 ] logger . info ( f \"Best hyperparameters found for { model_name } is as follows: \\n { grid . best_params_ } \" ) display ( best_hyperparams_df ) print () 2021-11-16,09:19:36 - Best hyperparameters found for LogisticRegression is as follows: {'model__C': 0.3593813663804626, 'model__penalty': 'l1'} 2021-11-16,09:19:36 - Best hyperparameters found for LogisticRegression is as follows: {'model__C': 0.3593813663804626, 'model__penalty': 'l1'} .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } penalty C mean_test_score std_test_score rank_test_score model_name 8 l1 0.359381 0.995419 0.002863 1 LogisticRegression 2021-11-16,09:19:36 - Best hyperparameters found for DecisionTreeClassifier is as follows: {'model__criterion': 'entropy', 'model__max_depth': 10, 'model__min_samples_leaf': 10} 2021-11-16,09:19:36 - Best hyperparameters found for DecisionTreeClassifier is as follows: {'model__criterion': 'entropy', 'model__max_depth': 10, 'model__min_samples_leaf': 10} .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } max_depth min_samples_leaf criterion mean_test_score std_test_score rank_test_score model_name 41 10 10 entropy 0.954515 0.015913 1 DecisionTreeClassifier 2021-11-16,09:19:37 - Best hyperparameters found for GradientBoostingClassifier is as follows: {'model__learning_rate': 0.1, 'model__max_depth': 3, 'model__subsample': 0.5} 2021-11-16,09:19:37 - Best hyperparameters found for GradientBoostingClassifier is as follows: {'model__learning_rate': 0.1, 'model__max_depth': 3, 'model__subsample': 0.5} .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } max_depth learning_rate subsample mean_test_score std_test_score rank_test_score model_name 1 3 0.1 0.5 0.991031 0.005869 1 GradientBoostingClassifier Success Our best performing set of hyperparameters for Logistic Regression {'model__C': 0.3593813663804626, 'model__penalty': 'l1'} gives rise to a mean cross validation score of \\(0.995419\\) , which is higher than the model with default hyperparameter scoring, \\(0.995\\) by a small margin. Not too surprising for Logistic Regression here since there aren't many things to tune, and should not see major improvements, but for Decesion Tree, it has increased from 0.907 to around 0.95, seeing quite a big jump with tuned params. DANGERRRRRRRRRRRRR I am being a bit hand wavy in terms of comparison here, I assumed THAT GridSearchCV used the exact same splitting strategy (yes it uses StratifiedKFold here) with the exact SEED/RANDOM_STATE , which I cannot promise as of now. Thus, a different splitting will, unfortunately, result in different results, although, I don't expect by a huge margin - so I think it is a no-go to compare like this. We can probably pass in a cv function into GridSearchCV to ensure seeding. This also highlights a problem that even K-fold splitting does not guarantee the reduction in variance. Room for Improvement Apart from the other methods to search for the optimal hyperparameters, we can also include preprocessing step as a tunable hyperparameter. More specifically, in our ReduceVIF() step, we hard coded two manual criterion in which the algorithm will stop; if the threshold reaches 10, or if the number of features removed hit 20; we can include them in the search space so we do not need to worry about how many features to remove!","title":"Search Space"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%207%20-%20Modelling%20%28Model%20Selection%20and%20Hyperparameter%20Tuning%29/#model-persistence-saving-models","text":"Model Persistence We save our models using joblib and we can load it back any time. Note Save it to wandb or GCP storage to store models for better consistency. model_path = \"/content/\" def save_model ( grids : List [ Callable ], path : str ): \"\"\"Save a model to a file\"\"\" for grid in grids : model_name = grid . best_estimator_ [ \"model\" ] . __class__ . __name__ path_to_save = Path ( path , f \" { model_name } _grid.joblib\" ) # Dump to local path dump ( grid , Path ( path , path_to_save )) # Dump to wandb cloud # \"model.h5\" is saved in wandb.run.dir & will be uploaded at the end of training wandb . save ( os . path . join ( wandb . run . dir , path_to_save )) Save the model!","title":"Model Persistence (Saving Models)"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%207%20-%20Modelling%20%28Model%20Selection%20and%20Hyperparameter%20Tuning%29/#wandb","text":"We first see how we save and load using wandb. save_model ( grids , model_path ) \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\") \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\") \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\") logistic_path = \"LogisticRegression_grid.joblib\" # restore the model file \"model.h5\" from a specific run by user \"lavanyashukla\" # in project \"save_and_restore\" from run \"10pr4joa\" best_model = wandb . restore ( logistic_path ) # use the \"name\" attribute of the returned object # if your framework expects a filename, e.g. as in Keras # model.load_weights(best_model.name)","title":"Wandb"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%207%20-%20Modelling%20%28Model%20Selection%20and%20Hyperparameter%20Tuning%29/#joblib","text":"We see how we use joblib to save and load. Load the model, and we can test it now if our loaded models is predicting correctly! logistic_grid = load ( \"/content/LogisticRegression_grid.joblib\" ) Great it seems to work!","title":"Joblib"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%207%20-%20Modelling%20%28Model%20Selection%20and%20Hyperparameter%20Tuning%29/#sanity-check","text":"Note We just make sure our loaded weight from path is the same as the one we trained. We can easily compare predictions (or coefficients) by the following. load ( best_model . name ) . predict ( X_train ) . all () == logistic_grid . predict ( X_train ) . all () == grids [ 0 ] . predict ( X_train ) . all () True metrics . roc_auc_score ( y_train , logistic_grid . predict_proba ( X_train )[:, 1 ] ) == metrics . roc_auc_score ( y_train , grids [ 0 ] . predict_proba ( X_train )[:, 1 ] ) == metrics . roc_auc_score ( y_train , load ( best_model . name ) . predict_proba ( X_train )[:, 1 ] ) True Seems like the save and load method works perfectly. Warning Do not call this directly. grids [ 0 ] . best_estimator_ [ \"model\" ] . predict ( X_train ) This is because grids[0].best_estimator_[\"model\"] is only referring to the Logistic Regression Model WITHOUT the pipeline (preprocessing) steps. And hence will raise error if the preprocessing steps has feature selection. But the main idea is, be careful when using the above. # grids[0].best_estimator_[\"model\"].predict(X_train)","title":"Sanity Check"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%207%20-%20Modelling%20%28Model%20Selection%20and%20Hyperparameter%20Tuning%29/#retrain-using-hyperparameters","text":"Retraining Methods From the discussion 1 , my doubts are cleared. Quoting verbatim from the discussion, we have: K-folds cross validation was devised as a way to assess model performance using training data. A great paper on this from Sebastian Raschka is a must read https://arxiv.org/abs/1811.12808. You use K-folds cv to tune you model, then retrain on all training data with best hyperparamters found. However, once you have run K-fold cv, you get \\(K\\) trained models. Kagglers quickly found that ensembling these models was giving good results at zero computation cost, rather than having to retrain a model on full data. It soon became a very common practice. Takeway For small-medium datasets, after finding the best hyperparameters \\(G\\) , we use \\(G\\) in our model \\(h\\) to train on the whole dataset \\(\\mathcal{X}\\) again to get the fitted parameters of \\(h\\) . Then you use the newly gained fitted parameters to then evaluate on the Test Set . For large and computationally expensive datasets, when you finished your K-folds, say 5 folds, you get 5 \"different\" models, \\(h_{i}, i \\in {1, 2, 3, 4, 5}\\) , what you can do is to save the weights (or in normal ML, weights refer to the parameters gained), and evaluate on the test set for each of the five models, you then get 5 different test predictions, and a common practice is the do a simple mean of these 5 set of predictions.","title":"Retrain using Hyperparameters"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%207%20-%20Modelling%20%28Model%20Selection%20and%20Hyperparameter%20Tuning%29/#retrain-on-k-folds","text":"TODO: This should be easy for me as I dabbled more in Kaggle comp and are more familiar with this methodology.","title":"Retrain on K-Folds"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%207%20-%20Modelling%20%28Model%20Selection%20and%20Hyperparameter%20Tuning%29/#retrain-on-the-whole-training-set","text":"A common practice after the hyperparameter tuning phase is to retrain the model on the whole dataset \\(X_{\\text{train}}\\) where we will get the estimator's coefficients obtained from the retraining. This is actually already done as the scikit-learn's GridSearchCV has a parameter refit ; if we select it to be true, then after the model selection process is done (i.e. getting the best hyperparameters after cross validation with grid search), the grid search object will retrain on the whole \\(X_{\\text{train}}\\) with the best hyperparameters internally, and return us back an object in which we can call predict etc. Paranoia Alert However, to be extra careful, we can retrain manually using the best hyperparameters and check if scikit-learn is true to its documentation. We will just reconstruct the pipeline using the grid's best hyper parameters. We will then test if the retrained model's coefficients coincide with the grid's best estimator's coefficients. If there difference is 0, this means they are trained under the same circumstances and we can be sure that the refit parameter is behaving true to its words. grid_best_hyperparams = grid . best_params_ print ( grid_best_hyperparams ) -> { 'model__C' : 0.3593813663804626 , 'model__penalty' : 'l1' } retrain_logistic_pipeline = pipeline . Pipeline ( [ ( \"standardize\" , preprocessing . StandardScaler ()), ( \"remove_multicollinearity\" , ReduceVIF ( thresh = 10 )), ( \"model\" , linear_model . LogisticRegression ( C = 0.3593813663804626 , max_iter = 10000 , random_state = 1992 , solver = \"saga\" , penalty = \"l1\" , ), ), ] ) _ = retrain_logistic_pipeline . fit ( X_train , y_train ) logistic_grid = grids [ 0 ] coef_diff = ( retrain_logistic_pipeline [ \"model\" ] . coef_ - logistic_grid . best_estimator_ [ \"model\" ] . coef_ ) print ( \"...\" ) assert np . all ( coef_diff == 0 ) == True logger . info ( \"Retraining Assertion Passed!\" ) 2021-11-16,09:19:38 - Retraining Assertion Passed! 2021-11-16,09:19:38 - Retraining Assertion Passed! ... https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/275883 \u21a9","title":"Retrain on the whole training set"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%208%20-%20Modelling%20%28Model%20Evaluation%20and%20Interpretation%29/","text":"Stage 8: Model Evaluation by Hongnan Gao Dependencies and Configuration %% capture ! pip install - q wandb # !pip install -q shap ! pip install - q mlxtend == 0.19.0 ! pip install - q statsmodels == 0.13.1 # !pip install gcloud == 0.18.3 import wandb wandb . login () <IPython.core.display.Javascript object> \u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc True import copy import csv import logging import os import random from dataclasses import asdict , dataclass , field from functools import wraps from pathlib import Path from time import time from typing import Any , Callable , Dict , List , Optional , Tuple , Union import matplotlib.pyplot as plt import mlxtend import numpy as np import pandas as pd import seaborn as sns from joblib import dump , load from mlxtend.evaluate import bias_variance_decomp , paired_ttest_5x2cv from scipy import stats from sklearn import ( base , decomposition , dummy , ensemble , feature_selection , linear_model , metrics , model_selection , neighbors , pipeline , preprocessing , svm , tree ) from statsmodels.regression.linear_model import OLS Utils and Configurations We need import ReduceVIF if not we cannot call our model. @dataclass class config : raw_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/raw/data.csv\" processed_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/processed.csv\" df_folds : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/df_folds.csv\" train_size : float = 0.9 seed : int = 1992 num_folds : int = 5 cv_schema : str = \"StratifiedKFold\" classification_type : str = \"binary\" target_col : List [ str ] = field ( default_factory = lambda : [ \"diagnosis\" ]) unwanted_cols : List [ str ] = field ( default_factory = lambda : [ \"id\" , \"Unnamed: 32\" ] ) # Plotting colors : List [ str ] = field ( default_factory = lambda : [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ] ) cmap_reversed = plt . cm . get_cmap ( \"mako_r\" ) def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert the config object to a dictionary. Returns: Dict: The config object as a dictionary. \"\"\" return asdict ( self ) # spot_checking_boxplot = \"../data/images/spot_checking_boxplot.png\" # oof_confusion_matrix = \"../data/images/oof_confusion_matrix.png\" # final_train_confusion_matrix = \"../data/images/final_train_confusion_matrix.png\" # precision_recall_threshold_plot = \"../data/images/precision_recall_threshold_plot.png\" # roc_plot = \"../data/images/roc_plot.png\" # feature_importance = \"../data/images/feature_importance.png\" def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) def init_logger ( log_file : str = \"info.log\" ): \"\"\" Initialize logger. \"\"\" logger = logging . getLogger ( __name__ ) logger . setLevel ( logging . INFO ) stream_handler = logging . StreamHandler () stream_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) file_handler = logging . FileHandler ( filename = log_file ) file_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) logger . addHandler ( stream_handler ) logger . addHandler ( file_handler ) return logger # Utils functions that we need def variance_inflation_factor ( exog , idx_kept , vif_idx ): \"\"\"Compute VIF for one feature. Args: exog (np.ndarray): Observations idx_kept (List[int]): Indices of features to consider vif_idx (int): Index of feature for which to compute VIF Returns: float: VIF for the selected feature \"\"\" exog = np . asarray ( exog ) x_i = exog [:, vif_idx ] mask = [ col for col in idx_kept if col != vif_idx ] x_noti = exog [:, mask ] r_squared_i = OLS ( x_i , x_noti ) . fit () . rsquared vif = 1. / ( 1. - r_squared_i ) return vif class ReduceVIF ( base . BaseEstimator , base . TransformerMixin ): \"\"\"The base of the class structure is implemented in https://www.kaggle.com/ffisegydd/sklearn-multicollinearity-class; I heavily modified the class such that it can take in numpy arrays and correctly implemented the fit and transform method. \"\"\" def __init__ ( self , thresh = 10 , max_drop = 20 ): self . thresh = thresh self . max_drop = max_drop self . column_indices_kept_ = [] self . feature_names_kept_ = None def reset ( self ): \"\"\"Resets the state of predictor columns after each fold.\"\"\" self . column_indices_kept_ = [] self . feature_names_kept_ = None def fit ( self , X , y = None ): \"\"\"Fits the Recursive VIF on the training folds and save the selected feature names in self.feature_names Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" self . column_indices_kept_ , self . feature_names_kept_ = self . calculate_vif ( X ) return self def transform ( self , X , y = None ): \"\"\"Transforms the Validation Set according to the selected feature names. Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" return X [:, self . column_indices_kept_ ] def calculate_vif ( self , X : Union [ np . ndarray , pd . DataFrame ]): \"\"\"Implements a VIF function that recursively eliminates features. Args: X (Union[np.ndarray, pd.DataFrame]): [description] Returns: [type]: [description] \"\"\" feature_names = None column_indices_kept = list ( range ( X . shape [ 1 ])) if isinstance ( X , pd . DataFrame ): feature_names = X . columns dropped = True count = 0 while dropped and count <= self . max_drop : dropped = False max_vif , max_vif_col = None , None for col in column_indices_kept : vif = variance_inflation_factor ( X , column_indices_kept , col ) if max_vif is None or vif > max_vif : max_vif = vif max_vif_col = col if max_vif > self . thresh : # print(f\"Dropping {max_vif_col} with vif={max_vif}\") column_indices_kept . remove ( max_vif_col ) if feature_names is not None : feature_names . pop ( max_vif_col ) dropped = True count += 1 return column_indices_kept , feature_names def prepare_y ( y : np . ndarray ) -> np . ndarray : \"\"\"Prepare the target variable for the model. If Binary Classification, we need to ravel the array to 1d. Args: y (np.ndarray): Target variable. Returns: np.ndarray: Transformed Target variable. \"\"\" return y . ravel () if config . classification_type == \"binary\" else y config = config () logger = init_logger () # set seeding for reproducibility _ = set_seeds ( seed = config . seed ) # read data df_folds = pd . read_csv ( config . df_folds ) # Assign predictors and target accordingly predictor_cols = df_folds . columns . to_list ()[: - 2 ] target_col = config . target_col Resume Run and Load Weights Here we resume wandb using its run_id and then load the model's weights. # Resume run by getting run_id # TODO: return id as an artifact so we can get it easily. run = wandb . init ( project = \"bcw\" , name = \"classification\" , resume = True , id = '3qh37hoo' ) \u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mreighns\u001b[0m (use `wandb login --relogin` to force relogin) Resuming run classification to Weights & Biases ( docs ). logistic_path = \"LogisticRegression_grid.joblib\" dt_path = \"DecisionTreeClassifier_grid.joblib\" gdb_path = \"GradientBoostingClassifier_grid.joblib\" logistic_best_weight = wandb . restore ( logistic_path ) logistic_best_model = load ( logistic_best_weight . name ) \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout. \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout. Interpretation of Results Interpretation of Coefficients As shown in the figure below, all else being equal, for every square unit increase in mean cell area, the odds of the tumor being malignant increases by a factor of \\(e^{1.43} = 4.19\\) . The variation (standard error) of the characteristics of cells also are deemed important by the model, for example, area se played an important role in determining whether a cell is malignant; intuitively, if some cells are noticably larger than the rest, then it is also a good indicator of malignancy. selected_features_by_vif_index = logistic_best_model . best_estimator_ [ 'remove_multicollinearity' ] . column_indices_kept_ selected_feature_names = np . asarray ( predictor_cols )[ selected_features_by_vif_index ] selected_features_coefficients = logistic_best_model . best_estimator_ [ 'model' ] . coef_ . flatten () # assertion #assert grid.best_estimator_['remove_multicollinearity'].feature_names_ == retrain_pipeline['remove_multicollinearity'].feature_names_ fig , ax = plt . subplots ( figsize = ( 15 , 15 )) # .abs() _ = pd . Series ( selected_features_coefficients , index = selected_feature_names ) . sort_values () . plot ( ax = ax , kind = 'barh' ) # fig.savefig(config.feature_importance, format=\"png\", dpi=300) Interpretation of Metric Scores on Train Set We are also interested in choosing an optimal threshold for the model such that it gives the lowest recall, or False Negatives. We note that the default threshold when calling predict() from a model is \\(0.5\\) . In this section, we will explore one way to get the best tradeoff we can when choosing a high recall, while maintaining a reasonable score for precision. def evaluate_train_test_set ( estimator : Callable , X : Union [ pd . DataFrame , np . ndarray ], y : Union [ pd . DataFrame , np . ndarray ] ) -> Dict [ str , Union [ float , np . ndarray ]]: \"\"\"This function takes in X and y and returns a dictionary of scores. Args: estimator (Callable): [description] X (Union[pd.DataFrame, np.ndarray]): [description] y (Union[pd.DataFrame, np.ndarray]): [description] Returns: Dict[str, Union[float, np.ndarray]]: [description] \"\"\" test_results = {} y_pred = estimator . predict ( X ) # This is the probability array of class 1 (malignant) y_prob = estimator . predict_proba ( X )[:, 1 ] test_brier = metrics . brier_score_loss ( y , y_prob ) test_roc = metrics . roc_auc_score ( y , y_prob ) test_results [ \"brier\" ] = test_brier test_results [ \"roc\" ] = test_roc test_results [ \"y\" ] = np . asarray ( y ) . flatten () test_results [ \"y_pred\" ] = y_pred . flatten () test_results [ \"y_prob\" ] = y_prob . flatten () return test_results def plot_precision_recall_vs_threshold ( precisions , recalls , thresholds ): \"\"\" Modified from: Hands-On Machine learning with Scikit-Learn and TensorFlow; p.89 and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 \"\"\" plt . figure ( figsize = ( 8 , 8 )) plt . title ( \"Precision and Recall Scores as a function of the decision threshold\" ) plt . plot ( thresholds , precisions [: - 1 ], \"b--\" , label = \"Precision\" ) plt . plot ( thresholds , recalls [: - 1 ], \"g-\" , label = \"Recall\" ) plt . ylabel ( \"Score\" ) plt . xlabel ( \"Decision Threshold\" ) plt . legend ( loc = 'best' ) plt . savefig ( config . precision_recall_threshold_plot , format = \"png\" , dpi = 300 ) def plot_roc_curve ( fpr , tpr , label = None ): \"\"\" The ROC curve, modified from Hands-On Machine learning with Scikit-Learn and TensorFlow; p.91 and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 \"\"\" plt . figure ( figsize = ( 8 , 8 )) plt . title ( 'ROC Curve' ) plt . plot ( fpr , tpr , linewidth = 2 , label = label ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], 'k--' ) plt . axis ([ - 0.005 , 1 , 0 , 1.005 ]) plt . xticks ( np . arange ( 0 , 1 , 0.05 ), rotation = 90 ) plt . xlabel ( \"False Positive Rate\" ) plt . ylabel ( \"True Positive Rate (Recall)\" ) plt . legend ( loc = 'best' ) plt . savefig ( config . roc_plot , format = \"png\" , dpi = 300 ) def adjusted_classes ( y_scores , t ): \"\"\" This function adjusts class predictions based on the prediction threshold (t). Will only work for binary classification problems. and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 \"\"\" return [ 1 if y >= t else 0 for y in y_scores ] The plots below show the tradeoffs between precision and recall, recall and false positive rate. The confusion matrix on the train set tells us that there is still more false negatives than false positives. We can choose a particular threshold in order to minimize false negatives, at some expense of false positive. X_train , y_train = df_folds [ predictor_cols ] . values , df_folds [ target_col ] . values y_train = prepare_y ( y_train ) train_results = evaluate_train_test_set ( logistic_best_model , X_train , y_train ) def plot_confusion_matrix ( y_true : np . ndarray , y_pred : np . ndarray , title : str , labels : List [ str ], tick_labels : List [ str ], ) -> None : \"\"\"Plots a Binary Confusion Matrix. Args: y_true (np.ndarray): the actual labels. y_pred (np.ndarray): the predicted labels. title (str): the title of the plot. tick_labels (List[str]): The labels for the ticks. \"\"\" # Unravel into tn, fp, fn and tp tn , fp , fn , tp = metrics . confusion_matrix ( y_true , y_pred , labels = labels ) . ravel () # reshape into tp, fp, fn, tn - this is personal preference reshaped_cm = np . asarray ([[ tp , fp ], [ fn , tn ]]) # flatten this 2d array cm_flattened = reshaped_cm . flatten () labels = [ \"True Positive\" , \"False Positive\" , \"False Negative\" , \"True Negative\" , ] annot = ( np . asarray ( [ f \" { label } \\n { cm_count } \" for label , cm_count in zip ( labels , cm_flattened ) ] ) ) . reshape ( 2 , 2 ) ax = plt . subplot () heatmap = sns . heatmap ( reshaped_cm , annot = annot , fmt = \"\" , cmap = \"Greens\" , ax = ax , xticklabels = tick_labels , yticklabels = tick_labels , ) ax . set_title ( title ) ax . set_xlabel ( \"Predicted labels\" ) ax . set_ylabel ( \"True labels\" ) plt . show () # CM y_true , y_pred = train_results [ 'y' ], train_results [ 'y_pred' ] plot_confusion_matrix ( y_true , y_pred , title = \"Confusion Matrix (Malignant as +)\" , labels = [ 0 , 1 ], tick_labels = [ \"benign\" , \"malignant\" ], ) # fig, ax = plt.subplots(figsize=(10, 10)) # # CM # cm_train = metrics.confusion_matrix(train_results['y'], train_results['y_pred']) # #### scores # auc = metrics.roc_auc_score(train_results['y'], train_results['y_prob']) # #### annotations # labels = [\"True Neg\", \"False Pos\", \"False Neg\", \"True Pos\"] # counts = [\"{0:0.0f}\".format(value) for value in cm_train.flatten()] # percentages = [\"{0:.2%}\".format(value) for value in cm_train.flatten() / np.sum(cm_train)] # #### final annotations # label = ( # np.array([f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(labels, counts, percentages)]) # ).reshape(2, 2) # # heatmap # sns.heatmap( # data=cm_train, # vmin=0, # vmax=330, # cmap=[\"#fe4a49\", \"#2ab7ca\", \"#fed766\", \"#59981A\"], # linewidth=2, # linecolor=\"white\", # square=True, # ax=ax, # annot=label, # fmt=\"\", # cbar=False, # annot_kws={\"size\": 10, \"color\": \"black\", \"weight\": \"bold\", \"alpha\": 0.8}, # alpha=1, # ) # ax.scatter(1, 1, s=3500, c=\"white\") # ax.text( # 0.72, # 1.0, # \"AUC: {}\".format(round(auc, 3)), # {\"size\": 10, \"color\": \"black\", \"weight\": \"bold\"}, # ) # ## ticks and labels # ax.set_xticklabels(\"\") # ax.set_yticklabels(\"\") # ## titles and text # fig.text(0, 1.05, \"Train Set Confusion Matrix\", {\"size\": 22, \"weight\": \"bold\"}, alpha=1) # fig.text( # 0, # 1, # \"\"\"Training Set Confusion Matrix.\"\"\", # {\"size\": 12, \"weight\": \"normal\"}, # alpha=0.98, # ) # fig.tight_layout(pad=2.5, w_pad=2.5, h_pad=2.5) # # fig.savefig(config.final_train_confusion_matrix, format='png', dpi=300) --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-26-3f454a3182d6> in <module>() 61 62 fig.tight_layout(pad=2.5, w_pad=2.5, h_pad=2.5) ---> 63 fig.savefig(config.final_train_confusion_matrix, format='png', dpi=300) AttributeError: 'config' object has no attribute 'final_train_confusion_matrix' # generate the precision recall curve precision , recall , pr_thresholds = metrics . precision_recall_curve ( train_results [ 'y' ], train_results [ 'y_prob' ]) fpr , tpr , roc_thresholds = metrics . roc_curve ( train_results [ 'y' ], train_results [ 'y_prob' ], pos_label = 1 ) # use the same p, r, thresholds that were previously calculated plot_precision_recall_vs_threshold ( precision , recall , pr_thresholds ) --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-28-1a8ed36985bb> in <module>() 1 # use the same p, r, thresholds that were previously calculated ----> 2 plot_precision_recall_vs_threshold(precision, recall, pr_thresholds) <ipython-input-18-2dad3ff8a911> in plot_precision_recall_vs_threshold(precisions, recalls, thresholds) 12 plt.xlabel(\"Decision Threshold\") 13 plt.legend(loc='best') ---> 14 plt.savefig(config.precision_recall_threshold_plot, format=\"png\", dpi=300) 15 16 def plot_roc_curve(fpr, tpr, label=None): AttributeError: 'config' object has no attribute 'precision_recall_threshold_plot' Based on the tradeoff plot above, a good threshold can be set at \\(t = 0.35\\) , let us see how it performs with this threshold. y_pred_adj = adjusted_classes ( train_results [ \"y_prob\" ], t = 0.35 ) print ( pd . DataFrame ( metrics . confusion_matrix ( train_results [ \"y\" ], y_pred_adj ), columns = [ \"pred_neg\" , \"pred_pos\" ], index = [ \"neg\" , \"pos\" ], ) ) pred_neg pred_pos neg 313 8 pos 5 186 print ( metrics . classification_report ( y_true = train_results [ \"y\" ], y_pred = y_pred_adj )) train_brier = train_results [ 'brier' ] print ( f \"train brier: { train_brier } \" ) precision recall f1-score support 0 0.98 0.98 0.98 321 1 0.96 0.97 0.97 191 accuracy 0.97 512 macro avg 0.97 0.97 0.97 512 weighted avg 0.97 0.97 0.97 512 train brier: 0.022402196649862854 The False Negatives reduced from 15 to 9, at the expense of increase False Positives from 6 to 14. We should take comfort that less patients are falsely classified as benign when in fact they need treatment. This is a tradeoff that we have to choose. The ROC curve below also paints a similar story, in order for you to have high recall, one must accept that there will more False Positives. plot_roc_curve ( fpr , tpr , 'recall_optimized' ) --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-31-893ec679a4c4> in <module>() ----> 1 plot_roc_curve(fpr, tpr, 'recall_optimized') <ipython-input-18-2dad3ff8a911> in plot_roc_curve(fpr, tpr, label) 29 plt.ylabel(\"True Positive Rate (Recall)\") 30 plt.legend(loc='best') ---> 31 plt.savefig(config.roc_plot, format=\"png\", dpi=300) 32 33 def adjusted_classes(y_scores, t): AttributeError: 'config' object has no attribute 'roc_plot' Evaluation on Test Set Ultimately, we are interested in finding the estimate of the generalization error of the model, and thus it is time for us to evaluate our model's performance on the \"unseen\" test set \\(X_{\\text{test}}\\) to get a good gauge on how well the model generalizes to unseen data. Take note that now everything has settled, we will use the exact hyperparameters to predict on test set, with the pre-defined threshold of 0.35. test_results = evaluate_train_test_set ( grid , X_test , y_test ) y_test_pred_adj = adjusted_classes ( test_results [ 'y_prob' ], t = 0.35 ) print ( pd . DataFrame ( metrics . confusion_matrix ( test_results [ 'y' ], y_test_pred_adj ), columns = [ 'pred_neg' , 'pred_pos' ], index = [ 'neg' , 'pos' ])) --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-32-f47e1573a96d> in <module>() ----> 1 test_results = evaluate_train_test_set(grid, X_test, y_test) 2 y_test_pred_adj = adjusted_classes(test_results['y_prob'], t=0.35) 3 4 print(pd.DataFrame(metrics.confusion_matrix(test_results['y'], y_test_pred_adj), 5 columns=['pred_neg', 'pred_pos'], NameError: name 'grid' is not defined test_roc = test_results [ 'roc' ] test_brier = test_results [ 'brier' ] print ( test_roc ) print ( test_brier ) print ( metrics . classification_report ( y_true = test_results [ \"y\" ], y_pred = y_test_pred_adj )) --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-33-0cc7fe660860> in <module>() ----> 1 test_roc = test_results['roc'] 2 test_brier = test_results['brier'] 3 print(test_roc) 4 print(test_brier) 5 print(metrics.classification_report(y_true=test_results[\"y\"], y_pred=y_test_pred_adj)) NameError: name 'test_results' is not defined Using the same threshold we used on training set, we see that the False Negative is quite low. The overall ROC score is 0.9828, and the corresponding Brier Score is 0.04136, both seem reasonably well performing. Benefit Structure Refer to health insurance project! https://ghnreigns.github.io/reighns-ml-website/metrics/classification_metrics/confusion_matrix/#benefit-structure # threshold_list : List[float] = [0.01, 0.1, 0.2, 0.5] # benefit_dict : Dict[str, int] = {\"old_structure\": {\"tp\":10, \"fn\":-10, \"fp\": -2, \"tp+fp\":-1}, # \"new_structure\": {\"tp\":100, \"fn\": -100, \"fp\": -2, \"tn+fp\":-1}} # columns = [\"threshold\", \"tp\", \"fn\", \"fp\", \"tn\", \"benefit_cost_old\", \"benefit_cost_new\"] # benefit_cost_list = [] # for t in threshold_list: # y_pred_adj = adjusted_classes(y_test_dt_prob, t=t) # cm = metrics.confusion_matrix(y_true=y_test_gt, y_pred = y_pred_adj) # tn, fp, fn, tp = metrics.confusion_matrix(y_true=y_test_gt, y_pred = y_pred_adj).ravel() # # this one check if it is correct formula # benefit_cost_old = tp*10 - fn*10 - fp*2 - (tp+fp)*1 # benefit_cost_new = tp*100 - fn*100 - fp*2 - (tp+fp)*1 # benefit_cost_list.append([t, tn, fn, fp, tn, benefit_cost_old, benefit_cost_new]) benefit_df = pd . DataFrame ( benefit_cost_list , columns = columns ) benefit_df Bias-Variance Tradeoff avg_expected_loss , avg_bias , avg_var = bias_variance_decomp ( grid . best_estimator_ [ 'model' ], X_train . values , y_train . values , X_test . values , y_test . values , loss = '0-1_loss' , random_seed = 123 ) print ( 'Average expected loss: %.3f ' % avg_expected_loss ) print ( 'Average bias: %.3f ' % avg_bias ) print ( 'Average variance: %.3f ' % avg_var ) We use the mlxtend library to estimate the Bias-Variance Tradeoff in our Logistic Regression model. The core idea behind this function is to use bagging and repeatedly sample from our training set so as to simulate that we are actually drawing samples from the \"true\" population over a distribution \\(\\mathcal{P}\\) . As expected, Logistic Regression being a linear model, its simplicity contributes to its high bias and low variance.","title":"Modelling (Model Evaluation)"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%208%20-%20Modelling%20%28Model%20Evaluation%20and%20Interpretation%29/#dependencies-and-configuration","text":"%% capture ! pip install - q wandb # !pip install -q shap ! pip install - q mlxtend == 0.19.0 ! pip install - q statsmodels == 0.13.1 # !pip install gcloud == 0.18.3 import wandb wandb . login () <IPython.core.display.Javascript object> \u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc True import copy import csv import logging import os import random from dataclasses import asdict , dataclass , field from functools import wraps from pathlib import Path from time import time from typing import Any , Callable , Dict , List , Optional , Tuple , Union import matplotlib.pyplot as plt import mlxtend import numpy as np import pandas as pd import seaborn as sns from joblib import dump , load from mlxtend.evaluate import bias_variance_decomp , paired_ttest_5x2cv from scipy import stats from sklearn import ( base , decomposition , dummy , ensemble , feature_selection , linear_model , metrics , model_selection , neighbors , pipeline , preprocessing , svm , tree ) from statsmodels.regression.linear_model import OLS","title":"Dependencies and Configuration"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%208%20-%20Modelling%20%28Model%20Evaluation%20and%20Interpretation%29/#utils-and-configurations","text":"We need import ReduceVIF if not we cannot call our model. @dataclass class config : raw_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/raw/data.csv\" processed_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/processed.csv\" df_folds : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/df_folds.csv\" train_size : float = 0.9 seed : int = 1992 num_folds : int = 5 cv_schema : str = \"StratifiedKFold\" classification_type : str = \"binary\" target_col : List [ str ] = field ( default_factory = lambda : [ \"diagnosis\" ]) unwanted_cols : List [ str ] = field ( default_factory = lambda : [ \"id\" , \"Unnamed: 32\" ] ) # Plotting colors : List [ str ] = field ( default_factory = lambda : [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ] ) cmap_reversed = plt . cm . get_cmap ( \"mako_r\" ) def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert the config object to a dictionary. Returns: Dict: The config object as a dictionary. \"\"\" return asdict ( self ) # spot_checking_boxplot = \"../data/images/spot_checking_boxplot.png\" # oof_confusion_matrix = \"../data/images/oof_confusion_matrix.png\" # final_train_confusion_matrix = \"../data/images/final_train_confusion_matrix.png\" # precision_recall_threshold_plot = \"../data/images/precision_recall_threshold_plot.png\" # roc_plot = \"../data/images/roc_plot.png\" # feature_importance = \"../data/images/feature_importance.png\" def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) def init_logger ( log_file : str = \"info.log\" ): \"\"\" Initialize logger. \"\"\" logger = logging . getLogger ( __name__ ) logger . setLevel ( logging . INFO ) stream_handler = logging . StreamHandler () stream_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) file_handler = logging . FileHandler ( filename = log_file ) file_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) logger . addHandler ( stream_handler ) logger . addHandler ( file_handler ) return logger # Utils functions that we need def variance_inflation_factor ( exog , idx_kept , vif_idx ): \"\"\"Compute VIF for one feature. Args: exog (np.ndarray): Observations idx_kept (List[int]): Indices of features to consider vif_idx (int): Index of feature for which to compute VIF Returns: float: VIF for the selected feature \"\"\" exog = np . asarray ( exog ) x_i = exog [:, vif_idx ] mask = [ col for col in idx_kept if col != vif_idx ] x_noti = exog [:, mask ] r_squared_i = OLS ( x_i , x_noti ) . fit () . rsquared vif = 1. / ( 1. - r_squared_i ) return vif class ReduceVIF ( base . BaseEstimator , base . TransformerMixin ): \"\"\"The base of the class structure is implemented in https://www.kaggle.com/ffisegydd/sklearn-multicollinearity-class; I heavily modified the class such that it can take in numpy arrays and correctly implemented the fit and transform method. \"\"\" def __init__ ( self , thresh = 10 , max_drop = 20 ): self . thresh = thresh self . max_drop = max_drop self . column_indices_kept_ = [] self . feature_names_kept_ = None def reset ( self ): \"\"\"Resets the state of predictor columns after each fold.\"\"\" self . column_indices_kept_ = [] self . feature_names_kept_ = None def fit ( self , X , y = None ): \"\"\"Fits the Recursive VIF on the training folds and save the selected feature names in self.feature_names Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" self . column_indices_kept_ , self . feature_names_kept_ = self . calculate_vif ( X ) return self def transform ( self , X , y = None ): \"\"\"Transforms the Validation Set according to the selected feature names. Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" return X [:, self . column_indices_kept_ ] def calculate_vif ( self , X : Union [ np . ndarray , pd . DataFrame ]): \"\"\"Implements a VIF function that recursively eliminates features. Args: X (Union[np.ndarray, pd.DataFrame]): [description] Returns: [type]: [description] \"\"\" feature_names = None column_indices_kept = list ( range ( X . shape [ 1 ])) if isinstance ( X , pd . DataFrame ): feature_names = X . columns dropped = True count = 0 while dropped and count <= self . max_drop : dropped = False max_vif , max_vif_col = None , None for col in column_indices_kept : vif = variance_inflation_factor ( X , column_indices_kept , col ) if max_vif is None or vif > max_vif : max_vif = vif max_vif_col = col if max_vif > self . thresh : # print(f\"Dropping {max_vif_col} with vif={max_vif}\") column_indices_kept . remove ( max_vif_col ) if feature_names is not None : feature_names . pop ( max_vif_col ) dropped = True count += 1 return column_indices_kept , feature_names def prepare_y ( y : np . ndarray ) -> np . ndarray : \"\"\"Prepare the target variable for the model. If Binary Classification, we need to ravel the array to 1d. Args: y (np.ndarray): Target variable. Returns: np.ndarray: Transformed Target variable. \"\"\" return y . ravel () if config . classification_type == \"binary\" else y config = config () logger = init_logger () # set seeding for reproducibility _ = set_seeds ( seed = config . seed ) # read data df_folds = pd . read_csv ( config . df_folds ) # Assign predictors and target accordingly predictor_cols = df_folds . columns . to_list ()[: - 2 ] target_col = config . target_col","title":"Utils and Configurations"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%208%20-%20Modelling%20%28Model%20Evaluation%20and%20Interpretation%29/#resume-run-and-load-weights","text":"Here we resume wandb using its run_id and then load the model's weights. # Resume run by getting run_id # TODO: return id as an artifact so we can get it easily. run = wandb . init ( project = \"bcw\" , name = \"classification\" , resume = True , id = '3qh37hoo' ) \u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mreighns\u001b[0m (use `wandb login --relogin` to force relogin) Resuming run classification to Weights & Biases ( docs ). logistic_path = \"LogisticRegression_grid.joblib\" dt_path = \"DecisionTreeClassifier_grid.joblib\" gdb_path = \"GradientBoostingClassifier_grid.joblib\" logistic_best_weight = wandb . restore ( logistic_path ) logistic_best_model = load ( logistic_best_weight . name ) \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout. \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout.","title":"Resume Run and Load Weights"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%208%20-%20Modelling%20%28Model%20Evaluation%20and%20Interpretation%29/#interpretation-of-results","text":"","title":"Interpretation of Results"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%208%20-%20Modelling%20%28Model%20Evaluation%20and%20Interpretation%29/#interpretation-of-coefficients","text":"As shown in the figure below, all else being equal, for every square unit increase in mean cell area, the odds of the tumor being malignant increases by a factor of \\(e^{1.43} = 4.19\\) . The variation (standard error) of the characteristics of cells also are deemed important by the model, for example, area se played an important role in determining whether a cell is malignant; intuitively, if some cells are noticably larger than the rest, then it is also a good indicator of malignancy. selected_features_by_vif_index = logistic_best_model . best_estimator_ [ 'remove_multicollinearity' ] . column_indices_kept_ selected_feature_names = np . asarray ( predictor_cols )[ selected_features_by_vif_index ] selected_features_coefficients = logistic_best_model . best_estimator_ [ 'model' ] . coef_ . flatten () # assertion #assert grid.best_estimator_['remove_multicollinearity'].feature_names_ == retrain_pipeline['remove_multicollinearity'].feature_names_ fig , ax = plt . subplots ( figsize = ( 15 , 15 )) # .abs() _ = pd . Series ( selected_features_coefficients , index = selected_feature_names ) . sort_values () . plot ( ax = ax , kind = 'barh' ) # fig.savefig(config.feature_importance, format=\"png\", dpi=300)","title":"Interpretation of Coefficients"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%208%20-%20Modelling%20%28Model%20Evaluation%20and%20Interpretation%29/#interpretation-of-metric-scores-on-train-set","text":"We are also interested in choosing an optimal threshold for the model such that it gives the lowest recall, or False Negatives. We note that the default threshold when calling predict() from a model is \\(0.5\\) . In this section, we will explore one way to get the best tradeoff we can when choosing a high recall, while maintaining a reasonable score for precision. def evaluate_train_test_set ( estimator : Callable , X : Union [ pd . DataFrame , np . ndarray ], y : Union [ pd . DataFrame , np . ndarray ] ) -> Dict [ str , Union [ float , np . ndarray ]]: \"\"\"This function takes in X and y and returns a dictionary of scores. Args: estimator (Callable): [description] X (Union[pd.DataFrame, np.ndarray]): [description] y (Union[pd.DataFrame, np.ndarray]): [description] Returns: Dict[str, Union[float, np.ndarray]]: [description] \"\"\" test_results = {} y_pred = estimator . predict ( X ) # This is the probability array of class 1 (malignant) y_prob = estimator . predict_proba ( X )[:, 1 ] test_brier = metrics . brier_score_loss ( y , y_prob ) test_roc = metrics . roc_auc_score ( y , y_prob ) test_results [ \"brier\" ] = test_brier test_results [ \"roc\" ] = test_roc test_results [ \"y\" ] = np . asarray ( y ) . flatten () test_results [ \"y_pred\" ] = y_pred . flatten () test_results [ \"y_prob\" ] = y_prob . flatten () return test_results def plot_precision_recall_vs_threshold ( precisions , recalls , thresholds ): \"\"\" Modified from: Hands-On Machine learning with Scikit-Learn and TensorFlow; p.89 and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 \"\"\" plt . figure ( figsize = ( 8 , 8 )) plt . title ( \"Precision and Recall Scores as a function of the decision threshold\" ) plt . plot ( thresholds , precisions [: - 1 ], \"b--\" , label = \"Precision\" ) plt . plot ( thresholds , recalls [: - 1 ], \"g-\" , label = \"Recall\" ) plt . ylabel ( \"Score\" ) plt . xlabel ( \"Decision Threshold\" ) plt . legend ( loc = 'best' ) plt . savefig ( config . precision_recall_threshold_plot , format = \"png\" , dpi = 300 ) def plot_roc_curve ( fpr , tpr , label = None ): \"\"\" The ROC curve, modified from Hands-On Machine learning with Scikit-Learn and TensorFlow; p.91 and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 \"\"\" plt . figure ( figsize = ( 8 , 8 )) plt . title ( 'ROC Curve' ) plt . plot ( fpr , tpr , linewidth = 2 , label = label ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], 'k--' ) plt . axis ([ - 0.005 , 1 , 0 , 1.005 ]) plt . xticks ( np . arange ( 0 , 1 , 0.05 ), rotation = 90 ) plt . xlabel ( \"False Positive Rate\" ) plt . ylabel ( \"True Positive Rate (Recall)\" ) plt . legend ( loc = 'best' ) plt . savefig ( config . roc_plot , format = \"png\" , dpi = 300 ) def adjusted_classes ( y_scores , t ): \"\"\" This function adjusts class predictions based on the prediction threshold (t). Will only work for binary classification problems. and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 \"\"\" return [ 1 if y >= t else 0 for y in y_scores ] The plots below show the tradeoffs between precision and recall, recall and false positive rate. The confusion matrix on the train set tells us that there is still more false negatives than false positives. We can choose a particular threshold in order to minimize false negatives, at some expense of false positive. X_train , y_train = df_folds [ predictor_cols ] . values , df_folds [ target_col ] . values y_train = prepare_y ( y_train ) train_results = evaluate_train_test_set ( logistic_best_model , X_train , y_train ) def plot_confusion_matrix ( y_true : np . ndarray , y_pred : np . ndarray , title : str , labels : List [ str ], tick_labels : List [ str ], ) -> None : \"\"\"Plots a Binary Confusion Matrix. Args: y_true (np.ndarray): the actual labels. y_pred (np.ndarray): the predicted labels. title (str): the title of the plot. tick_labels (List[str]): The labels for the ticks. \"\"\" # Unravel into tn, fp, fn and tp tn , fp , fn , tp = metrics . confusion_matrix ( y_true , y_pred , labels = labels ) . ravel () # reshape into tp, fp, fn, tn - this is personal preference reshaped_cm = np . asarray ([[ tp , fp ], [ fn , tn ]]) # flatten this 2d array cm_flattened = reshaped_cm . flatten () labels = [ \"True Positive\" , \"False Positive\" , \"False Negative\" , \"True Negative\" , ] annot = ( np . asarray ( [ f \" { label } \\n { cm_count } \" for label , cm_count in zip ( labels , cm_flattened ) ] ) ) . reshape ( 2 , 2 ) ax = plt . subplot () heatmap = sns . heatmap ( reshaped_cm , annot = annot , fmt = \"\" , cmap = \"Greens\" , ax = ax , xticklabels = tick_labels , yticklabels = tick_labels , ) ax . set_title ( title ) ax . set_xlabel ( \"Predicted labels\" ) ax . set_ylabel ( \"True labels\" ) plt . show () # CM y_true , y_pred = train_results [ 'y' ], train_results [ 'y_pred' ] plot_confusion_matrix ( y_true , y_pred , title = \"Confusion Matrix (Malignant as +)\" , labels = [ 0 , 1 ], tick_labels = [ \"benign\" , \"malignant\" ], ) # fig, ax = plt.subplots(figsize=(10, 10)) # # CM # cm_train = metrics.confusion_matrix(train_results['y'], train_results['y_pred']) # #### scores # auc = metrics.roc_auc_score(train_results['y'], train_results['y_prob']) # #### annotations # labels = [\"True Neg\", \"False Pos\", \"False Neg\", \"True Pos\"] # counts = [\"{0:0.0f}\".format(value) for value in cm_train.flatten()] # percentages = [\"{0:.2%}\".format(value) for value in cm_train.flatten() / np.sum(cm_train)] # #### final annotations # label = ( # np.array([f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(labels, counts, percentages)]) # ).reshape(2, 2) # # heatmap # sns.heatmap( # data=cm_train, # vmin=0, # vmax=330, # cmap=[\"#fe4a49\", \"#2ab7ca\", \"#fed766\", \"#59981A\"], # linewidth=2, # linecolor=\"white\", # square=True, # ax=ax, # annot=label, # fmt=\"\", # cbar=False, # annot_kws={\"size\": 10, \"color\": \"black\", \"weight\": \"bold\", \"alpha\": 0.8}, # alpha=1, # ) # ax.scatter(1, 1, s=3500, c=\"white\") # ax.text( # 0.72, # 1.0, # \"AUC: {}\".format(round(auc, 3)), # {\"size\": 10, \"color\": \"black\", \"weight\": \"bold\"}, # ) # ## ticks and labels # ax.set_xticklabels(\"\") # ax.set_yticklabels(\"\") # ## titles and text # fig.text(0, 1.05, \"Train Set Confusion Matrix\", {\"size\": 22, \"weight\": \"bold\"}, alpha=1) # fig.text( # 0, # 1, # \"\"\"Training Set Confusion Matrix.\"\"\", # {\"size\": 12, \"weight\": \"normal\"}, # alpha=0.98, # ) # fig.tight_layout(pad=2.5, w_pad=2.5, h_pad=2.5) # # fig.savefig(config.final_train_confusion_matrix, format='png', dpi=300) --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-26-3f454a3182d6> in <module>() 61 62 fig.tight_layout(pad=2.5, w_pad=2.5, h_pad=2.5) ---> 63 fig.savefig(config.final_train_confusion_matrix, format='png', dpi=300) AttributeError: 'config' object has no attribute 'final_train_confusion_matrix' # generate the precision recall curve precision , recall , pr_thresholds = metrics . precision_recall_curve ( train_results [ 'y' ], train_results [ 'y_prob' ]) fpr , tpr , roc_thresholds = metrics . roc_curve ( train_results [ 'y' ], train_results [ 'y_prob' ], pos_label = 1 ) # use the same p, r, thresholds that were previously calculated plot_precision_recall_vs_threshold ( precision , recall , pr_thresholds ) --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-28-1a8ed36985bb> in <module>() 1 # use the same p, r, thresholds that were previously calculated ----> 2 plot_precision_recall_vs_threshold(precision, recall, pr_thresholds) <ipython-input-18-2dad3ff8a911> in plot_precision_recall_vs_threshold(precisions, recalls, thresholds) 12 plt.xlabel(\"Decision Threshold\") 13 plt.legend(loc='best') ---> 14 plt.savefig(config.precision_recall_threshold_plot, format=\"png\", dpi=300) 15 16 def plot_roc_curve(fpr, tpr, label=None): AttributeError: 'config' object has no attribute 'precision_recall_threshold_plot' Based on the tradeoff plot above, a good threshold can be set at \\(t = 0.35\\) , let us see how it performs with this threshold. y_pred_adj = adjusted_classes ( train_results [ \"y_prob\" ], t = 0.35 ) print ( pd . DataFrame ( metrics . confusion_matrix ( train_results [ \"y\" ], y_pred_adj ), columns = [ \"pred_neg\" , \"pred_pos\" ], index = [ \"neg\" , \"pos\" ], ) ) pred_neg pred_pos neg 313 8 pos 5 186 print ( metrics . classification_report ( y_true = train_results [ \"y\" ], y_pred = y_pred_adj )) train_brier = train_results [ 'brier' ] print ( f \"train brier: { train_brier } \" ) precision recall f1-score support 0 0.98 0.98 0.98 321 1 0.96 0.97 0.97 191 accuracy 0.97 512 macro avg 0.97 0.97 0.97 512 weighted avg 0.97 0.97 0.97 512 train brier: 0.022402196649862854 The False Negatives reduced from 15 to 9, at the expense of increase False Positives from 6 to 14. We should take comfort that less patients are falsely classified as benign when in fact they need treatment. This is a tradeoff that we have to choose. The ROC curve below also paints a similar story, in order for you to have high recall, one must accept that there will more False Positives. plot_roc_curve ( fpr , tpr , 'recall_optimized' ) --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-31-893ec679a4c4> in <module>() ----> 1 plot_roc_curve(fpr, tpr, 'recall_optimized') <ipython-input-18-2dad3ff8a911> in plot_roc_curve(fpr, tpr, label) 29 plt.ylabel(\"True Positive Rate (Recall)\") 30 plt.legend(loc='best') ---> 31 plt.savefig(config.roc_plot, format=\"png\", dpi=300) 32 33 def adjusted_classes(y_scores, t): AttributeError: 'config' object has no attribute 'roc_plot'","title":"Interpretation of Metric Scores on Train Set"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%208%20-%20Modelling%20%28Model%20Evaluation%20and%20Interpretation%29/#evaluation-on-test-set","text":"Ultimately, we are interested in finding the estimate of the generalization error of the model, and thus it is time for us to evaluate our model's performance on the \"unseen\" test set \\(X_{\\text{test}}\\) to get a good gauge on how well the model generalizes to unseen data. Take note that now everything has settled, we will use the exact hyperparameters to predict on test set, with the pre-defined threshold of 0.35. test_results = evaluate_train_test_set ( grid , X_test , y_test ) y_test_pred_adj = adjusted_classes ( test_results [ 'y_prob' ], t = 0.35 ) print ( pd . DataFrame ( metrics . confusion_matrix ( test_results [ 'y' ], y_test_pred_adj ), columns = [ 'pred_neg' , 'pred_pos' ], index = [ 'neg' , 'pos' ])) --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-32-f47e1573a96d> in <module>() ----> 1 test_results = evaluate_train_test_set(grid, X_test, y_test) 2 y_test_pred_adj = adjusted_classes(test_results['y_prob'], t=0.35) 3 4 print(pd.DataFrame(metrics.confusion_matrix(test_results['y'], y_test_pred_adj), 5 columns=['pred_neg', 'pred_pos'], NameError: name 'grid' is not defined test_roc = test_results [ 'roc' ] test_brier = test_results [ 'brier' ] print ( test_roc ) print ( test_brier ) print ( metrics . classification_report ( y_true = test_results [ \"y\" ], y_pred = y_test_pred_adj )) --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-33-0cc7fe660860> in <module>() ----> 1 test_roc = test_results['roc'] 2 test_brier = test_results['brier'] 3 print(test_roc) 4 print(test_brier) 5 print(metrics.classification_report(y_true=test_results[\"y\"], y_pred=y_test_pred_adj)) NameError: name 'test_results' is not defined Using the same threshold we used on training set, we see that the False Negative is quite low. The overall ROC score is 0.9828, and the corresponding Brier Score is 0.04136, both seem reasonably well performing.","title":"Evaluation on Test Set"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%208%20-%20Modelling%20%28Model%20Evaluation%20and%20Interpretation%29/#benefit-structure","text":"Refer to health insurance project! https://ghnreigns.github.io/reighns-ml-website/metrics/classification_metrics/confusion_matrix/#benefit-structure # threshold_list : List[float] = [0.01, 0.1, 0.2, 0.5] # benefit_dict : Dict[str, int] = {\"old_structure\": {\"tp\":10, \"fn\":-10, \"fp\": -2, \"tp+fp\":-1}, # \"new_structure\": {\"tp\":100, \"fn\": -100, \"fp\": -2, \"tn+fp\":-1}} # columns = [\"threshold\", \"tp\", \"fn\", \"fp\", \"tn\", \"benefit_cost_old\", \"benefit_cost_new\"] # benefit_cost_list = [] # for t in threshold_list: # y_pred_adj = adjusted_classes(y_test_dt_prob, t=t) # cm = metrics.confusion_matrix(y_true=y_test_gt, y_pred = y_pred_adj) # tn, fp, fn, tp = metrics.confusion_matrix(y_true=y_test_gt, y_pred = y_pred_adj).ravel() # # this one check if it is correct formula # benefit_cost_old = tp*10 - fn*10 - fp*2 - (tp+fp)*1 # benefit_cost_new = tp*100 - fn*100 - fp*2 - (tp+fp)*1 # benefit_cost_list.append([t, tn, fn, fp, tn, benefit_cost_old, benefit_cost_new]) benefit_df = pd . DataFrame ( benefit_cost_list , columns = columns ) benefit_df","title":"Benefit Structure"},{"location":"reighns_ml_journey/projects/breast_cancer_wisconsin/Stage%208%20-%20Modelling%20%28Model%20Evaluation%20and%20Interpretation%29/#bias-variance-tradeoff","text":"avg_expected_loss , avg_bias , avg_var = bias_variance_decomp ( grid . best_estimator_ [ 'model' ], X_train . values , y_train . values , X_test . values , y_test . values , loss = '0-1_loss' , random_seed = 123 ) print ( 'Average expected loss: %.3f ' % avg_expected_loss ) print ( 'Average bias: %.3f ' % avg_bias ) print ( 'Average variance: %.3f ' % avg_var ) We use the mlxtend library to estimate the Bias-Variance Tradeoff in our Logistic Regression model. The core idea behind this function is to use bagging and repeatedly sample from our training set so as to simulate that we are actually drawing samples from the \"true\" population over a distribution \\(\\mathcal{P}\\) . As expected, Logistic Regression being a linear model, its simplicity contributes to its high bias and low variance.","title":"Bias-Variance Tradeoff"},{"location":"reighns_ml_journey/projects/gj_regression/DBA%203803%20Project%20-%20Hongnan/","text":"import csv import random from functools import wraps from time import time from typing import Callable , Dict , List , Union , Any , Optional import copy import matplotlib.pyplot as plt import mlxtend import numpy as np import pandas as pd import seaborn as sns from mlxtend.evaluate import paired_ttest_5x2cv , bias_variance_decomp from scipy import stats from sklearn import ( base , decomposition , dummy , ensemble , feature_selection , linear_model , metrics , model_selection , neighbors , pipeline , preprocessing , svm , tree ) # Created a config class to write global parameters. class global_config : # File Path raw_data = \"../data/raw/data.csv\" processed_data = \"Project2_Train.csv\" test_data = \"Project2_Test.csv\" spot_checking_boxplot = \"../data/images/spot_checking_boxplot.png\" oof_confusion_matrix = \"../data/images/oof_confusion_matrix.png\" final_train_confusion_matrix = \"../data/images/final_train_confusion_matrix.png\" precision_recall_threshold_plot = \"../data/images/precision_recall_threshold_plot.png\" roc_plot = \"../data/images/roc_plot.png\" feature_importance = \"../data/images/feature_importance.png\" # Data Information target = [ \"Response\" ] unwanted_cols = [ \"id\" , \"Unnamed: 32\" ] classification_type = \"binary\" # Plotting colors = [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ] cmap_reversed = plt . cm . get_cmap ( 'mako_r' ) # Seed Number seed = 1992 # Cross Validation num_folds = 5 cv_schema = \"StratifiedKFold\" split_size = { \"train_size\" : 1 , \"test_size\" : 0 } def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) # Read the csv file to a pandas dataframe. config = global_config # set seeding for reproducibility _ = set_seeds ( seed = config . seed ) # read data df = pd . read_csv ( config . processed_data ) df_test = pd . read_csv ( config . test_data ) # Drop the redundant column id. df . drop ( columns = [ \"id\" ], inplace = True ) df_test . drop ( columns = [ \"id\" ], inplace = True ) We first manually map the categorical features, Note the manual mapping we mapped letters to numbers, but for now, we will keep the 0, 1, 2 etc as strings, as function calls such as pd.get_dummies and OneHotEncoder work better when they see strings (which is a major signal for them to convert those columns). categorical_feat = [ 'Gender' , 'Vehicle_Age' , 'Vehicle_Damage' , 'Region_Code' ] continuous_feat = [ 'Age' , 'Annual_Premium' , 'Vintage' ] class_feat = [ 'Response' ] # assert that these columns are of the correct dtype! df = df . astype ( { \"Age\" : \"int64\" , \"Region_Code\" : \"int64\" , \"Annual_Premium\" : \"int64\" , \"Vintage\" : \"int64\" , \"Gender\" : \"str\" , \"Vehicle_Age\" : \"str\" , \"Vehicle_Damage\" : \"str\" , \"Response\" : \"int64\" } ) df_test = df_test . astype ( { \"Age\" : \"int64\" , \"Region_Code\" : \"int64\" , \"Annual_Premium\" : \"int64\" , \"Vintage\" : \"int64\" , \"Gender\" : \"str\" , \"Vehicle_Age\" : \"str\" , \"Vehicle_Damage\" : \"str\" , \"Response\" : \"int64\" } ) gender_map = { \"Female\" : \"0\" , \"Male\" : \"1\" } vehicle_age_map = { \"< 1 Year\" : \"0\" , \"1-2 Year\" : \"1\" , \"> 2 Years\" : \"2\" } vehicle_dmg_map = { \"No\" : \"0\" , \"Yes\" : \"1\" } region_code_map = { code : code for code in df . Region_Code . unique ()} cat_feature_map = { \"Gender\" : gender_map , \"Vehicle_Age\" : vehicle_age_map , \"Vehicle_Damage\" : vehicle_dmg_map , \"Region_Code\" : region_code_map } for cat_col in categorical_feat : df [ cat_col ] = df [ cat_col ] . map ( cat_feature_map [ cat_col ]) df_test [ cat_col ] = df_test [ cat_col ] . map ( cat_feature_map [ cat_col ]) X = df . copy () y = X . pop ( \"Response\" ) X_test = df_test . copy () y_test = X_test . pop ( \"Response\" ) # The below 3 code cells are just applying one hot encoding to gender and vehicle damage columns import logging log = logging . getLogger ( __name__ ) logging . basicConfig ( level = logging . INFO ) class CategoricalDummyCoder ( base . TransformerMixin ): \"\"\"https://stackoverflow.com/questions/39923927/pandas-sklearn-one-hot-encoding-dataframe-or-numpy Identifies categorical columns by dtype of object and dummy codes them. Optionally a pandas.DataFrame can be returned where categories are of pandas.Category dtype and not binarized for better coding strategies than dummy coding.\"\"\" def __init__ ( self , only_categoricals = False , selected_columns = None , ** get_dummies_kwargs ): self . categorical_variables = [] self . categories_per_column = {} self . only_categoricals = only_categoricals self . selected_columns = selected_columns self . get_dummies_kwargs = get_dummies_kwargs [ 'get_dummies_kwargs' ] def fit ( self , X , y = None ): if self . selected_columns is None : logging . info ( \"No selected columns by user, we will use all columns that is string\" ) self . categorical_variables = list ( X . select_dtypes ( include = [ 'object' ]) . columns ) else : logging . info ( \"Users have chosen selected columns to fit\" ) self . categorical_variables = list ( self . selected_columns ) logging . debug ( f 'identified the following categorical variables: { self . categorical_variables } ' ) for col in self . categorical_variables : self . categories_per_column [ col ] = X [ col ] . astype ( 'category' ) . cat . categories logging . debug ( 'fitted categories' ) return self def transform ( self , X ): for col in self . categorical_variables : logging . debug ( f 'transforming cat col: { col } ' ) X [ col ] = pd . Categorical ( X [ col ], categories = self . categories_per_column [ col ]) if self . only_categoricals : X [ col ] = X [ col ] . cat . codes if not self . only_categoricals : selected_cat_feat_df = pd . get_dummies ( X [ self . categorical_variables ], ** self . get_dummies_kwargs ) concat_df = pd . concat ([ X , selected_cat_feat_df ], axis = 1 ) . drop ( columns = self . categorical_variables , axis = 1 ) return concat_df else : return X one_hot_encoder = CategoricalDummyCoder ( only_categoricals = False , selected_columns = [ \"Vehicle_Age\" , 'Vehicle_Damage' , \"Gender\" , \"Region_Code\" ], get_dummies_kwargs = { \"sparse\" : True , \"drop_first\" : False }) X = one_hot_encoder . fit_transform ( X ) X_test = one_hot_encoder . transform ( X_test ) INFO:root:Users have chosen selected columns to fit predictor_cols = X . columns . to_list () # Split train - test X_train , y_train = X . copy (), y . copy () print ( f \"Y Train Distribution is : { y_train . value_counts ( normalize = True ) . to_dict () } \" ) print ( f \"Y Test Distribution is : { y_test . value_counts ( normalize = True ) . to_dict () } \" ) Y Train Distribution is : {0: 0.8674, 1: 0.1326} Y Test Distribution is : {0: 0.8636, 1: 0.1364} # The make fold function will further split the x training set into 5 different folds, and we stratified them accordingly. def make_folds ( df : pd . DataFrame , num_folds : int , cv_schema : str , seed : int , predictor_col : List , target_col : List , ) -> pd . DataFrame : \"\"\"Split the given dataframe into training folds. Args: df (pd.DataFrame): [description] num_folds (int): [description] cv_schema (str): [description] seed (int): [description] Returns: pd.DataFrame: [description] \"\"\" if cv_schema == \"KFold\" : df_folds = df . copy () kf = model_selection . KFold ( n_splits = num_folds , shuffle = True , random_state = seed ) for fold , ( train_idx , val_idx ) in enumerate ( kf . split ( X = df_folds [ predictor_col ], y = df_folds [ target_col ]) ): df_folds . loc [ val_idx , \"fold\" ] = int ( fold + 1 ) df_folds [ \"fold\" ] = df_folds [ \"fold\" ] . astype ( int ) elif cv_schema == \"StratifiedKFold\" : df_folds = df . copy () skf = model_selection . StratifiedKFold ( n_splits = num_folds , shuffle = True , random_state = seed ) for fold , ( train_idx , val_idx ) in enumerate ( skf . split ( X = df_folds [ predictor_col ], y = df_folds [ target_col ]) ): df_folds . loc [ val_idx , \"fold\" ] = int ( fold + 1 ) df_folds [ \"fold\" ] = df_folds [ \"fold\" ] . astype ( int ) print ( df_folds . groupby ([ \"fold\" , target_col [ 0 ]]) . size ()) return df_folds X_y_train = pd . concat ([ X_train , y_train ], axis = 1 ) . reset_index ( drop = True ) df_folds = make_folds ( X_y_train , num_folds = config . num_folds , cv_schema = config . cv_schema , seed = config . seed , predictor_col = predictor_cols , target_col = config . target ) fold Response 1 0 1735 1 265 2 0 1735 1 265 3 0 1735 1 265 4 0 1735 1 265 5 0 1734 1 266 dtype: int64 # The make_pipeline function will first standardize our training data, then fit the model. # create a feature preparation pipeline for a model def make_pipeline ( model ): \"\"\"Make a Pipeline for Training. Args: model ([type]): [description] Returns: [type]: [description] \"\"\" steps = list () # standardization steps . append (( 'standardize' , preprocessing . StandardScaler ())) # the model steps . append (( 'model' , model )) # create pipeline _pipeline = pipeline . Pipeline ( steps = steps ) return _pipeline # Now we define a list of classifier, which is a few models of our choice. classifiers = [ # linear model linear_model . LogisticRegression ( random_state = config . seed , solver = \"liblinear\" ), # # tree tree . DecisionTreeClassifier ( random_state = config . seed ), # # ensemble ensemble . RandomForestClassifier ( n_estimators = 100 , random_state = config . seed ), ] classifiers = [ make_pipeline ( model ) for model in classifiers ] Utils Results Class This class is an abstract data structure that takes in default result names and store the results in columnwise format. This is abstract also because if you call model_results which is a dictionary, it returns {'DummyClassifier': <__main__.Results at 0x1fd24436ca0>} . default_result_names = [ \"y_true\" , \"y_pred\" , \"y_prob\" , \"brier_loss\" , \"roc\" , \"precision\" , \"recall\" , \"f1\" , \"confusion_matrix\" , ] default_logit_names = [ \"y_true\" , \"y_pred\" , \"y_prob\" ] default_score_names = [ \"brier_loss\" , \"roc\" , \"precision\" , \"recall\" , \"f1\" , \"confusion_matrix\" ] class Results : \"\"\"Stores results for model training in columnwise format.\"\"\" _result_dict : Dict logit_names : List [ str ] score_names : List [ str ] def __init__ ( self , logit_names : List [ str ] = default_logit_names , score_names : List [ str ] = default_score_names , existing_dict : Optional [ Dict ] = None , ): \"\"\"Construct a new results store.\"\"\" self . logit_names = logit_names self . score_names = score_names if existing_dict is not None : self . _result_dict = copy . deepcopy ( existing_dict ) return dict_keys = [ \"identifier\" , * logit_names , * score_names ] self . _result_dict = { key : [] for key in dict_keys } def add ( self , identifier : str , results : Dict , in_place = False ): \"\"\"Add a new results row.\"\"\" if not in_place : return Results ( self . logit_names , self . score_names , self . _result_dict ) . add ( identifier , results , in_place = True ) self . _result_dict [ \"identifier\" ] . append ( identifier ) for result_name in set ([ * results . keys (), * self . logit_names , * self . score_names ]): result_value = results . get ( result_name , np . nan ) self . _result_dict [ result_name ] . append ( result_value ) return self def get_result ( self , result_name : str ) -> Dict [ str , Any ]: \"\"\"Get a map of identifiers to result values for a result.\"\"\" return { identifier : result_value for identifier , result_value in zip ( self . _result_dict [ \"identifier\" ], self . _result_dict [ result_name ]) } def get_result_values ( self , result_name : str ) -> List [ Any ]: \"\"\"Get a list of values for a result.\"\"\" return self . _result_dict [ result_name ] def to_dataframe ( self ) -> pd . DataFrame : \"\"\"Get a Data Frame containing the results.\"\"\" return pd . DataFrame . from_dict ( self . _result_dict ) def to_dict ( self ) -> Dict : \"\"\"Get a dictionary containing the results. Returns: Dict[str, List[Any]]: Dictionary of result columns \"\"\" return self . _result_dict Metrics def multiclass_label_binarize ( y , classes , pos_label = 1 , neg_label = 0 ): \"\"\"Binarize labels in one-vs-all fashion. Args: y (np.ndarray) Sequence of integer labels to encode classes (array-like) Labels for each class pos_label (int) Value for positive labels neg_label (int) Value for negative labels Returns: np.ndarray of shape (n_samples, n_classes) Encoded dataset \"\"\" columns = [ np . where ( y == label , pos_label , neg_label ) for label in classes ] return np . column_stack ( columns ) def multiclass_roc_auc_score ( y_true , y_score , classes = None ): \"\"\"Compute ROC-AUC score for each class in a multiclass dataset. Args: y_true (np.ndarray of shape (n_samples, n_classes)) True labels y_score (np.ndarray of shape (n_samples, n_classes)) Target scores classes (array-like of shape (n_classes,)) List of dataset classes. If `None`, the lexicographical order of the labels in `y_true` is used. Returns: array-like: ROC-AUC score for each class, in the same order as `classes` \"\"\" classes = ( np . unique ( y_true ) if classes is None else classes ) y_true_multiclass = multiclass_label_binarize ( y_true , classes = classes ) def oneclass_roc_auc_score ( class_id ): y_true_class = y_true_multiclass [:, class_id ] y_score_class = y_score [:, class_id ] fpr , tpr , _ = metrics . roc_curve ( y_true = y_true_class , y_score = y_score_class , pos_label = 1 ) return metrics . auc ( fpr , tpr ) return [ oneclass_roc_auc_score ( class_id ) for class_id in range ( len ( classes )) ] def compute_metrics ( logits : Dict [ str , np . ndarray ]) -> Dict [ str , Any ]: \"\"\"Compute metrics from logits.\"\"\" y_val , y_val_pred , y_val_prob = logits [ \"y_true\" ], logits [ \"y_pred\" ], logits [ \"y_prob\" ] #val_score = metrics.roc_auc_score( # y_true=y_val, # y_score=y_val_prob #) val_score = multiclass_roc_auc_score ( y_true = y_val , y_score = y_val_prob ) precision , recall , fbeta_score , _ = metrics . precision_recall_fscore_support ( y_true = y_val , y_pred = y_val_pred , labels = np . unique ( y_val ), average = None ) brier_loss = ( metrics . brier_score_loss ( y_true = y_val , y_prob = y_val_prob [:, 1 ] ) if config . classification_type == \"binary\" else np . nan ) confusion_matrix = metrics . confusion_matrix ( y_val , y_val_pred ) return { \"roc\" : val_score , \"precision\" : precision , \"recall\" : recall , \"f1\" : fbeta_score , \"brier_loss\" : brier_loss , \"confusion_matrix\" : confusion_matrix } def prepare_y ( y ): return ( y . ravel () if config . classification_type == \"binary\" else y ) def mean_score ( score_values ) -> Union [ float , np . ndarray ]: \"\"\"Compute the mean score.\"\"\" score_values = np . array ( score_values ) shape = score_values . shape if len ( shape ) == 1 : return score_values . mean () return score_values . mean ( axis = 0 ) def mean_cv_results ( model_results : Results ) -> Dict : \"\"\"Add mean cross-validation results. This method computes the mean value for all score types in the model_results, including for scores (e.g., confusion matrix) where the mean value may contain decimal places. \"\"\" cv_logits = { y_result : np . concatenate ( model_results . get_result_values ( y_result )) for y_result in model_results . logit_names } cv_scores = { score : mean_score ( model_results . get_result_values ( score ) ) for score in model_results . score_names } return { ** cv_logits , ** cv_scores , } def oof_cv_results ( model_results : Results ) -> Dict : \"\"\"Add OOF cross-validation results.\"\"\" cv_logits = { y_result : np . concatenate ( model_results . get_result_values ( y_result ) ) for y_result in model_results . logit_names } cv_scores = compute_metrics ( cv_logits ) return { ** cv_logits , ** cv_scores , } def add_cv_results ( model_results : Results ): \"\"\"Add cross-validation results. This method returns a copy of the given model results with summary columns for mean and CV cross-validation. \"\"\" mean_cv = mean_cv_results ( model_results ) oof_cv = oof_cv_results ( model_results ) return ( model_results . add ( \"mean_cv\" , mean_cv ) . add ( \"oof_cv\" , oof_cv ) ) def train_on_fold ( df_folds : pd . DataFrame , models : List [ Callable ], num_folds : int , predictor_col : List , target_col : List , ) -> Dict [ str , List ]: \"\"\"Take in a dataframe with fold number as column, and a models which holds a list of callable models, we will loop through and return a dictionary of cv results. Args: df_folds (pd.DataFrame): [description] model (Callable): [description] num_folds (int): [description] predictor_col (List): [description] target_col (List): [description] Returns: Dict[str, List]: [description] \"\"\" y_true = df_folds [ target_col ] . values . flatten () # test_pred_arr: np.ndarray = np.zeros(len(X_test)) model_dict = {} for model in models : model_results = Results () if isinstance ( model , pipeline . Pipeline ): model_name = model [ \"model\" ] . __class__ . __name__ else : model_name = model . __class__ . __name__ # out-of-fold validation predictions oof_pred_arr : np . ndarray = np . zeros ( len ( df_folds )) for fold in range ( 1 , num_folds + 1 ): train_df = df_folds [ df_folds [ \"fold\" ] != fold ] . reset_index ( drop = True ) val_df = df_folds [ df_folds [ \"fold\" ] == fold ] . reset_index ( drop = True ) val_idx = df_folds [ df_folds [ \"fold\" ] == fold ] . index . values X_train , y_train = train_df [ predictor_col ] . values , prepare_y ( train_df [ target_col ] . values ) X_val , y_val = val_df [ predictor_col ] . values , prepare_y ( val_df [ target_col ] . values ) model . fit ( X_train , y_train ) y_val_pred = model . predict ( X_val ) y_val_prob = model . predict_proba ( X_val ) logits = { \"y_true\" : y_val , \"y_pred\" : y_val_pred , \"y_prob\" : y_val_prob , } metrics = compute_metrics ( logits ) model_results . add ( f \"fold { fold } \" , { ** logits , ** metrics }, in_place = True ) if model_name not in model_dict : model_dict [ model_name ] = model_results return model_dict model_dict = train_on_fold ( df_folds , models = classifiers , num_folds = 5 , predictor_col = predictor_cols , target_col = config . target ) C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) model_dict_with_summary = { model : add_cv_results ( model_results ) for model , model_results in model_dict . items () } results_df = pd . concat ({ name : results . to_dataframe () . T for name , results in model_dict_with_summary . items () }, axis = 0 ) results_df . columns = [ 'fold 1' , 'fold 2' , 'fold 3' , 'fold 4' , 'fold 5' , 'mean_cv' , 'oof_cv' ] results_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv LogisticRegression identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ... [0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... y_pred [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... y_prob [[0.9913007976279888, 0.00869920237201111], [0... [[0.6945486216835197, 0.30545137831648034], [0... [[0.9958858145499713, 0.004114185450028691], [... [[0.8859877852400798, 0.11401221475992017], [0... [[0.9977204955737388, 0.0022795044262612914], ... [[0.9913007976279888, 0.00869920237201111], [0... [[0.9913007976279888, 0.00869920237201111], [0... brier_loss 0.095722 0.097785 0.096882 0.096771 0.09971 0.097374 0.097374 roc [0.829466586917514, 0.8294665869175141] [0.811457778261106, 0.811457778261106] [0.82251101082051, 0.82251101082051] [0.8232026534718069, 0.8232026534718069] [0.7994488817198706, 0.7994488817198706] [0.8172173822381614, 0.8172173822381614] [0.816942573130776, 0.8169425731307758] precision [0.8674337168584292, 0.0] [0.8675, 0.0] [0.8675, 0.0] [0.8678017025538307, 0.3333333333333333] [0.867, 0.0] [0.8674470838824521, 0.06666666666666667] [0.8674469787915166, 0.25] recall [0.9994236311239193, 0.0] [1.0, 0.0] [1.0, 0.0] [0.9988472622478386, 0.0037735849056603774] [1.0, 0.0] [0.9996541786743517, 0.0007547169811320754] [0.999654138805626, 0.0007541478129713424] f1 [0.9287627209426889, 0.0] [0.92904953145917, 0.0] [0.92904953145917, 0.0] [0.9287245444801714, 0.007462686567164179] [0.9287627209426887, 0.0] [0.9288698098567778, 0.0014925373134328358] [0.9288698446705945, 0.0015037593984962407] confusion_matrix [[1734, 1], [265, 0]] [[1735, 0], [265, 0]] [[1735, 0], [265, 0]] [[1733, 2], [264, 1]] [[1734, 0], [266, 0]] [[1734.2, 0.6], [265.0, 0.2]] [[8671, 3], [1325, 1]] DecisionTreeClassifier identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ... [0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... y_pred [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ... [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... y_prob [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0... [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0... [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0... [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0... [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0... [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0... [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0... brier_loss 0.194 0.188 0.2045 0.186 0.1895 0.1924 0.1924 roc [0.6164210755260726, 0.6164210755260725] [0.6198792887825567, 0.6198792887825567] [0.5656081779131097, 0.5656081779131097] [0.5938556902832908, 0.5938556902832907] [0.6090464049396849, 0.6090464049396849] [0.600962127488943, 0.600962127488943] [0.6009676462415547, 0.6009676462415547] precision [0.8992294013040901, 0.3035143769968051] [0.899941141848146, 0.31561461794019935] [0.8850174216027874, 0.24100719424460432] [0.8923431203223949, 0.2965779467680608] [0.8964306612053833, 0.30584192439862545] [0.8945923492565603, 0.292511212069659] [0.894552256254384, 0.29322268326417705] recall [0.8743515850144092, 0.3584905660377358] [0.8812680115273775, 0.3584905660377358] [0.878386167146974, 0.2528301886792453] [0.8933717579250721, 0.2943396226415094] [0.8835063437139562, 0.33458646616541354] [0.8821767730655579, 0.319747481912328] [0.8821766197832603, 0.31975867269984914] f1 [0.886616014026885, 0.328719723183391] [0.8905066977285965, 0.33568904593639576] [0.8816893260052068, 0.24677716390423574] [0.8928571428571428, 0.2954545454545454] [0.8899215800174267, 0.31956912028725315] [0.8883181521270516, 0.3052419197531642] [0.8883213373577897, 0.3059163059163059] confusion_matrix [[1517, 218], [170, 95]] [[1529, 206], [170, 95]] [[1524, 211], [198, 67]] [[1550, 185], [187, 78]] [[1532, 202], [177, 89]] [[1530.4, 204.4], [180.4, 84.8]] [[7652, 1022], [902, 424]] RandomForestClassifier identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ... [0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... y_pred [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... y_prob [[1.0, 0.0], [0.32, 0.68], [0.53, 0.47], [0.9,... [[0.81, 0.19], [0.88, 0.12], [1.0, 0.0], [1.0,... [[1.0, 0.0], [1.0, 0.0], [0.64, 0.36], [0.97, ... [[0.98, 0.02], [0.49, 0.51], [0.99, 0.01], [0.... [[1.0, 0.0], [0.54, 0.46], [1.0, 0.0], [0.69, ... [[1.0, 0.0], [0.32, 0.68], [0.53, 0.47], [0.9,... [[1.0, 0.0], [0.32, 0.68], [0.53, 0.47], [0.9,... brier_loss 0.107495 0.106624 0.106167 0.103634 0.10682 0.106148 0.106148 roc [0.7979500842803545, 0.7979500842803545] [0.7956076341688869, 0.7956076341688869] [0.798345930074493, 0.798345930074493] [0.8149660159860801, 0.8149660159860802] [0.7988808526506578, 0.7988808526506577] [0.8011501034320945, 0.8011501034320944] [0.8012694444763238, 0.8012694444763238] precision [0.8798511430090378, 0.3277310924369748] [0.8757861635220126, 0.30434782608695654] [0.8771097046413502, 0.3076923076923077] [0.8760460251046025, 0.3181818181818182] [0.8754598003152917, 0.29896907216494845] [0.876850567318459, 0.31138442331260113] [0.8768421052631579, 0.312] recall [0.9538904899135446, 0.1471698113207547] [0.9631123919308358, 0.10566037735849057] [0.9585014409221903, 0.12075471698113208] [0.9654178674351584, 0.10566037735849057] [0.9607843137254902, 0.10902255639097744] [0.9603413007854439, 0.11765356788196908] [0.9603412497117824, 0.11764705882352941] f1 [0.9153761061946902, 0.203125] [0.9173757891847379, 0.1568627450980392] [0.9160011016248968, 0.1734417344173442] [0.9185632026323005, 0.15864022662889515] [0.9161396755567776, 0.15977961432506887] [0.9166911750386806, 0.1703698640938695] [0.9166941784967537, 0.17086527929901427] confusion_matrix [[1655, 80], [226, 39]] [[1671, 64], [237, 28]] [[1663, 72], [233, 32]] [[1675, 60], [237, 28]] [[1666, 68], [237, 29]] [[1666.0, 68.8], [234.0, 31.2]] [[8330, 344], [1170, 156]] def get_result ( self , result_name : str ) -> Dict [ str , Any ]: \"\"\"Get a map of identifiers to result values for a result.\"\"\" return { identifier : result_value for identifier , result_value in zip ( self . _result_dict [ \"identifier\" ], self . _result_dict [ result_name ]) } results_df = pd . concat ({ name : results . to_dataframe () . T for name , results in model_dict_with_summary . items () }, axis = 0 ) results_df . columns = [ 'fold 1' , 'fold 2' , 'fold 3' , 'fold 4' , 'fold 5' , 'mean_cv' , 'oof_cv' ] results_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv LogisticRegression identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ... [0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... y_pred [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... y_prob [[0.9913007976279888, 0.00869920237201111], [0... [[0.6945486216835197, 0.30545137831648034], [0... [[0.9958858145499713, 0.004114185450028691], [... [[0.8859877852400798, 0.11401221475992017], [0... [[0.9977204955737388, 0.0022795044262612914], ... [[0.9913007976279888, 0.00869920237201111], [0... [[0.9913007976279888, 0.00869920237201111], [0... brier_loss 0.095722 0.097785 0.096882 0.096771 0.09971 0.097374 0.097374 roc [0.829466586917514, 0.8294665869175141] [0.811457778261106, 0.811457778261106] [0.82251101082051, 0.82251101082051] [0.8232026534718069, 0.8232026534718069] [0.7994488817198706, 0.7994488817198706] [0.8172173822381614, 0.8172173822381614] [0.816942573130776, 0.8169425731307758] precision [0.8674337168584292, 0.0] [0.8675, 0.0] [0.8675, 0.0] [0.8678017025538307, 0.3333333333333333] [0.867, 0.0] [0.8674470838824521, 0.06666666666666667] [0.8674469787915166, 0.25] recall [0.9994236311239193, 0.0] [1.0, 0.0] [1.0, 0.0] [0.9988472622478386, 0.0037735849056603774] [1.0, 0.0] [0.9996541786743517, 0.0007547169811320754] [0.999654138805626, 0.0007541478129713424] f1 [0.9287627209426889, 0.0] [0.92904953145917, 0.0] [0.92904953145917, 0.0] [0.9287245444801714, 0.007462686567164179] [0.9287627209426887, 0.0] [0.9288698098567778, 0.0014925373134328358] [0.9288698446705945, 0.0015037593984962407] confusion_matrix [[1734, 1], [265, 0]] [[1735, 0], [265, 0]] [[1735, 0], [265, 0]] [[1733, 2], [264, 1]] [[1734, 0], [266, 0]] [[1734.2, 0.6], [265.0, 0.2]] [[8671, 3], [1325, 1]] DecisionTreeClassifier identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ... [0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... y_pred [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ... [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... y_prob [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0... [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0... [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0... [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0... [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0... [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0... [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0... brier_loss 0.194 0.188 0.2045 0.186 0.1895 0.1924 0.1924 roc [0.6164210755260726, 0.6164210755260725] [0.6198792887825567, 0.6198792887825567] [0.5656081779131097, 0.5656081779131097] [0.5938556902832908, 0.5938556902832907] [0.6090464049396849, 0.6090464049396849] [0.600962127488943, 0.600962127488943] [0.6009676462415547, 0.6009676462415547] precision [0.8992294013040901, 0.3035143769968051] [0.899941141848146, 0.31561461794019935] [0.8850174216027874, 0.24100719424460432] [0.8923431203223949, 0.2965779467680608] [0.8964306612053833, 0.30584192439862545] [0.8945923492565603, 0.292511212069659] [0.894552256254384, 0.29322268326417705] recall [0.8743515850144092, 0.3584905660377358] [0.8812680115273775, 0.3584905660377358] [0.878386167146974, 0.2528301886792453] [0.8933717579250721, 0.2943396226415094] [0.8835063437139562, 0.33458646616541354] [0.8821767730655579, 0.319747481912328] [0.8821766197832603, 0.31975867269984914] f1 [0.886616014026885, 0.328719723183391] [0.8905066977285965, 0.33568904593639576] [0.8816893260052068, 0.24677716390423574] [0.8928571428571428, 0.2954545454545454] [0.8899215800174267, 0.31956912028725315] [0.8883181521270516, 0.3052419197531642] [0.8883213373577897, 0.3059163059163059] confusion_matrix [[1517, 218], [170, 95]] [[1529, 206], [170, 95]] [[1524, 211], [198, 67]] [[1550, 185], [187, 78]] [[1532, 202], [177, 89]] [[1530.4, 204.4], [180.4, 84.8]] [[7652, 1022], [902, 424]] RandomForestClassifier identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ... [0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... y_pred [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... y_prob [[1.0, 0.0], [0.32, 0.68], [0.53, 0.47], [0.9,... [[0.81, 0.19], [0.88, 0.12], [1.0, 0.0], [1.0,... [[1.0, 0.0], [1.0, 0.0], [0.64, 0.36], [0.97, ... [[0.98, 0.02], [0.49, 0.51], [0.99, 0.01], [0.... [[1.0, 0.0], [0.54, 0.46], [1.0, 0.0], [0.69, ... [[1.0, 0.0], [0.32, 0.68], [0.53, 0.47], [0.9,... [[1.0, 0.0], [0.32, 0.68], [0.53, 0.47], [0.9,... brier_loss 0.107495 0.106624 0.106167 0.103634 0.10682 0.106148 0.106148 roc [0.7979500842803545, 0.7979500842803545] [0.7956076341688869, 0.7956076341688869] [0.798345930074493, 0.798345930074493] [0.8149660159860801, 0.8149660159860802] [0.7988808526506578, 0.7988808526506577] [0.8011501034320945, 0.8011501034320944] [0.8012694444763238, 0.8012694444763238] precision [0.8798511430090378, 0.3277310924369748] [0.8757861635220126, 0.30434782608695654] [0.8771097046413502, 0.3076923076923077] [0.8760460251046025, 0.3181818181818182] [0.8754598003152917, 0.29896907216494845] [0.876850567318459, 0.31138442331260113] [0.8768421052631579, 0.312] recall [0.9538904899135446, 0.1471698113207547] [0.9631123919308358, 0.10566037735849057] [0.9585014409221903, 0.12075471698113208] [0.9654178674351584, 0.10566037735849057] [0.9607843137254902, 0.10902255639097744] [0.9603413007854439, 0.11765356788196908] [0.9603412497117824, 0.11764705882352941] f1 [0.9153761061946902, 0.203125] [0.9173757891847379, 0.1568627450980392] [0.9160011016248968, 0.1734417344173442] [0.9185632026323005, 0.15864022662889515] [0.9161396755567776, 0.15977961432506887] [0.9166911750386806, 0.1703698640938695] [0.9166941784967537, 0.17086527929901427] confusion_matrix [[1655, 80], [226, 39]] [[1671, 64], [237, 28]] [[1663, 72], [233, 32]] [[1675, 60], [237, 28]] [[1666, 68], [237, 29]] [[1666.0, 68.8], [234.0, 31.2]] [[8330, 344], [1170, 156]] def summarize_metrics ( model_dict : Dict , metric_name : str = \"roc\" , pos_label : int = 1 ): \"\"\" Summarize metrics of each fold with its standard error. We also plot a boxplot to show the results. \"\"\" results = [] for model_name , model_results in model_dict . items (): result_dict = model_results . get_result ( result_name = metric_name ) tmp_score = [] for fold , metric in result_dict . items (): pos_class_score = metric [ pos_label ] results . append (( model_name , fold , pos_class_score )) tmp_score . append ( pos_class_score ) # append the Standard Error of K folds results . append (( model_name , \"SE\" , np . std ( tmp_score , ddof = 1 ) / len ( tmp_score ) ** 0.5 )) summary_df = pd . DataFrame ( results , columns = [ \"model\" , \"fold\" , metric_name ]) fig , ax = plt . subplots ( figsize = ( 15 , 8 )) _ = sns . boxplot ( x = \"model\" , y = metric_name , data = summary_df [( summary_df [ 'model' ] != 'DummyClassifier' ) & ( summary_df [ 'fold' ] != 'SE' )], ax = ax ) # fig.savefig(config.spot_checking_boxplot, format='png', dpi=300) return summary_df summary_df = summarize_metrics ( model_dict = model_dict , metric_name = \"roc\" ) display ( summary_df . tail ( 12 )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } model fold roc 6 DecisionTreeClassifier fold 1 0.616421 7 DecisionTreeClassifier fold 2 0.619879 8 DecisionTreeClassifier fold 3 0.565608 9 DecisionTreeClassifier fold 4 0.593856 10 DecisionTreeClassifier fold 5 0.609046 11 DecisionTreeClassifier SE 0.009906 12 RandomForestClassifier fold 1 0.797950 13 RandomForestClassifier fold 2 0.795608 14 RandomForestClassifier fold 3 0.798346 15 RandomForestClassifier fold 4 0.814966 16 RandomForestClassifier fold 5 0.798881 17 RandomForestClassifier SE 0.003499 model_names = [ model for model in model_dict . keys ()] fig , ax = plt . subplots ( 1 , 3 , figsize = ( 10 , 10 )) for axes , algo in zip ( ax . ravel (), model_names ): cf_mat = results_df . oof_cv [ algo ] . confusion_matrix #### scores positive_class_auroc = results_df . oof_cv [ algo ] . roc [ 1 ] #### annotations labels = [ \"True Neg\" , \"False Pos\" , \"False Neg\" , \"True Pos\" ] counts = [ \" {0:0.0f} \" . format ( value ) for value in cf_mat . flatten ()] percentages = [ \" {0:.2%} \" . format ( value ) for value in cf_mat . flatten () / np . sum ( cf_mat )] #### final annotations label = ( np . array ([ f \" { v1 } \\n { v2 } \\n { v3 } \" for v1 , v2 , v3 in zip ( labels , counts , percentages )]) ) . reshape ( 2 , 2 ) # heatmap sns . heatmap ( data = cf_mat , vmin = 0 , vmax = 330 , cmap = [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ], linewidth = 2 , linecolor = \"white\" , square = True , ax = axes , annot = label , fmt = \"\" , cbar = False , annot_kws = { \"size\" : 10 , \"color\" : \"black\" , \"weight\" : \"bold\" , \"alpha\" : 0.8 }, alpha = 1 , ) axes . text ( 0 , - 0 , \" {} \" . format ( algo ), { \"size\" : 12 , \"color\" : \"black\" , \"weight\" : \"bold\" }) axes . scatter ( 1 , 1 , s = 3500 , c = \"white\" ) axes . text ( 0.72 , 1.0 , \"AUC: {} \" . format ( np . round ( positive_class_auroc , 3 )), { \"size\" : 10 , \"color\" : \"black\" , \"weight\" : \"bold\" }, ) ## ticks and labels axes . set_xticklabels ( \"\" ) axes . set_yticklabels ( \"\" ) ## titles and text fig . text ( 0 , 1.05 , \"Out Of Fold Confusion Matrix\" , { \"size\" : 22 , \"weight\" : \"bold\" }, alpha = 1 ) fig . text ( 0 , 1 , \"\"\"This Visualization show the results of various classifiers and there respective results.\"\"\" , { \"size\" : 14 , \"weight\" : \"normal\" }, alpha = 0.98 , ) fig . tight_layout ( pad = 2.5 , w_pad = 2.5 , h_pad = 2.5 ) # fig.savefig(config.oof_confusion_matrix, format='png', dpi=300) Hyperparameter # create a feature preparation pipeline for a model def make_finetuning_pipeline ( model ): \"\"\"Make a Pipeline for Training. Args: model ([type]): [description] Returns: [type]: [description] \"\"\" steps = list () # standardization steps . append (( 'standardize' , preprocessing . StandardScaler ())) # the model steps . append (( 'model' , model )) # create pipeline _pipeline = pipeline . Pipeline ( steps = steps ) return _pipeline pipeline_logistic = make_finetuning_pipeline ( linear_model . LogisticRegression ( random_state = config . seed , solver = 'liblinear' )) pipeline_decision_tree = make_finetuning_pipeline ( tree . DecisionTreeClassifier ( random_state = config . seed )) debug = True if debug : pipeline_random_forest = make_finetuning_pipeline ( ensemble . RandomForestClassifier ( n_estimators = 1 , random_state = config . seed )) else : pipeline_random_forest = make_finetuning_pipeline ( ensemble . RandomForestClassifier ( n_estimators = 100 , random_state = config . seed )) logistic_param_grid = dict ( model__C = np . logspace ( - 4 , 4 , 10 ), model__penalty = [ \"l1\" , \"l2\" ]) # https://stackoverflow.com/questions/46480457/difference-between-min-samples-split-and-min-samples-leaf-in-sklearn-decisiontre # min samples in a leaf should necessarily be odd to avoid tie in majority voting. # https://stackoverflow.com/questions/43963365/how-to-tune-sklearns-randomforest-max-depth-vs-min-samples-leaf dt_param_grid = dict ( model__criterion = [ \"gini\" , \"entropy\" ], model__max_depth = [ i for i in range ( 1 , 15 , 1 )], model__min_samples_leaf = [ 3 , 5 , 7 , 9 , 11 , 13 ], ) rf_param_grid = dict ( model__criterion = [ \"gini\" , \"entropy\" ], model__max_depth = [ i for i in range ( 1 , 15 , 1 )], model__min_samples_leaf = [ 3 , 5 , 7 , 9 , 11 , 13 ], ) grid_logistic = model_selection . GridSearchCV ( pipeline_logistic , param_grid = logistic_param_grid , cv = 5 , refit = True , verbose = 0 , scoring = \"roc_auc\" ) _ = grid_logistic . fit ( X_train . values , y_train . values ) grid_dt = model_selection . GridSearchCV ( pipeline_decision_tree , param_grid = dt_param_grid , cv = 5 , refit = True , verbose = 0 , scoring = \"roc_auc\" ) _ = grid_dt . fit ( X_train . values , y_train . values ) grid_rf = model_selection . GridSearchCV ( pipeline_random_forest , param_grid = rf_param_grid , cv = 5 , refit = True , verbose = 0 , scoring = \"roc_auc\" ) _ = grid_rf . fit ( X_train . values , y_train . values ) cv_results = pd . DataFrame ( grid_logistic . cv_results_ ) . sort_values ( \"mean_test_score\" , ascending = False ) # get the parameter names column_results = [ f \"param_ { name } \" for name in logistic_param_grid . keys ()] column_results += [ \"mean_test_score\" , \"std_test_score\" , \"rank_test_score\" ] cv_results = cv_results [ column_results ] def shorten_param ( param_name ): if \"__\" in param_name : return param_name . rsplit ( \"__\" , 1 )[ 1 ] return param_name cv_results = cv_results . rename ( shorten_param , axis = 1 ) cv_results .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } C penalty mean_test_score std_test_score rank_test_score 8 0.359381 l1 0.817990 0.007830 1 10 2.782559 l1 0.817900 0.007826 2 12 21.544347 l1 0.817868 0.007823 3 9 0.359381 l2 0.817868 0.007848 4 11 2.782559 l2 0.817867 0.007828 5 14 166.810054 l1 0.817860 0.007818 6 17 1291.549665 l2 0.817859 0.007815 7 19 10000.0 l2 0.817859 0.007815 7 15 166.810054 l2 0.817858 0.007815 9 16 1291.549665 l1 0.817858 0.007817 10 18 10000.0 l1 0.817858 0.007817 10 13 21.544347 l2 0.817857 0.007817 12 6 0.046416 l1 0.817795 0.008225 13 7 0.046416 l2 0.817775 0.008023 14 5 0.005995 l2 0.816659 0.008399 15 3 0.000774 l2 0.807662 0.006027 16 4 0.005995 l1 0.795119 0.004219 17 1 0.0001 l2 0.792937 0.002506 18 2 0.000774 l1 0.500000 0.000000 19 0 0.0001 l1 0.500000 0.000000 19 cv_results = pd . DataFrame ( grid_dt . cv_results_ ) . sort_values ( \"mean_test_score\" , ascending = False ) # get the parameter names column_results = [ f \"param_ { name } \" for name in dt_param_grid . keys ()] column_results += [ \"mean_test_score\" , \"std_test_score\" , \"rank_test_score\" ] cv_results = cv_results [ column_results ] def shorten_param ( param_name ): if \"__\" in param_name : return param_name . rsplit ( \"__\" , 1 )[ 1 ] return param_name cv_results = cv_results . rename ( shorten_param , axis = 1 ) cv_results .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } criterion max_depth min_samples_leaf mean_test_score std_test_score rank_test_score 21 gini 4 9 0.818482 0.005350 1 22 gini 4 11 0.818390 0.005198 2 23 gini 4 13 0.818338 0.005103 3 20 gini 4 7 0.818105 0.005469 4 19 gini 4 5 0.817740 0.005811 5 ... ... ... ... ... ... ... 163 entropy 14 5 0.754388 0.017172 164 79 gini 14 5 0.750641 0.016970 165 162 entropy 14 3 0.748512 0.018148 166 72 gini 13 3 0.746442 0.020769 167 78 gini 14 3 0.736971 0.025007 168 168 rows \u00d7 6 columns cv_results = pd . DataFrame ( grid_rf . cv_results_ ) . sort_values ( \"mean_test_score\" , ascending = False ) # get the parameter names column_results = [ f \"param_ { name } \" for name in rf_param_grid . keys ()] column_results += [ \"mean_test_score\" , \"std_test_score\" , \"rank_test_score\" ] cv_results = cv_results [ column_results ] def shorten_param ( param_name ): if \"__\" in param_name : return param_name . rsplit ( \"__\" , 1 )[ 1 ] return param_name cv_results = cv_results . rename ( shorten_param , axis = 1 ) cv_results .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } criterion max_depth min_samples_leaf mean_test_score std_test_score rank_test_score 19 gini 4 5 0.809695 0.008776 1 106 entropy 4 11 0.809282 0.006761 2 107 entropy 4 13 0.809282 0.006761 2 23 gini 4 13 0.808963 0.009294 4 22 gini 4 11 0.808963 0.009294 4 ... ... ... ... ... ... ... 144 entropy 11 3 0.755646 0.011515 164 156 entropy 13 3 0.743200 0.030837 165 78 gini 14 3 0.742452 0.013079 166 162 entropy 14 3 0.740631 0.022496 167 72 gini 13 3 0.734308 0.009049 168 168 rows \u00d7 6 columns Export Tree Viz import graphviz dot_data = tree . export_graphviz ( grid_dt . best_estimator_ [ 'model' ], out_file = None , filled = True , rounded = True , feature_names = predictor_cols , class_names = [ '0' , '1' ], special_characters = True ) graph = graphviz . Source ( dot_data ) # graph.format = \"png\" # graph.render(\"health\") fig = plt . figure ( figsize = ( 50 , 35 ), dpi = 300 ) _ = tree . plot_tree ( grid_dt . best_estimator_ [ 'model' ], filled = True , rounded = True , feature_names = predictor_cols , class_names = [ '0' , '1' ]) fig . savefig ( \"dt_plot.png\" , dpi = 300 ) # plt.show() Feature Importance def get_feature_importance ( estimator : Callable , predictor_cols : List [ str ], has_feature_selection : bool = False ): \"\"\" If there is custom preprocessing feature selection step, like VIF, then we need to use different method to extract the features. predictor_cols: must be in sequence, how you pass in to train should preserve sequence. \"\"\" linear_models = [ 'LinearRegression' , 'LogisticRegression' ] tree_models = [ 'DecisionTreeClassifier' , 'RandomForestClassifier' ] estimator_name = estimator . __class__ . __name__ if has_feature_selection : pass else : if estimator_name in linear_models : coefficient = pd . DataFrame ( estimator . coef_ . flatten (), columns = [ 'Coefficients' ], index = predictor_cols ) coefficient . sort_values ( by = 'Coefficients' ) . plot ( kind = 'barh' , figsize = ( 9 , 7 )) plt . title ( f \" { estimator_name } \" ) plt . axvline ( x = 0 , color = '.5' ) plt . subplots_adjust ( left = .3 ) elif estimator_name in tree_models : coefficient = pd . DataFrame ( estimator . feature_importances_ , columns = [ 'Coefficients' ], index = predictor_cols ) coefficient . sort_values ( by = 'Coefficients' ) . plot ( kind = 'barh' , figsize = ( 9 , 7 )) plt . title ( f \" { estimator_name } \" ) plt . axvline ( x = 0 , color = '.5' ) plt . subplots_adjust ( left = .3 ) return coefficient predictor_cols = X_train . columns coef_logistic = get_feature_importance ( estimator = grid_logistic . best_estimator_ [ 'model' ], predictor_cols = predictor_cols ) coef_dt = get_feature_importance ( estimator = grid_dt . best_estimator_ [ 'model' ], predictor_cols = predictor_cols ) coef_rf = get_feature_importance ( estimator = grid_rf . best_estimator_ [ 'model' ], predictor_cols = predictor_cols ) Evaluate Performance on Test Set def plot_roc_curve ( fpr , tpr , label = None ): \"\"\" The ROC curve, modified from Hands-On Machine learning with Scikit-Learn and TensorFlow; p.91 and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 \"\"\" plt . figure ( figsize = ( 8 , 8 )) plt . title ( 'ROC Curve' ) plt . plot ( fpr , tpr , linewidth = 2 , label = label ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], 'k--' ) plt . axis ([ - 0.005 , 1 , 0 , 1.005 ]) plt . xticks ( np . arange ( 0 , 1 , 0.05 ), rotation = 90 ) plt . xlabel ( \"False Positive Rate\" ) plt . ylabel ( \"True Positive Rate (Recall)\" ) plt . legend ( loc = 'best' ) # plt.savefig(config.roc_plot, format=\"png\", dpi=300) y_test_gt = np . asarray ( y_test ) y_test_logistic_prob = grid_logistic . predict_proba ( X_test . values )[:, 1 ] roc_auc_logistic = metrics . roc_auc_score ( y_test_gt , y_test_logistic_prob ) print ( roc_auc_logistic ) 0.81786215276976 # generate the precision recall curve fpr , tpr , roc_thresholds = metrics . roc_curve ( y_test_gt , y_test_logistic_prob , pos_label = 1 ) plot_roc_curve ( fpr , tpr , label = \"ROC Curve on Test Set using Logistic\" ) Benefit Structure def adjusted_classes ( y_scores , t ): \"\"\" This function adjusts class predictions based on the prediction threshold (t). Will only work for binary classification problems. and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 \"\"\" return [ 1 if y >= t else 0 for y in y_scores ] def return_benefit_structure ( thresholds : List [ float ], y_true : np . ndarray , y_prob : np . ndarray ): columns = [ \"threshold\" , \"tp\" , \"fn\" , \"fp\" , \"tn\" , \"benefit_cost_old\" , \"benefit_cost_new\" ] benefit_cost_list = [] for t in threshold_list : y_pred_adj = adjusted_classes ( y_prob , t = t ) cm = metrics . confusion_matrix ( y_true = y_true , y_pred = y_pred_adj ) tn , fp , fn , tp = metrics . confusion_matrix ( y_true = y_true , y_pred = y_pred_adj ) . ravel () # this one check if it is correct formula benefit_cost_old = tp * 10 - fn * 10 - fp * 2 - ( tp + fp ) * 1 benefit_cost_new = tp * 100 - fn * 100 - fp * 2 - ( tp + fp ) * 1 benefit_cost_list . append ([ t , tn , fn , fp , tn , benefit_cost_old , benefit_cost_new ]) benefit_df = pd . DataFrame ( benefit_cost_list , columns = columns ) return benefit_df threshold_list : List [ float ] = [ 0.01 , 0.1 , 0.2 , 0.5 ] y_test_gt = np . asarray ( y_test ) y_test_logistic_prob = grid_logistic . predict_proba ( X_test . values )[:, 1 ] y_test_dt_prob = grid_dt . predict_proba ( X_test . values )[:, 1 ] y_test_rf_prob = grid_rf . predict_proba ( X_test . values )[:, 1 ] return_benefit_structure ( thresholds = threshold_list , y_true = y_test_gt , y_prob = y_test_logistic_prob ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } threshold tp fn fp tn benefit_cost_old benefit_cost_new 0 0.01 4622 19 4014 4622 -127 119213 1 0.10 4780 29 3856 4780 157 117697 2 0.20 5906 233 2730 5906 -341 80479 3 0.50 8636 1364 0 8636 -13640 -136400 return_benefit_structure ( thresholds = threshold_list , y_true = y_test_gt , y_prob = y_test_dt_prob ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } threshold tp fn fp tn benefit_cost_old benefit_cost_new 0 0.01 4412 16 4224 4412 -700 119180 1 0.10 5092 68 3544 5092 352 110872 2 0.20 6111 245 2525 6111 46 78706 3 0.50 8636 1364 0 8636 -13640 -136400 return_benefit_structure ( thresholds = threshold_list , y_true = y_test_gt , y_prob = y_test_rf_prob ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } threshold tp fn fp tn benefit_cost_old benefit_cost_new 0 0.01 3821 10 4815 3821 -2359 118601 1 0.10 5017 50 3619 5017 469 114229 2 0.20 5868 187 2768 5868 419 89519 3 0.50 8636 1364 0 8636 -13640 -136400 Bias-Variance Tradeoff # avg_expected_loss, avg_bias, avg_var = bias_variance_decomp( # grid.best_estimator_['model'], X_train.values, y_train.values, X_test.values, y_test.values, # loss='0-1_loss', # random_seed=123) # print('Average expected loss: %.3f' % avg_expected_loss) # print('Average bias: %.3f' % avg_bias) # print('Average variance: %.3f' % avg_var)","title":"DBA 3803 Project   Hongnan"},{"location":"reighns_ml_journey/projects/gj_regression/DBA%203803%20Project%20-%20Hongnan/#utils","text":"","title":"Utils"},{"location":"reighns_ml_journey/projects/gj_regression/DBA%203803%20Project%20-%20Hongnan/#results-class","text":"This class is an abstract data structure that takes in default result names and store the results in columnwise format. This is abstract also because if you call model_results which is a dictionary, it returns {'DummyClassifier': <__main__.Results at 0x1fd24436ca0>} . default_result_names = [ \"y_true\" , \"y_pred\" , \"y_prob\" , \"brier_loss\" , \"roc\" , \"precision\" , \"recall\" , \"f1\" , \"confusion_matrix\" , ] default_logit_names = [ \"y_true\" , \"y_pred\" , \"y_prob\" ] default_score_names = [ \"brier_loss\" , \"roc\" , \"precision\" , \"recall\" , \"f1\" , \"confusion_matrix\" ] class Results : \"\"\"Stores results for model training in columnwise format.\"\"\" _result_dict : Dict logit_names : List [ str ] score_names : List [ str ] def __init__ ( self , logit_names : List [ str ] = default_logit_names , score_names : List [ str ] = default_score_names , existing_dict : Optional [ Dict ] = None , ): \"\"\"Construct a new results store.\"\"\" self . logit_names = logit_names self . score_names = score_names if existing_dict is not None : self . _result_dict = copy . deepcopy ( existing_dict ) return dict_keys = [ \"identifier\" , * logit_names , * score_names ] self . _result_dict = { key : [] for key in dict_keys } def add ( self , identifier : str , results : Dict , in_place = False ): \"\"\"Add a new results row.\"\"\" if not in_place : return Results ( self . logit_names , self . score_names , self . _result_dict ) . add ( identifier , results , in_place = True ) self . _result_dict [ \"identifier\" ] . append ( identifier ) for result_name in set ([ * results . keys (), * self . logit_names , * self . score_names ]): result_value = results . get ( result_name , np . nan ) self . _result_dict [ result_name ] . append ( result_value ) return self def get_result ( self , result_name : str ) -> Dict [ str , Any ]: \"\"\"Get a map of identifiers to result values for a result.\"\"\" return { identifier : result_value for identifier , result_value in zip ( self . _result_dict [ \"identifier\" ], self . _result_dict [ result_name ]) } def get_result_values ( self , result_name : str ) -> List [ Any ]: \"\"\"Get a list of values for a result.\"\"\" return self . _result_dict [ result_name ] def to_dataframe ( self ) -> pd . DataFrame : \"\"\"Get a Data Frame containing the results.\"\"\" return pd . DataFrame . from_dict ( self . _result_dict ) def to_dict ( self ) -> Dict : \"\"\"Get a dictionary containing the results. Returns: Dict[str, List[Any]]: Dictionary of result columns \"\"\" return self . _result_dict","title":"Results Class"},{"location":"reighns_ml_journey/projects/gj_regression/DBA%203803%20Project%20-%20Hongnan/#metrics","text":"def multiclass_label_binarize ( y , classes , pos_label = 1 , neg_label = 0 ): \"\"\"Binarize labels in one-vs-all fashion. Args: y (np.ndarray) Sequence of integer labels to encode classes (array-like) Labels for each class pos_label (int) Value for positive labels neg_label (int) Value for negative labels Returns: np.ndarray of shape (n_samples, n_classes) Encoded dataset \"\"\" columns = [ np . where ( y == label , pos_label , neg_label ) for label in classes ] return np . column_stack ( columns ) def multiclass_roc_auc_score ( y_true , y_score , classes = None ): \"\"\"Compute ROC-AUC score for each class in a multiclass dataset. Args: y_true (np.ndarray of shape (n_samples, n_classes)) True labels y_score (np.ndarray of shape (n_samples, n_classes)) Target scores classes (array-like of shape (n_classes,)) List of dataset classes. If `None`, the lexicographical order of the labels in `y_true` is used. Returns: array-like: ROC-AUC score for each class, in the same order as `classes` \"\"\" classes = ( np . unique ( y_true ) if classes is None else classes ) y_true_multiclass = multiclass_label_binarize ( y_true , classes = classes ) def oneclass_roc_auc_score ( class_id ): y_true_class = y_true_multiclass [:, class_id ] y_score_class = y_score [:, class_id ] fpr , tpr , _ = metrics . roc_curve ( y_true = y_true_class , y_score = y_score_class , pos_label = 1 ) return metrics . auc ( fpr , tpr ) return [ oneclass_roc_auc_score ( class_id ) for class_id in range ( len ( classes )) ] def compute_metrics ( logits : Dict [ str , np . ndarray ]) -> Dict [ str , Any ]: \"\"\"Compute metrics from logits.\"\"\" y_val , y_val_pred , y_val_prob = logits [ \"y_true\" ], logits [ \"y_pred\" ], logits [ \"y_prob\" ] #val_score = metrics.roc_auc_score( # y_true=y_val, # y_score=y_val_prob #) val_score = multiclass_roc_auc_score ( y_true = y_val , y_score = y_val_prob ) precision , recall , fbeta_score , _ = metrics . precision_recall_fscore_support ( y_true = y_val , y_pred = y_val_pred , labels = np . unique ( y_val ), average = None ) brier_loss = ( metrics . brier_score_loss ( y_true = y_val , y_prob = y_val_prob [:, 1 ] ) if config . classification_type == \"binary\" else np . nan ) confusion_matrix = metrics . confusion_matrix ( y_val , y_val_pred ) return { \"roc\" : val_score , \"precision\" : precision , \"recall\" : recall , \"f1\" : fbeta_score , \"brier_loss\" : brier_loss , \"confusion_matrix\" : confusion_matrix } def prepare_y ( y ): return ( y . ravel () if config . classification_type == \"binary\" else y ) def mean_score ( score_values ) -> Union [ float , np . ndarray ]: \"\"\"Compute the mean score.\"\"\" score_values = np . array ( score_values ) shape = score_values . shape if len ( shape ) == 1 : return score_values . mean () return score_values . mean ( axis = 0 ) def mean_cv_results ( model_results : Results ) -> Dict : \"\"\"Add mean cross-validation results. This method computes the mean value for all score types in the model_results, including for scores (e.g., confusion matrix) where the mean value may contain decimal places. \"\"\" cv_logits = { y_result : np . concatenate ( model_results . get_result_values ( y_result )) for y_result in model_results . logit_names } cv_scores = { score : mean_score ( model_results . get_result_values ( score ) ) for score in model_results . score_names } return { ** cv_logits , ** cv_scores , } def oof_cv_results ( model_results : Results ) -> Dict : \"\"\"Add OOF cross-validation results.\"\"\" cv_logits = { y_result : np . concatenate ( model_results . get_result_values ( y_result ) ) for y_result in model_results . logit_names } cv_scores = compute_metrics ( cv_logits ) return { ** cv_logits , ** cv_scores , } def add_cv_results ( model_results : Results ): \"\"\"Add cross-validation results. This method returns a copy of the given model results with summary columns for mean and CV cross-validation. \"\"\" mean_cv = mean_cv_results ( model_results ) oof_cv = oof_cv_results ( model_results ) return ( model_results . add ( \"mean_cv\" , mean_cv ) . add ( \"oof_cv\" , oof_cv ) ) def train_on_fold ( df_folds : pd . DataFrame , models : List [ Callable ], num_folds : int , predictor_col : List , target_col : List , ) -> Dict [ str , List ]: \"\"\"Take in a dataframe with fold number as column, and a models which holds a list of callable models, we will loop through and return a dictionary of cv results. Args: df_folds (pd.DataFrame): [description] model (Callable): [description] num_folds (int): [description] predictor_col (List): [description] target_col (List): [description] Returns: Dict[str, List]: [description] \"\"\" y_true = df_folds [ target_col ] . values . flatten () # test_pred_arr: np.ndarray = np.zeros(len(X_test)) model_dict = {} for model in models : model_results = Results () if isinstance ( model , pipeline . Pipeline ): model_name = model [ \"model\" ] . __class__ . __name__ else : model_name = model . __class__ . __name__ # out-of-fold validation predictions oof_pred_arr : np . ndarray = np . zeros ( len ( df_folds )) for fold in range ( 1 , num_folds + 1 ): train_df = df_folds [ df_folds [ \"fold\" ] != fold ] . reset_index ( drop = True ) val_df = df_folds [ df_folds [ \"fold\" ] == fold ] . reset_index ( drop = True ) val_idx = df_folds [ df_folds [ \"fold\" ] == fold ] . index . values X_train , y_train = train_df [ predictor_col ] . values , prepare_y ( train_df [ target_col ] . values ) X_val , y_val = val_df [ predictor_col ] . values , prepare_y ( val_df [ target_col ] . values ) model . fit ( X_train , y_train ) y_val_pred = model . predict ( X_val ) y_val_prob = model . predict_proba ( X_val ) logits = { \"y_true\" : y_val , \"y_pred\" : y_val_pred , \"y_prob\" : y_val_prob , } metrics = compute_metrics ( logits ) model_results . add ( f \"fold { fold } \" , { ** logits , ** metrics }, in_place = True ) if model_name not in model_dict : model_dict [ model_name ] = model_results return model_dict model_dict = train_on_fold ( df_folds , models = classifiers , num_folds = 5 , predictor_col = predictor_cols , target_col = config . target ) C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) model_dict_with_summary = { model : add_cv_results ( model_results ) for model , model_results in model_dict . items () } results_df = pd . concat ({ name : results . to_dataframe () . T for name , results in model_dict_with_summary . items () }, axis = 0 ) results_df . columns = [ 'fold 1' , 'fold 2' , 'fold 3' , 'fold 4' , 'fold 5' , 'mean_cv' , 'oof_cv' ] results_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv LogisticRegression identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ... [0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... y_pred [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... y_prob [[0.9913007976279888, 0.00869920237201111], [0... [[0.6945486216835197, 0.30545137831648034], [0... [[0.9958858145499713, 0.004114185450028691], [... [[0.8859877852400798, 0.11401221475992017], [0... [[0.9977204955737388, 0.0022795044262612914], ... [[0.9913007976279888, 0.00869920237201111], [0... [[0.9913007976279888, 0.00869920237201111], [0... brier_loss 0.095722 0.097785 0.096882 0.096771 0.09971 0.097374 0.097374 roc [0.829466586917514, 0.8294665869175141] [0.811457778261106, 0.811457778261106] [0.82251101082051, 0.82251101082051] [0.8232026534718069, 0.8232026534718069] [0.7994488817198706, 0.7994488817198706] [0.8172173822381614, 0.8172173822381614] [0.816942573130776, 0.8169425731307758] precision [0.8674337168584292, 0.0] [0.8675, 0.0] [0.8675, 0.0] [0.8678017025538307, 0.3333333333333333] [0.867, 0.0] [0.8674470838824521, 0.06666666666666667] [0.8674469787915166, 0.25] recall [0.9994236311239193, 0.0] [1.0, 0.0] [1.0, 0.0] [0.9988472622478386, 0.0037735849056603774] [1.0, 0.0] [0.9996541786743517, 0.0007547169811320754] [0.999654138805626, 0.0007541478129713424] f1 [0.9287627209426889, 0.0] [0.92904953145917, 0.0] [0.92904953145917, 0.0] [0.9287245444801714, 0.007462686567164179] [0.9287627209426887, 0.0] [0.9288698098567778, 0.0014925373134328358] [0.9288698446705945, 0.0015037593984962407] confusion_matrix [[1734, 1], [265, 0]] [[1735, 0], [265, 0]] [[1735, 0], [265, 0]] [[1733, 2], [264, 1]] [[1734, 0], [266, 0]] [[1734.2, 0.6], [265.0, 0.2]] [[8671, 3], [1325, 1]] DecisionTreeClassifier identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ... [0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... y_pred [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ... [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... y_prob [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0... [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0... [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0... [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0... [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0... [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0... [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0... brier_loss 0.194 0.188 0.2045 0.186 0.1895 0.1924 0.1924 roc [0.6164210755260726, 0.6164210755260725] [0.6198792887825567, 0.6198792887825567] [0.5656081779131097, 0.5656081779131097] [0.5938556902832908, 0.5938556902832907] [0.6090464049396849, 0.6090464049396849] [0.600962127488943, 0.600962127488943] [0.6009676462415547, 0.6009676462415547] precision [0.8992294013040901, 0.3035143769968051] [0.899941141848146, 0.31561461794019935] [0.8850174216027874, 0.24100719424460432] [0.8923431203223949, 0.2965779467680608] [0.8964306612053833, 0.30584192439862545] [0.8945923492565603, 0.292511212069659] [0.894552256254384, 0.29322268326417705] recall [0.8743515850144092, 0.3584905660377358] [0.8812680115273775, 0.3584905660377358] [0.878386167146974, 0.2528301886792453] [0.8933717579250721, 0.2943396226415094] [0.8835063437139562, 0.33458646616541354] [0.8821767730655579, 0.319747481912328] [0.8821766197832603, 0.31975867269984914] f1 [0.886616014026885, 0.328719723183391] [0.8905066977285965, 0.33568904593639576] [0.8816893260052068, 0.24677716390423574] [0.8928571428571428, 0.2954545454545454] [0.8899215800174267, 0.31956912028725315] [0.8883181521270516, 0.3052419197531642] [0.8883213373577897, 0.3059163059163059] confusion_matrix [[1517, 218], [170, 95]] [[1529, 206], [170, 95]] [[1524, 211], [198, 67]] [[1550, 185], [187, 78]] [[1532, 202], [177, 89]] [[1530.4, 204.4], [180.4, 84.8]] [[7652, 1022], [902, 424]] RandomForestClassifier identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ... [0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... y_pred [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... y_prob [[1.0, 0.0], [0.32, 0.68], [0.53, 0.47], [0.9,... [[0.81, 0.19], [0.88, 0.12], [1.0, 0.0], [1.0,... [[1.0, 0.0], [1.0, 0.0], [0.64, 0.36], [0.97, ... [[0.98, 0.02], [0.49, 0.51], [0.99, 0.01], [0.... [[1.0, 0.0], [0.54, 0.46], [1.0, 0.0], [0.69, ... [[1.0, 0.0], [0.32, 0.68], [0.53, 0.47], [0.9,... [[1.0, 0.0], [0.32, 0.68], [0.53, 0.47], [0.9,... brier_loss 0.107495 0.106624 0.106167 0.103634 0.10682 0.106148 0.106148 roc [0.7979500842803545, 0.7979500842803545] [0.7956076341688869, 0.7956076341688869] [0.798345930074493, 0.798345930074493] [0.8149660159860801, 0.8149660159860802] [0.7988808526506578, 0.7988808526506577] [0.8011501034320945, 0.8011501034320944] [0.8012694444763238, 0.8012694444763238] precision [0.8798511430090378, 0.3277310924369748] [0.8757861635220126, 0.30434782608695654] [0.8771097046413502, 0.3076923076923077] [0.8760460251046025, 0.3181818181818182] [0.8754598003152917, 0.29896907216494845] [0.876850567318459, 0.31138442331260113] [0.8768421052631579, 0.312] recall [0.9538904899135446, 0.1471698113207547] [0.9631123919308358, 0.10566037735849057] [0.9585014409221903, 0.12075471698113208] [0.9654178674351584, 0.10566037735849057] [0.9607843137254902, 0.10902255639097744] [0.9603413007854439, 0.11765356788196908] [0.9603412497117824, 0.11764705882352941] f1 [0.9153761061946902, 0.203125] [0.9173757891847379, 0.1568627450980392] [0.9160011016248968, 0.1734417344173442] [0.9185632026323005, 0.15864022662889515] [0.9161396755567776, 0.15977961432506887] [0.9166911750386806, 0.1703698640938695] [0.9166941784967537, 0.17086527929901427] confusion_matrix [[1655, 80], [226, 39]] [[1671, 64], [237, 28]] [[1663, 72], [233, 32]] [[1675, 60], [237, 28]] [[1666, 68], [237, 29]] [[1666.0, 68.8], [234.0, 31.2]] [[8330, 344], [1170, 156]] def get_result ( self , result_name : str ) -> Dict [ str , Any ]: \"\"\"Get a map of identifiers to result values for a result.\"\"\" return { identifier : result_value for identifier , result_value in zip ( self . _result_dict [ \"identifier\" ], self . _result_dict [ result_name ]) } results_df = pd . concat ({ name : results . to_dataframe () . T for name , results in model_dict_with_summary . items () }, axis = 0 ) results_df . columns = [ 'fold 1' , 'fold 2' , 'fold 3' , 'fold 4' , 'fold 5' , 'mean_cv' , 'oof_cv' ] results_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv LogisticRegression identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ... [0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... y_pred [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... y_prob [[0.9913007976279888, 0.00869920237201111], [0... [[0.6945486216835197, 0.30545137831648034], [0... [[0.9958858145499713, 0.004114185450028691], [... [[0.8859877852400798, 0.11401221475992017], [0... [[0.9977204955737388, 0.0022795044262612914], ... [[0.9913007976279888, 0.00869920237201111], [0... [[0.9913007976279888, 0.00869920237201111], [0... brier_loss 0.095722 0.097785 0.096882 0.096771 0.09971 0.097374 0.097374 roc [0.829466586917514, 0.8294665869175141] [0.811457778261106, 0.811457778261106] [0.82251101082051, 0.82251101082051] [0.8232026534718069, 0.8232026534718069] [0.7994488817198706, 0.7994488817198706] [0.8172173822381614, 0.8172173822381614] [0.816942573130776, 0.8169425731307758] precision [0.8674337168584292, 0.0] [0.8675, 0.0] [0.8675, 0.0] [0.8678017025538307, 0.3333333333333333] [0.867, 0.0] [0.8674470838824521, 0.06666666666666667] [0.8674469787915166, 0.25] recall [0.9994236311239193, 0.0] [1.0, 0.0] [1.0, 0.0] [0.9988472622478386, 0.0037735849056603774] [1.0, 0.0] [0.9996541786743517, 0.0007547169811320754] [0.999654138805626, 0.0007541478129713424] f1 [0.9287627209426889, 0.0] [0.92904953145917, 0.0] [0.92904953145917, 0.0] [0.9287245444801714, 0.007462686567164179] [0.9287627209426887, 0.0] [0.9288698098567778, 0.0014925373134328358] [0.9288698446705945, 0.0015037593984962407] confusion_matrix [[1734, 1], [265, 0]] [[1735, 0], [265, 0]] [[1735, 0], [265, 0]] [[1733, 2], [264, 1]] [[1734, 0], [266, 0]] [[1734.2, 0.6], [265.0, 0.2]] [[8671, 3], [1325, 1]] DecisionTreeClassifier identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ... [0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... y_pred [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ... [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... y_prob [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0... [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0... [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0... [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0... [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0... [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0... [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0... brier_loss 0.194 0.188 0.2045 0.186 0.1895 0.1924 0.1924 roc [0.6164210755260726, 0.6164210755260725] [0.6198792887825567, 0.6198792887825567] [0.5656081779131097, 0.5656081779131097] [0.5938556902832908, 0.5938556902832907] [0.6090464049396849, 0.6090464049396849] [0.600962127488943, 0.600962127488943] [0.6009676462415547, 0.6009676462415547] precision [0.8992294013040901, 0.3035143769968051] [0.899941141848146, 0.31561461794019935] [0.8850174216027874, 0.24100719424460432] [0.8923431203223949, 0.2965779467680608] [0.8964306612053833, 0.30584192439862545] [0.8945923492565603, 0.292511212069659] [0.894552256254384, 0.29322268326417705] recall [0.8743515850144092, 0.3584905660377358] [0.8812680115273775, 0.3584905660377358] [0.878386167146974, 0.2528301886792453] [0.8933717579250721, 0.2943396226415094] [0.8835063437139562, 0.33458646616541354] [0.8821767730655579, 0.319747481912328] [0.8821766197832603, 0.31975867269984914] f1 [0.886616014026885, 0.328719723183391] [0.8905066977285965, 0.33568904593639576] [0.8816893260052068, 0.24677716390423574] [0.8928571428571428, 0.2954545454545454] [0.8899215800174267, 0.31956912028725315] [0.8883181521270516, 0.3052419197531642] [0.8883213373577897, 0.3059163059163059] confusion_matrix [[1517, 218], [170, 95]] [[1529, 206], [170, 95]] [[1524, 211], [198, 67]] [[1550, 185], [187, 78]] [[1532, 202], [177, 89]] [[1530.4, 204.4], [180.4, 84.8]] [[7652, 1022], [902, 424]] RandomForestClassifier identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ... [0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... y_pred [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... y_prob [[1.0, 0.0], [0.32, 0.68], [0.53, 0.47], [0.9,... [[0.81, 0.19], [0.88, 0.12], [1.0, 0.0], [1.0,... [[1.0, 0.0], [1.0, 0.0], [0.64, 0.36], [0.97, ... [[0.98, 0.02], [0.49, 0.51], [0.99, 0.01], [0.... [[1.0, 0.0], [0.54, 0.46], [1.0, 0.0], [0.69, ... [[1.0, 0.0], [0.32, 0.68], [0.53, 0.47], [0.9,... [[1.0, 0.0], [0.32, 0.68], [0.53, 0.47], [0.9,... brier_loss 0.107495 0.106624 0.106167 0.103634 0.10682 0.106148 0.106148 roc [0.7979500842803545, 0.7979500842803545] [0.7956076341688869, 0.7956076341688869] [0.798345930074493, 0.798345930074493] [0.8149660159860801, 0.8149660159860802] [0.7988808526506578, 0.7988808526506577] [0.8011501034320945, 0.8011501034320944] [0.8012694444763238, 0.8012694444763238] precision [0.8798511430090378, 0.3277310924369748] [0.8757861635220126, 0.30434782608695654] [0.8771097046413502, 0.3076923076923077] [0.8760460251046025, 0.3181818181818182] [0.8754598003152917, 0.29896907216494845] [0.876850567318459, 0.31138442331260113] [0.8768421052631579, 0.312] recall [0.9538904899135446, 0.1471698113207547] [0.9631123919308358, 0.10566037735849057] [0.9585014409221903, 0.12075471698113208] [0.9654178674351584, 0.10566037735849057] [0.9607843137254902, 0.10902255639097744] [0.9603413007854439, 0.11765356788196908] [0.9603412497117824, 0.11764705882352941] f1 [0.9153761061946902, 0.203125] [0.9173757891847379, 0.1568627450980392] [0.9160011016248968, 0.1734417344173442] [0.9185632026323005, 0.15864022662889515] [0.9161396755567776, 0.15977961432506887] [0.9166911750386806, 0.1703698640938695] [0.9166941784967537, 0.17086527929901427] confusion_matrix [[1655, 80], [226, 39]] [[1671, 64], [237, 28]] [[1663, 72], [233, 32]] [[1675, 60], [237, 28]] [[1666, 68], [237, 29]] [[1666.0, 68.8], [234.0, 31.2]] [[8330, 344], [1170, 156]] def summarize_metrics ( model_dict : Dict , metric_name : str = \"roc\" , pos_label : int = 1 ): \"\"\" Summarize metrics of each fold with its standard error. We also plot a boxplot to show the results. \"\"\" results = [] for model_name , model_results in model_dict . items (): result_dict = model_results . get_result ( result_name = metric_name ) tmp_score = [] for fold , metric in result_dict . items (): pos_class_score = metric [ pos_label ] results . append (( model_name , fold , pos_class_score )) tmp_score . append ( pos_class_score ) # append the Standard Error of K folds results . append (( model_name , \"SE\" , np . std ( tmp_score , ddof = 1 ) / len ( tmp_score ) ** 0.5 )) summary_df = pd . DataFrame ( results , columns = [ \"model\" , \"fold\" , metric_name ]) fig , ax = plt . subplots ( figsize = ( 15 , 8 )) _ = sns . boxplot ( x = \"model\" , y = metric_name , data = summary_df [( summary_df [ 'model' ] != 'DummyClassifier' ) & ( summary_df [ 'fold' ] != 'SE' )], ax = ax ) # fig.savefig(config.spot_checking_boxplot, format='png', dpi=300) return summary_df summary_df = summarize_metrics ( model_dict = model_dict , metric_name = \"roc\" ) display ( summary_df . tail ( 12 )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } model fold roc 6 DecisionTreeClassifier fold 1 0.616421 7 DecisionTreeClassifier fold 2 0.619879 8 DecisionTreeClassifier fold 3 0.565608 9 DecisionTreeClassifier fold 4 0.593856 10 DecisionTreeClassifier fold 5 0.609046 11 DecisionTreeClassifier SE 0.009906 12 RandomForestClassifier fold 1 0.797950 13 RandomForestClassifier fold 2 0.795608 14 RandomForestClassifier fold 3 0.798346 15 RandomForestClassifier fold 4 0.814966 16 RandomForestClassifier fold 5 0.798881 17 RandomForestClassifier SE 0.003499 model_names = [ model for model in model_dict . keys ()] fig , ax = plt . subplots ( 1 , 3 , figsize = ( 10 , 10 )) for axes , algo in zip ( ax . ravel (), model_names ): cf_mat = results_df . oof_cv [ algo ] . confusion_matrix #### scores positive_class_auroc = results_df . oof_cv [ algo ] . roc [ 1 ] #### annotations labels = [ \"True Neg\" , \"False Pos\" , \"False Neg\" , \"True Pos\" ] counts = [ \" {0:0.0f} \" . format ( value ) for value in cf_mat . flatten ()] percentages = [ \" {0:.2%} \" . format ( value ) for value in cf_mat . flatten () / np . sum ( cf_mat )] #### final annotations label = ( np . array ([ f \" { v1 } \\n { v2 } \\n { v3 } \" for v1 , v2 , v3 in zip ( labels , counts , percentages )]) ) . reshape ( 2 , 2 ) # heatmap sns . heatmap ( data = cf_mat , vmin = 0 , vmax = 330 , cmap = [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ], linewidth = 2 , linecolor = \"white\" , square = True , ax = axes , annot = label , fmt = \"\" , cbar = False , annot_kws = { \"size\" : 10 , \"color\" : \"black\" , \"weight\" : \"bold\" , \"alpha\" : 0.8 }, alpha = 1 , ) axes . text ( 0 , - 0 , \" {} \" . format ( algo ), { \"size\" : 12 , \"color\" : \"black\" , \"weight\" : \"bold\" }) axes . scatter ( 1 , 1 , s = 3500 , c = \"white\" ) axes . text ( 0.72 , 1.0 , \"AUC: {} \" . format ( np . round ( positive_class_auroc , 3 )), { \"size\" : 10 , \"color\" : \"black\" , \"weight\" : \"bold\" }, ) ## ticks and labels axes . set_xticklabels ( \"\" ) axes . set_yticklabels ( \"\" ) ## titles and text fig . text ( 0 , 1.05 , \"Out Of Fold Confusion Matrix\" , { \"size\" : 22 , \"weight\" : \"bold\" }, alpha = 1 ) fig . text ( 0 , 1 , \"\"\"This Visualization show the results of various classifiers and there respective results.\"\"\" , { \"size\" : 14 , \"weight\" : \"normal\" }, alpha = 0.98 , ) fig . tight_layout ( pad = 2.5 , w_pad = 2.5 , h_pad = 2.5 ) # fig.savefig(config.oof_confusion_matrix, format='png', dpi=300)","title":"Metrics"},{"location":"reighns_ml_journey/projects/gj_regression/DBA%203803%20Project%20-%20Hongnan/#hyperparameter","text":"# create a feature preparation pipeline for a model def make_finetuning_pipeline ( model ): \"\"\"Make a Pipeline for Training. Args: model ([type]): [description] Returns: [type]: [description] \"\"\" steps = list () # standardization steps . append (( 'standardize' , preprocessing . StandardScaler ())) # the model steps . append (( 'model' , model )) # create pipeline _pipeline = pipeline . Pipeline ( steps = steps ) return _pipeline pipeline_logistic = make_finetuning_pipeline ( linear_model . LogisticRegression ( random_state = config . seed , solver = 'liblinear' )) pipeline_decision_tree = make_finetuning_pipeline ( tree . DecisionTreeClassifier ( random_state = config . seed )) debug = True if debug : pipeline_random_forest = make_finetuning_pipeline ( ensemble . RandomForestClassifier ( n_estimators = 1 , random_state = config . seed )) else : pipeline_random_forest = make_finetuning_pipeline ( ensemble . RandomForestClassifier ( n_estimators = 100 , random_state = config . seed )) logistic_param_grid = dict ( model__C = np . logspace ( - 4 , 4 , 10 ), model__penalty = [ \"l1\" , \"l2\" ]) # https://stackoverflow.com/questions/46480457/difference-between-min-samples-split-and-min-samples-leaf-in-sklearn-decisiontre # min samples in a leaf should necessarily be odd to avoid tie in majority voting. # https://stackoverflow.com/questions/43963365/how-to-tune-sklearns-randomforest-max-depth-vs-min-samples-leaf dt_param_grid = dict ( model__criterion = [ \"gini\" , \"entropy\" ], model__max_depth = [ i for i in range ( 1 , 15 , 1 )], model__min_samples_leaf = [ 3 , 5 , 7 , 9 , 11 , 13 ], ) rf_param_grid = dict ( model__criterion = [ \"gini\" , \"entropy\" ], model__max_depth = [ i for i in range ( 1 , 15 , 1 )], model__min_samples_leaf = [ 3 , 5 , 7 , 9 , 11 , 13 ], ) grid_logistic = model_selection . GridSearchCV ( pipeline_logistic , param_grid = logistic_param_grid , cv = 5 , refit = True , verbose = 0 , scoring = \"roc_auc\" ) _ = grid_logistic . fit ( X_train . values , y_train . values ) grid_dt = model_selection . GridSearchCV ( pipeline_decision_tree , param_grid = dt_param_grid , cv = 5 , refit = True , verbose = 0 , scoring = \"roc_auc\" ) _ = grid_dt . fit ( X_train . values , y_train . values ) grid_rf = model_selection . GridSearchCV ( pipeline_random_forest , param_grid = rf_param_grid , cv = 5 , refit = True , verbose = 0 , scoring = \"roc_auc\" ) _ = grid_rf . fit ( X_train . values , y_train . values ) cv_results = pd . DataFrame ( grid_logistic . cv_results_ ) . sort_values ( \"mean_test_score\" , ascending = False ) # get the parameter names column_results = [ f \"param_ { name } \" for name in logistic_param_grid . keys ()] column_results += [ \"mean_test_score\" , \"std_test_score\" , \"rank_test_score\" ] cv_results = cv_results [ column_results ] def shorten_param ( param_name ): if \"__\" in param_name : return param_name . rsplit ( \"__\" , 1 )[ 1 ] return param_name cv_results = cv_results . rename ( shorten_param , axis = 1 ) cv_results .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } C penalty mean_test_score std_test_score rank_test_score 8 0.359381 l1 0.817990 0.007830 1 10 2.782559 l1 0.817900 0.007826 2 12 21.544347 l1 0.817868 0.007823 3 9 0.359381 l2 0.817868 0.007848 4 11 2.782559 l2 0.817867 0.007828 5 14 166.810054 l1 0.817860 0.007818 6 17 1291.549665 l2 0.817859 0.007815 7 19 10000.0 l2 0.817859 0.007815 7 15 166.810054 l2 0.817858 0.007815 9 16 1291.549665 l1 0.817858 0.007817 10 18 10000.0 l1 0.817858 0.007817 10 13 21.544347 l2 0.817857 0.007817 12 6 0.046416 l1 0.817795 0.008225 13 7 0.046416 l2 0.817775 0.008023 14 5 0.005995 l2 0.816659 0.008399 15 3 0.000774 l2 0.807662 0.006027 16 4 0.005995 l1 0.795119 0.004219 17 1 0.0001 l2 0.792937 0.002506 18 2 0.000774 l1 0.500000 0.000000 19 0 0.0001 l1 0.500000 0.000000 19 cv_results = pd . DataFrame ( grid_dt . cv_results_ ) . sort_values ( \"mean_test_score\" , ascending = False ) # get the parameter names column_results = [ f \"param_ { name } \" for name in dt_param_grid . keys ()] column_results += [ \"mean_test_score\" , \"std_test_score\" , \"rank_test_score\" ] cv_results = cv_results [ column_results ] def shorten_param ( param_name ): if \"__\" in param_name : return param_name . rsplit ( \"__\" , 1 )[ 1 ] return param_name cv_results = cv_results . rename ( shorten_param , axis = 1 ) cv_results .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } criterion max_depth min_samples_leaf mean_test_score std_test_score rank_test_score 21 gini 4 9 0.818482 0.005350 1 22 gini 4 11 0.818390 0.005198 2 23 gini 4 13 0.818338 0.005103 3 20 gini 4 7 0.818105 0.005469 4 19 gini 4 5 0.817740 0.005811 5 ... ... ... ... ... ... ... 163 entropy 14 5 0.754388 0.017172 164 79 gini 14 5 0.750641 0.016970 165 162 entropy 14 3 0.748512 0.018148 166 72 gini 13 3 0.746442 0.020769 167 78 gini 14 3 0.736971 0.025007 168 168 rows \u00d7 6 columns cv_results = pd . DataFrame ( grid_rf . cv_results_ ) . sort_values ( \"mean_test_score\" , ascending = False ) # get the parameter names column_results = [ f \"param_ { name } \" for name in rf_param_grid . keys ()] column_results += [ \"mean_test_score\" , \"std_test_score\" , \"rank_test_score\" ] cv_results = cv_results [ column_results ] def shorten_param ( param_name ): if \"__\" in param_name : return param_name . rsplit ( \"__\" , 1 )[ 1 ] return param_name cv_results = cv_results . rename ( shorten_param , axis = 1 ) cv_results .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } criterion max_depth min_samples_leaf mean_test_score std_test_score rank_test_score 19 gini 4 5 0.809695 0.008776 1 106 entropy 4 11 0.809282 0.006761 2 107 entropy 4 13 0.809282 0.006761 2 23 gini 4 13 0.808963 0.009294 4 22 gini 4 11 0.808963 0.009294 4 ... ... ... ... ... ... ... 144 entropy 11 3 0.755646 0.011515 164 156 entropy 13 3 0.743200 0.030837 165 78 gini 14 3 0.742452 0.013079 166 162 entropy 14 3 0.740631 0.022496 167 72 gini 13 3 0.734308 0.009049 168 168 rows \u00d7 6 columns","title":"Hyperparameter"},{"location":"reighns_ml_journey/projects/gj_regression/DBA%203803%20Project%20-%20Hongnan/#export-tree-viz","text":"import graphviz dot_data = tree . export_graphviz ( grid_dt . best_estimator_ [ 'model' ], out_file = None , filled = True , rounded = True , feature_names = predictor_cols , class_names = [ '0' , '1' ], special_characters = True ) graph = graphviz . Source ( dot_data ) # graph.format = \"png\" # graph.render(\"health\") fig = plt . figure ( figsize = ( 50 , 35 ), dpi = 300 ) _ = tree . plot_tree ( grid_dt . best_estimator_ [ 'model' ], filled = True , rounded = True , feature_names = predictor_cols , class_names = [ '0' , '1' ]) fig . savefig ( \"dt_plot.png\" , dpi = 300 ) # plt.show()","title":"Export Tree Viz"},{"location":"reighns_ml_journey/projects/gj_regression/DBA%203803%20Project%20-%20Hongnan/#feature-importance","text":"def get_feature_importance ( estimator : Callable , predictor_cols : List [ str ], has_feature_selection : bool = False ): \"\"\" If there is custom preprocessing feature selection step, like VIF, then we need to use different method to extract the features. predictor_cols: must be in sequence, how you pass in to train should preserve sequence. \"\"\" linear_models = [ 'LinearRegression' , 'LogisticRegression' ] tree_models = [ 'DecisionTreeClassifier' , 'RandomForestClassifier' ] estimator_name = estimator . __class__ . __name__ if has_feature_selection : pass else : if estimator_name in linear_models : coefficient = pd . DataFrame ( estimator . coef_ . flatten (), columns = [ 'Coefficients' ], index = predictor_cols ) coefficient . sort_values ( by = 'Coefficients' ) . plot ( kind = 'barh' , figsize = ( 9 , 7 )) plt . title ( f \" { estimator_name } \" ) plt . axvline ( x = 0 , color = '.5' ) plt . subplots_adjust ( left = .3 ) elif estimator_name in tree_models : coefficient = pd . DataFrame ( estimator . feature_importances_ , columns = [ 'Coefficients' ], index = predictor_cols ) coefficient . sort_values ( by = 'Coefficients' ) . plot ( kind = 'barh' , figsize = ( 9 , 7 )) plt . title ( f \" { estimator_name } \" ) plt . axvline ( x = 0 , color = '.5' ) plt . subplots_adjust ( left = .3 ) return coefficient predictor_cols = X_train . columns coef_logistic = get_feature_importance ( estimator = grid_logistic . best_estimator_ [ 'model' ], predictor_cols = predictor_cols ) coef_dt = get_feature_importance ( estimator = grid_dt . best_estimator_ [ 'model' ], predictor_cols = predictor_cols ) coef_rf = get_feature_importance ( estimator = grid_rf . best_estimator_ [ 'model' ], predictor_cols = predictor_cols )","title":"Feature Importance"},{"location":"reighns_ml_journey/projects/gj_regression/DBA%203803%20Project%20-%20Hongnan/#evaluate-performance-on-test-set","text":"def plot_roc_curve ( fpr , tpr , label = None ): \"\"\" The ROC curve, modified from Hands-On Machine learning with Scikit-Learn and TensorFlow; p.91 and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 \"\"\" plt . figure ( figsize = ( 8 , 8 )) plt . title ( 'ROC Curve' ) plt . plot ( fpr , tpr , linewidth = 2 , label = label ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], 'k--' ) plt . axis ([ - 0.005 , 1 , 0 , 1.005 ]) plt . xticks ( np . arange ( 0 , 1 , 0.05 ), rotation = 90 ) plt . xlabel ( \"False Positive Rate\" ) plt . ylabel ( \"True Positive Rate (Recall)\" ) plt . legend ( loc = 'best' ) # plt.savefig(config.roc_plot, format=\"png\", dpi=300) y_test_gt = np . asarray ( y_test ) y_test_logistic_prob = grid_logistic . predict_proba ( X_test . values )[:, 1 ] roc_auc_logistic = metrics . roc_auc_score ( y_test_gt , y_test_logistic_prob ) print ( roc_auc_logistic ) 0.81786215276976 # generate the precision recall curve fpr , tpr , roc_thresholds = metrics . roc_curve ( y_test_gt , y_test_logistic_prob , pos_label = 1 ) plot_roc_curve ( fpr , tpr , label = \"ROC Curve on Test Set using Logistic\" )","title":"Evaluate Performance on Test Set"},{"location":"reighns_ml_journey/projects/gj_regression/DBA%203803%20Project%20-%20Hongnan/#benefit-structure","text":"def adjusted_classes ( y_scores , t ): \"\"\" This function adjusts class predictions based on the prediction threshold (t). Will only work for binary classification problems. and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 \"\"\" return [ 1 if y >= t else 0 for y in y_scores ] def return_benefit_structure ( thresholds : List [ float ], y_true : np . ndarray , y_prob : np . ndarray ): columns = [ \"threshold\" , \"tp\" , \"fn\" , \"fp\" , \"tn\" , \"benefit_cost_old\" , \"benefit_cost_new\" ] benefit_cost_list = [] for t in threshold_list : y_pred_adj = adjusted_classes ( y_prob , t = t ) cm = metrics . confusion_matrix ( y_true = y_true , y_pred = y_pred_adj ) tn , fp , fn , tp = metrics . confusion_matrix ( y_true = y_true , y_pred = y_pred_adj ) . ravel () # this one check if it is correct formula benefit_cost_old = tp * 10 - fn * 10 - fp * 2 - ( tp + fp ) * 1 benefit_cost_new = tp * 100 - fn * 100 - fp * 2 - ( tp + fp ) * 1 benefit_cost_list . append ([ t , tn , fn , fp , tn , benefit_cost_old , benefit_cost_new ]) benefit_df = pd . DataFrame ( benefit_cost_list , columns = columns ) return benefit_df threshold_list : List [ float ] = [ 0.01 , 0.1 , 0.2 , 0.5 ] y_test_gt = np . asarray ( y_test ) y_test_logistic_prob = grid_logistic . predict_proba ( X_test . values )[:, 1 ] y_test_dt_prob = grid_dt . predict_proba ( X_test . values )[:, 1 ] y_test_rf_prob = grid_rf . predict_proba ( X_test . values )[:, 1 ] return_benefit_structure ( thresholds = threshold_list , y_true = y_test_gt , y_prob = y_test_logistic_prob ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } threshold tp fn fp tn benefit_cost_old benefit_cost_new 0 0.01 4622 19 4014 4622 -127 119213 1 0.10 4780 29 3856 4780 157 117697 2 0.20 5906 233 2730 5906 -341 80479 3 0.50 8636 1364 0 8636 -13640 -136400 return_benefit_structure ( thresholds = threshold_list , y_true = y_test_gt , y_prob = y_test_dt_prob ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } threshold tp fn fp tn benefit_cost_old benefit_cost_new 0 0.01 4412 16 4224 4412 -700 119180 1 0.10 5092 68 3544 5092 352 110872 2 0.20 6111 245 2525 6111 46 78706 3 0.50 8636 1364 0 8636 -13640 -136400 return_benefit_structure ( thresholds = threshold_list , y_true = y_test_gt , y_prob = y_test_rf_prob ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } threshold tp fn fp tn benefit_cost_old benefit_cost_new 0 0.01 3821 10 4815 3821 -2359 118601 1 0.10 5017 50 3619 5017 469 114229 2 0.20 5868 187 2768 5868 419 89519 3 0.50 8636 1364 0 8636 -13640 -136400","title":"Benefit Structure"},{"location":"reighns_ml_journey/projects/gj_regression/DBA%203803%20Project%20-%20Hongnan/#bias-variance-tradeoff","text":"# avg_expected_loss, avg_bias, avg_var = bias_variance_decomp( # grid.best_estimator_['model'], X_train.values, y_train.values, X_test.values, y_test.values, # loss='0-1_loss', # random_seed=123) # print('Average expected loss: %.3f' % avg_expected_loss) # print('Average bias: %.3f' % avg_bias) # print('Average variance: %.3f' % avg_var)","title":"Bias-Variance Tradeoff"},{"location":"reighns_ml_journey/projects/gj_regression/house_sales_in_king_country_usa/","text":"Quick Navigation Dependencies and Configuration Stage 4: Modelling How EDA helped us? Modelling Spot Checking Algorithms Make Basic Pipeline (Say No to Data Leakage!) Define Metrics Comparison of Cross-Validated Models Out-of-Fold Confusion Matrix Hypothesis Testing Across Models Model Selection: Hyperparameter Tuning with GridSearchCV Retrain on the whole training set Retrain using Optimal Hyperparameters Interpretation of Results Interpretation of Coefficients Interpretation of Metric Scores on Train Set Evaluation on Test Set Bias-Variance Tradeoff Dependencies and Configuration import csv import random from functools import wraps from time import time from typing import Callable , Dict , List , Union , Optional , Any import matplotlib.pyplot as plt import copy import mlxtend import numpy as np import pandas as pd import seaborn as sns from mlxtend.evaluate import paired_ttest_5x2cv , bias_variance_decomp from scipy import stats from sklearn import ( base , decomposition , dummy , ensemble , feature_selection , linear_model , metrics , model_selection , neighbors , pipeline , preprocessing , svm , tree ) from statsmodels.regression.linear_model import OLS #from statsmodels.stats.outliers_influence import variance_inflation_factor from dataclasses import dataclass import logging @dataclass class config : raw_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/regression/ %20ho use-sales-in-king-country-usa/data/raw/kc_house_data.csv\" train_size : float = 0.8 seed : int = 1992 num_folds : int = 5 cv_schema : str = \"KFold\" def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) def init_logger ( log_file : str = \"info.log\" ): \"\"\" Initialize logger. \"\"\" logger = logging . getLogger ( __name__ ) logger . setLevel ( logging . INFO ) stream_handler = logging . StreamHandler () stream_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) file_handler = logging . FileHandler ( filename = log_file ) file_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) logger . addHandler ( stream_handler ) logger . addHandler ( file_handler ) return logger # set seeding for reproducibility _ = set_seeds ( seed = config . seed ) # set logger logger = init_logger () # read data df = pd . read_csv ( config . raw_data ) How EDA helped us? Insights derived from EDA: To fill in. Cross-Validation Strategy Generalization: Ultimately, we are interested in the Generalization Error made by the model, that is, how well the model perform on unseen data that is not taken from our sample set $\\mathcal{D}$. In general, we use validation set for Model Selection and the test set for an estimate of generalization error on new data. - Refactored from Elements of Statistical Learning, Chapter 7.2 Step 1: Train-Test-Split: Since this dataset is relatively small, we will not use the train-validation-test split and only split into train and test in a ratio of 9:1, whereby the split is stratified on our target, using stratify=y parameter in train_test_split() to ensure that our target has equal representation in both train and test. We note that this is a relatively small dataset and in practice, we need a large sample size to get a reliable/stable split, it is also recommended to retrain the whole dataset (without the \"unseen\" test set) after we have done the model selection process (eg. finding best hyperparameters). Step 2: Resampling Stategy: Note that we will be performing StratifiedKFold as our resampling strategy. After our split in Step 1, we have a training set $X_{\\text{train}}$, we will then perform our resampling strategy on this $X_{\\text{train}}$. We will choose our choice of $K = 5$. The choice of $K$ is somewhat arbitrary, and is derived empirically . To recap, we have the following: Training Set ( \\(X_{\\text{train}}\\) ) : This will be further split into K validation sets during our cross-validation. This set is used to fit a particular hypothesis \\(h \\in \\mathcal{H}\\) . Validation Set ( \\(X_{\\text{val}}\\) ) : This is split from our \\(X_{\\text{train}}\\) during cross-validation. This set is used for model selection (i.e. find best hyperparameters, attempt to produce a best hypothesis \\(g \\in \\mathcal{H}\\) ). Test Set ( \\(X_{\\text{test}}\\) ) : This is an unseen test set, and we will only use it after we finish tuning our model/hypothesis. Suppose we have a final best model \\(g\\) , we will use \\(g\\) to predict on the test set to get an estimate of the generalization error (also called out-of-sample error). Courtesy of scikit-learn on a typical Cross-Validation workflow. predictor_cols = df . columns [ 3 :] . tolist () target_col = [ \"price\" ] logger . info ( f \" \\n The predictor columns are \\n { predictor_cols } \" ) 2021-11-10,12:09:44 - The predictor columns are ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15'] X = df [ predictor_cols ] . copy () y = df [ target_col ] . copy () # Split train - test X_train , X_test , y_train , y_test = model_selection . train_test_split ( X , y , train_size = config . train_size , shuffle = True , random_state = config . seed ) logger . info ( f \" \\n Shape of train: { X_train . shape } \\n Shape of test: { X_test . shape } \" ) 2021-11-10,12:09:44 - Shape of train: (17290, 18) Shape of test: (4323, 18) def make_folds ( df : pd . DataFrame , num_folds : int , cv_schema : str , seed : int , predictor_col : List , target_col : List , ) -> pd . DataFrame : \"\"\"Split the given dataframe into training folds. Args: df (pd.DataFrame): [description] num_folds (int): [description] cv_schema (str): [description] seed (int): [description] Returns: pd.DataFrame: [description] \"\"\" if cv_schema == \"KFold\" : df_folds = df . copy () kf = model_selection . KFold ( n_splits = num_folds , shuffle = True , random_state = seed ) for fold , ( train_idx , val_idx ) in enumerate ( kf . split ( X = df_folds [ predictor_col ], y = df_folds [ target_col ]) ): df_folds . loc [ val_idx , \"fold\" ] = int ( fold + 1 ) df_folds [ \"fold\" ] = df_folds [ \"fold\" ] . astype ( int ) elif cv_schema == \"StratifiedKFold\" : df_folds = df . copy () skf = model_selection . StratifiedKFold ( n_splits = num_folds , shuffle = True , random_state = seed ) for fold , ( train_idx , val_idx ) in enumerate ( skf . split ( X = df_folds [ predictor_col ], y = df_folds [ target_col ]) ): df_folds . loc [ val_idx , \"fold\" ] = int ( fold + 1 ) df_folds [ \"fold\" ] = df_folds [ \"fold\" ] . astype ( int ) print ( df_folds . groupby ([ \"fold\" , \"diagnosis\" ]) . size ()) return df_folds X_y_train = pd . concat ([ X_train , y_train ], axis = 1 ) . reset_index ( drop = True ) df_folds = make_folds ( X_y_train , num_folds = config . num_folds , cv_schema = config . cv_schema , seed = config . seed , predictor_col = predictor_cols , target_col = target_col ) Looks good! All our five folds are now in df_fold ! Modelling Spot Checking Algorithms Terminology Alert! This method is advocated by Jason Brownlee PhD and this serves as the first stage of my modelling process. We will rapidly test (spot check) different classifier algorithms, from DummyClassifier , to LinearModel to more sophisticated ensemble trees like RandomForest . I also note to the readers that we need to think of a few things when choosing the \"optimal\" machine learning algorithm: No Lunch Free Theorem intuitively says that no single optimization algorithm can work best in all situations. Therefore, spot checking can help us form a basis of which algorithm might work better in this particular scenario. Occam's Razor often appears in many Machine Learning textbook, and the narrative is that a simpler model more often times generalizes better than a complex model. This is not unfamiliar when we think of the bias-variance tradeoff, and that is why there is always a tradeoff that we must make. Make Basic Pipeline (Say No to Data Leakage!) Say No to Data Leakage: This has been emphasized throughout and we must be careful as we should never touch the test set when fitting the model. This means that preprocessing steps such as StandardScaling() should only be fitted on the training data, and then apply the same transformation (mean and std) on the test data. In other words, do not apply scaling on the whole dataset before splitting. However, it is also equally important to take note not to contaminate our validation set, which is often overlooked, resulting in over optimistic results from model selection phase, but perform badly on unseen test set. As a result, when we use a 5 fold cross validation, we should be careful during fitting that the preprocessing steps are only applied on the training folds, and not on all 5 folds. The same idea is also applied to our ReduceVIF() preprocessing step. We should also include this in our pipeline and not select the features outside the cross-validation loop. Scikit Learn's Pipeline object will prevent us from data leakage, as the steps in a pipeline is already pre-defined. There is also a lot of flexibility in this object, as you can even write custom functions in your pipeline! def variance_inflation_factor ( exog , idx_kept , vif_idx ): \"\"\"Compute VIF for one feature. Args: exog (np.ndarray): Observations idx_kept (List[int]): Indices of features to consider vif_idx (int): Index of feature for which to compute VIF Returns: float: VIF for the selected feature \"\"\" exog = np . asarray ( exog ) x_i = exog [:, vif_idx ] mask = [ col for col in idx_kept if col != vif_idx ] x_noti = exog [:, mask ] r_squared_i = OLS ( x_i , x_noti ) . fit () . rsquared vif = 1. / ( 1. - r_squared_i ) return vif class ReduceVIF ( base . BaseEstimator , base . TransformerMixin ): \"\"\"The base of the class structure is implemented in https://www.kaggle.com/ffisegydd/sklearn-multicollinearity-class; I heavily modified the class such that it can take in numpy arrays and correctly implemented the fit and transform method. \"\"\" def __init__ ( self , thresh = 10 , max_drop = 20 ): self . thresh = thresh self . max_drop = max_drop self . column_indices_kept_ = [] self . feature_names_kept_ = None def reset ( self ): \"\"\"Resets the state of predictor columns after each fold.\"\"\" self . column_indices_kept_ = [] self . feature_names_kept_ = None def fit ( self , X , y = None ): \"\"\"Fits the Recursive VIF on the training folds and save the selected feature names in self.feature_names Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" self . column_indices_kept_ , self . feature_names_kept_ = self . calculate_vif ( X ) return self def transform ( self , X , y = None ): \"\"\"Transforms the Validation Set according to the selected feature names. Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" return X [:, self . column_indices_kept_ ] def calculate_vif ( self , X : Union [ np . ndarray , pd . DataFrame ]): \"\"\"Implements a VIF function that recursively eliminates features. Args: X (Union[np.ndarray, pd.DataFrame]): [description] Returns: [type]: [description] \"\"\" feature_names = None column_indices_kept = list ( range ( X . shape [ 1 ])) if isinstance ( X , pd . DataFrame ): feature_names = X . columns dropped = True count = 0 while dropped and count <= self . max_drop : dropped = False max_vif , max_vif_col = None , None for col in column_indices_kept : vif = variance_inflation_factor ( X , column_indices_kept , col ) if max_vif is None or vif > max_vif : max_vif = vif max_vif_col = col if max_vif > self . thresh : print ( f \"Dropping { max_vif_col } with vif= { max_vif } \" ) column_indices_kept . remove ( max_vif_col ) if feature_names is not None : feature_names . pop ( max_vif_col ) dropped = True count += 1 return column_indices_kept , feature_names # create a feature preparation pipeline for a model def make_pipeline ( model ): \"\"\"Make a Pipeline for Training. Args: model ([type]): [description] Returns: [type]: [description] \"\"\" steps = list () # standardization steps . append (( 'standardize' , preprocessing . StandardScaler ())) # reduce VIF # steps.append((\"remove_multicollinearity\", ReduceVIF(thresh=10))) # the model steps . append (( 'model' , model )) # create pipeline _pipeline = pipeline . Pipeline ( steps = steps ) return _pipeline regressors = [ # baseline model dummy . DummyRegressor ( strategy = \"mean\" ), # linear model linear_model . LinearRegression ( fit_intercept = True ), linear_model . Ridge ( random_state = config . seed , alpha = 1 , fit_intercept = True ), linear_model . Lasso ( random_state = config . seed , alpha = 1 , fit_intercept = True , ), linear_model . ElasticNet ( random_state = config . seed , alpha = 1 , l1_ratio = 0.5 , fit_intercept = True , ), # tree tree . DecisionTreeRegressor ( random_state = config . seed , criterion = \"squared_error\" ), # ensemble # ensemble.RandomForestClassifier(random_state=config.seed), ] regressors = [ make_pipeline ( model ) for model in regressors ] Define Metrics default_result_names = [ \"y_true\" , \"y_pred\" , ] default_logit_names = [ \"y_true\" , \"y_pred\" , ] default_score_names = [ \"explained_variance_score\" , \"mean_squared_error\" , \"mean_absolute_error\" , \"root_mean_squared_error\" , \"r2_score\" , \"mean_absolute_percentage_error\" , ] custom_score_names = [ \"adjusted_r2\" ] def adjusted_r2 ( r2 : float , n : int , k : int ) -> float : \"\"\"Calculate adjusted R^2. Args: r2 (float): r2 score of the model/ n (int): number of samples. k (int): number of features minus the constant bias term. Returns: adjusted_r2_score (float): r2 * (n - 1) / (n - k - 1) \"\"\" adjusted_r2_score = r2 * ( n - 1 ) / ( n - k - 1 ) return adjusted_r2_score class Results : \"\"\"Stores results for model training in columnwise format.\"\"\" _result_dict : Dict logit_names : List [ str ] score_names : List [ str ] def __init__ ( self , logit_names : List [ str ] = default_logit_names , score_names : List [ str ] = default_score_names , existing_dict : Optional [ Dict ] = None , ): \"\"\"Construct a new results store.\"\"\" self . logit_names = logit_names self . score_names = score_names if existing_dict is not None : self . _result_dict = copy . deepcopy ( existing_dict ) return dict_keys = [ \"identifier\" , * logit_names , * score_names ] self . _result_dict = { key : [] for key in dict_keys } def add ( self , identifier : str , results : Dict , in_place = False ): \"\"\"Add a new results row.\"\"\" if not in_place : return Results ( self . logit_names , self . score_names , self . _result_dict ) . add ( identifier , results , in_place = True ) self . _result_dict [ \"identifier\" ] . append ( identifier ) for result_name in set ([ * results . keys (), * self . logit_names , * self . score_names ]): result_value = results . get ( result_name , np . nan ) self . _result_dict [ result_name ] . append ( result_value ) return self def get_result ( self , result_name : str ) -> Dict [ str , Any ]: \"\"\"Get a map of identifiers to result values for a result.\"\"\" return { identifier : result_value for identifier , result_value in zip ( self . _result_dict [ \"identifier\" ], self . _result_dict [ result_name ]) } def get_result_values ( self , result_name : str ) -> List [ Any ]: \"\"\"Get a list of values for a result.\"\"\" return self . _result_dict [ result_name ] def to_dataframe ( self ) -> pd . DataFrame : \"\"\"Get a Data Frame containing the results.\"\"\" return pd . DataFrame . from_dict ( self . _result_dict ) def to_dict ( self ) -> Dict : \"\"\"Get a dictionary containing the results. Returns: Dict[str, List[Any]]: Dictionary of result columns \"\"\" return self . _result_dict def compute_metrics ( logits : Dict [ str , np . ndarray ]) -> Dict [ str , Any ]: \"\"\"Compute metrics from logits.\"\"\" y_true , y_pred = logits [ \"y_true\" ], logits [ \"y_pred\" ] default_score_names = [ \"explained_variance_score\" , \"mean_squared_error\" , \"mean_absolute_error\" , \"root_mean_squared_error\" , \"r2_score\" , \"mean_absolute_percentage_error\" , ] default_metrics_dict : Dict [ str , float ] = {} custom_metrics_dict : Dict [ str , float ] = {} for metric_name in default_score_names : if hasattr ( metrics . _regression , metric_name ): # TODO: get metric score with default parameters, consider adding kwargs if you want to configure parameters metric_score = getattr ( metrics . _regression , metric_name )( y_true , y_pred ) else : # logger.info(f\"{metrics._regression} has no such attribute {metric_name}!\") # add custom metrics here rmse = metrics . _regression . mean_squared_error ( y_true , y_pred , squared = False ) custom_metrics_dict [ \"root_mean_squared_error\" ] = rmse if metric_name not in default_metrics_dict : default_metrics_dict [ metric_name ] = metric_score metrics_dict = { ** default_metrics_dict , ** custom_metrics_dict } return metrics_dict def mean_score ( score_values ) -> Union [ float , np . ndarray ]: \"\"\"Compute the mean score.\"\"\" score_values = np . array ( score_values ) shape = score_values . shape if len ( shape ) == 1 : return score_values . mean () return score_values . mean ( axis = 0 ) def mean_cv_results ( model_results : Results ) -> Dict : \"\"\"Add mean cross-validation results. This method computes the mean value for all score types in the model_results, including for scores (e.g., confusion matrix) where the mean value may contain decimal places. \"\"\" cv_logits = { y_result : np . concatenate ( model_results . get_result_values ( y_result )) for y_result in model_results . logit_names } cv_scores = { score : mean_score ( model_results . get_result_values ( score ) ) for score in model_results . score_names } return { ** cv_logits , ** cv_scores , } def oof_cv_results ( model_results : Results ) -> Dict : \"\"\"Add OOF cross-validation results.\"\"\" cv_logits = { y_result : np . concatenate ( model_results . get_result_values ( y_result ) ) for y_result in model_results . logit_names } cv_scores = compute_metrics ( cv_logits ) return { ** cv_logits , ** cv_scores , } def add_cv_results ( model_results : Results ): \"\"\"Add cross-validation results. This method returns a copy of the given model results with summary columns for mean and CV cross-validation. \"\"\" mean_cv = mean_cv_results ( model_results ) oof_cv = oof_cv_results ( model_results ) return ( model_results . add ( \"mean_cv\" , mean_cv ) . add ( \"oof_cv\" , oof_cv ) ) def train_on_fold ( df_folds : pd . DataFrame , models : List [ Callable ], num_folds : int , predictor_col : List , target_col : List , ) -> Dict [ str , List ]: \"\"\"Take in a dataframe with fold number as column, and a models which holds a list of callable models, we will loop through and return a dictionary of cv results. Args: df_folds (pd.DataFrame): [description] model (Callable): [description] num_folds (int): [description] predictor_col (List): [description] target_col (List): [description] Returns: Dict[str, List]: [description] \"\"\" y_true = df_folds [ target_col ] . values . flatten () # test_pred_arr: np.ndarray = np.zeros(len(X_test)) model_dict = {} for model in models : model_results = Results () if isinstance ( model , pipeline . Pipeline ): model_name = model [ \"model\" ] . __class__ . __name__ else : model_name = model . __class__ . __name__ # out-of-fold validation predictions oof_pred_arr : np . ndarray = np . zeros ( len ( df_folds )) for fold in range ( 1 , num_folds + 1 ): train_df = df_folds [ df_folds [ \"fold\" ] != fold ] . reset_index ( drop = True ) val_df = df_folds [ df_folds [ \"fold\" ] == fold ] . reset_index ( drop = True ) val_idx = df_folds [ df_folds [ \"fold\" ] == fold ] . index . values X_train , y_train = train_df [ predictor_col ] . values , train_df [ target_col ] . values X_val , y_val = val_df [ predictor_col ] . values , val_df [ target_col ] . values model . fit ( X_train , y_train ) y_val_pred = model . predict ( X_val ) logits = { \"y_true\" : y_val , \"y_pred\" : y_val_pred , } metrics = compute_metrics ( logits ) model_results . add ( f \"fold { fold } \" , { ** logits , ** metrics }, in_place = True ) if model_name not in model_dict : model_dict [ model_name ] = model_results return model_dict model_dict = train_on_fold ( df_folds , models = regressors , num_folds = 5 , predictor_col = predictor_cols , target_col = target_col ) 2021-11-10,12:09:44 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:44 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:44 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:44 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:44 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.676e+13, tolerance: 1.868e+11 model = cd_fast.enet_coordinate_descent( 2021-11-10,12:09:46 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+13, tolerance: 1.862e+11 model = cd_fast.enet_coordinate_descent( 2021-11-10,12:09:48 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.603e+13, tolerance: 1.760e+11 model = cd_fast.enet_coordinate_descent( 2021-11-10,12:09:49 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.718e+13, tolerance: 1.897e+11 model = cd_fast.enet_coordinate_descent( 2021-11-10,12:09:50 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.810e+13, tolerance: 1.913e+11 model = cd_fast.enet_coordinate_descent( 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:53 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:53 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! model_dict_with_summary = { model : add_cv_results ( model_results ) for model , model_results in model_dict . items () } 2021-11-10,12:19:12 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:19:12 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:19:12 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:19:12 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:19:12 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:19:12 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! results_df = pd . concat ({ name : results . to_dataframe () . T for name , results in model_dict_with_summary . items () }, axis = 0 ) results_df . columns = [ 'fold 1' , 'fold 2' , 'fold 3' , 'fold 4' , 'fold 5' , 'mean_cv' , 'oof_cv' ] results_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv DummyRegressor identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [[945000.0], [352500.0], [560000.0], [500000.0... [[850000.0], [653000.0], [532000.0], [385000.0... [[1274950.0], [392137.0], [850000.0], [520000.... [[450000.0], [495000.0], [395000.0], [280000.0... [[754999.0], [588500.0], [525000.0], [525000.0... [[945000.0], [352500.0], [560000.0], [500000.0... [[945000.0], [352500.0], [560000.0], [500000.0... y_pred [541073.7741469058, 541073.7741469058, 541073.... [539415.1803788317, 539415.1803788317, 539415.... [536694.9069548872, 536694.9069548872, 536694.... [539857.5267495662, 539857.5267495662, 539857.... [540990.9316078658, 540990.9316078658, 540990.... [541073.7741469058, 541073.7741469058, 541073.... [541073.7741469058, 541073.7741469058, 541073.... explained_variance_score -0.0 0.0 0.0 0.0 -0.0 -0.0 -0.000169 mean_squared_error 132247512137.06163 133951959536.310318 163296091978.429077 123686085780.066254 119223815039.600311 134481092894.293533 134481092894.293488 mean_absolute_error 229404.508989 232306.56185 241944.838637 232308.320526 230976.128917 233388.071784 233388.071784 root_mean_squared_error 363658.510332 365994.480199 404099.111578 351690.326538 345288.017515 366146.089233 366716.63842 r2_score -0.000407 -0.000007 -0.0013 -0.000013 -0.000402 -0.000426 -0.000169 mean_absolute_percentage_error 0.542257 0.530016 0.531438 0.534771 0.540719 0.53584 0.53584 LinearRegression identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [[945000.0], [352500.0], [560000.0], [500000.0... [[850000.0], [653000.0], [532000.0], [385000.0... [[1274950.0], [392137.0], [850000.0], [520000.... [[450000.0], [495000.0], [395000.0], [280000.0... [[754999.0], [588500.0], [525000.0], [525000.0... [[945000.0], [352500.0], [560000.0], [500000.0... [[945000.0], [352500.0], [560000.0], [500000.0... y_pred [[667876.5428875468], [614410.8399372244], [73... [[694869.3745167988], [618367.5827104518], [46... [[1023189.3043934056], [304317.4518826463], [9... [[477332.04737439007], [492396.46923110134], [... [[853190.5779001702], [574292.3295129627], [77... [[667876.5428875468], [614410.8399372244], [73... [[667876.5428875468], [614410.8399372244], [73... explained_variance_score 0.680095 0.705119 0.685252 0.693963 0.721335 0.697153 0.696083 mean_squared_error 42290033057.306816 39515957195.400787 51430470051.222527 37873842125.074295 33210290838.463482 40864118653.493584 40864118653.493584 mean_absolute_error 127943.982537 124266.336776 130373.471382 126075.417537 122514.978064 126234.837259 126234.837259 root_mean_squared_error 205645.406118 198786.209772 226782.869836 194612.029754 182236.908552 201612.684806 202148.753777 r2_score 0.68009 0.704997 0.684638 0.693787 0.721334 0.696969 0.696083 mean_absolute_percentage_error 0.265815 0.257898 0.250641 0.251933 0.255471 0.256352 0.256352 Ridge identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [[945000.0], [352500.0], [560000.0], [500000.0... [[850000.0], [653000.0], [532000.0], [385000.0... [[1274950.0], [392137.0], [850000.0], [520000.... [[450000.0], [495000.0], [395000.0], [280000.0... [[754999.0], [588500.0], [525000.0], [525000.0... [[945000.0], [352500.0], [560000.0], [500000.0... [[945000.0], [352500.0], [560000.0], [500000.0... y_pred [[667923.954879363], [614423.8176756387], [731... [[694094.2870407818], [617981.9933915635], [46... [[1023626.0773182933], [304085.70358412666], [... [[477416.1959023318], [492474.1026423527], [30... [[853485.3791880516], [574203.5276395974], [77... [[667923.954879363], [614423.8176756387], [731... [[667923.954879363], [614423.8176756387], [731... explained_variance_score 0.680109 0.70512 0.685333 0.693972 0.721308 0.697168 0.696102 mean_squared_error 42288266590.795799 39515834657.582001 51417532361.73008 37872817115.509499 33213482338.320721 40861586612.787613 40861586612.787621 mean_absolute_error 127934.770004 124362.619797 130426.951817 126064.678687 122549.911589 126267.786379 126267.786379 root_mean_squared_error 205641.11114 198785.901556 226754.343645 194609.396267 182245.6648 201607.283482 202142.490864 r2_score 0.680104 0.704998 0.684718 0.693795 0.721307 0.696984 0.696102 mean_absolute_percentage_error 0.265796 0.258152 0.250808 0.251901 0.255562 0.256444 0.256444 Lasso identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [[945000.0], [352500.0], [560000.0], [500000.0... [[850000.0], [653000.0], [532000.0], [385000.0... [[1274950.0], [392137.0], [850000.0], [520000.... [[450000.0], [495000.0], [395000.0], [280000.0... [[754999.0], [588500.0], [525000.0], [525000.0... [[945000.0], [352500.0], [560000.0], [500000.0... [[945000.0], [352500.0], [560000.0], [500000.0... y_pred [667918.069528075, 614415.1560338883, 731173.9... [694081.9059207196, 617992.7838152755, 463279.... [1023674.2634054194, 304066.4170321575, 955604... [477415.1327845714, 492463.7396309793, 305057.... [853505.6953654164, 574205.7699892861, 771097.... [667918.069528075, 614415.1560338883, 731173.9... [667918.069528075, 614415.1560338883, 731173.9... explained_variance_score 0.680106 0.70512 0.685338 0.693969 0.721305 0.697168 0.696102 mean_squared_error 42288598880.705284 39515796822.947304 51416606662.838509 37873203587.611603 33213918899.943615 40861624970.809258 40861624970.809265 mean_absolute_error 127937.742862 124365.678646 130429.153983 126067.441635 122553.005403 126270.604506 126270.604506 root_mean_squared_error 205641.919075 198785.806392 226752.302442 194610.389208 182246.862524 201607.455928 202142.585743 r2_score 0.680101 0.704998 0.684723 0.693792 0.721303 0.696984 0.696102 mean_absolute_percentage_error 0.265806 0.258161 0.250816 0.25191 0.255571 0.256453 0.256453 ElasticNet identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [[945000.0], [352500.0], [560000.0], [500000.0... [[850000.0], [653000.0], [532000.0], [385000.0... [[1274950.0], [392137.0], [850000.0], [520000.... [[450000.0], [495000.0], [395000.0], [280000.0... [[754999.0], [588500.0], [525000.0], [525000.0... [[945000.0], [352500.0], [560000.0], [500000.0... [[945000.0], [352500.0], [560000.0], [500000.0... y_pred [656773.6005291657, 614707.2479590292, 745914.... [685212.4217947914, 599436.3647276227, 483018.... [907890.7497500202, 369129.65691215475, 862824... [469736.2740988735, 539209.0261494196, 379320.... [804965.135403886, 571735.3944930851, 737716.4... [656773.6005291657, 614707.2479590292, 745914.... [656773.6005291657, 614707.2479590292, 745914.... explained_variance_score 0.65603 0.673131 0.638955 0.670126 0.695926 0.666834 0.664827 mean_squared_error 45475490348.208717 43800728719.55619 59005250118.409103 40812502573.376862 36239917129.190384 45066777777.748253 45066777777.748253 mean_absolute_error 124419.180393 122248.905619 131347.081356 124722.490988 120613.606068 124670.252885 124670.252885 root_mean_squared_error 213249.830828 209286.236336 242909.962987 202021.044877 190367.846889 211566.984383 212289.372739 r2_score 0.655993 0.673009 0.638191 0.670027 0.695912 0.666627 0.664827 mean_absolute_percentage_error 0.247878 0.245323 0.243383 0.244499 0.243172 0.244851 0.244851 DecisionTreeRegressor identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [[945000.0], [352500.0], [560000.0], [500000.0... [[850000.0], [653000.0], [532000.0], [385000.0... [[1274950.0], [392137.0], [850000.0], [520000.... [[450000.0], [495000.0], [395000.0], [280000.0... [[754999.0], [588500.0], [525000.0], [525000.0... [[945000.0], [352500.0], [560000.0], [500000.0... [[945000.0], [352500.0], [560000.0], [500000.0... y_pred [915000.0, 300000.0, 316000.0, 643000.0, 28200... [535000.0, 630000.0, 410000.0, 335000.0, 72000... [1605000.0, 479000.0, 850000.0, 614000.0, 6505... [449000.0, 555000.0, 440000.0, 247500.0, 35000... [715000.0, 545000.0, 597000.0, 530000.0, 50000... [915000.0, 300000.0, 316000.0, 643000.0, 28200... [915000.0, 300000.0, 316000.0, 643000.0, 28200... explained_variance_score 0.752389 0.756145 0.770122 0.743783 0.767617 0.758011 0.758577 mean_squared_error 32751471474.735687 32664895113.423656 37510896623.824608 31693198492.168594 27707113797.312031 32465515100.292915 32465515100.292915 mean_absolute_error 101035.96819 99810.482071 105829.744939 101019.912377 96233.899075 100786.00133 100786.00133 root_mean_squared_error 180973.676193 180734.321902 193677.300229 178025.836586 166454.539732 179973.134928 180181.894485 r2_score 0.752246 0.756143 0.769991 0.743758 0.767511 0.75793 0.758546 mean_absolute_percentage_error 0.192566 0.186579 0.189517 0.183729 0.184638 0.187406 0.187406 Comparison of Cross-Validated Models The point of the following comparison is to check how different models are performing across folds. More specifically, if we have 5 folds, we will have a metric score for each fold, subsequently, we can find the standard error of model's performance. We need to be aware of models that have high variance across folds in terms of the metrics performance. This can indicate that the model is highly unstable, and may be a sign of overfitting. def summarize_metrics ( metric_name ): ls = [] for model_name , inner_dict in model_dict . items (): folds = inner_dict [ \"identifier\" ][: - 2 ] all_obs = [] for idx , obs in enumerate ( inner_dict [ metric_name ][: - 2 ]): ls . append (( model_name , folds [ idx ], obs )) all_obs . append ( obs ) ls . append (( model_name , \"SE\" , np . std ( all_obs , ddof = 1 ) / len ( all_obs ) ** 0.5 )) fig , ax = plt . subplots ( figsize = ( 15 , 8 )) summary_df = pd . DataFrame ( ls , columns = [ \"model\" , \"fold\" , metric_name ]) # summary_df.to_csv _ = sns . boxplot ( x = \"model\" , y = metric_name , data = summary_df [( summary_df [ 'model' ] != 'DummyClassifier' ) & ( summary_df [ 'fold' ] != 'SE' )], ax = ax ) fig . savefig ( config . spot_checking_boxplot , format = 'png' , dpi = 300 ) return summary_df summary_df = summarize_metrics ( \"roc\" ) display ( summary_df . tail ( 12 )) Out-of-Fold Confusion Matrix We do have information on the performance of each folds, we now look at the performance of all 5 folds together. Typicall there are two ways to do it, one is to simply take the average of the score of five folds, the other is to take a look at out of folds predictions. From the confusion matrix of the out of fold performance, Logistic Regression does seem to be a model we can explore on, although slightly lower in terms of overall AUROC score than SVC, it seems to have the quite low False Negatives amongst all. With further hyperparameter tuning and threshold optimization, we can make it better. model_names = [ model for model in model_dict . keys ()] fig , ax = plt . subplots ( 2 , 3 , figsize = ( 10 , 10 )) for axes , algo in zip ( ax . ravel (), model_names ): cf_mat = results_df . oof_cv [ algo ] . confusion_matrix #### scores auc = results_df . oof_cv [ algo ] . roc #### annotations labels = [ \"True Neg\" , \"False Pos\" , \"False Neg\" , \"True Pos\" ] counts = [ \" {0:0.0f} \" . format ( value ) for value in cf_mat . flatten ()] percentages = [ \" {0:.2%} \" . format ( value ) for value in cf_mat . flatten () / np . sum ( cf_mat )] #### final annotations label = ( np . array ([ f \" { v1 } \\n { v2 } \\n { v3 } \" for v1 , v2 , v3 in zip ( labels , counts , percentages )]) ) . reshape ( 2 , 2 ) # heatmap sns . heatmap ( data = cf_mat , vmin = 0 , vmax = 330 , cmap = [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ], linewidth = 2 , linecolor = \"white\" , square = True , ax = axes , annot = label , fmt = \"\" , cbar = False , annot_kws = { \"size\" : 10 , \"color\" : \"black\" , \"weight\" : \"bold\" , \"alpha\" : 0.8 }, alpha = 1 , ) axes . text ( 0 , - 0 , \" {} \" . format ( algo ), { \"size\" : 12 , \"color\" : \"black\" , \"weight\" : \"bold\" }) axes . scatter ( 1 , 1 , s = 3500 , c = \"white\" ) axes . text ( 0.72 , 1.0 , \"AUC: {} \" . format ( round ( auc , 3 )), { \"size\" : 10 , \"color\" : \"black\" , \"weight\" : \"bold\" }, ) ## ticks and labels axes . set_xticklabels ( \"\" ) axes . set_yticklabels ( \"\" ) ## titles and text fig . text ( 0 , 1.05 , \"Out Of Fold Confusion Matrix\" , { \"size\" : 22 , \"weight\" : \"bold\" }, alpha = 1 ) fig . text ( 0 , 1 , \"\"\"This Visualization show the results of various classifiers and there respective results.\"\"\" , { \"size\" : 14 , \"weight\" : \"normal\" }, alpha = 0.98 , ) fig . tight_layout ( pad = 2.5 , w_pad = 2.5 , h_pad = 2.5 ) fig . savefig ( config . oof_confusion_matrix , format = 'png' , dpi = 300 ) Hypothesis Testing Across Models I am slightly shocked at the performance of plain LogisticRegression, I decide to use an idea from Hypothesis Testing Across Models to check if the difference is really by chance or not. Note that I will be modifying his code as his code does not split using StratifiedKFold. The basic idea is to test if two model's difference in scores (in this case roc), is statistically significant or not. However, we note that this method may violate an assumption of Student's t test. Null Hypothesis \\(H_0\\) : The difference in the performance score of two classifiers is Statistically Significant. Alternate Hypothesis \\(H_1\\) : The difference in the performance score of two classifiers is not Statistically Significant. def paired_ttest_skfold_cv ( estimator1 , estimator2 , X , y , cv = 10 , scoring = None , shuffle = False , random_seed = None ): \"\"\"Modified from https://github.com/rasbt/mlxtend/blob/master/mlxtend/evaluate/ttest.py to accomodate StratifiedKFold\"\"\" if not shuffle : skf = model_selection . StratifiedKFold ( n_splits = cv , shuffle = shuffle ) else : skf = model_selection . StratifiedKFold ( n_splits = cv , random_state = random_seed , shuffle = shuffle ) if scoring is None : if estimator1 . _estimator_type == \"classifier\" : scoring = \"accuracy\" elif estimator1 . _estimator_type == \"regressor\" : scoring = \"r2\" else : raise AttributeError ( \"Estimator must \" \"be a Classifier or Regressor.\" ) if isinstance ( scoring , str ): scorer = metrics . get_scorer ( scoring ) else : scorer = scoring score_diff = [] for train_index , test_index in skf . split ( X = X , y = y ): X_train , X_test = X [ train_index ], X [ test_index ] y_train , y_test = y [ train_index ], y [ test_index ] estimator1 . fit ( X_train , y_train ) estimator2 . fit ( X_train , y_train ) est1_score = scorer ( estimator1 , X_test , y_test ) est2_score = scorer ( estimator2 , X_test , y_test ) score_diff . append ( est1_score - est2_score ) avg_diff = np . mean ( score_diff ) numerator = avg_diff * np . sqrt ( cv ) denominator = np . sqrt ( sum ([( diff - avg_diff ) ** 2 for diff in score_diff ]) / ( cv - 1 )) t_stat = numerator / denominator pvalue = stats . t . sf ( np . abs ( t_stat ), cv - 1 ) * 2.0 return float ( t_stat ), float ( pvalue ) # check if difference between algorithms is real X_tmp = X_y_train [ predictor_cols ] . values y_tmp = X_y_train [ 'diagnosis' ] . values t , p = paired_ttest_skfold_cv ( estimator1 = classifiers [ 1 ], estimator2 = classifiers [ - 1 ], shuffle = True , cv = 5 , X = X_tmp , y = y_tmp , scoring = 'roc_auc' , random_seed = config . seed ) print ( 'P-value: %.3f , t-Statistic: %.3f ' % ( p , t )) Since P value is quite high, and more the basic threshold of 0.05 or 0.1, we fail to reject the null hypothesis, and say that there is no significant difference between these two models. Model Selection: Hyperparameter Tuning with GridSearchCV Hyperparameter Tuning: We have done a quick spot checking on algorithms and realized that LogisticRegression is doing well for this task. For this purpose, I will just perform hyperparameter tuning on this single algorithm. However, in practice and if resources are allowed, I will also tune other models such as RandomForest() , or gradient boosting algorithms such as XGBoost , as I believe they will perform no worse than our Logistic Regression model given the right hyperparameters. Grid Search: We will use an old-fashioned way to search for hyperparameters, which is brute force method. The time complexity of Grid Search is high and if you have many hyperparameters to tune, I recommend trying out Random Grid Search or libraries like Optuna that uses Bayesian Optimization. def make_finetuning_pipeline ( model ): \"\"\"Make a Pipeline for Training. Args: model ([type]): [description] Returns: [type]: [description] \"\"\" steps = list () # standardization steps . append (( 'standardize' , preprocessing . StandardScaler ())) # reduce VIF steps . append (( 'remove_multicollinearity' , ReduceVIF ( thresh = 10 ))) # the model steps . append (( 'model' , model )) # create pipeline _pipeline = pipeline . Pipeline ( steps = steps ) return _pipeline Reconstruct our pipeline but now only taking in LogisticRegression . pipeline_logistic = make_finetuning_pipeline ( linear_model . LogisticRegression ( solver = \"saga\" , random_state = config . seed , max_iter = 10000 , n_jobs = None , fit_intercept = True ) ) Define our search space for the hyperparameters: param_grid = { model__penalty = [ \"l1\" , \"l2\" ], model__C = np . logspace ( - 4 , 4 , 10 )} param_grid = dict ( model__penalty = [ \"l1\" , \"l2\" ], model__C = np . logspace ( - 4 , 4 , 10 ), ) Run our hyperparameter search with cross-validation. For example, our param_grid has \\(2 \\times 10 = 20\\) combinations, and our cross validation has 5 folds, then there will be a total of 100 fits. Below details the pseudo code of what happens under the hood: Define \\(G\\) as the set of combination of hyperparamters. Define number of splits to be \\(K\\) . For each set of hyperparameter \\(z \\in Z\\) : for fold \\(j\\) in K: Set \\(F_{\\text{train}}=\\bigcup\\limits_{i\\neq k}^{K} F_{i}\\) Set \\(F_{\\text{val}} = F_{j}\\) as the validation set Perform Standard Scaling on \\(F_{\\text{train}}\\) and find the mean and std Perform VIF recursively on \\(F_{\\text{train}}\\) and find the selected features Transform \\(F_{\\text{val}}\\) using the mean and std found using \\(F_{\\text{train}}\\) Transform \\(F_{\\text{val}}\\) to have only the selected features from \\(F_{\\text{train}}\\) Train and fit on \\(F_{\\text{train}}\\) Evaluate the fitted parameters on \\(F_{\\text{val}}\\) to obtain \\(\\mathcal{M}\\) grid = model_selection . GridSearchCV ( pipeline_logistic , param_grid = param_grid , cv = 5 , refit = True , verbose = 3 , scoring = \"roc_auc\" ) _ = grid . fit ( X_train , y_train ) We can save our results in a dataframe, we will also look at the top performing hyperparameter by querying the below: grid_cv_df = pd . DataFrame ( grid . cv_results_ ) grid_cv_df . loc [ grid_cv_df [ 'rank_test_score' ] == 1 ] grid_cv_df = pd . DataFrame ( grid . cv_results_ ) best_cv = grid_cv_df . loc [ grid_cv_df [ 'rank_test_score' ] == 1 ] display ( best_cv ) best_hyperparams = grid . best_params_ print ( f \"Best Hyperparameters found is { best_hyperparams } \" ) Our best performing set of hyperparameters {'model__C': 0.3593813663804626, 'model__penalty': 'l2'} gives rise to a mean cross validation score of \\(0.988739\\) , which is higher than the model with default hyperparameter scoring, \\(0.987136\\) . Room for Improvement: Apart from the other methods to search for the optimal hyperparameters, we can also include preprocessing step as a tunable hyperparameter. More specifically, in our ReduceVIF() step, we hard coded two manual criterion in which the algorithm will stop; if the threshold reaches 10, or if the number of features removed hit 20; we can include them in the search space. Retrain on the whole training set A common practice after the hyperparameter tuning phase is to retrain the model on the whole dataset \\(X_{\\text{train}}\\) where we will get the estimator's coefficients obtained from the retraining. This is actually already done as the scikit-learn's GridSearchCV has a parameter refit ; if we select it to be true, then after the model selection process is done (i.e. getting the best hyperparameters after cross validation with grid search), the grid search object will retrain on the whole \\(X_{\\text{train}}\\) with the best hyperparameters internally, and return us back an object in which we can call predict etc. Retrain using optimal hyperparameters However, to be extra careful, we can retrain manually using the best hyperparameters and check if scikit-learn is true to its documentation. We will just reconstruct the pipeline using the grid's best hyper parameters. We will then test if the retrained model's coefficients coincide with the grid's best estimator's coefficients. If there difference is 0, this means they are trained under the same circumstances and we can be sure that the refit parameter is behaving true to its words. grid_best_hyperparams = grid . best_params_ print ( grid_best_hyperparams ) -> { 'model__C' : 0.3593813663804626 , 'model__penalty' : 'l2' } retrain_pipeline = pipeline . Pipeline ( [ ( \"standardize\" , preprocessing . StandardScaler ()), ( 'remove_multicollinearity' , ReduceVIF ( thresh = 10 )), ( \"model\" , linear_model . LogisticRegression ( C = 0.3593813663804626 , max_iter = 10000 , random_state = 1992 , solver = \"saga\" , penalty = \"l1\" ), ), ] ) _ = retrain_pipeline . fit ( X_train , y_train ) coef_diff = retrain_pipeline [ 'model' ] . coef_ - grid . best_estimator_ [ 'model' ] . coef_ print ( \"...\" ) assert np . all ( coef_diff == 0 ) == True print ( \"Retraining Assertion Passed!\" ) Interpretation of Results Interpretation of Coefficients As shown in the figure below, all else being equal, for every square unit increase in mean cell area, the odds of the tumor being malignant increases by a factor of \\(e^{1.43} = 4.19\\) . The variation (standard error) of the characteristics of cells also are deemed important by the model, for example, area se played an important role in determining whether a cell is malignant; intuitively, if some cells are noticably larger than the rest, then it is also a good indicator of malignancy. selected_features_by_vif_index = grid . best_estimator_ [ 'remove_multicollinearity' ] . column_indices_kept_ selected_feature_names = np . asarray ( predictor_cols )[ selected_features_by_vif_index ] selected_features_coefficients = grid . best_estimator_ [ 'model' ] . coef_ . flatten () # assertion #assert grid.best_estimator_['remove_multicollinearity'].feature_names_ == retrain_pipeline['remove_multicollinearity'].feature_names_ fig , ax = plt . subplots ( figsize = ( 15 , 15 )) # .abs() _ = pd . Series ( selected_features_coefficients , index = selected_feature_names ) . sort_values () . plot ( ax = ax , kind = 'barh' ) fig . savefig ( config . feature_importance , format = \"png\" , dpi = 300 ) Interpretation of Metric Scores on Train Set We are also interested in choosing an optimal threshold for the model such that it gives the lowest recall, or False Negatives. We note that the default threshold when calling predict() from a model is \\(0.5\\) . In this section, we will explore one way to get the best tradeoff we can when choosing a high recall, while maintaining a reasonable score for precision. def evaluate_train_test_set ( estimator : Callable , X : Union [ pd . DataFrame , np . ndarray ], y : Union [ pd . DataFrame , np . ndarray ] ) -> Dict [ str , Union [ float , np . ndarray ]]: \"\"\"This function takes in X and y and returns a dictionary of scores. Args: estimator (Callable): [description] X (Union[pd.DataFrame, np.ndarray]): [description] y (Union[pd.DataFrame, np.ndarray]): [description] Returns: Dict[str, Union[float, np.ndarray]]: [description] \"\"\" test_results = {} y_pred = estimator . predict ( X ) # This is the probability array of class 1 (malignant) y_prob = estimator . predict_proba ( X )[:, 1 ] test_brier = metrics . brier_score_loss ( y , y_prob ) test_roc = metrics . roc_auc_score ( y , y_prob ) test_results [ \"brier\" ] = test_brier test_results [ \"roc\" ] = test_roc test_results [ \"y\" ] = np . asarray ( y ) . flatten () test_results [ \"y_pred\" ] = y_pred . flatten () test_results [ \"y_prob\" ] = y_prob . flatten () return test_results def plot_precision_recall_vs_threshold ( precisions , recalls , thresholds ): \"\"\" Modified from: Hands-On Machine learning with Scikit-Learn and TensorFlow; p.89 and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 \"\"\" plt . figure ( figsize = ( 8 , 8 )) plt . title ( \"Precision and Recall Scores as a function of the decision threshold\" ) plt . plot ( thresholds , precisions [: - 1 ], \"b--\" , label = \"Precision\" ) plt . plot ( thresholds , recalls [: - 1 ], \"g-\" , label = \"Recall\" ) plt . ylabel ( \"Score\" ) plt . xlabel ( \"Decision Threshold\" ) plt . legend ( loc = 'best' ) plt . savefig ( config . precision_recall_threshold_plot , format = \"png\" , dpi = 300 ) def plot_roc_curve ( fpr , tpr , label = None ): \"\"\" The ROC curve, modified from Hands-On Machine learning with Scikit-Learn and TensorFlow; p.91 and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 \"\"\" plt . figure ( figsize = ( 8 , 8 )) plt . title ( 'ROC Curve' ) plt . plot ( fpr , tpr , linewidth = 2 , label = label ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], 'k--' ) plt . axis ([ - 0.005 , 1 , 0 , 1.005 ]) plt . xticks ( np . arange ( 0 , 1 , 0.05 ), rotation = 90 ) plt . xlabel ( \"False Positive Rate\" ) plt . ylabel ( \"True Positive Rate (Recall)\" ) plt . legend ( loc = 'best' ) plt . savefig ( config . roc_plot , format = \"png\" , dpi = 300 ) def adjusted_classes ( y_scores , t ): \"\"\" This function adjusts class predictions based on the prediction threshold (t). Will only work for binary classification problems. and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 \"\"\" return [ 1 if y >= t else 0 for y in y_scores ] The plots below show the tradeoffs between precision and recall, recall and false positive rate. The confusion matrix on the train set tells us that there is still more false negatives than false positives. We can choose a particular threshold in order to minimize false negatives, at some expense of false positive. train_results = evaluate_train_test_set ( grid , X_train , y_train ) fig , ax = plt . subplots ( figsize = ( 10 , 10 )) # CM cm_train = metrics . confusion_matrix ( train_results [ 'y' ], train_results [ 'y_pred' ]) #### scores auc = metrics . roc_auc_score ( train_results [ 'y' ], train_results [ 'y_prob' ]) #### annotations labels = [ \"True Neg\" , \"False Pos\" , \"False Neg\" , \"True Pos\" ] counts = [ \" {0:0.0f} \" . format ( value ) for value in cm_train . flatten ()] percentages = [ \" {0:.2%} \" . format ( value ) for value in cm_train . flatten () / np . sum ( cm_train )] #### final annotations label = ( np . array ([ f \" { v1 } \\n { v2 } \\n { v3 } \" for v1 , v2 , v3 in zip ( labels , counts , percentages )]) ) . reshape ( 2 , 2 ) # heatmap sns . heatmap ( data = cm_train , vmin = 0 , vmax = 330 , cmap = [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ], linewidth = 2 , linecolor = \"white\" , square = True , ax = ax , annot = label , fmt = \"\" , cbar = False , annot_kws = { \"size\" : 10 , \"color\" : \"black\" , \"weight\" : \"bold\" , \"alpha\" : 0.8 }, alpha = 1 , ) ax . scatter ( 1 , 1 , s = 3500 , c = \"white\" ) ax . text ( 0.72 , 1.0 , \"AUC: {} \" . format ( round ( auc , 3 )), { \"size\" : 10 , \"color\" : \"black\" , \"weight\" : \"bold\" }, ) ## ticks and labels ax . set_xticklabels ( \"\" ) ax . set_yticklabels ( \"\" ) ## titles and text fig . text ( 0 , 1.05 , \"Train Set Confusion Matrix\" , { \"size\" : 22 , \"weight\" : \"bold\" }, alpha = 1 ) fig . text ( 0 , 1 , \"\"\"Training Set Confusion Matrix.\"\"\" , { \"size\" : 12 , \"weight\" : \"normal\" }, alpha = 0.98 , ) fig . tight_layout ( pad = 2.5 , w_pad = 2.5 , h_pad = 2.5 ) fig . savefig ( config . final_train_confusion_matrix , format = 'png' , dpi = 300 ) # generate the precision recall curve precision , recall , pr_thresholds = metrics . precision_recall_curve ( train_results [ 'y' ], train_results [ 'y_prob' ]) fpr , tpr , roc_thresholds = metrics . roc_curve ( train_results [ 'y' ], train_results [ 'y_prob' ], pos_label = 1 ) # use the same p, r, thresholds that were previously calculated plot_precision_recall_vs_threshold ( precision , recall , pr_thresholds ) Based on the tradeoff plot above, a good threshold can be set at \\(t = 0.35\\) , let us see how it performs with this threshold. y_pred_adj = adjusted_classes ( train_results [ \"y_prob\" ], t = 0.35 ) print ( pd . DataFrame ( metrics . confusion_matrix ( train_results [ \"y\" ], y_pred_adj ), columns = [ \"pred_neg\" , \"pred_pos\" ], index = [ \"neg\" , \"pos\" ], ) ) print ( metrics . classification_report ( y_true = train_results [ \"y\" ], y_pred = y_pred_adj )) train_brier = train_results [ 'brier' ] print ( f \"train brier: { train_brier } \" ) The False Negatives reduced from 15 to 9, at the expense of increase False Positives from 6 to 14. We should take comfort that less patients are falsely classified as benign when in fact they need treatment. This is a tradeoff that we have to choose. The ROC curve below also paints a similar story, in order for you to have high recall, one must accept that there will more False Positives. plot_roc_curve ( fpr , tpr , 'recall_optimized' ) Evaluation on Test Set Ultimately, we are interested in finding the estimate of the generalization error of the model, and thus it is time for us to evaluate our model's performance on the \"unseen\" test set \\(X_{\\text{test}}\\) to get a good gauge on how well the model generalizes to unseen data. Take note that now everything has settled, we will use the exact hyperparameters to predict on test set, with the pre-defined threshold of 0.35. test_results = evaluate_train_test_set ( grid , X_test , y_test ) y_test_pred_adj = adjusted_classes ( test_results [ 'y_prob' ], t = 0.35 ) print ( pd . DataFrame ( metrics . confusion_matrix ( test_results [ 'y' ], y_test_pred_adj ), columns = [ 'pred_neg' , 'pred_pos' ], index = [ 'neg' , 'pos' ])) test_roc = test_results [ 'roc' ] test_brier = test_results [ 'brier' ] print ( test_roc ) print ( test_brier ) print ( metrics . classification_report ( y_true = test_results [ \"y\" ], y_pred = y_test_pred_adj )) Using the same threshold we used on training set, we see that the False Negative is quite low. The overall ROC score is 0.9828, and the corresponding Brier Score is 0.04136, both seem reasonably well performing. Bias-Variance Tradeoff avg_expected_loss , avg_bias , avg_var = bias_variance_decomp ( grid . best_estimator_ [ 'model' ], X_train . values , y_train . values , X_test . values , y_test . values , loss = '0-1_loss' , random_seed = 123 ) print ( 'Average expected loss: %.3f ' % avg_expected_loss ) print ( 'Average bias: %.3f ' % avg_bias ) print ( 'Average variance: %.3f ' % avg_var ) We use the mlxtend library to estimate the Bias-Variance Tradeoff in our Logistic Regression model. The core idea behind this function is to use bagging and repeatedly sample from our training set so as to simulate that we are actually drawing samples from the \"true\" population over a distribution \\(\\mathcal{P}\\) . As expected, Logistic Regression being a linear model, its simplicity contributes to its high bias and low variance.","title":"House sales in king country usa"},{"location":"reighns_ml_journey/projects/gj_regression/house_sales_in_king_country_usa/#dependencies-and-configuration","text":"import csv import random from functools import wraps from time import time from typing import Callable , Dict , List , Union , Optional , Any import matplotlib.pyplot as plt import copy import mlxtend import numpy as np import pandas as pd import seaborn as sns from mlxtend.evaluate import paired_ttest_5x2cv , bias_variance_decomp from scipy import stats from sklearn import ( base , decomposition , dummy , ensemble , feature_selection , linear_model , metrics , model_selection , neighbors , pipeline , preprocessing , svm , tree ) from statsmodels.regression.linear_model import OLS #from statsmodels.stats.outliers_influence import variance_inflation_factor from dataclasses import dataclass import logging @dataclass class config : raw_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/regression/ %20ho use-sales-in-king-country-usa/data/raw/kc_house_data.csv\" train_size : float = 0.8 seed : int = 1992 num_folds : int = 5 cv_schema : str = \"KFold\" def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) def init_logger ( log_file : str = \"info.log\" ): \"\"\" Initialize logger. \"\"\" logger = logging . getLogger ( __name__ ) logger . setLevel ( logging . INFO ) stream_handler = logging . StreamHandler () stream_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) file_handler = logging . FileHandler ( filename = log_file ) file_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) logger . addHandler ( stream_handler ) logger . addHandler ( file_handler ) return logger # set seeding for reproducibility _ = set_seeds ( seed = config . seed ) # set logger logger = init_logger () # read data df = pd . read_csv ( config . raw_data )","title":"Dependencies and Configuration"},{"location":"reighns_ml_journey/projects/gj_regression/house_sales_in_king_country_usa/#how-eda-helped-us","text":"Insights derived from EDA: To fill in.","title":"How EDA helped us?"},{"location":"reighns_ml_journey/projects/gj_regression/house_sales_in_king_country_usa/#cross-validation-strategy","text":"Generalization: Ultimately, we are interested in the Generalization Error made by the model, that is, how well the model perform on unseen data that is not taken from our sample set $\\mathcal{D}$. In general, we use validation set for Model Selection and the test set for an estimate of generalization error on new data. - Refactored from Elements of Statistical Learning, Chapter 7.2 Step 1: Train-Test-Split: Since this dataset is relatively small, we will not use the train-validation-test split and only split into train and test in a ratio of 9:1, whereby the split is stratified on our target, using stratify=y parameter in train_test_split() to ensure that our target has equal representation in both train and test. We note that this is a relatively small dataset and in practice, we need a large sample size to get a reliable/stable split, it is also recommended to retrain the whole dataset (without the \"unseen\" test set) after we have done the model selection process (eg. finding best hyperparameters). Step 2: Resampling Stategy: Note that we will be performing StratifiedKFold as our resampling strategy. After our split in Step 1, we have a training set $X_{\\text{train}}$, we will then perform our resampling strategy on this $X_{\\text{train}}$. We will choose our choice of $K = 5$. The choice of $K$ is somewhat arbitrary, and is derived empirically . To recap, we have the following: Training Set ( \\(X_{\\text{train}}\\) ) : This will be further split into K validation sets during our cross-validation. This set is used to fit a particular hypothesis \\(h \\in \\mathcal{H}\\) . Validation Set ( \\(X_{\\text{val}}\\) ) : This is split from our \\(X_{\\text{train}}\\) during cross-validation. This set is used for model selection (i.e. find best hyperparameters, attempt to produce a best hypothesis \\(g \\in \\mathcal{H}\\) ). Test Set ( \\(X_{\\text{test}}\\) ) : This is an unseen test set, and we will only use it after we finish tuning our model/hypothesis. Suppose we have a final best model \\(g\\) , we will use \\(g\\) to predict on the test set to get an estimate of the generalization error (also called out-of-sample error). Courtesy of scikit-learn on a typical Cross-Validation workflow. predictor_cols = df . columns [ 3 :] . tolist () target_col = [ \"price\" ] logger . info ( f \" \\n The predictor columns are \\n { predictor_cols } \" ) 2021-11-10,12:09:44 - The predictor columns are ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15'] X = df [ predictor_cols ] . copy () y = df [ target_col ] . copy () # Split train - test X_train , X_test , y_train , y_test = model_selection . train_test_split ( X , y , train_size = config . train_size , shuffle = True , random_state = config . seed ) logger . info ( f \" \\n Shape of train: { X_train . shape } \\n Shape of test: { X_test . shape } \" ) 2021-11-10,12:09:44 - Shape of train: (17290, 18) Shape of test: (4323, 18) def make_folds ( df : pd . DataFrame , num_folds : int , cv_schema : str , seed : int , predictor_col : List , target_col : List , ) -> pd . DataFrame : \"\"\"Split the given dataframe into training folds. Args: df (pd.DataFrame): [description] num_folds (int): [description] cv_schema (str): [description] seed (int): [description] Returns: pd.DataFrame: [description] \"\"\" if cv_schema == \"KFold\" : df_folds = df . copy () kf = model_selection . KFold ( n_splits = num_folds , shuffle = True , random_state = seed ) for fold , ( train_idx , val_idx ) in enumerate ( kf . split ( X = df_folds [ predictor_col ], y = df_folds [ target_col ]) ): df_folds . loc [ val_idx , \"fold\" ] = int ( fold + 1 ) df_folds [ \"fold\" ] = df_folds [ \"fold\" ] . astype ( int ) elif cv_schema == \"StratifiedKFold\" : df_folds = df . copy () skf = model_selection . StratifiedKFold ( n_splits = num_folds , shuffle = True , random_state = seed ) for fold , ( train_idx , val_idx ) in enumerate ( skf . split ( X = df_folds [ predictor_col ], y = df_folds [ target_col ]) ): df_folds . loc [ val_idx , \"fold\" ] = int ( fold + 1 ) df_folds [ \"fold\" ] = df_folds [ \"fold\" ] . astype ( int ) print ( df_folds . groupby ([ \"fold\" , \"diagnosis\" ]) . size ()) return df_folds X_y_train = pd . concat ([ X_train , y_train ], axis = 1 ) . reset_index ( drop = True ) df_folds = make_folds ( X_y_train , num_folds = config . num_folds , cv_schema = config . cv_schema , seed = config . seed , predictor_col = predictor_cols , target_col = target_col ) Looks good! All our five folds are now in df_fold !","title":"Cross-Validation Strategy"},{"location":"reighns_ml_journey/projects/gj_regression/house_sales_in_king_country_usa/#modelling","text":"","title":"Modelling"},{"location":"reighns_ml_journey/projects/gj_regression/house_sales_in_king_country_usa/#spot-checking-algorithms","text":"Terminology Alert! This method is advocated by Jason Brownlee PhD and this serves as the first stage of my modelling process. We will rapidly test (spot check) different classifier algorithms, from DummyClassifier , to LinearModel to more sophisticated ensemble trees like RandomForest . I also note to the readers that we need to think of a few things when choosing the \"optimal\" machine learning algorithm: No Lunch Free Theorem intuitively says that no single optimization algorithm can work best in all situations. Therefore, spot checking can help us form a basis of which algorithm might work better in this particular scenario. Occam's Razor often appears in many Machine Learning textbook, and the narrative is that a simpler model more often times generalizes better than a complex model. This is not unfamiliar when we think of the bias-variance tradeoff, and that is why there is always a tradeoff that we must make.","title":"Spot Checking Algorithms"},{"location":"reighns_ml_journey/projects/gj_regression/house_sales_in_king_country_usa/#make-basic-pipeline-say-no-to-data-leakage","text":"Say No to Data Leakage: This has been emphasized throughout and we must be careful as we should never touch the test set when fitting the model. This means that preprocessing steps such as StandardScaling() should only be fitted on the training data, and then apply the same transformation (mean and std) on the test data. In other words, do not apply scaling on the whole dataset before splitting. However, it is also equally important to take note not to contaminate our validation set, which is often overlooked, resulting in over optimistic results from model selection phase, but perform badly on unseen test set. As a result, when we use a 5 fold cross validation, we should be careful during fitting that the preprocessing steps are only applied on the training folds, and not on all 5 folds. The same idea is also applied to our ReduceVIF() preprocessing step. We should also include this in our pipeline and not select the features outside the cross-validation loop. Scikit Learn's Pipeline object will prevent us from data leakage, as the steps in a pipeline is already pre-defined. There is also a lot of flexibility in this object, as you can even write custom functions in your pipeline! def variance_inflation_factor ( exog , idx_kept , vif_idx ): \"\"\"Compute VIF for one feature. Args: exog (np.ndarray): Observations idx_kept (List[int]): Indices of features to consider vif_idx (int): Index of feature for which to compute VIF Returns: float: VIF for the selected feature \"\"\" exog = np . asarray ( exog ) x_i = exog [:, vif_idx ] mask = [ col for col in idx_kept if col != vif_idx ] x_noti = exog [:, mask ] r_squared_i = OLS ( x_i , x_noti ) . fit () . rsquared vif = 1. / ( 1. - r_squared_i ) return vif class ReduceVIF ( base . BaseEstimator , base . TransformerMixin ): \"\"\"The base of the class structure is implemented in https://www.kaggle.com/ffisegydd/sklearn-multicollinearity-class; I heavily modified the class such that it can take in numpy arrays and correctly implemented the fit and transform method. \"\"\" def __init__ ( self , thresh = 10 , max_drop = 20 ): self . thresh = thresh self . max_drop = max_drop self . column_indices_kept_ = [] self . feature_names_kept_ = None def reset ( self ): \"\"\"Resets the state of predictor columns after each fold.\"\"\" self . column_indices_kept_ = [] self . feature_names_kept_ = None def fit ( self , X , y = None ): \"\"\"Fits the Recursive VIF on the training folds and save the selected feature names in self.feature_names Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" self . column_indices_kept_ , self . feature_names_kept_ = self . calculate_vif ( X ) return self def transform ( self , X , y = None ): \"\"\"Transforms the Validation Set according to the selected feature names. Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" return X [:, self . column_indices_kept_ ] def calculate_vif ( self , X : Union [ np . ndarray , pd . DataFrame ]): \"\"\"Implements a VIF function that recursively eliminates features. Args: X (Union[np.ndarray, pd.DataFrame]): [description] Returns: [type]: [description] \"\"\" feature_names = None column_indices_kept = list ( range ( X . shape [ 1 ])) if isinstance ( X , pd . DataFrame ): feature_names = X . columns dropped = True count = 0 while dropped and count <= self . max_drop : dropped = False max_vif , max_vif_col = None , None for col in column_indices_kept : vif = variance_inflation_factor ( X , column_indices_kept , col ) if max_vif is None or vif > max_vif : max_vif = vif max_vif_col = col if max_vif > self . thresh : print ( f \"Dropping { max_vif_col } with vif= { max_vif } \" ) column_indices_kept . remove ( max_vif_col ) if feature_names is not None : feature_names . pop ( max_vif_col ) dropped = True count += 1 return column_indices_kept , feature_names # create a feature preparation pipeline for a model def make_pipeline ( model ): \"\"\"Make a Pipeline for Training. Args: model ([type]): [description] Returns: [type]: [description] \"\"\" steps = list () # standardization steps . append (( 'standardize' , preprocessing . StandardScaler ())) # reduce VIF # steps.append((\"remove_multicollinearity\", ReduceVIF(thresh=10))) # the model steps . append (( 'model' , model )) # create pipeline _pipeline = pipeline . Pipeline ( steps = steps ) return _pipeline regressors = [ # baseline model dummy . DummyRegressor ( strategy = \"mean\" ), # linear model linear_model . LinearRegression ( fit_intercept = True ), linear_model . Ridge ( random_state = config . seed , alpha = 1 , fit_intercept = True ), linear_model . Lasso ( random_state = config . seed , alpha = 1 , fit_intercept = True , ), linear_model . ElasticNet ( random_state = config . seed , alpha = 1 , l1_ratio = 0.5 , fit_intercept = True , ), # tree tree . DecisionTreeRegressor ( random_state = config . seed , criterion = \"squared_error\" ), # ensemble # ensemble.RandomForestClassifier(random_state=config.seed), ] regressors = [ make_pipeline ( model ) for model in regressors ]","title":"Make Basic Pipeline (Say No to Data Leakage!)"},{"location":"reighns_ml_journey/projects/gj_regression/house_sales_in_king_country_usa/#define-metrics","text":"default_result_names = [ \"y_true\" , \"y_pred\" , ] default_logit_names = [ \"y_true\" , \"y_pred\" , ] default_score_names = [ \"explained_variance_score\" , \"mean_squared_error\" , \"mean_absolute_error\" , \"root_mean_squared_error\" , \"r2_score\" , \"mean_absolute_percentage_error\" , ] custom_score_names = [ \"adjusted_r2\" ] def adjusted_r2 ( r2 : float , n : int , k : int ) -> float : \"\"\"Calculate adjusted R^2. Args: r2 (float): r2 score of the model/ n (int): number of samples. k (int): number of features minus the constant bias term. Returns: adjusted_r2_score (float): r2 * (n - 1) / (n - k - 1) \"\"\" adjusted_r2_score = r2 * ( n - 1 ) / ( n - k - 1 ) return adjusted_r2_score class Results : \"\"\"Stores results for model training in columnwise format.\"\"\" _result_dict : Dict logit_names : List [ str ] score_names : List [ str ] def __init__ ( self , logit_names : List [ str ] = default_logit_names , score_names : List [ str ] = default_score_names , existing_dict : Optional [ Dict ] = None , ): \"\"\"Construct a new results store.\"\"\" self . logit_names = logit_names self . score_names = score_names if existing_dict is not None : self . _result_dict = copy . deepcopy ( existing_dict ) return dict_keys = [ \"identifier\" , * logit_names , * score_names ] self . _result_dict = { key : [] for key in dict_keys } def add ( self , identifier : str , results : Dict , in_place = False ): \"\"\"Add a new results row.\"\"\" if not in_place : return Results ( self . logit_names , self . score_names , self . _result_dict ) . add ( identifier , results , in_place = True ) self . _result_dict [ \"identifier\" ] . append ( identifier ) for result_name in set ([ * results . keys (), * self . logit_names , * self . score_names ]): result_value = results . get ( result_name , np . nan ) self . _result_dict [ result_name ] . append ( result_value ) return self def get_result ( self , result_name : str ) -> Dict [ str , Any ]: \"\"\"Get a map of identifiers to result values for a result.\"\"\" return { identifier : result_value for identifier , result_value in zip ( self . _result_dict [ \"identifier\" ], self . _result_dict [ result_name ]) } def get_result_values ( self , result_name : str ) -> List [ Any ]: \"\"\"Get a list of values for a result.\"\"\" return self . _result_dict [ result_name ] def to_dataframe ( self ) -> pd . DataFrame : \"\"\"Get a Data Frame containing the results.\"\"\" return pd . DataFrame . from_dict ( self . _result_dict ) def to_dict ( self ) -> Dict : \"\"\"Get a dictionary containing the results. Returns: Dict[str, List[Any]]: Dictionary of result columns \"\"\" return self . _result_dict def compute_metrics ( logits : Dict [ str , np . ndarray ]) -> Dict [ str , Any ]: \"\"\"Compute metrics from logits.\"\"\" y_true , y_pred = logits [ \"y_true\" ], logits [ \"y_pred\" ] default_score_names = [ \"explained_variance_score\" , \"mean_squared_error\" , \"mean_absolute_error\" , \"root_mean_squared_error\" , \"r2_score\" , \"mean_absolute_percentage_error\" , ] default_metrics_dict : Dict [ str , float ] = {} custom_metrics_dict : Dict [ str , float ] = {} for metric_name in default_score_names : if hasattr ( metrics . _regression , metric_name ): # TODO: get metric score with default parameters, consider adding kwargs if you want to configure parameters metric_score = getattr ( metrics . _regression , metric_name )( y_true , y_pred ) else : # logger.info(f\"{metrics._regression} has no such attribute {metric_name}!\") # add custom metrics here rmse = metrics . _regression . mean_squared_error ( y_true , y_pred , squared = False ) custom_metrics_dict [ \"root_mean_squared_error\" ] = rmse if metric_name not in default_metrics_dict : default_metrics_dict [ metric_name ] = metric_score metrics_dict = { ** default_metrics_dict , ** custom_metrics_dict } return metrics_dict def mean_score ( score_values ) -> Union [ float , np . ndarray ]: \"\"\"Compute the mean score.\"\"\" score_values = np . array ( score_values ) shape = score_values . shape if len ( shape ) == 1 : return score_values . mean () return score_values . mean ( axis = 0 ) def mean_cv_results ( model_results : Results ) -> Dict : \"\"\"Add mean cross-validation results. This method computes the mean value for all score types in the model_results, including for scores (e.g., confusion matrix) where the mean value may contain decimal places. \"\"\" cv_logits = { y_result : np . concatenate ( model_results . get_result_values ( y_result )) for y_result in model_results . logit_names } cv_scores = { score : mean_score ( model_results . get_result_values ( score ) ) for score in model_results . score_names } return { ** cv_logits , ** cv_scores , } def oof_cv_results ( model_results : Results ) -> Dict : \"\"\"Add OOF cross-validation results.\"\"\" cv_logits = { y_result : np . concatenate ( model_results . get_result_values ( y_result ) ) for y_result in model_results . logit_names } cv_scores = compute_metrics ( cv_logits ) return { ** cv_logits , ** cv_scores , } def add_cv_results ( model_results : Results ): \"\"\"Add cross-validation results. This method returns a copy of the given model results with summary columns for mean and CV cross-validation. \"\"\" mean_cv = mean_cv_results ( model_results ) oof_cv = oof_cv_results ( model_results ) return ( model_results . add ( \"mean_cv\" , mean_cv ) . add ( \"oof_cv\" , oof_cv ) ) def train_on_fold ( df_folds : pd . DataFrame , models : List [ Callable ], num_folds : int , predictor_col : List , target_col : List , ) -> Dict [ str , List ]: \"\"\"Take in a dataframe with fold number as column, and a models which holds a list of callable models, we will loop through and return a dictionary of cv results. Args: df_folds (pd.DataFrame): [description] model (Callable): [description] num_folds (int): [description] predictor_col (List): [description] target_col (List): [description] Returns: Dict[str, List]: [description] \"\"\" y_true = df_folds [ target_col ] . values . flatten () # test_pred_arr: np.ndarray = np.zeros(len(X_test)) model_dict = {} for model in models : model_results = Results () if isinstance ( model , pipeline . Pipeline ): model_name = model [ \"model\" ] . __class__ . __name__ else : model_name = model . __class__ . __name__ # out-of-fold validation predictions oof_pred_arr : np . ndarray = np . zeros ( len ( df_folds )) for fold in range ( 1 , num_folds + 1 ): train_df = df_folds [ df_folds [ \"fold\" ] != fold ] . reset_index ( drop = True ) val_df = df_folds [ df_folds [ \"fold\" ] == fold ] . reset_index ( drop = True ) val_idx = df_folds [ df_folds [ \"fold\" ] == fold ] . index . values X_train , y_train = train_df [ predictor_col ] . values , train_df [ target_col ] . values X_val , y_val = val_df [ predictor_col ] . values , val_df [ target_col ] . values model . fit ( X_train , y_train ) y_val_pred = model . predict ( X_val ) logits = { \"y_true\" : y_val , \"y_pred\" : y_val_pred , } metrics = compute_metrics ( logits ) model_results . add ( f \"fold { fold } \" , { ** logits , ** metrics }, in_place = True ) if model_name not in model_dict : model_dict [ model_name ] = model_results return model_dict model_dict = train_on_fold ( df_folds , models = regressors , num_folds = 5 , predictor_col = predictor_cols , target_col = target_col ) 2021-11-10,12:09:44 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:44 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:44 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:44 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:44 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.676e+13, tolerance: 1.868e+11 model = cd_fast.enet_coordinate_descent( 2021-11-10,12:09:46 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+13, tolerance: 1.862e+11 model = cd_fast.enet_coordinate_descent( 2021-11-10,12:09:48 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.603e+13, tolerance: 1.760e+11 model = cd_fast.enet_coordinate_descent( 2021-11-10,12:09:49 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.718e+13, tolerance: 1.897e+11 model = cd_fast.enet_coordinate_descent( 2021-11-10,12:09:50 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.810e+13, tolerance: 1.913e+11 model = cd_fast.enet_coordinate_descent( 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:53 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:53 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! model_dict_with_summary = { model : add_cv_results ( model_results ) for model , model_results in model_dict . items () } 2021-11-10,12:19:12 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:19:12 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:19:12 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:19:12 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:19:12 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:19:12 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! results_df = pd . concat ({ name : results . to_dataframe () . T for name , results in model_dict_with_summary . items () }, axis = 0 ) results_df . columns = [ 'fold 1' , 'fold 2' , 'fold 3' , 'fold 4' , 'fold 5' , 'mean_cv' , 'oof_cv' ] results_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv DummyRegressor identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [[945000.0], [352500.0], [560000.0], [500000.0... [[850000.0], [653000.0], [532000.0], [385000.0... [[1274950.0], [392137.0], [850000.0], [520000.... [[450000.0], [495000.0], [395000.0], [280000.0... [[754999.0], [588500.0], [525000.0], [525000.0... [[945000.0], [352500.0], [560000.0], [500000.0... [[945000.0], [352500.0], [560000.0], [500000.0... y_pred [541073.7741469058, 541073.7741469058, 541073.... [539415.1803788317, 539415.1803788317, 539415.... [536694.9069548872, 536694.9069548872, 536694.... [539857.5267495662, 539857.5267495662, 539857.... [540990.9316078658, 540990.9316078658, 540990.... [541073.7741469058, 541073.7741469058, 541073.... [541073.7741469058, 541073.7741469058, 541073.... explained_variance_score -0.0 0.0 0.0 0.0 -0.0 -0.0 -0.000169 mean_squared_error 132247512137.06163 133951959536.310318 163296091978.429077 123686085780.066254 119223815039.600311 134481092894.293533 134481092894.293488 mean_absolute_error 229404.508989 232306.56185 241944.838637 232308.320526 230976.128917 233388.071784 233388.071784 root_mean_squared_error 363658.510332 365994.480199 404099.111578 351690.326538 345288.017515 366146.089233 366716.63842 r2_score -0.000407 -0.000007 -0.0013 -0.000013 -0.000402 -0.000426 -0.000169 mean_absolute_percentage_error 0.542257 0.530016 0.531438 0.534771 0.540719 0.53584 0.53584 LinearRegression identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [[945000.0], [352500.0], [560000.0], [500000.0... [[850000.0], [653000.0], [532000.0], [385000.0... [[1274950.0], [392137.0], [850000.0], [520000.... [[450000.0], [495000.0], [395000.0], [280000.0... [[754999.0], [588500.0], [525000.0], [525000.0... [[945000.0], [352500.0], [560000.0], [500000.0... [[945000.0], [352500.0], [560000.0], [500000.0... y_pred [[667876.5428875468], [614410.8399372244], [73... [[694869.3745167988], [618367.5827104518], [46... [[1023189.3043934056], [304317.4518826463], [9... [[477332.04737439007], [492396.46923110134], [... [[853190.5779001702], [574292.3295129627], [77... [[667876.5428875468], [614410.8399372244], [73... [[667876.5428875468], [614410.8399372244], [73... explained_variance_score 0.680095 0.705119 0.685252 0.693963 0.721335 0.697153 0.696083 mean_squared_error 42290033057.306816 39515957195.400787 51430470051.222527 37873842125.074295 33210290838.463482 40864118653.493584 40864118653.493584 mean_absolute_error 127943.982537 124266.336776 130373.471382 126075.417537 122514.978064 126234.837259 126234.837259 root_mean_squared_error 205645.406118 198786.209772 226782.869836 194612.029754 182236.908552 201612.684806 202148.753777 r2_score 0.68009 0.704997 0.684638 0.693787 0.721334 0.696969 0.696083 mean_absolute_percentage_error 0.265815 0.257898 0.250641 0.251933 0.255471 0.256352 0.256352 Ridge identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [[945000.0], [352500.0], [560000.0], [500000.0... [[850000.0], [653000.0], [532000.0], [385000.0... [[1274950.0], [392137.0], [850000.0], [520000.... [[450000.0], [495000.0], [395000.0], [280000.0... [[754999.0], [588500.0], [525000.0], [525000.0... [[945000.0], [352500.0], [560000.0], [500000.0... [[945000.0], [352500.0], [560000.0], [500000.0... y_pred [[667923.954879363], [614423.8176756387], [731... [[694094.2870407818], [617981.9933915635], [46... [[1023626.0773182933], [304085.70358412666], [... [[477416.1959023318], [492474.1026423527], [30... [[853485.3791880516], [574203.5276395974], [77... [[667923.954879363], [614423.8176756387], [731... [[667923.954879363], [614423.8176756387], [731... explained_variance_score 0.680109 0.70512 0.685333 0.693972 0.721308 0.697168 0.696102 mean_squared_error 42288266590.795799 39515834657.582001 51417532361.73008 37872817115.509499 33213482338.320721 40861586612.787613 40861586612.787621 mean_absolute_error 127934.770004 124362.619797 130426.951817 126064.678687 122549.911589 126267.786379 126267.786379 root_mean_squared_error 205641.11114 198785.901556 226754.343645 194609.396267 182245.6648 201607.283482 202142.490864 r2_score 0.680104 0.704998 0.684718 0.693795 0.721307 0.696984 0.696102 mean_absolute_percentage_error 0.265796 0.258152 0.250808 0.251901 0.255562 0.256444 0.256444 Lasso identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [[945000.0], [352500.0], [560000.0], [500000.0... [[850000.0], [653000.0], [532000.0], [385000.0... [[1274950.0], [392137.0], [850000.0], [520000.... [[450000.0], [495000.0], [395000.0], [280000.0... [[754999.0], [588500.0], [525000.0], [525000.0... [[945000.0], [352500.0], [560000.0], [500000.0... [[945000.0], [352500.0], [560000.0], [500000.0... y_pred [667918.069528075, 614415.1560338883, 731173.9... [694081.9059207196, 617992.7838152755, 463279.... [1023674.2634054194, 304066.4170321575, 955604... [477415.1327845714, 492463.7396309793, 305057.... [853505.6953654164, 574205.7699892861, 771097.... [667918.069528075, 614415.1560338883, 731173.9... [667918.069528075, 614415.1560338883, 731173.9... explained_variance_score 0.680106 0.70512 0.685338 0.693969 0.721305 0.697168 0.696102 mean_squared_error 42288598880.705284 39515796822.947304 51416606662.838509 37873203587.611603 33213918899.943615 40861624970.809258 40861624970.809265 mean_absolute_error 127937.742862 124365.678646 130429.153983 126067.441635 122553.005403 126270.604506 126270.604506 root_mean_squared_error 205641.919075 198785.806392 226752.302442 194610.389208 182246.862524 201607.455928 202142.585743 r2_score 0.680101 0.704998 0.684723 0.693792 0.721303 0.696984 0.696102 mean_absolute_percentage_error 0.265806 0.258161 0.250816 0.25191 0.255571 0.256453 0.256453 ElasticNet identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [[945000.0], [352500.0], [560000.0], [500000.0... [[850000.0], [653000.0], [532000.0], [385000.0... [[1274950.0], [392137.0], [850000.0], [520000.... [[450000.0], [495000.0], [395000.0], [280000.0... [[754999.0], [588500.0], [525000.0], [525000.0... [[945000.0], [352500.0], [560000.0], [500000.0... [[945000.0], [352500.0], [560000.0], [500000.0... y_pred [656773.6005291657, 614707.2479590292, 745914.... [685212.4217947914, 599436.3647276227, 483018.... [907890.7497500202, 369129.65691215475, 862824... [469736.2740988735, 539209.0261494196, 379320.... [804965.135403886, 571735.3944930851, 737716.4... [656773.6005291657, 614707.2479590292, 745914.... [656773.6005291657, 614707.2479590292, 745914.... explained_variance_score 0.65603 0.673131 0.638955 0.670126 0.695926 0.666834 0.664827 mean_squared_error 45475490348.208717 43800728719.55619 59005250118.409103 40812502573.376862 36239917129.190384 45066777777.748253 45066777777.748253 mean_absolute_error 124419.180393 122248.905619 131347.081356 124722.490988 120613.606068 124670.252885 124670.252885 root_mean_squared_error 213249.830828 209286.236336 242909.962987 202021.044877 190367.846889 211566.984383 212289.372739 r2_score 0.655993 0.673009 0.638191 0.670027 0.695912 0.666627 0.664827 mean_absolute_percentage_error 0.247878 0.245323 0.243383 0.244499 0.243172 0.244851 0.244851 DecisionTreeRegressor identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [[945000.0], [352500.0], [560000.0], [500000.0... [[850000.0], [653000.0], [532000.0], [385000.0... [[1274950.0], [392137.0], [850000.0], [520000.... [[450000.0], [495000.0], [395000.0], [280000.0... [[754999.0], [588500.0], [525000.0], [525000.0... [[945000.0], [352500.0], [560000.0], [500000.0... [[945000.0], [352500.0], [560000.0], [500000.0... y_pred [915000.0, 300000.0, 316000.0, 643000.0, 28200... [535000.0, 630000.0, 410000.0, 335000.0, 72000... [1605000.0, 479000.0, 850000.0, 614000.0, 6505... [449000.0, 555000.0, 440000.0, 247500.0, 35000... [715000.0, 545000.0, 597000.0, 530000.0, 50000... [915000.0, 300000.0, 316000.0, 643000.0, 28200... [915000.0, 300000.0, 316000.0, 643000.0, 28200... explained_variance_score 0.752389 0.756145 0.770122 0.743783 0.767617 0.758011 0.758577 mean_squared_error 32751471474.735687 32664895113.423656 37510896623.824608 31693198492.168594 27707113797.312031 32465515100.292915 32465515100.292915 mean_absolute_error 101035.96819 99810.482071 105829.744939 101019.912377 96233.899075 100786.00133 100786.00133 root_mean_squared_error 180973.676193 180734.321902 193677.300229 178025.836586 166454.539732 179973.134928 180181.894485 r2_score 0.752246 0.756143 0.769991 0.743758 0.767511 0.75793 0.758546 mean_absolute_percentage_error 0.192566 0.186579 0.189517 0.183729 0.184638 0.187406 0.187406","title":"Define Metrics"},{"location":"reighns_ml_journey/projects/gj_regression/house_sales_in_king_country_usa/#comparison-of-cross-validated-models","text":"The point of the following comparison is to check how different models are performing across folds. More specifically, if we have 5 folds, we will have a metric score for each fold, subsequently, we can find the standard error of model's performance. We need to be aware of models that have high variance across folds in terms of the metrics performance. This can indicate that the model is highly unstable, and may be a sign of overfitting. def summarize_metrics ( metric_name ): ls = [] for model_name , inner_dict in model_dict . items (): folds = inner_dict [ \"identifier\" ][: - 2 ] all_obs = [] for idx , obs in enumerate ( inner_dict [ metric_name ][: - 2 ]): ls . append (( model_name , folds [ idx ], obs )) all_obs . append ( obs ) ls . append (( model_name , \"SE\" , np . std ( all_obs , ddof = 1 ) / len ( all_obs ) ** 0.5 )) fig , ax = plt . subplots ( figsize = ( 15 , 8 )) summary_df = pd . DataFrame ( ls , columns = [ \"model\" , \"fold\" , metric_name ]) # summary_df.to_csv _ = sns . boxplot ( x = \"model\" , y = metric_name , data = summary_df [( summary_df [ 'model' ] != 'DummyClassifier' ) & ( summary_df [ 'fold' ] != 'SE' )], ax = ax ) fig . savefig ( config . spot_checking_boxplot , format = 'png' , dpi = 300 ) return summary_df summary_df = summarize_metrics ( \"roc\" ) display ( summary_df . tail ( 12 ))","title":"Comparison of Cross-Validated Models"},{"location":"reighns_ml_journey/projects/gj_regression/house_sales_in_king_country_usa/#out-of-fold-confusion-matrix","text":"We do have information on the performance of each folds, we now look at the performance of all 5 folds together. Typicall there are two ways to do it, one is to simply take the average of the score of five folds, the other is to take a look at out of folds predictions. From the confusion matrix of the out of fold performance, Logistic Regression does seem to be a model we can explore on, although slightly lower in terms of overall AUROC score than SVC, it seems to have the quite low False Negatives amongst all. With further hyperparameter tuning and threshold optimization, we can make it better. model_names = [ model for model in model_dict . keys ()] fig , ax = plt . subplots ( 2 , 3 , figsize = ( 10 , 10 )) for axes , algo in zip ( ax . ravel (), model_names ): cf_mat = results_df . oof_cv [ algo ] . confusion_matrix #### scores auc = results_df . oof_cv [ algo ] . roc #### annotations labels = [ \"True Neg\" , \"False Pos\" , \"False Neg\" , \"True Pos\" ] counts = [ \" {0:0.0f} \" . format ( value ) for value in cf_mat . flatten ()] percentages = [ \" {0:.2%} \" . format ( value ) for value in cf_mat . flatten () / np . sum ( cf_mat )] #### final annotations label = ( np . array ([ f \" { v1 } \\n { v2 } \\n { v3 } \" for v1 , v2 , v3 in zip ( labels , counts , percentages )]) ) . reshape ( 2 , 2 ) # heatmap sns . heatmap ( data = cf_mat , vmin = 0 , vmax = 330 , cmap = [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ], linewidth = 2 , linecolor = \"white\" , square = True , ax = axes , annot = label , fmt = \"\" , cbar = False , annot_kws = { \"size\" : 10 , \"color\" : \"black\" , \"weight\" : \"bold\" , \"alpha\" : 0.8 }, alpha = 1 , ) axes . text ( 0 , - 0 , \" {} \" . format ( algo ), { \"size\" : 12 , \"color\" : \"black\" , \"weight\" : \"bold\" }) axes . scatter ( 1 , 1 , s = 3500 , c = \"white\" ) axes . text ( 0.72 , 1.0 , \"AUC: {} \" . format ( round ( auc , 3 )), { \"size\" : 10 , \"color\" : \"black\" , \"weight\" : \"bold\" }, ) ## ticks and labels axes . set_xticklabels ( \"\" ) axes . set_yticklabels ( \"\" ) ## titles and text fig . text ( 0 , 1.05 , \"Out Of Fold Confusion Matrix\" , { \"size\" : 22 , \"weight\" : \"bold\" }, alpha = 1 ) fig . text ( 0 , 1 , \"\"\"This Visualization show the results of various classifiers and there respective results.\"\"\" , { \"size\" : 14 , \"weight\" : \"normal\" }, alpha = 0.98 , ) fig . tight_layout ( pad = 2.5 , w_pad = 2.5 , h_pad = 2.5 ) fig . savefig ( config . oof_confusion_matrix , format = 'png' , dpi = 300 )","title":"Out-of-Fold Confusion Matrix"},{"location":"reighns_ml_journey/projects/gj_regression/house_sales_in_king_country_usa/#hypothesis-testing-across-models","text":"I am slightly shocked at the performance of plain LogisticRegression, I decide to use an idea from Hypothesis Testing Across Models to check if the difference is really by chance or not. Note that I will be modifying his code as his code does not split using StratifiedKFold. The basic idea is to test if two model's difference in scores (in this case roc), is statistically significant or not. However, we note that this method may violate an assumption of Student's t test. Null Hypothesis \\(H_0\\) : The difference in the performance score of two classifiers is Statistically Significant. Alternate Hypothesis \\(H_1\\) : The difference in the performance score of two classifiers is not Statistically Significant. def paired_ttest_skfold_cv ( estimator1 , estimator2 , X , y , cv = 10 , scoring = None , shuffle = False , random_seed = None ): \"\"\"Modified from https://github.com/rasbt/mlxtend/blob/master/mlxtend/evaluate/ttest.py to accomodate StratifiedKFold\"\"\" if not shuffle : skf = model_selection . StratifiedKFold ( n_splits = cv , shuffle = shuffle ) else : skf = model_selection . StratifiedKFold ( n_splits = cv , random_state = random_seed , shuffle = shuffle ) if scoring is None : if estimator1 . _estimator_type == \"classifier\" : scoring = \"accuracy\" elif estimator1 . _estimator_type == \"regressor\" : scoring = \"r2\" else : raise AttributeError ( \"Estimator must \" \"be a Classifier or Regressor.\" ) if isinstance ( scoring , str ): scorer = metrics . get_scorer ( scoring ) else : scorer = scoring score_diff = [] for train_index , test_index in skf . split ( X = X , y = y ): X_train , X_test = X [ train_index ], X [ test_index ] y_train , y_test = y [ train_index ], y [ test_index ] estimator1 . fit ( X_train , y_train ) estimator2 . fit ( X_train , y_train ) est1_score = scorer ( estimator1 , X_test , y_test ) est2_score = scorer ( estimator2 , X_test , y_test ) score_diff . append ( est1_score - est2_score ) avg_diff = np . mean ( score_diff ) numerator = avg_diff * np . sqrt ( cv ) denominator = np . sqrt ( sum ([( diff - avg_diff ) ** 2 for diff in score_diff ]) / ( cv - 1 )) t_stat = numerator / denominator pvalue = stats . t . sf ( np . abs ( t_stat ), cv - 1 ) * 2.0 return float ( t_stat ), float ( pvalue ) # check if difference between algorithms is real X_tmp = X_y_train [ predictor_cols ] . values y_tmp = X_y_train [ 'diagnosis' ] . values t , p = paired_ttest_skfold_cv ( estimator1 = classifiers [ 1 ], estimator2 = classifiers [ - 1 ], shuffle = True , cv = 5 , X = X_tmp , y = y_tmp , scoring = 'roc_auc' , random_seed = config . seed ) print ( 'P-value: %.3f , t-Statistic: %.3f ' % ( p , t )) Since P value is quite high, and more the basic threshold of 0.05 or 0.1, we fail to reject the null hypothesis, and say that there is no significant difference between these two models.","title":"Hypothesis Testing Across Models"},{"location":"reighns_ml_journey/projects/gj_regression/house_sales_in_king_country_usa/#model-selection-hyperparameter-tuning-with-gridsearchcv","text":"Hyperparameter Tuning: We have done a quick spot checking on algorithms and realized that LogisticRegression is doing well for this task. For this purpose, I will just perform hyperparameter tuning on this single algorithm. However, in practice and if resources are allowed, I will also tune other models such as RandomForest() , or gradient boosting algorithms such as XGBoost , as I believe they will perform no worse than our Logistic Regression model given the right hyperparameters. Grid Search: We will use an old-fashioned way to search for hyperparameters, which is brute force method. The time complexity of Grid Search is high and if you have many hyperparameters to tune, I recommend trying out Random Grid Search or libraries like Optuna that uses Bayesian Optimization. def make_finetuning_pipeline ( model ): \"\"\"Make a Pipeline for Training. Args: model ([type]): [description] Returns: [type]: [description] \"\"\" steps = list () # standardization steps . append (( 'standardize' , preprocessing . StandardScaler ())) # reduce VIF steps . append (( 'remove_multicollinearity' , ReduceVIF ( thresh = 10 ))) # the model steps . append (( 'model' , model )) # create pipeline _pipeline = pipeline . Pipeline ( steps = steps ) return _pipeline Reconstruct our pipeline but now only taking in LogisticRegression . pipeline_logistic = make_finetuning_pipeline ( linear_model . LogisticRegression ( solver = \"saga\" , random_state = config . seed , max_iter = 10000 , n_jobs = None , fit_intercept = True ) ) Define our search space for the hyperparameters: param_grid = { model__penalty = [ \"l1\" , \"l2\" ], model__C = np . logspace ( - 4 , 4 , 10 )} param_grid = dict ( model__penalty = [ \"l1\" , \"l2\" ], model__C = np . logspace ( - 4 , 4 , 10 ), ) Run our hyperparameter search with cross-validation. For example, our param_grid has \\(2 \\times 10 = 20\\) combinations, and our cross validation has 5 folds, then there will be a total of 100 fits. Below details the pseudo code of what happens under the hood: Define \\(G\\) as the set of combination of hyperparamters. Define number of splits to be \\(K\\) . For each set of hyperparameter \\(z \\in Z\\) : for fold \\(j\\) in K: Set \\(F_{\\text{train}}=\\bigcup\\limits_{i\\neq k}^{K} F_{i}\\) Set \\(F_{\\text{val}} = F_{j}\\) as the validation set Perform Standard Scaling on \\(F_{\\text{train}}\\) and find the mean and std Perform VIF recursively on \\(F_{\\text{train}}\\) and find the selected features Transform \\(F_{\\text{val}}\\) using the mean and std found using \\(F_{\\text{train}}\\) Transform \\(F_{\\text{val}}\\) to have only the selected features from \\(F_{\\text{train}}\\) Train and fit on \\(F_{\\text{train}}\\) Evaluate the fitted parameters on \\(F_{\\text{val}}\\) to obtain \\(\\mathcal{M}\\) grid = model_selection . GridSearchCV ( pipeline_logistic , param_grid = param_grid , cv = 5 , refit = True , verbose = 3 , scoring = \"roc_auc\" ) _ = grid . fit ( X_train , y_train ) We can save our results in a dataframe, we will also look at the top performing hyperparameter by querying the below: grid_cv_df = pd . DataFrame ( grid . cv_results_ ) grid_cv_df . loc [ grid_cv_df [ 'rank_test_score' ] == 1 ] grid_cv_df = pd . DataFrame ( grid . cv_results_ ) best_cv = grid_cv_df . loc [ grid_cv_df [ 'rank_test_score' ] == 1 ] display ( best_cv ) best_hyperparams = grid . best_params_ print ( f \"Best Hyperparameters found is { best_hyperparams } \" ) Our best performing set of hyperparameters {'model__C': 0.3593813663804626, 'model__penalty': 'l2'} gives rise to a mean cross validation score of \\(0.988739\\) , which is higher than the model with default hyperparameter scoring, \\(0.987136\\) . Room for Improvement: Apart from the other methods to search for the optimal hyperparameters, we can also include preprocessing step as a tunable hyperparameter. More specifically, in our ReduceVIF() step, we hard coded two manual criterion in which the algorithm will stop; if the threshold reaches 10, or if the number of features removed hit 20; we can include them in the search space.","title":"Model Selection: Hyperparameter Tuning with GridSearchCV"},{"location":"reighns_ml_journey/projects/gj_regression/house_sales_in_king_country_usa/#retrain-on-the-whole-training-set","text":"A common practice after the hyperparameter tuning phase is to retrain the model on the whole dataset \\(X_{\\text{train}}\\) where we will get the estimator's coefficients obtained from the retraining. This is actually already done as the scikit-learn's GridSearchCV has a parameter refit ; if we select it to be true, then after the model selection process is done (i.e. getting the best hyperparameters after cross validation with grid search), the grid search object will retrain on the whole \\(X_{\\text{train}}\\) with the best hyperparameters internally, and return us back an object in which we can call predict etc.","title":"Retrain on the whole training set"},{"location":"reighns_ml_journey/projects/gj_regression/house_sales_in_king_country_usa/#retrain-using-optimal-hyperparameters","text":"However, to be extra careful, we can retrain manually using the best hyperparameters and check if scikit-learn is true to its documentation. We will just reconstruct the pipeline using the grid's best hyper parameters. We will then test if the retrained model's coefficients coincide with the grid's best estimator's coefficients. If there difference is 0, this means they are trained under the same circumstances and we can be sure that the refit parameter is behaving true to its words. grid_best_hyperparams = grid . best_params_ print ( grid_best_hyperparams ) -> { 'model__C' : 0.3593813663804626 , 'model__penalty' : 'l2' } retrain_pipeline = pipeline . Pipeline ( [ ( \"standardize\" , preprocessing . StandardScaler ()), ( 'remove_multicollinearity' , ReduceVIF ( thresh = 10 )), ( \"model\" , linear_model . LogisticRegression ( C = 0.3593813663804626 , max_iter = 10000 , random_state = 1992 , solver = \"saga\" , penalty = \"l1\" ), ), ] ) _ = retrain_pipeline . fit ( X_train , y_train ) coef_diff = retrain_pipeline [ 'model' ] . coef_ - grid . best_estimator_ [ 'model' ] . coef_ print ( \"...\" ) assert np . all ( coef_diff == 0 ) == True print ( \"Retraining Assertion Passed!\" )","title":"Retrain using optimal hyperparameters"},{"location":"reighns_ml_journey/projects/gj_regression/house_sales_in_king_country_usa/#interpretation-of-results","text":"","title":"Interpretation of Results"},{"location":"reighns_ml_journey/projects/gj_regression/house_sales_in_king_country_usa/#interpretation-of-coefficients","text":"As shown in the figure below, all else being equal, for every square unit increase in mean cell area, the odds of the tumor being malignant increases by a factor of \\(e^{1.43} = 4.19\\) . The variation (standard error) of the characteristics of cells also are deemed important by the model, for example, area se played an important role in determining whether a cell is malignant; intuitively, if some cells are noticably larger than the rest, then it is also a good indicator of malignancy. selected_features_by_vif_index = grid . best_estimator_ [ 'remove_multicollinearity' ] . column_indices_kept_ selected_feature_names = np . asarray ( predictor_cols )[ selected_features_by_vif_index ] selected_features_coefficients = grid . best_estimator_ [ 'model' ] . coef_ . flatten () # assertion #assert grid.best_estimator_['remove_multicollinearity'].feature_names_ == retrain_pipeline['remove_multicollinearity'].feature_names_ fig , ax = plt . subplots ( figsize = ( 15 , 15 )) # .abs() _ = pd . Series ( selected_features_coefficients , index = selected_feature_names ) . sort_values () . plot ( ax = ax , kind = 'barh' ) fig . savefig ( config . feature_importance , format = \"png\" , dpi = 300 )","title":"Interpretation of Coefficients"},{"location":"reighns_ml_journey/projects/gj_regression/house_sales_in_king_country_usa/#interpretation-of-metric-scores-on-train-set","text":"We are also interested in choosing an optimal threshold for the model such that it gives the lowest recall, or False Negatives. We note that the default threshold when calling predict() from a model is \\(0.5\\) . In this section, we will explore one way to get the best tradeoff we can when choosing a high recall, while maintaining a reasonable score for precision. def evaluate_train_test_set ( estimator : Callable , X : Union [ pd . DataFrame , np . ndarray ], y : Union [ pd . DataFrame , np . ndarray ] ) -> Dict [ str , Union [ float , np . ndarray ]]: \"\"\"This function takes in X and y and returns a dictionary of scores. Args: estimator (Callable): [description] X (Union[pd.DataFrame, np.ndarray]): [description] y (Union[pd.DataFrame, np.ndarray]): [description] Returns: Dict[str, Union[float, np.ndarray]]: [description] \"\"\" test_results = {} y_pred = estimator . predict ( X ) # This is the probability array of class 1 (malignant) y_prob = estimator . predict_proba ( X )[:, 1 ] test_brier = metrics . brier_score_loss ( y , y_prob ) test_roc = metrics . roc_auc_score ( y , y_prob ) test_results [ \"brier\" ] = test_brier test_results [ \"roc\" ] = test_roc test_results [ \"y\" ] = np . asarray ( y ) . flatten () test_results [ \"y_pred\" ] = y_pred . flatten () test_results [ \"y_prob\" ] = y_prob . flatten () return test_results def plot_precision_recall_vs_threshold ( precisions , recalls , thresholds ): \"\"\" Modified from: Hands-On Machine learning with Scikit-Learn and TensorFlow; p.89 and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 \"\"\" plt . figure ( figsize = ( 8 , 8 )) plt . title ( \"Precision and Recall Scores as a function of the decision threshold\" ) plt . plot ( thresholds , precisions [: - 1 ], \"b--\" , label = \"Precision\" ) plt . plot ( thresholds , recalls [: - 1 ], \"g-\" , label = \"Recall\" ) plt . ylabel ( \"Score\" ) plt . xlabel ( \"Decision Threshold\" ) plt . legend ( loc = 'best' ) plt . savefig ( config . precision_recall_threshold_plot , format = \"png\" , dpi = 300 ) def plot_roc_curve ( fpr , tpr , label = None ): \"\"\" The ROC curve, modified from Hands-On Machine learning with Scikit-Learn and TensorFlow; p.91 and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 \"\"\" plt . figure ( figsize = ( 8 , 8 )) plt . title ( 'ROC Curve' ) plt . plot ( fpr , tpr , linewidth = 2 , label = label ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], 'k--' ) plt . axis ([ - 0.005 , 1 , 0 , 1.005 ]) plt . xticks ( np . arange ( 0 , 1 , 0.05 ), rotation = 90 ) plt . xlabel ( \"False Positive Rate\" ) plt . ylabel ( \"True Positive Rate (Recall)\" ) plt . legend ( loc = 'best' ) plt . savefig ( config . roc_plot , format = \"png\" , dpi = 300 ) def adjusted_classes ( y_scores , t ): \"\"\" This function adjusts class predictions based on the prediction threshold (t). Will only work for binary classification problems. and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 \"\"\" return [ 1 if y >= t else 0 for y in y_scores ] The plots below show the tradeoffs between precision and recall, recall and false positive rate. The confusion matrix on the train set tells us that there is still more false negatives than false positives. We can choose a particular threshold in order to minimize false negatives, at some expense of false positive. train_results = evaluate_train_test_set ( grid , X_train , y_train ) fig , ax = plt . subplots ( figsize = ( 10 , 10 )) # CM cm_train = metrics . confusion_matrix ( train_results [ 'y' ], train_results [ 'y_pred' ]) #### scores auc = metrics . roc_auc_score ( train_results [ 'y' ], train_results [ 'y_prob' ]) #### annotations labels = [ \"True Neg\" , \"False Pos\" , \"False Neg\" , \"True Pos\" ] counts = [ \" {0:0.0f} \" . format ( value ) for value in cm_train . flatten ()] percentages = [ \" {0:.2%} \" . format ( value ) for value in cm_train . flatten () / np . sum ( cm_train )] #### final annotations label = ( np . array ([ f \" { v1 } \\n { v2 } \\n { v3 } \" for v1 , v2 , v3 in zip ( labels , counts , percentages )]) ) . reshape ( 2 , 2 ) # heatmap sns . heatmap ( data = cm_train , vmin = 0 , vmax = 330 , cmap = [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ], linewidth = 2 , linecolor = \"white\" , square = True , ax = ax , annot = label , fmt = \"\" , cbar = False , annot_kws = { \"size\" : 10 , \"color\" : \"black\" , \"weight\" : \"bold\" , \"alpha\" : 0.8 }, alpha = 1 , ) ax . scatter ( 1 , 1 , s = 3500 , c = \"white\" ) ax . text ( 0.72 , 1.0 , \"AUC: {} \" . format ( round ( auc , 3 )), { \"size\" : 10 , \"color\" : \"black\" , \"weight\" : \"bold\" }, ) ## ticks and labels ax . set_xticklabels ( \"\" ) ax . set_yticklabels ( \"\" ) ## titles and text fig . text ( 0 , 1.05 , \"Train Set Confusion Matrix\" , { \"size\" : 22 , \"weight\" : \"bold\" }, alpha = 1 ) fig . text ( 0 , 1 , \"\"\"Training Set Confusion Matrix.\"\"\" , { \"size\" : 12 , \"weight\" : \"normal\" }, alpha = 0.98 , ) fig . tight_layout ( pad = 2.5 , w_pad = 2.5 , h_pad = 2.5 ) fig . savefig ( config . final_train_confusion_matrix , format = 'png' , dpi = 300 ) # generate the precision recall curve precision , recall , pr_thresholds = metrics . precision_recall_curve ( train_results [ 'y' ], train_results [ 'y_prob' ]) fpr , tpr , roc_thresholds = metrics . roc_curve ( train_results [ 'y' ], train_results [ 'y_prob' ], pos_label = 1 ) # use the same p, r, thresholds that were previously calculated plot_precision_recall_vs_threshold ( precision , recall , pr_thresholds ) Based on the tradeoff plot above, a good threshold can be set at \\(t = 0.35\\) , let us see how it performs with this threshold. y_pred_adj = adjusted_classes ( train_results [ \"y_prob\" ], t = 0.35 ) print ( pd . DataFrame ( metrics . confusion_matrix ( train_results [ \"y\" ], y_pred_adj ), columns = [ \"pred_neg\" , \"pred_pos\" ], index = [ \"neg\" , \"pos\" ], ) ) print ( metrics . classification_report ( y_true = train_results [ \"y\" ], y_pred = y_pred_adj )) train_brier = train_results [ 'brier' ] print ( f \"train brier: { train_brier } \" ) The False Negatives reduced from 15 to 9, at the expense of increase False Positives from 6 to 14. We should take comfort that less patients are falsely classified as benign when in fact they need treatment. This is a tradeoff that we have to choose. The ROC curve below also paints a similar story, in order for you to have high recall, one must accept that there will more False Positives. plot_roc_curve ( fpr , tpr , 'recall_optimized' )","title":"Interpretation of Metric Scores on Train Set"},{"location":"reighns_ml_journey/projects/gj_regression/house_sales_in_king_country_usa/#evaluation-on-test-set","text":"Ultimately, we are interested in finding the estimate of the generalization error of the model, and thus it is time for us to evaluate our model's performance on the \"unseen\" test set \\(X_{\\text{test}}\\) to get a good gauge on how well the model generalizes to unseen data. Take note that now everything has settled, we will use the exact hyperparameters to predict on test set, with the pre-defined threshold of 0.35. test_results = evaluate_train_test_set ( grid , X_test , y_test ) y_test_pred_adj = adjusted_classes ( test_results [ 'y_prob' ], t = 0.35 ) print ( pd . DataFrame ( metrics . confusion_matrix ( test_results [ 'y' ], y_test_pred_adj ), columns = [ 'pred_neg' , 'pred_pos' ], index = [ 'neg' , 'pos' ])) test_roc = test_results [ 'roc' ] test_brier = test_results [ 'brier' ] print ( test_roc ) print ( test_brier ) print ( metrics . classification_report ( y_true = test_results [ \"y\" ], y_pred = y_test_pred_adj )) Using the same threshold we used on training set, we see that the False Negative is quite low. The overall ROC score is 0.9828, and the corresponding Brier Score is 0.04136, both seem reasonably well performing.","title":"Evaluation on Test Set"},{"location":"reighns_ml_journey/projects/gj_regression/house_sales_in_king_country_usa/#bias-variance-tradeoff","text":"avg_expected_loss , avg_bias , avg_var = bias_variance_decomp ( grid . best_estimator_ [ 'model' ], X_train . values , y_train . values , X_test . values , y_test . values , loss = '0-1_loss' , random_seed = 123 ) print ( 'Average expected loss: %.3f ' % avg_expected_loss ) print ( 'Average bias: %.3f ' % avg_bias ) print ( 'Average variance: %.3f ' % avg_var ) We use the mlxtend library to estimate the Bias-Variance Tradeoff in our Logistic Regression model. The core idea behind this function is to use bagging and repeatedly sample from our training set so as to simulate that we are actually drawing samples from the \"true\" population over a distribution \\(\\mathcal{P}\\) . As expected, Logistic Regression being a linear model, its simplicity contributes to its high bias and low variance.","title":"Bias-Variance Tradeoff"}]}