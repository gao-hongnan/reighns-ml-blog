{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f9cc59d-a850-4943-8950-3b66d5246e2f",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=FppOzcDvaDI&list=PLhhyoLH6Ijfw0TpCTVTNk42NN08H6UvNq&index=4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c87cf0-1ee5-4456-93ab-d73de1b3b782",
   "metadata": {},
   "source": [
    "1. Get all bounding box predicitons on our test set. Green is GT and red is predicted.\n",
    "2. Associated bbox confidence score is something like IOU score but not exactly.\n",
    "3. How to define TP or FP is to see IOU, if IOU threshold is 0.5, then if a **predicted bbox** has an IOU of > 0.5 with a gt bbox, then it is TP, else it is FP.\n",
    "\n",
    "- if IoU â‰¥0.5, classify the object detection as True Positive(TP)\n",
    "- if Iou <0.5, then it is a wrong detection and classify it as False Positive(FP)\n",
    "- When a ground truth is present in the image and model failed to detect the object, classify it as False Negative(FN).\n",
    "- True Negative (TN): TN is every part of the image where we did not predict an object. This metrics is not useful for object detection, hence we ignore TN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851a1636-2435-40ac-83da-7d793f506d7e",
   "metadata": {},
   "source": [
    "4. At minute 3.10, important, he sort the table of all test predictions, sort by descending confidence score.\n",
    "    - precision = TP / (TP + FP)\n",
    "    - recall = TP / (TP + FN)\n",
    "    - In the table he tried to calculate the precision and recall per row. but imo for me its easy just calculate it column wise! like in the table there are 7 rows, with 3 tp and 4 fp, so precision is just 3/7 and recall is 3/4, dont get confused why recall is not 1 here, cause one didnt see any fn in the table, but TP + FN is also the total num of positive samples, which is the total num of gt bboxes which is 4. a bit confusing i must say. but col wise cant plot so a bit confusing\n",
    "    \n",
    "5. at min 7, he said avg precision for class dog.??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c1f498-a849-4dc6-8cc7-05b2a6645aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [\"positive\", \"negative\", \"positive\", \"negative\", \"positive\", \"positive\", \"positive\", \"negative\", \"positive\", \"negative\"]\n",
    "\n",
    "pred_scores = [0.7, 0.3, 0.5, 0.6, 0.55, 0.9, 0.75, 0.2, 0.8, 0.3]\n",
    "\n",
    "thresholds = numpy.arange(start=0.2, stop=0.9, step=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836a42de-a35f-4f99-93ca-e4bc145168bc",
   "metadata": {},
   "source": [
    "**Important: you should follow this tutorial https://towardsdatascience.com/on-object-detection-metrics-with-worked-example-216f173ed31e and list out the table to calculate MAP, and a much better understanding ensues!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d778ef-fbac-4613-9ddd-48f298cef82e",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/metrics/computer_vision_metrics/mean_average_precision/blood_cells_map_calc.png\" align=\"center\"/></p>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/metrics/computer_vision_metrics/mean_average_precision/blood_cells_map_calc_table.PNG\" align=\"center\"/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417321b5-e8e1-4919-b522-d3ed42295b58",
   "metadata": {},
   "source": [
    "- Let us label the ground truth bboxes above to be: $[1, 2, 3, 4, 5, 6, 7, 8, 9]$ (**rmb to label them above when I have time**) which are matched with\n",
    "- the predicted bboxes $\\{1: [a], 2: [b], 3: [c, d], 4: [e], 5: [f, g], 6: [h], 7: [], 8: [i, k], 9: [j], ?: [l] \\}$\n",
    "- Notice that gt bbox 7 has no predicted bbox and pred bbox $l$ has no match gt bbox.\n",
    "- And also the IOUs are listed above in the table, for example pred bbox c and d have IOU of 47 and 42 with gt bbox 3.\n",
    "- The MAP curve is parametrized by confidence threshold but TP/FP cutoff is parametrized by IOU (this is important to grasp!) unlike normal classification where for eg AUROC curve is parametrized by probability threshold and TP/FP are also parametrized by probablity threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76221524-f49a-46b0-9724-10af64d4936f",
   "metadata": {},
   "source": [
    "**Some Rules**\n",
    "\n",
    "- We establish the IOU threshold to be 0.5, this will determine if a bbox is TP or FP.\n",
    "- We establish the case of gt bbox 3, both of them are FP since they are below IOU threshold 0.5;\n",
    "- We establish the case of gt bbox 5, where we take pred bbox g to be TP but pred bbox f to be FP. \n",
    "    - The source: Some detectors can output multiple detections overlapping a single ground truth. For those cases the detection with the highest confidence is considered a TP and the others are considered as FP, as applied by the PASCAL VOC 2012 challenge. -Source: A Survey on Performance Metrics for Object-Detection Algorithms paper.\n",
    "    - This is intuitive as well since this is object detection problem, therefore a gt bbox should only have 1 corresponding pred bbox. If you do not punish the additional pred bboxes, then it will inflate the TP value!\n",
    "\n",
    "**Algorithm**\n",
    "\n",
    "- Sort all pred bboxes by their confidence scores, in here we have 12 predicted bboxes with 9 gt bboxes.\n",
    "- At the first row and also the first *confidence threshold*, we denote using our rules to check if this pred bbox $e$ is a TP or FP. \n",
    "    - It is TP and so our precision is 1 and recall is $1/9 = 0.11$. \n",
    "    - Note $TP + FN = 9$ always for object detection since there are 9 gt bboxes. \n",
    "    - Now we have a tuple of $(precision, recall)$ for this particular confidence threshold.\n",
    "    - You now can think of our usual classification problem, where we use probability logits as threshold to get each pair of precision recall.\n",
    "    - Another way to think is: if given the stringent confidence score (since sorted as highest), let's see how our model perform. Let us have this thinking.\n",
    "    - Also, take note that in the first row, we are also asking: ok so we can only predict 1 bbox, how good is our model prediction?\n",
    "\n",
    "- At the second row, we see that $a$ is a TP still.\n",
    "    - precision is 1 and recall 0.22;\n",
    "    - Note carefully that precision here is **cumulative** and so is **recall**, that is precision is calculated as 2/2 and recall as 2/9.\n",
    "    - We have a new tuple of $(precision, recall)$ for this threshold of 0.98.\n",
    "    - We can now think again: if given the confidence at 0.98 cutoff, how does our model perform? *note carefully that at 0.98 cutoff, we are including the first row's 0.99*.\n",
    "    - We can now think again: if the model can predict 2 bboxes, how good is our predictions?\n",
    "    \n",
    "- We continue this process:\n",
    "\n",
    "> at each confidence level (threshold), we ask what is the precision-recall score of the predictions of **all bounding boxes at a specific IOU** while discarding off those below the threshold, then average them over all thresholds? That's MAP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
