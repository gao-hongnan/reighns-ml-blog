{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40c87cf0-1ee5-4456-93ab-d73de1b3b782",
   "metadata": {},
   "source": [
    "1. Get all bounding box predicitons on our test set. Green is GT and red is predicted.\n",
    "2. Associated bbox confidence score is something like IOU score but not exactly.\n",
    "3. How to define TP or FP is to see IOU, if IOU threshold is 0.5, then if a **predicted bbox** has an IOU of > 0.5 with a gt bbox, then it is TP, else it is FP.\n",
    "\n",
    "- if IoU â‰¥0.5, classify the object detection as True Positive(TP)\n",
    "- if Iou <0.5, then it is a wrong detection and classify it as False Positive(FP)\n",
    "- When a ground truth is present in the image and model failed to detect the object, classify it as False Negative(FN).\n",
    "- True Negative (TN): TN is every part of the image where we did not predict an object. This metrics is not useful for object detection, hence we ignore TN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851a1636-2435-40ac-83da-7d793f506d7e",
   "metadata": {},
   "source": [
    "4. At minute 3.10, important, he sort the table of all test predictions, sort by descending confidence score.\n",
    "    - precision = TP / (TP + FP)\n",
    "    - recall = TP / (TP + FN)\n",
    "    - In the table he tried to calculate the precision and recall per row. but imo for me its easy just calculate it column wise! like in the table there are 7 rows, with 3 tp and 4 fp, so precision is just 3/7 and recall is 3/4, dont get confused why recall is not 1 here, cause one didnt see any fn in the table, but TP + FN is also the total num of positive samples, which is the total num of gt bboxes which is 4. a bit confusing i must say. but col wise cant plot so a bit confusing\n",
    "    \n",
    "5. at min 7, he said avg precision for class dog.??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c1f498-a849-4dc6-8cc7-05b2a6645aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [\"positive\", \"negative\", \"positive\", \"negative\", \"positive\", \"positive\", \"positive\", \"negative\", \"positive\", \"negative\"]\n",
    "\n",
    "pred_scores = [0.7, 0.3, 0.5, 0.6, 0.55, 0.9, 0.75, 0.2, 0.8, 0.3]\n",
    "\n",
    "thresholds = numpy.arange(start=0.2, stop=0.9, step=0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
