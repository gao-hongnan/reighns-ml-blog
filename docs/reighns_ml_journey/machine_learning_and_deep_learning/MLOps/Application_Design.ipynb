{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIESAz_wezJG"
   },
   "source": [
    "## Clarifying Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfXQz94nfFYN"
   },
   "source": [
    "### What is the Business Problem?\n",
    "\n",
    "What is our business problem? Understanding the business requirements and motivations from our stakeholders will save us a lot of time. After all, one does not wish to design a model just to know it is not what the user/stakeholder wants.\n",
    "\n",
    "So the first step in this is to discuss and understand the business requirements and lay out some technical constraints and assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8q0AjYCKgW00"
   },
   "source": [
    "### Clarifying The Business Problem\n",
    "\n",
    "Sometimes it is good to prompt new questions in case the stakeholders miss it.\n",
    "\n",
    "Potential questions:\n",
    "\n",
    "- Was there already a baseline to your problem?\n",
    "- Is there a benefit structure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DUdLQTVf_9f"
   },
   "source": [
    "## Clarifying Constraints\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efyHNuOwg4H0"
   },
   "source": [
    "### Data\n",
    "\n",
    "As always, we always want to ask:\n",
    "\n",
    "> How much data would we have access to? The consideration can be in line with the modelling process later. As a very naive example, for smaller datasets less complex models would be more appropriate but for bigger datasets, larger models like deep neural networks would work better. \n",
    "\n",
    "If there are little data, or rather say we are working with a classification problem, we have little data in the minority class, can we ask:\n",
    "\n",
    "> Are we able to collect more data? Is it very expensive to do so (both manually and financially). \n",
    "\n",
    "However, if the minority class is inherently rare (i.e. cancer), then we can keep it as it is sometimes since it represents the population distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kgpJ6nAhhqf_"
   },
   "source": [
    "### Hardware Constraints\n",
    "\n",
    "Do we have sufficient hardware for a complex task at hand? GPU matters quite a fair bit especially in deep learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3e5t72UtjHim"
   },
   "source": [
    "#### Where should the model be deployed on?\n",
    "\n",
    "Does the model need to be fit on a smartphone? I remembered download **YOLOv5**'s IOS app and had fun using it to detect objects around me. I reckon they have a real time inference in the app itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fL4RLhrTh949"
   },
   "source": [
    "### Latency\n",
    "\n",
    "When deployed, does the model need extremely fast inference time? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9g0QOYgiSE3"
   },
   "source": [
    "#### Oneline (Real Time)\n",
    "\n",
    "Is the application real time? A classic example is Google's autocomplete feature. I really like how the autocomplete knows roughly what I want to ask when I just typed a few words, however, if the model inference behind is slow (i.e. only recommending me after I finished typing the query), then the application usefulness is moot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEPjOVwqj09s"
   },
   "source": [
    "#### Offline (Non-Real Time)\n",
    "\n",
    "Maybe our model just want to do **Customer Segmentation** and we can afford more time to do inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBeOE0JpkKVA"
   },
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ir_AtvzMkLhx"
   },
   "source": [
    "### Offline Metrics\n",
    "\n",
    "The offline metrics are pretty much set in stone and are usually accustomed to the type of machine learning problem at hand.\n",
    "\n",
    "- Classification: Metrics like AUROC, F1 etc. Note accuracy may not be the best as it can be misleading.\n",
    "- Regression: The typical R-squared, MSE, RMSE and MAPE.\n",
    "- Object Detection: MAP, IOU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrkSxbW1kpEa"
   },
   "source": [
    "### Online Metrics\n",
    "\n",
    "This metric is after we deployed the model in production and say it is an web application taking in user uploaded images and predicting what animal it is. \n",
    "\n",
    "Then we can design metrics like **Click Through Rate (CTR)** because we want users to come to our websites more.\n",
    "\n",
    "In the case of a small example (modified), I always see my Iphone having **scan credit card** to autofill for me the details.\n",
    "\n",
    "- I envision this as an object detection problem behind the scenes where it first localize and classify the credit card (as sometimes user puts the credit card on the table and the camera captures other objects).\n",
    "- After this, we perform an OCR on it and gives the user the much needed details.\n",
    "\n",
    "Then a good online metric will be how many times the user **failed and return back to manual keying in**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHunovcBl7fR"
   },
   "source": [
    "### Non-Functional Metrics\n",
    "\n",
    "- Training speed etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Registry (Artifacts and Experiment Tracking) [^Comprehensive Article on Model Registry][^Made With ML's Experiment Tracking Section][^Weights & Biases as Model Registry]\n",
    "\n",
    "This is where MLOps come into play. We basically stores each version of model and its corresponding artifacts into a centralized store. Teams can view each others version on the cloud. \n",
    "\n",
    "Tracking model eases the deployment process by finding the best model and deploy it.\n",
    "\n",
    "\n",
    "\n",
    "[^Comprehensive Article on Model Registry]: Comprehensive Article on Model Registry: [https://neptune.ai/blog/ml-model-registry](https://neptune.ai/blog/ml-model-registry)\n",
    "[^Made With ML's Experiment Tracking Section]: Made With ML's Experiment Tracking Section: [https://madewithml.com/courses/mlops/experiment-tracking/](https://madewithml.com/courses/mlops/experiment-tracking/)\n",
    "[^Weights & Biases as Model Registry]: Weights & Biases as Model Registry: [https://docs.wandb.ai/guides/models](https://docs.wandb.ai/guides/models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "- [PyTorch](https://github.com/pytorch/serve/blob/master/README.md#serve-a-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sy45dC2h3Aim"
   },
   "source": [
    "## MLOps [^Google Cloud's Comprehensive Overview of MLOps][^Neptune's Blog on MLOps Architecture]\n",
    "\n",
    "\n",
    "[^Google Cloud's Comprehensive Overview of MLOps]: Google Cloud's Comprehensive Overview of MLOps: [https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)\n",
    "[^Neptune's Blog on MLOps Architecture]: Neptune's Blog on MLOps Architecture: [https://neptune.ai/blog/mlops-architecture-guide](https://neptune.ai/blog/mlops-architecture-guide) has a lot of good diagrams to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Science Steps for ML\n",
    "\n",
    "Google's blog detailed some common steps in a ML pipeline. Most of the consensus over this pipeline is similar, with only varying of terminologies. What the article wants to say is that, each step below defines the **maturity of your MLOps and DevOps**. \n",
    "\n",
    "In any ML project, after you define the business use case and establish the success criteria, the process of delivering an ML model to production involves the following steps. These steps can be completed manually or can be completed by an automatic pipeline.\n",
    "\n",
    "1. Data extraction: You select and integrate the relevant data from various data sources for the ML task.\n",
    "2. Data analysis: You perform exploratory data analysis (EDA) to understand the available data for building the ML model. This process leads to the following:\n",
    "    - Understanding the data schema and characteristics that are expected by the model.\n",
    "    - Identifying the data preparation and feature engineering that are needed for the model.\n",
    "3. Data preparation: The data is prepared for the ML task. This preparation involves data cleaning, where you split the data into training, validation, and test sets. You also apply data transformations and feature engineering to the model that solves the target task. The output of this step are the data splits in the prepared format.\n",
    "4. Model training: The data scientist implements different algorithms with the prepared data to train various ML models. In addition, you subject the implemented algorithms to hyperparameter tuning to get the best performing ML model. The output of this step is a trained model.\n",
    "5. Model evaluation: The model is evaluated on a holdout test set to evaluate the model quality. The output of this step is a set of metrics to assess the quality of the model.\n",
    "6. Model validation: The model is confirmed to be adequate for deploymentâ€”that its predictive performance is better than a certain baseline.\n",
    "7. Model serving: The validated model is deployed to a target environment to serve predictions. This deployment can be one of the following:\n",
    "    - Microservices with a REST API to serve online predictions.\n",
    "    - An embedded model to an edge or mobile device.\n",
    "    - Part of a batch prediction system.\n",
    "8. Model monitoring: The model predictive performance is monitored to potentially invoke a new iteration in the ML process.\n",
    "\n",
    "\n",
    "The level of automation of these steps defines the maturity of the ML process, which reflects the velocity of training new models given new data or training new models given new implementations. The following sections describe three levels of MLOps, starting from the most common level, which involves no automation, up to automating both ML and CI/CD pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLHZALuYl313"
   },
   "source": [
    "### MLOps Architecture Level 0 \n",
    "\n",
    "Many teams have data scientists and ML researchers who can build state-of-the-art models, but their process for building and deploying ML models is entirely manual. This is considered the basic level of maturity, or level 0. The following diagram shows the workflow of this process.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/deep_learning/MLOps/architecture.svg\" style=\"margin-left:auto; margin-right:auto\"/>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>MLOps Level 0: Manual ML steps to serve the model as a prediction service.</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Characteristics\n",
    "\n",
    "The following list highlights the characteristics of the MLOps level 0 process, as shown in the figure above:\n",
    "\n",
    "- Manual, script-driven, and interactive process: Every step is manual, including data analysis, data preparation, model training, and validation. It requires manual execution of each step, and manual transition from one step to another. This process is usually driven by experimental code that is written and executed in notebooks by data scientist interactively, until a workable model is produced.\n",
    "\n",
    "- Disconnection between ML and operations: The process separates data scientists who create the model and engineers who serve the model as a prediction service. The data scientists hand over a trained model as an artifact to the engineering team to deploy on their API infrastructure. This handoff can include putting the trained model in a storage location, checking the model object into a code repository, or uploading it to a models registry. Then engineers who deploy the model need to make the required features available in production for low-latency serving, which can lead to training-serving skew.\n",
    "\n",
    "- Infrequent release iterations: The process assumes that your data science team manages a few models that don't change frequentlyâ€”either changing model implementation or retraining the model with new data. A new model version is deployed only a couple of times per year.\n",
    "\n",
    "- No CI: Because few implementation changes are assumed, CI is ignored. Usually, testing the code is part of the notebooks or script execution. The scripts and notebooks that implement the experiment steps are source controlled, and they produce artifacts such as trained models, evaluation metrics, and visualizations.\n",
    "\n",
    "- No CD: Because there aren't frequent model version deployments, CD isn't considered.\n",
    "\n",
    "- Deployment refers to the prediction service: The process is concerned only with deploying the trained model as a prediction service (for example, a microservice with a REST API), rather than deploying the entire ML system.\n",
    "\n",
    "- Lack of active performance monitoring: The process doesn't track or log the model predictions and actions, which are required in order to detect model performance degradation and other model behavioral drifts.\n",
    "\n",
    "The engineering team might have their own complex setup for API configuration, testing, and deployment, including security, regression, and load and canary testing. In addition, production deployment of a new version of an ML model usually goes through A/B testing or online experiments before the model is promoted to serve all the prediction request traffic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenges\n",
    "\n",
    "MLOps level 0 is common in many businesses that are beginning to apply ML to their use cases. This manual, data-scientist-driven process might be sufficient when models are rarely changed or trained. In practice, models often break when they are deployed in the real world. The models fail to adapt to changes in the dynamics of the environment, or changes in the data that describes the environment. For more information, see Why Machine Learning Models Crash and Burn in Production.\n",
    "\n",
    "To address these challenges and to maintain your model's accuracy in production, you need to do the following:\n",
    "\n",
    "- Actively monitor the quality of your model in production: Monitoring lets you detect performance degradation and model staleness. It acts as a cue to a new experimentation iteration and (manual) retraining of the model on new data.\n",
    "\n",
    "- Frequently retrain your production models: To capture the evolving and emerging patterns, you need to retrain your model with the most recent data. For example, if your app recommends fashion products using ML, its recommendations should adapt to the latest trends and products.\n",
    "\n",
    "- Continuously experiment with new implementations to produce the model: To harness the latest ideas and advances in technology, you need to try out new implementations such as feature engineering, model architecture, and hyperparameters. For example, if you use computer vision in face detection, face patterns are fixed, but better new techniques can improve the detection accuracy.\n",
    "\n",
    "To address the challenges of this manual process, MLOps practices for CI/CD and CT are helpful. By deploying an ML training pipeline, you can enable CT, and you can set up a CI/CD system to rapidly test, build, and deploy new implementations of the ML pipeline. These features are discussed in more detail in the next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChvqTiveiVpq"
   },
   "source": [
    "- https://www.mle-interviews.com/ml-design-template\n",
    "- Ace the Machine Learning Interview\n",
    "- https://madewithml.com/courses/mlops\n",
    "- [Minimizing real-time prediction serving latency in machine learning ](https://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning): Talks about things like Online and Offline predictions etc.\n",
    "- [Google Cloud's Tutorial on MLOps](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning): Quite a good read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Application Design.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
