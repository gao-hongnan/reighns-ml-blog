{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries and Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T04:16:55.931110Z",
     "start_time": "2020-06-02T04:16:55.820406Z"
    }
   },
   "outputs": [],
   "source": [
    "# from preamble import *\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np \n",
    "import scipy as sp \n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm \n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "#from pandas.tools.plotting import scatter_matrix\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import string\n",
    "import math\n",
    "import sys\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sklearn\n",
    "\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "\n",
    "#importing from sklearn\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import learning_curve,GridSearchCV\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.formula.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "#import xgboost as xgb\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import scikitplot as skplt #conda install -c conda-forge scikit-plot\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/titanic_train.csv')\n",
    "test = pd.read_csv(\"data/titanic_test.csv\")\n",
    "test_labels = pd.read_csv(\"data/titanic_test_labels.csv\")\n",
    "\n",
    "\n",
    "\n",
    "median_fare = test.groupby(['Pclass', 'Parch']).Fare.median()[3][0]\n",
    "# Filling the missing value in Fare with the median Fare of 3rd class passenger who has Parch 0.\n",
    "test['Fare'] = test['Fare'].fillna(median_fare)\n",
    "train['Embarked'] = train['Embarked'].fillna('S')\n",
    "train['Age']  = train.groupby(['Pclass', 'SibSp'])['Age'].apply(lambda x: x.fillna(x.median()))\n",
    "\n",
    "# remember to fill in here, this one really very special case pertaining to this project\n",
    "\n",
    "train['Age'] = train.Age.fillna(11)\n",
    "test['Age'] = test.groupby(['Pclass', 'SibSp'])['Age'].apply(lambda x: x.fillna(x.median()))\n",
    "\n",
    "train['Cabin'] = train['Cabin'].fillna('M').astype(str).apply(lambda cabin: cabin[0])\n",
    "idx = train[train['Cabin'] == 'T'].index\n",
    "train.loc[idx, 'Cabin'] = 'A'\n",
    "\n",
    "test['Cabin'] = test['Cabin'].fillna('M').astype(str).apply(lambda cabin: cabin[0])\n",
    "\n",
    "# Create function that take name and separates it into title, family name and deletes all puntuation from name column:\n",
    "def name_sep(data):\n",
    "    families=[]\n",
    "    titles = []\n",
    "    new_name = []\n",
    "    #for each row in dataset:\n",
    "    for i in range(len(data)):\n",
    "        name = data.iloc[i]\n",
    "        # extract name inside brakets into name_bracket:\n",
    "        if '(' in name:\n",
    "            name_no_bracket = name.split('(')[0] \n",
    "        else:\n",
    "            name_no_bracket = name\n",
    "            \n",
    "        family = name_no_bracket.split(\",\")[0]\n",
    "        title = name_no_bracket.split(\",\")[1].strip().split(\" \")[0]\n",
    "        \n",
    "        #remove punctuations accept brackets:\n",
    "        for c in string.punctuation:\n",
    "            name = name.replace(c,\"\").strip()\n",
    "            family = family.replace(c,\"\").strip()\n",
    "            title = title.replace(c,\"\").strip()\n",
    "            \n",
    "        families.append(family)\n",
    "        titles.append(title)\n",
    "        new_name.append(name)\n",
    "            \n",
    "    return families, titles, new_name \n",
    "\n",
    "\n",
    "train['Surname'], train['Title'], train['Newname']  = name_sep(train.Name)\n",
    "test['Surname'], test['Title'], test['Newname'] = name_sep(test.Name)\n",
    "train['Title'] = train['Title'].replace(['Ms', 'Mlle'],'Miss')\n",
    "train['Title'] = train['Title'].replace(['Mme'],'Mrs')\n",
    "train['Title'] = train['Title'].replace(['Dr','Rev','the','Jonkheer','Lady','Sir', 'Don'],'Nobles')\n",
    "train['Title'] = train['Title'].replace(['Major','Col', 'Capt'],'Navy')\n",
    "train.Title.value_counts()\n",
    "\n",
    "\n",
    "\n",
    "test['Title'] = test['Title'].replace(['Ms','Dona'],'Miss')\n",
    "test['Title'] = test['Title'].replace(['Dr','Rev'],'Nobles')\n",
    "test['Title'] = test['Title'].replace(['Col'],'Navy')\n",
    "test.Title.value_counts()\n",
    "\n",
    "\n",
    "\n",
    "train_categorical_features = ['Pclass', 'Sex','Title','Cabin', 'Embarked']\n",
    "\n",
    "# No need to use sklearn's encoders\n",
    "# pandas has a pandas.get_dummies() function that takes in a series\n",
    "#     and returns a HOT encoded dataframe of that series\n",
    "#     use the add_prefix() method of dataframe to add the feature name in front of the category name\n",
    "#     then join the dataframe sideways (similar to pd.concat([train, dummies], axis=1))\n",
    "for feature in train_categorical_features:\n",
    "    dummies = pd.get_dummies(train[feature]).add_prefix(feature + '_')\n",
    "    train = train.join(dummies)\n",
    "    \n",
    "    \n",
    "    \n",
    "test_categorical_features = ['Pclass', 'Sex','Title', 'Cabin', 'Embarked']\n",
    "\n",
    "# No need to use sklearn's encoders\n",
    "# pandas has a pandas.get_dummies() function that takes in a series\n",
    "#     and returns a HOT encoded dataframe of that series\n",
    "#     use the add_prefix() method of dataframe to add the feature name in front of the category name\n",
    "#     then join the dataframe sideways (similar to pd.concat([train, dummies], axis=1))\n",
    "for feature in test_categorical_features:\n",
    "    dummies = pd.get_dummies(test[feature]).add_prefix(feature + '_')\n",
    "    test = test.join(dummies)\n",
    "    \n",
    "    \n",
    "    \n",
    "drop_column = ['Pclass','Name','Sex','Cabin', 'Embarked','Surname','Title','Newname', 'Ticket', 'PassengerId']\n",
    "train.drop(drop_column, axis=1, inplace = True)\n",
    "\n",
    "drop_column = ['Pclass','Name','Sex','Cabin', 'Embarked','Surname','Title','Newname', 'Ticket', 'PassengerId']\n",
    "test.drop(drop_column, axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start reading, do read this [link on why in practice, researchers like to first split the given data into train-validation-test set instead of just performing cross validation via train-test](https://stats.stackexchange.com/questions/152907/how-do-you-use-the-test-dataset-after-cross-validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the idea of Cross Validation with Train Test Split\n",
    "\n",
    "[Scikt learn Cross Validation Document](https://scikit-learn.org/stable/modules/cross_validation.html#computing-cross-validated-metrics)\n",
    "\n",
    "\n",
    "Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called **overfitting**. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set **X_test, y_test**. Note that the word “experiment” is not intended to denote academic use only, because even in commercial settings machine learning usually starts out experimentally. Here is a flowchart of typical cross validation workflow in model training. The best parameters can be determined by **[grid search techniques](https://scikit-learn.org/stable/modules/grid_search.html).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/images/cv.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before performing out modelling to our test data, we can split the training data in two using **`train_test_split`** for training and validating your model. Then, when you have a good performance on the validation part of the training data and are happy with your model, you can apply the model to the test data and submit your predicted survival values for the test. For beginners who do not know what this step is, I will provide a brief yet intuitive explanation. But do try and fiddle around with the 4 new variables I created, namely, x_train, y_train, x_test, y_test. \n",
    "\n",
    "\n",
    "So the idea is we split our training data **X** into 4 datasets, x_train means the training set without the outcome values (no survived column), x_train will serve as the training set. Now y_train set is the set of survived values corresponding to the x_train set. Similarly, we have x_test which is also extracted from the training set (PAY SPECIAL ATTENTION THAT x_test IS NOT REFERRING TO THE TEST SET given to us), but serving the purpose as test set here. And y_test set is the set of survived values corresponding to the x_test set.\n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                            random_state=0, shuffle = True)\n",
    "\n",
    "Now `train_size = 0.8`, `test_size = 0.2` just means we split the data set into 80 percent of training data, and 20 percent test data.\n",
    "\n",
    "\n",
    "\n",
    "`Random_state` is basically used for reproducing your problem the same every time it is run. If you do not use a random state in **`train_test_split`**, every time you make the split you might get a different set of train and test data points and will not help you in debugging in case you get an issue. Note, it does not matter which integer value you use for random state, as long as you set it to one integer value and keep to it throughout.\n",
    "\n",
    "\n",
    "We can then build a model, say **logistic regression** and call **`fit`** on the our training set x_train (which is 80% of our X) and evaluate this fitted set of parameters on our x_test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#1 In this tutorial, we will used the titanic data set.\n",
    "X = train.drop(\"Survived\", axis=1)\n",
    "y = train[\"Survived\"]\n",
    "\n",
    "#2 split data and labels into a training and a test set. Note the parameters we can put inside, we will explain\n",
    "#  the argument shuffle later on in the chapter.\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.8, test_size=0.2, random_state=0, shuffle = True)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "#3 instantiate a model logistic regression and fit it to the training set, note you can pass in many tuning parameters\n",
    "#  in the LogisticRegression() but for now we leave it in the basic form\n",
    "logreg = LogisticRegression().fit(x_train, y_train)\n",
    "\n",
    "#4 evaluate the model on the test set\n",
    "print(\"Test set score: {:.8f}\".format(logreg.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, the reason we split our data into training and test sets is that we are interested in measuring how well our model generalizes to new, previously unseen data. We are not interested in how well our model fit the training set, but rather in how well it can make predictions for data that was not observed during training. So we got a decent score of 82% on the supposedly test set.\n",
    "\n",
    "When evaluating different settings (“hyperparameters”) for estimators, such as the ridge setting that must be manually set for a Logistic Regression, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance. To solve this problem, yet another part of the dataset can be held out as a so-called “validation set”: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.\n",
    "\n",
    "However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A solution to this problem is a procedure called cross-validation (CV for short). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called **k-fold CV**, the training set is split into k smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the k “folds”:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation: K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general procedure for **K-fold Cross Validation** is as follows.\n",
    "\n",
    "1. Shuffle the dataset X randomly - provided you have **`shuffle = True`** in your parameter. It should be on by default.\n",
    "\n",
    "\n",
    "2. Split the dataset into **k** groups of approximately equal size depending on the **k** value and the size of the dataset (if k = 10, but X.shape = 999, then you cannot divide equally); **k** is a number you can choose, convention is **k** = 5 or 10.\n",
    "\n",
    "\n",
    "3. A sequence of **models** will be trained **k** number of times. Say **k = 5**, then for each **unique group**, the first model is trained using the first fold as the test set, and the remaining folds (2–5) are used as the training set. The model is built using the data in folds 2–5, and then the accuracy is evaluated on fold 1. Then another model is built, this time using fold 2 as the test set and the data in folds 1, 3, 4, and 5 as the training set. This process is repeated using folds 3, 4, and 5 as test sets. For each of these five splits of the data into training and test sets, we compute the accuracy (or the desired performance metrics).  In the end, we have collected five accuracy values. The process is illustrated in the Figures below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usually, the first fifth of the data is the first fold, the second fifth of the data is the second fold, and so on.\n",
    "mglearn.plots.plot_cross_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/images/cv2.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Cross Validated Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is implemented in scikit-learn using the **`cross_val_score`** function from the `model_selection` module. The parameters of the **`cross_val_score`** function are the **model** we want to evaluate, the **training data X**, and the **ground-truth labels y**. Let’s evaluate LogisticRegression on the titanic dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the **`sklearn.model_selection.cross_val_score()`** here and the [sklearn website documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score) is here.\n",
    "\n",
    "**`Default class: sklearn.model_selection.cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv=None, n_jobs=None, verbose=0, fit_params=None, pre_dispatch='2*n_jobs', error_score=nan)`** \n",
    "\n",
    "**Returns:** Array of scores of the estimator for each run of the cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#1 Default: class sklearn.model_selection.KFold(n_splits=5, shuffle=False, random_state=None)\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "#2 normalize it by using our StandardScaler(), will mention more on this later.\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "#3 instantiate a model logistic regression and fit it to the training set.\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "\n",
    "#4 Here, cross_val_score performed 10-fold cross-validation and therefore returns 10 scores, the scoring\n",
    "#  system is based on the accuracy metric.\n",
    "\n",
    "print(\"Accuracy Scores: \" + format(cross_val_score(logreg,X,y,cv=k_fold,scoring=\"accuracy\")))\n",
    "print(\" \") \n",
    "\n",
    "#5 A common way to summarize the cross-validation accuracy is to compute the mean of the 10 folds.\n",
    "\n",
    "print(\"Mean Accuracy Score: \", cross_val_score(logreg,X,y,cv=k_fold,scoring=\"accuracy\").mean())\n",
    "print(\" \")\n",
    "\n",
    "#6 Knowing the mean score may not be enough, we can have a 95% confidence interval of the score estimate given by:\n",
    "#  A variance value of 0.0007 score seems quite reasonably small and tells us the 10 scores are closely tied together\n",
    "#  and don't fluctuate much, which is good news.\n",
    "print(\"Variance of Accuracy:\", cross_val_score(logreg,X,y,cv=k_fold,scoring=\"accuracy\").std()**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we choose our way to train the model and get the result and outcomes, it is natural for us to consider the performances of the model! Now the very first step we want to check is the accuracy of the model! The formula for accuracy is simply given by \n",
    "\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{\\text{Number of correctly classified cases}}{\\text{Number of all cases}}$$\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "And as seen above, we first showed all our accuracy scores for each fold in the K-Fold CV validation, and the values are pretty close to each other with a standard deviation of 0.02733 - which in my opinion is a very acceptable range (one can further do some testing to show that the SD is justifiable low).\n",
    "\n",
    "<br>\n",
    "\n",
    "So our final accuracy score for Logistic Regression is 82.82 percent. But is that good enough? No. Let me give you one more problem. \n",
    "\n",
    "<br>\n",
    "\n",
    "For example, in our Titanic Problem, we want to classify data into survive or dead. And to make the example simple, we have 1000 data set in which we achieved an 80% accuracy, that is we predicted 800/1000 correctly. This 80% accuracy seems reasonable, but however it can still be extremely bad if we have many False Positives or False Negatives. In our case, let's say we have exactly 500 people who died and 500 people who lived - Our algorithm predicts 500/500 correctly for those people who survived, however only 300/500 correctly for those who died. Yes, the total accuracy does add up to 80 percent and it looks good on paper, but we realised that this algorithm is extremely incapable of prediction for people who died, and that can be a big problem because in reality, we may have say, 900 people who died and 100 who survived, one can imagine how disastrous it can be if we applied the algorithm and it will give you a much lower total accuracy rate!\n",
    "\n",
    "<br>\n",
    "\n",
    "**We shall explore some popular metrics to further assess the performance of the model - An extension to evaluate how accurate the model performs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the score computed at each CV iteration is the score method of the estimator. It is possible to change this by using the scoring parameter in **`cross_val_score`**. For example, if you want to check the `precision, recall or f1 score` you can pass it in as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#1 Default: class sklearn.model_selection.KFold(n_splits=5, shuffle=False, random_state=None)\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "#2 normalize it by using our StandardScaler(), will mention more on this later.\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "#3 instantiate a model logistic regression and fit it to the training set.\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "\n",
    "#4 Here, cross_val_score performed 10-fold cross-validation and therefore returns 10 scores,\n",
    "#  the scoring metric here demonstrates f1, recall and precision respectively.\n",
    "\n",
    "print(\"f1 Scores: \" + format(cross_val_score(logreg,X,y,cv=k_fold,scoring=\"f1\")))\n",
    "print(\"recall Scores: \" + format(cross_val_score(logreg,X,y,cv=k_fold,scoring=\"recall\")))\n",
    "print(\"precision Scores: \" + format(cross_val_score(logreg,X,y,cv=k_fold,scoring=\"precision\")))\n",
    "print(\" \") \n",
    "\n",
    "#5 A common way to summarize the cross-validation accuracy is to compute the mean of the 10 folds in their\n",
    "#  respective scoring metrics.\n",
    "\n",
    "print(\"Mean f1 Score: \", cross_val_score(logreg,X,y,cv=k_fold,scoring=\"f1\").mean())\n",
    "print(\"Mean recall Score: \", cross_val_score(logreg,X,y,cv=k_fold,scoring=\"recall\").mean())\n",
    "print(\"Mean precision Score: \", cross_val_score(logreg,X,y,cv=k_fold,scoring=\"precision\").mean())\n",
    "print(\" \")\n",
    "\n",
    "#6 Knowing the mean score may not be enough, we can have a 95% confidence interval of the score estimate for\n",
    "#  the respective scoring metrics given by:\n",
    "\n",
    "print(\"Variance of f1:\", cross_val_score(logreg,X,y,cv=k_fold,scoring=\"f1\").std()**2)\n",
    "print(\"Variance of recall:\", cross_val_score(logreg,X,y,cv=k_fold,scoring=\"recall\").std()**2)\n",
    "print(\"Variance of precision:\", cross_val_score(logreg,X,y,cv=k_fold,scoring=\"precision\").std()**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The other Cross Validate function\n",
    "\n",
    "There is a second function you can use for cross-validation, called **`cross_validate`**. \n",
    "\n",
    "The **`cross_validate`** function differs from **`cross_val_score`** in two ways:\n",
    "\n",
    "- It allows specifying multiple metrics for evaluation.\n",
    "\n",
    "- It returns a dict containing fit-times, score-times (and optionally training scores as well as fitted estimators) in addition to the test score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class is listed as follows: [sklearn website documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html)\n",
    "\n",
    "**`sklearn.model_selection.cross_validate(estimator, X, y=None, groups=None, scoring=None, cv=None, n_jobs=None, verbose=0, fit_params=None, pre_dispatch='2*n_jobs', return_train_score=False, return_estimator=False, error_score=nan)`**\n",
    "\n",
    "Notice that the main difference here is we can specify our **scoring metric** to be more than just 1 metric. This is useful as we can evaluate a few metric at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scoring_metrics = ['accuracy', 'f1', 'recall']\n",
    "\n",
    "\n",
    "#1 Default: class sklearn.model_selection.KFold(n_splits=5, shuffle=False, random_state=None)\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "#2 normalize it by using our StandardScaler(), will mention more on this later.\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "#3 instantiate a model logistic regression and fit it to the training set.\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "\n",
    "#4 Here, cross_val_score performed 10-fold cross-validation and therefore returns 10 scores, the scoring\n",
    "#  system is based on the accuracy, f1 and recall metric.\n",
    "\n",
    "print(\"Different Metric Scores: \" + format(cross_validate(logreg, X, y,\n",
    "                                                          cv=k_fold, scoring = scoring_metrics, return_train_score=False)))\n",
    "print(\" \") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pandas, we can nicely display these results and compute summaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(logreg, X, y, cv=k_fold, scoring = scoring_metrics, return_train_score=False)\n",
    "df = pd.DataFrame(scores)\n",
    "display(df)\n",
    "print(\"Mean times and scores:\\n{}\".format(df.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several benefits to using cross-validation instead of a single split into a training and a test set. First, remember that `train_test_split` performs a random split of the data. Imagine that we are “lucky” when randomly splitting the data, and all examples that are hard to classify end up in the training set. In that case, the test set will only contain “easy” examples, and our test set accuracy will be unrealistically high. Conversely, if we are “unlucky,” we might have randomly put all the hard-to-classify examples in the test set and consequently obtain an unrealistically low score. However, when using cross-validation, each example will be in the test set exactly once: each example is in one of the folds, and each fold is the test set once. Therefore, the model needs to generalize well to all of the samples in the dataset for all of the cross-validation scores (and their mean) to be high.\n",
    "\n",
    "Having multiple splits of the data also provides some information about how sensitive our model is to the selection of the training dataset. For the titanic dataset, we saw accuracies between 80% and 100%. This is quite a range, and it provides us with an idea about how the model might perform in the worst case and best case scenarios when applied to new data.\n",
    "\n",
    "Another benefit of cross-validation as compared to using a single split of the data is that we use our data more effectively. When using `train_test_split`, we usually use 75% of the data for training and 25% of the data for evaluation. When using five-fold cross-validation, in each iteration we can use four-fifths of the data (80%) to fit the model. When using 10-fold cross-validation, we can use nine-tenths of the data (90%) to fit the model. More data will usually result in more accurate models.\n",
    "\n",
    "The main disadvantage of cross-validation is increased computational cost. As we are now training k models instead of a single model, cross-validation will be roughly k times slower than doing a single split of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split() Super Important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold, (train_idx, val_idx) in enumerate(k_fold.split(X, y)):\n",
    "    print(fold, len(train_idx), len(val_idx))\n",
    "    #df.loc[val_idx, 'kfold'] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuing from the titanic example\n",
    "loop = 1\n",
    "for train, validation in k_fold.split(X):\n",
    "    print(\"\\n \\nLoop {}:\\n \\n train:{}\\n \\n validation:{}\".format(loop,train, validation))\n",
    "    loop = loop + 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is simple, each loop of `k_fold.split(X)` generates the `train and validation`'s folds **indices index**: each fold is constituted by 2 arrays/lists, the first list is the training set, while the second list is the test/validation set. This is extremely useful later on each section 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to use Different Cross Validation Methods/Iterators\n",
    "\n",
    "There are more than one cross validation methods. **K-fold** is merely one of them, but if there are so many cross validation strategies, which one should we use? This is important and a general rule of thumb is as follows.\n",
    "\n",
    "I believe one should also read this [stackexchange link to gain some intuition](https://stats.stackexchange.com/questions/103459/how-do-i-know-which-method-of-cross-validation-is-best)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When your data is independent i.i.d\n",
    "\n",
    "Assuming that some data is Independent and Identically Distributed (i.i.d.) is making the assumption that all samples stem from the same generative process and that the generative process is assumed to have no memory of past generated samples.\n",
    "In Laymen terms, i.i.d just says that each and every training rows you have must be independent.\n",
    "\n",
    "The following cross-validators can be used in such cases.\n",
    "\n",
    "**Note:** While i.i.d. data is a common assumption in machine learning theory, it rarely holds in practice. If one knows that the samples have been generated using a time-dependent process, it’s safer to use a [time-series aware cross-validation scheme](https://scikit-learn.org/stable/modules/cross_validation.html#timeseries-cv). Similarly if we know that the generative process has a group structure (samples from collected from different subjects, experiments, measurement devices) it safer to use [group-wise cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html#group-cv)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " I had a rough time trying to understand what does it mean for a dataset to be i.i.d.\n",
    " \n",
    " I refer you to some great links/articles [here](https://stats.stackexchange.com/questions/213464/on-the-importance-of-the-i-i-d-assumption-in-statistical-learning) and [here](https://www.ijcai.org/Proceedings/07/Papers/121.pdf) and [here](https://www.statisticshowto.datasciencecentral.com/assumption-of-independence/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set is independent/dependent can be illustrated **naively** with examples below. A more detailed and curated version will appear in **Section 2.3**.\n",
    "\n",
    "*If the occurrence of one event does not affect the occurrence or non-occurrence of the other, we say the events are independent.*\n",
    "\n",
    "Let the selection of a red ball be event A, and the selection of a black ball be event B. \n",
    "\n",
    "[Example](http://www.math.usu.edu/cfairbourn/SCORM/IVC/Chance_sco/Chance_print.html)\n",
    "\n",
    "**Independent:** A box contains two red balls and four green balls. We randomly select two balls with replacement from the box. These six balls constitute a population. \"With replacement\" means that once we select the first ball we put it back in the box before we select the second ball. Drawing with replacement makes the draws independent of each other, since the color of the first ball drawn does not affect the color of the second ball. **Event A and Event B are independent since the occurrence of Event A does not affect the occurrence of Event B**.\n",
    "\n",
    "**Non-Independent:** A box contains two red balls and four green balls. Suppose this time we randomly select two balls without replacement from the box. \"Without replacement\" means that once we select the first ball we do not put it back in the box before we select the second ball. Drawing without replacement makes the draws dependent events, since the color of the first ball drawn does affect the color of the second ball. **Event A and Event B are dependent since the occurrence of Event A does affect the occurrence of Event B and vice versa**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First method: K-Fold\n",
    "\n",
    "**K-fold** has been explained in section 2, remember we can pass **K-fold** as the **cv** parameter in **cross_val_score**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second method: Repeated K-Fold\n",
    "\n",
    "[Repeated K-Fold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedKFold.html#sklearn.model_selection.RepeatedKFold) is documented in sklearn here. In repeated cross-validation the data is randomly split into **k** partitions **j** times. The performance of the model can thereby be averaged over several runs, but this is rarely desirable in practice. \n",
    "\n",
    "We illustrate it with the same titanic data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "#1 Default: class sklearn.model_selection.RepeatedKFold(n_splits=5, n_repeats=10, random_state=None)\n",
    "repeated_k_fold = RepeatedKFold(n_splits=10, n_repeats = 10, random_state=0)\n",
    "\n",
    "#2 normalize it by using our StandardScaler(), will mention more on this later.\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "#3 instantiate a model logistic regression and fit it to the training set.\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "\n",
    "#4 Here, cross_val_score performed 10-fold cross-validation and therefore returns 10 scores, the scoring\n",
    "#  system is based on the accuracy metric.\n",
    "\n",
    "print(\"Accuracy Scores: \" + format(cross_val_score(logreg,X,y,cv = repeated_k_fold,scoring=\"accuracy\")))\n",
    "print(\" \") \n",
    "\n",
    "#5 A common way to summarize the cross-validation accuracy is to compute the mean of the 10 folds.\n",
    "\n",
    "print(\"Mean Accuracy Score: \", cross_val_score(logreg,X,y, cv=repeated_k_fold,scoring=\"accuracy\").mean())\n",
    "print(\" \")\n",
    "\n",
    "#6 Knowing the mean score may not be enough, we can have a 95% confidence interval of the score estimate given by:\n",
    "#  A variance value of 0.0007 score seems quite reasonably small and tells us the 10 scores are closely tied together\n",
    "#  and don't fluctuate much, which is good news.\n",
    "print(\"Variance:\", cross_val_score(logreg,X,y, cv=repeated_k_fold,scoring=\"accuracy\").std()**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Method: Leave One Out (LOO) and Leave P out (LPO)\n",
    "\n",
    "We will not discuss this method because according to a general rule from literatures, authors and empirical evidence, a 5- or 10- fold cross validation should be preferred to LOO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth Method: Random permutations cross-validation a.k.a. Shuffle & Split¶\n",
    "\n",
    "The **ShuffleSplit** iterator will generate a user defined number of independent train / test dataset splits. Samples are first shuffled and then split into a pair of train and test sets.\n",
    "\n",
    "It is possible to control the randomness for reproducibility of the results by explicitly seeding the random_state pseudo random number generator.\n",
    "\n",
    "Have not used myself yet - to be updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When your Data is highly imbalanced\n",
    "\n",
    "Some classification problems can exhibit a large imbalance in the distribution of the target classes: for instance there could be several times more negative samples than positive samples. In such cases it is recommended to use stratified sampling as implemented in StratifiedKFold and StratifiedShuffleSplit to ensure that relative class frequencies is approximately preserved in each train and validation fold.\n",
    "\n",
    "\n",
    "**Stratification preserves the same target distribution over different folds (read 6.2.1.1's example).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Method: Stratified K-Fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balanced number of classes but with an ordered sequence\n",
    "\n",
    "The first example will showcase a dataset with a balanced number of classes but with an ordered sequence.\n",
    "\n",
    "Let us deviate a while from titanic and take a look at the iris data set, we print out the Iris labels, the target class is **0, 1 and 2** with **50 occurences each in sequence**. So indeed its target class is balanced because each class 0, 1 and 2 has an equal number of occurences. But the problem here is the sequence.\n",
    "\n",
    "Why, one may ask. As you can see, the first third of the data is the class 0, the second third is the class 1, and the last third is the class 2. Imagine doing **three-fold cross-validation** on this dataset. The first fold would be only class 0, so in the first split of the data, the test set would be only class 0, and the training set would be only classes 1 and 2. As the classes in training and test sets would be different for all three splits, the three-fold cross-validation accuracy would be zero on this dataset. That is not very helpful, as we can do much better than 0% accuracy on iris. \n",
    "\n",
    "Some people might be confused, why would the accuracy of our classifier be 0%? Let us dissect this intuition with an example classifier **Logistic Regression**. The formula for logistic regression is basically something like the following, I am being pedantic here, so don't mind the details: $$P(Y = \\text{class i}~|~ X = \\text{inputs}) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ...$$\n",
    "\n",
    "We look at our first fold, our training set **only have classes 1 and 2**, and if we define our class 1 to be variable $x_1$, class 2 to be variable $x_2$ and class 3 to be variable $x_3$, then one should realise in a intuitive manner that the **Logistic Regression** model can only spit out something like $$P(Y = \\text{class i=1,2}~|~ X = \\text{inputs}) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2$$ \n",
    "\n",
    "There is no $x_3$ in our logistic model's equation, simply because there isn't any class 3 in our training set. Therefore the logic is easy, your test set has class 3 labels, but our model can **only ever predict** class 1 and 2. And in this extreme scenario, the **test set** in the first split only consists of class 3 labels, consequently, our classifier never predicts anything correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "print(\"Iris labels:\\n{}\".format(iris.target))\n",
    "unique, counts = np.unique(iris.target, return_counts=True)\n",
    "print(\"Iris class labels counts:\\n\", dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_stratified_cross_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see what the **K-Fold** will yield us, with **shuffle = False** at first! Because *secret, secret*, once you turn on the **shuffle = True**, it actually solves this particular scenario.\n",
    "\n",
    "So in this code below, we **ONLY SPECIFY** the number of folds to be 3 and validate that our intuition was correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "#1 Default: class sklearn.model_selection.KFold(n_splits=5, shuffle = False, random_state=None)\n",
    "k_fold = KFold(n_splits = 3,  shuffle = False, random_state=0)\n",
    "\n",
    "\n",
    "#2 instantiate a model logistic regression and fit it to the training set.\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "\n",
    "#3 Here, cross_val_score performed 10-fold cross-validation and therefore returns 10 scores, the scoring\n",
    "#  system is based on the accuracy metric.\n",
    "\n",
    "print(\"Accuracy Scores: \" + format(cross_val_score(logreg, iris.data, iris.target, cv = k_fold, scoring=\"accuracy\")))\n",
    "print(\" \") \n",
    "\n",
    "#4 A common way to summarize the cross-validation accuracy is to compute the mean of the 10 folds.\n",
    "\n",
    "print(\"Mean Accuracy Score: \", cross_val_score(logreg, iris.data, iris.target, cv=k_fold,scoring=\"accuracy\").mean())\n",
    "print(\" \")\n",
    "\n",
    "#5 Knowing the mean score may not be enough, we can have a 95% confidence interval of the score estimate given by:\n",
    "#  A variance value of 0.0007 score seems quite reasonably small and tells us the 10 scores are closely tied together\n",
    "#  and don't fluctuate much, which is good news.\n",
    "print(\"Standard Deviation:\", cross_val_score(logreg, iris.data, iris.target, cv=k_fold,scoring=\"accuracy\").std()**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the accuracy for **3 folds** is 0. But if we increase our folds, the accuracy will change drastically for the better. Remember: each fold corresponds to one of the classes in the iris dataset, and so nothing can be learned. Another way to resolve this problem is to shuffle the data instead of stratifying the folds, to remove the ordering of the samples by label. We can do that by setting the shuffle parameter of KFold to True. If we shuffle the data, we also need to fix the random_state to get a reproducible shuffling. Otherwise, each run of cross_val_score would yield a different result, as each time a different split would be used (this might not be a problem, but can be surprising). Shuffling the data before splitting it yields a much better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "#1 Default: class sklearn.model_selection.KFold(n_splits=5, shuffle = False, random_state=None)\n",
    "k_fold = KFold(n_splits = 3,  shuffle = True, random_state=0)\n",
    "\n",
    "\n",
    "#2 instantiate a model logistic regression and fit it to the training set.\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "\n",
    "#3 Here, cross_val_score performed 10-fold cross-validation and therefore returns 10 scores, the scoring\n",
    "#  system is based on the accuracy metric.\n",
    "\n",
    "print(\"Accuracy Scores: \" + format(cross_val_score(logreg, iris.data, iris.target, cv = k_fold, scoring=\"accuracy\")))\n",
    "print(\" \") \n",
    "\n",
    "#4 A common way to summarize the cross-validation accuracy is to compute the mean of the 10 folds.\n",
    "\n",
    "print(\"Mean Accuracy Score: \", cross_val_score(logreg, iris.data, iris.target, cv=k_fold,scoring=\"accuracy\").mean())\n",
    "print(\" \")\n",
    "\n",
    "#5 Knowing the mean score may not be enough, we can have a 95% confidence interval of the score estimate given by:\n",
    "#  A variance value of 0.0007 score seems quite reasonably small and tells us the 10 scores are closely tied together\n",
    "#  and don't fluctuate much, which is good news.\n",
    "print(\"Variance:\", cross_val_score(logreg, iris.data, iris.target, cv=k_fold,scoring=\"accuracy\").std()**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note as mentioned, the above problem is mitigated when you turn on **shuffle = True** in **K-Fold**, because when you shuffle the data prior to performing **K-Fold**, then the sequences will be randomly mixed up as well. Which brings us to the next point on why one must perform **Stratified K-Fold** over **K-Fold**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imbalanced Number of classes\n",
    "\n",
    "This is a generalization of the above example, for example, if I have a dataset where 90% of the samples are of **class A** and only 10% of the samples are of **class B**, then this is an imbalanced number of classes. This will cause a problem because if you use the normal **K-Fold (with 10 folds)** method, then there is a very high possibility that at step 2 (refer to my general steps for k-fold), **one of the 10 folds (call it fold i)** contains **many many** samples with labels **on class A (due to the sheer amount in class A)**; this is a problem because it obstructs the **real performance of the classifier**. If your classifier is a baseline classifier which naively predicts all data to be of **class A**, then our **fold i as previously defined** will have a close to 100% accuracy rate on that fold! But does it mean that our classifier is good? Not really."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Stratified K-Fold's sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold)**: **class sklearn.model_selection.StratifiedKFold(n_splits=5, shuffle=False, random_state=None)**\n",
    "\n",
    "**The implementation is designed to:**\n",
    "\n",
    "- Generate test sets such that all contain the same distribution of classes, or as close as possible.\n",
    "\n",
    "\n",
    "- Be invariant to class label: relabelling y = [\"Happy\", \"Sad\"] to y = [1, 0] should not change the indices generated.\n",
    "\n",
    "\n",
    "- Preserve order dependencies in the dataset ordering, when shuffle=False: all samples from class k in some test set were contiguous in y, or separated in y by samples from classes other than k.\n",
    "\n",
    "\n",
    "- Generate test sets where the smallest and largest differ by at most one sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StratifiedKFold** is a variation of **K-Fold** which returns stratified folds: each set contains approximately the same percentage of samples of each target class as the complete set.\n",
    "\n",
    "Here is an example of stratified 5-fold cross-validation on a dataset with 50 samples from two unbalanced classes. We show the number of samples in each class and compare with KFold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general procedure for **Stratified K-fold Cross Validation** is as follows.\n",
    "\n",
    "1. Shuffle the dataset X randomly - provided you have **shuffle = True** in your parameter. But note that shuffling the data in **Stratified K-Fold** will not change each fold's percentage on each class.\n",
    "\n",
    "\n",
    "2. Find the total number m of training samples in the dataset (use X.shape). Find out the number of unique classes in the dataset X, denote this number by n. For each unique class i = 0 to i = n, count the number of occurrences of each class i, $i \\in [0,n]$, denote the occurrences in each class by $o_i$. Finally, divide $o_i$ by the total number m to get $$\\text{percentage of class i} = \\dfrac{o_i}{m}$$\n",
    "\n",
    "\n",
    "\n",
    "3. In each class i, split the dataset in class i into **k** groups of approximately equal size depending on the k value; consequently, define each k-fold to be a combination of each fold split according to the class i so that each fold will have $o_i$ percentage of each class.\n",
    "\n",
    "\n",
    "4. A sequence of **models** will be trained **k** number of times. Say **k = 5**, then for each **unique group**, the first model is trained using the first fold as the test set, and the remaining folds (2–5) are used as the training set. The model is built using the data in folds 2–5, and then the accuracy is evaluated on fold 1. Then another model is built, this time using fold 2 as the test set and the data in folds 1, 3, 4, and 5 as the training set. This process is repeated using folds 3, 4, and 5 as test sets. For each of these five splits of the data into training and test sets, we compute the accuracy.  In the end, we have collected five accuracy values. The process is illustrated in the Figures below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Illustration**\n",
    "\n",
    "The above definition is a bit difficult to chew on, so let me illustrate it with a picture and an example. \n",
    "\n",
    "**At step 2,** suppose we have a dataset X with 50 training samples, so m = 50. Then there are 2 classes in this dataset, class 0 and class 1, so n = 2. The occurrences of class 0 is 45/50 times while the occurrences of class 1 is only 5/50 times. We calculate the percentage of class 0 to be $45/50 = 90\\%$ and the percentage of class 1 to be $5/50 = 5\\%$.\n",
    "\n",
    "**At step 3,** if we define k to be 5; then we split as follows: \n",
    "\n",
    "- class 0 -> 45/5 = 9; class 1 -> 5/5 = 1\n",
    "- Fold 1 = 9 + 1 = 10\n",
    "- Fold 2 = 9 + 1 = 10\n",
    "- ...\n",
    "\n",
    "As a result, in our first model, if we use fold 1 as test set, then we will be guaranteed to have 9 class 0s and 1 class 1; whereas our training set will guarantee to have 36 class 0s and 4 class 1s. This ensures an equal weightage in assigning the classes - 90% to class 0 and 10% to class 1 - which solves the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing I like to add here is, with or without shuffle, we can see that in each fold, there will be 90% of class 0 and 10% of class 1, as illustrated below. On the contrary, if you use **K-Fold** method to do, you can see how imbalanced each fold is, **EVEN IF** you set **shuffle = True** for **K-Fold**, there is a good chance that the test set (or even the train set) consists of only the imbalanced class (class 0) re: 3rd fold in the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/images/cv3.png\" width=\"300\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imbalanced, y_imbalanced = np.ones((50, 1)), np.hstack(([0] * 45, [1] * 5))\n",
    "\n",
    "X_imbalanced.shape\n",
    "y_imbalanced.shape\n",
    "\n",
    "counter = 1\n",
    "k_fold = KFold(n_splits=5, shuffle = False, random_state = 0)\n",
    "for train, test in k_fold.split(X_imbalanced, y_imbalanced):    \n",
    "    print('Fold {}: K-Fold shuffle off: train -  {}   |   test -  {}'.format(\n",
    "         counter, np.bincount(y[train]), np.bincount(y[test])))\n",
    "    counter = counter + 1\n",
    "\n",
    "print(\"---\"*40)\n",
    "\n",
    "\n",
    "k_fold = KFold(n_splits=5, shuffle = True, random_state = 0)\n",
    "\n",
    "counter = 1\n",
    "for train, test in k_fold.split(X_imbalanced, y_imbalanced):\n",
    "    print('Fold {}: K-Fold shuffle on: train -  {}   |   test -  {}'.format(\n",
    "         counter, np.bincount(y[train]), np.bincount(y[test])))\n",
    "    counter = counter + 1\n",
    "\n",
    "print(\"---\"*40)\n",
    "\n",
    "\n",
    "strat_k_fold = StratifiedKFold(n_splits=5, shuffle = False, random_state = 0)\n",
    "\n",
    "counter = 1\n",
    "for train, test in strat_k_fold.split(X_imbalanced, y_imbalanced):\n",
    "    print('Fold {}: Stratified K-Fold Shuffle off: train -  {}   |   test -  {}'.format(\n",
    "        counter, np.bincount(y[train]), np.bincount(y[test])))\n",
    "    counter = counter + 1\n",
    "    \n",
    "print(\"---\"*40)\n",
    "\n",
    "strat_k_fold = StratifiedKFold(n_splits=5, shuffle = True, random_state = 0)\n",
    "\n",
    "counter = 1\n",
    "for train, test in strat_k_fold.split(X_imbalanced, y_imbalanced):\n",
    "    print('Fold {}: Stratified K-Fold Shuffle on: train -  {}   |   test -  {}'.format(\n",
    "         counter, np.bincount(y[train]), np.bincount(y[test])))\n",
    "    counter = counter + 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply our **Stratified K-Fold** to our titanic. This is because when I checked the class in the Titanic dataset, the class imbalance is kind of huge with 549 dead and 342 survived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 In this tutorial, we will used the titanic data set, we check the unique classes of titanic.\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#1 Default: class sklearn.model_selection.KFold(n_splits=5, shuffle=False, random_state=None)\n",
    "strat_k_fold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "#2 normalize it by using our StandardScaler(), will mention more on this later.\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "#3 instantiate a model logistic regression and fit it to the training set.\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "\n",
    "#4 Here, cross_val_score performed 10-fold cross-validation and therefore returns 10 scores, the scoring\n",
    "#  system is based on the accuracy metric.\n",
    "\n",
    "print(\"Accuracy Scores: \" + format(cross_val_score(logreg,X,y,cv=strat_k_fold,scoring=\"accuracy\")))\n",
    "print(\" \") \n",
    "\n",
    "#5 A common way to summarize the cross-validation accuracy is to compute the mean of the 10 folds.\n",
    "\n",
    "print(\"Mean Accuracy Score: \", cross_val_score(logreg,X,y,cv=strat_k_fold,scoring=\"accuracy\").mean())\n",
    "print(\" \")\n",
    "\n",
    "#6 Knowing the mean score may not be enough, we can have a 95% confidence interval of the score estimate given by:\n",
    "#  A variance value of 0.0007 score seems quite reasonably small and tells us the 10 scores are closely tied together\n",
    "#  and don't fluctuate much, which is good news.\n",
    "print(\"Variance of Accuracy:\", cross_val_score(logreg,X,y,cv=strat_k_fold,scoring=\"accuracy\").std()**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Method: Repeated Stratified K-Fold\n",
    "\n",
    "See Repeated K-Fold for reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation iterators for grouped data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand this idea, we have to go back to section 6.1 and quote **Assuming that some data is Independent and Identically Distributed (i.i.d.) is making the assumption that all samples stem from the same generative process and that the generative process is assumed to have no memory of past generated samples.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider the generated dataset below. There are a total of 11 training samples (i.e. 11 rows).  In an almost universal paradigm for Diabetes prediction, the features will definitely include the Cholesterol Level and the Blood Sugar level. But take note that multiple patients appear in the same dataset. When we take samples from the same patients, our training samples (the 11 rows) are no longer considered independent. Under this paradigm, each patient has an unique underlying, anatomical structure that helps your model to \"recognize\" this patient's label when the model receives the patient's input features. These input features of the same patient may differ slightly (as you can see from the datase), but in general sense they are highly correlated. A person's internal structure and readings will not fluctuate in a short amount of time should give you some intuition why repeated samples of the same patient may adversely aid your model to overfit in a **cross validation** scheme should both of your train and test set contain the same person."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main idea of i.i.d**\n",
    "\n",
    "The main idea of i.i.d in training samples (rows) is to guarantee that techniques such as cross-validation can indeed be used to infer a reliable measure of the model's capability of generalising well. This is because a stronger case of `i.i.d amongst each training samples` has an **if and only if** relationship with the [Exchangeability Theorem](https://en.wikipedia.org/wiki/Exchangeable_random_variables), where the latter guarantees a reliable measure of cross validation (no overfitting).\n",
    "\n",
    "\n",
    "**Exchangeable Theorem**\n",
    "\n",
    "\n",
    "In statistics, an exchangeable sequence of random variables (also sometimes interchangeable) is a sequence $X_1, X_2, X_3, ...$ (which may be finitely or infinitely long) whose joint probability distribution does not change when the positions in the sequence in which finitely many of them appear are altered. Thus, for example the sequences\n",
    "\n",
    "$$X_{1},X_{2},X_{3},X_{4},X_{5},X_{6} {\\text{ and }} X_{3},X_{6},X_{1},X_{5},X_{2},X_{4}$$\n",
    "both have the same joint probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate why **exchangeable Theorem is so important**, I will lay out the example clearly.\n",
    "\n",
    "To make a model do its magic, we generally have to assume each training samples come from the same distribution, and any permutations within the samples should not alter the distribution. However, consider employing a simple logistic regression model on this `diabetes` dateset, the **Logistic Model takes in these 11 samples, in theory, if the training samples are i.i.d, then at each training sample, our model should not remember the previous training sample such that it affects his current decision to classify a person**. (Check the definition of i.i.d to further understand this sentence.) However, since our dataset samples has a dependency structure (same person), then you will meet problems when you perform K-Fold cross validation.\n",
    "\n",
    "\n",
    "**Scenario 1: No shuffling**\n",
    "\n",
    "Consider 5 folds cross validation, our split will look like \n",
    "\n",
    "- Fold 1 = $[John, John]$\n",
    "- Fold 2 = $[John, John]$\n",
    "- Fold 3 = $[John, May]$\n",
    "- Fold 4 = $[May, May]$\n",
    "- Fold 5 = $[Kim, Kim]$\n",
    "\n",
    "\n",
    "\n",
    "If we do not shuffle the data, the folds look like the above, so let us say we use Fold 1 to 4 as the training set, our model will take into account the inherent dependency structure and when it is presented with Fold 5 = $[Kim, Kim]$, it is seeing an entirely new set of data and hence it really puts our model to the test (pun intended) on whether it can accurately predict unseen data while taking **the dependency structure in**. For example it predicts one out of two correctly in Fold 5, yielding a 50% accuracy. (of course in real datasets there will be much much more data points in Fold 5.) Although a low accuracy, at least it did not give us a false sense of security.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Scenario 2: Shuffling**\n",
    "\n",
    "If we shuffle our data beforehand, the problem comes. Consider the scenario where our fold looks like this.\n",
    "\n",
    "- Fold 1 = $[John, Kim]$\n",
    "- Fold 2 = $[John, May]$\n",
    "- Fold 3 = $[John, May]$\n",
    "- Fold 4 = $[Kim, May]$\n",
    "- Fold 5 = $[John, May]$\n",
    "\n",
    "If we take Fold 1 to 4 as the training set, and asked to predict Fold 5, our model may not take into account the inherent dependency structure, and happily ingest the first 4 folds and happily predict Fold 5 with 100% accuracy simply because it has seen how John and May behave. Although each of John and May sample have slightly different values, but each sample of the same individual are so **correlated** such that the model has already find out the **underlying structure of John and May, whatever it may be**, and simply cause the model to memorize their labels and when it sees the same person in the test set, it can predict well. But one fatal point is, this often leads to over optimistic accuracy scores in our cross validation process and hence overfits a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic dataset\n",
    "\n",
    "d = {'Name': ['John','John','John','John','John', 'May','May','May','Kim','Kim'],\n",
    "     'Cholesterol Level (mg/dL)': [150.1, 150.3,150.1,152.2,150.09,112.01,112.9,113,180,180.2],\n",
    "     'Blood Sugar (mg/dL)': [120.2,120.3,120.1,120.1,121.2,99.9,99.8,99.8,150.1,150.2],\n",
    "     'Diabetes': [1, 1, 1, 1, 1, 0,0,0,1,1,]\n",
    "     }\n",
    "\n",
    "diabetes = pd.DataFrame(data=d)\n",
    "diabetes.set_index('Name', inplace = True)\n",
    "diabetes\n",
    "\n",
    "# X = diabetes[['Cholesterol Level (mg/dL)', 'Blood Sugar (mg/dL)']]\n",
    "# y = [1,1,1,1,1,0,0,0,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot scatter plot to show that if you group the 11 data points into 3 categrories (John, May and Kim)\n",
    "# you can see that these 3 categories actually form a cluster by themselves, indicating\n",
    "# correlation within each group/category.\n",
    "\n",
    "groups = diabetes.groupby('Name')\n",
    "x = 'Cholesterol Level (mg/dL)'\n",
    "y = 'Diabetes'\n",
    "\n",
    "\n",
    "#Plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "for name, group in groups:\n",
    "    ax.plot(group[x], group[y], marker='o', linestyle='', ms=12, label=name)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "groups = diabetes.groupby('Name')\n",
    "x = 'Blood Sugar (mg/dL)'\n",
    "y = 'Diabetes'\n",
    "\n",
    "\n",
    "#Plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "for name, group in groups:\n",
    "    ax.plot(group[x], group[y], marker='o', linestyle='', ms=12, label=name)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Another example by the book:** If your datasets have training rows that are highly correlated to their groups, then the normal **K-Fold** cross validation strategy will pose an overly optimistic result for your local cv score. This is because if your training set fold (say fold 2-5) has group 1's data (say there are 100 Mary's samples, 80 of which appeared in this fold) and on the same test set fold you also have the remaining 20 Mary's samples, albeit different samples, but belonging to the same group a.k.a Mary! This will make the classifier have an extremely easy job to predict this test set because it is used to **the group identifier: Mary\", in other words, we are possibly and potentially leaking future information to the test set!**. Now the classifier will have an easier time predicting because it sees a lot of Mary's examples already, and under this paradigm, correlations clearly exist among both the features and the labels of candidates that refer to the same\n",
    "underlying structure, blood results, patient, etc. \n",
    "\n",
    "So the classifier might actually not **generalize well to unseen data** because it has similar groups. But if we use a normal **cross validation**, we might be lured into a false sense of security that the classifier is performing well simply because our test set may contain similar groups as our training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Third Example: Here is an example taken from a github](https://github.com/ogrisel/notebooks/blob/master/Non%20IID%20cross-validation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The digits dataset of scikit-learn is the test set of the UCI optdigits dataset. Apparently consecutive samples are more likely to stem from the same writer on this dataset. Hence the samples are not independent and identically distributed (iid) as different writing styles grouped togethers effectively introduce a dependency. Unfortunately the exact per-sample authorship metadata has not be kept in the optdigits dataset.\n",
    "\n",
    "This is highlighted by the fact that shuffling the data significantly affects the test score estimated by K-Fold cross-validation. Let us build a model with non-optimal parameters to highlight the impact of dependent samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm  import SVC\n",
    "\n",
    "model = SVC(C=10, gamma=0.005)\n",
    "\n",
    "\n",
    "\n",
    "def print_cv_score_summary(model, X, y, cv):\n",
    "    scores = cross_val_score(model, X, y, cv=cv, n_jobs=-1)\n",
    "    print(\"mean: {:3f}, stdev: {:3f}\".format(\n",
    "        np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    \n",
    "#1 KFold does not shuffle the data by default hence takes the dependency structure of the dataset\n",
    "# into account for small number of folds such as k=5.\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=False, random_state=0)\n",
    "print_cv_score_summary(model, X, y, cv)\n",
    "\n",
    "\n",
    "#2 If we shuffle the data, the estimated test score is much higher as we hide the dependency\n",
    "#  structure to the model hence we cannot detect the overfitting caused by the author writing styles\n",
    "\n",
    "\n",
    "cv = KFold(n_splits=5,  shuffle=True, random_state=0)\n",
    "print_cv_score_summary(model, X, y, cv)\n",
    "\n",
    "\n",
    "#3 There is almost 7% discrepancy between the estimated score probably caused by the dependency\n",
    "#between samples. Those shuffled KFold cv scores are in-line with equivalent ShuffleSplit:\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "cv = ShuffleSplit(n_splits = 5, test_size=0.2, random_state=0)\n",
    "print_cv_score_summary(model, X, y, cv)\n",
    "\n",
    "\n",
    "#4 Note that StratifiedKFold sorts the samples by classes prior to computing the folds hence\n",
    "#  breaks the dependency too (at least in scikit-learn 0.14)\n",
    "\n",
    "cv =  KFold(n_splits = 5,  shuffle = True, random_state=0)\n",
    "print_cv_score_summary(model, X, y, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Group K-Fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The syntax is **class sklearn.model_selection.GroupKFold(n_splits=5)**.\n",
    "\n",
    "K-fold iterator variant with non-overlapping groups. The same group will not appear in two different folds (the number of distinct groups has to be at least equal to the number of folds). The folds are approximately balanced in the sense that the number of distinct groups is approximately the same in each fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an example of using a synthetic dataset with a grouping given by the groups array. The dataset consists of 12 data points, and for each of the data points, groups specifies which group (think patient) the point belongs to. The groups specify that there are four groups, and the first three samples belong to the first group, the next four samples belong to the second group, and so on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T04:38:22.163029Z",
     "start_time": "2020-06-02T04:38:22.144050Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logreg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-cd3f647778ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# then the next four, etc.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogreg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mGroupKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cross-validation scores:\\n{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logreg' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.datasets import make_blobs\n",
    "# create synthetic dataset\n",
    "X, y = make_blobs(n_samples=12, random_state=0)\n",
    "# assume the first three samples belong to the same group,\n",
    "# then the next four, etc.\n",
    "groups = [0, 0, 0, 1, 1, 1, 1, 2, 2, 3, 3, 3]\n",
    "scores = cross_val_score(logreg, X, y, groups, cv=GroupKFold(n_splits=3))\n",
    "print(\"Cross-validation scores:\\n{}\".format(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The samples don’t need to be ordered by group; we just did this for illustration purposes. The splits that are calculated based on these labels are visualized in Figure below. As you can see, for each split, each group is either entirely in the training set or entirely in the test set.\n",
    "\n",
    "This gives us a **non-overlapping grouping system** in which the same group will not appear in both the training fold and the corresponding test fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_group_kfold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Group K-Fold on our example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes2 = diabetes.append(diabetes)\n",
    "y = diabetes2['Diabetes']\n",
    "groups = [0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 2, 2]\n",
    "scores = cross_val_score(KNeighborsClassifier(), diabetes2, y, groups, cv=GroupKFold(n_splits=3), scoring = \"accuracy\")\n",
    "scores\n",
    "\n",
    "\n",
    "print(\"Accuracy Scores: \" + format(cross_val_score(KNeighborsClassifier(),diabetes2,y,groups,cv=GroupKFold(n_splits=3),scoring=\"accuracy\")))\n",
    "print(\" \") \n",
    "\n",
    "#5 A common way to summarize the cross-validation accuracy is to compute the mean of the 10 folds.\n",
    "\n",
    "print(\"Mean Accuracy Score: \", cross_val_score(KNeighborsClassifier(),diabetes2,y,groups,cv=GroupKFold(n_splits=3),scoring=\"accuracy\").mean())\n",
    "print(\" \")\n",
    "\n",
    "#6 Knowing the mean score may not be enough, we can have a 95% confidence interval of the score estimate given by:\n",
    "#  A variance value of 0.0007 score seems quite reasonably small and tells us the 10 scores are closely tied together\n",
    "#  and don't fluctuate much, which is good news.\n",
    "print(\"Variance of Accuracy:\", cross_val_score(KNeighborsClassifier(),diabetes2,y,groups,cv=GroupKFold(n_splits=3),scoring=\"accuracy\").std()**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes2 = diabetes.append(diabetes)\n",
    "y = diabetes2['Diabetes']\n",
    "groups = list(diabetes2.index)\n",
    "scores = cross_val_score(KNeighborsClassifier(), diabetes2, y, groups, cv=GroupKFold(n_splits=3), scoring = \"accuracy\")\n",
    "scores\n",
    "\n",
    "\n",
    "print(\"Accuracy Scores: \" + format(cross_val_score(KNeighborsClassifier(),diabetes2,y,groups,cv=GroupKFold(n_splits=3),scoring=\"accuracy\")))\n",
    "print(\" \") \n",
    "\n",
    "#5 A common way to summarize the cross-validation accuracy is to compute the mean of the 10 folds.\n",
    "\n",
    "print(\"Mean Accuracy Score: \", cross_val_score(KNeighborsClassifier(),diabetes2,y,groups,cv=GroupKFold(n_splits=3),scoring=\"accuracy\").mean())\n",
    "print(\" \")\n",
    "\n",
    "#6 Knowing the mean score may not be enough, we can have a 95% confidence interval of the score estimate given by:\n",
    "#  A variance value of 0.0007 score seems quite reasonably small and tells us the 10 scores are closely tied together\n",
    "#  and don't fluctuate much, which is good news.\n",
    "print(\"Variance of Accuracy:\", cross_val_score(KNeighborsClassifier(),diabetes2,y,groups,cv=GroupKFold(n_splits=3),scoring=\"accuracy\").std()**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T04:53:05.053365Z",
     "start_time": "2020-06-02T04:53:05.050373Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection: Caution on the wrong way of using CV\n",
    "\n",
    "There is one major caveat [read standford example pdf here](https://web.stanford.edu/class/stats202/content/lec11-cond.pdf) and [here](https://web.stanford.edu/class/stats202/content/lab11) to be read when I approach this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline with CV\n",
    "\n",
    "Just as it is important to test a predictor on data held-out from training, preprocessing (such as standardization, feature selection, etc.) and similar data transformations similarly should be learnt from a training set and applied to held-out data for prediction. Here I will not go into details how to use pipeline, we will dedicate a whole chapter to that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ask ivan, what is perplexing is this result below is the same as my above result, it should be different?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/titanic_train.csv')\n",
    "test = pd.read_csv(\"data/titanic_test.csv\")\n",
    "test_labels = pd.read_csv(\"data/titanic_test_labels.csv\")\n",
    "\n",
    "\n",
    "\n",
    "median_fare = test.groupby(['Pclass', 'Parch']).Fare.median()[3][0]\n",
    "# Filling the missing value in Fare with the median Fare of 3rd class passenger who has Parch 0.\n",
    "test['Fare'] = test['Fare'].fillna(median_fare)\n",
    "train['Embarked'] = train['Embarked'].fillna('S')\n",
    "train['Age']  = train.groupby(['Pclass', 'SibSp'])['Age'].apply(lambda x: x.fillna(x.median()))\n",
    "\n",
    "# remember to fill in here, this one really very special case pertaining to this project\n",
    "\n",
    "train['Age'] = train.Age.fillna(11)\n",
    "test['Age'] = test.groupby(['Pclass', 'SibSp'])['Age'].apply(lambda x: x.fillna(x.median()))\n",
    "\n",
    "train['Cabin'] = train['Cabin'].fillna('M').astype(str).apply(lambda cabin: cabin[0])\n",
    "idx = train[train['Cabin'] == 'T'].index\n",
    "train.loc[idx, 'Cabin'] = 'A'\n",
    "\n",
    "test['Cabin'] = test['Cabin'].fillna('M').astype(str).apply(lambda cabin: cabin[0])\n",
    "\n",
    "# Create function that take name and separates it into title, family name and deletes all puntuation from name column:\n",
    "def name_sep(data):\n",
    "    families=[]\n",
    "    titles = []\n",
    "    new_name = []\n",
    "    #for each row in dataset:\n",
    "    for i in range(len(data)):\n",
    "        name = data.iloc[i]\n",
    "        # extract name inside brakets into name_bracket:\n",
    "        if '(' in name:\n",
    "            name_no_bracket = name.split('(')[0] \n",
    "        else:\n",
    "            name_no_bracket = name\n",
    "            \n",
    "        family = name_no_bracket.split(\",\")[0]\n",
    "        title = name_no_bracket.split(\",\")[1].strip().split(\" \")[0]\n",
    "        \n",
    "        #remove punctuations accept brackets:\n",
    "        for c in string.punctuation:\n",
    "            name = name.replace(c,\"\").strip()\n",
    "            family = family.replace(c,\"\").strip()\n",
    "            title = title.replace(c,\"\").strip()\n",
    "            \n",
    "        families.append(family)\n",
    "        titles.append(title)\n",
    "        new_name.append(name)\n",
    "            \n",
    "    return families, titles, new_name \n",
    "\n",
    "\n",
    "train['Surname'], train['Title'], train['Newname']  = name_sep(train.Name)\n",
    "test['Surname'], test['Title'], test['Newname'] = name_sep(test.Name)\n",
    "train['Title'] = train['Title'].replace(['Ms', 'Mlle'],'Miss')\n",
    "train['Title'] = train['Title'].replace(['Mme'],'Mrs')\n",
    "train['Title'] = train['Title'].replace(['Dr','Rev','the','Jonkheer','Lady','Sir', 'Don'],'Nobles')\n",
    "train['Title'] = train['Title'].replace(['Major','Col', 'Capt'],'Navy')\n",
    "train.Title.value_counts()\n",
    "\n",
    "\n",
    "\n",
    "test['Title'] = test['Title'].replace(['Ms','Dona'],'Miss')\n",
    "test['Title'] = test['Title'].replace(['Dr','Rev'],'Nobles')\n",
    "test['Title'] = test['Title'].replace(['Col'],'Navy')\n",
    "test.Title.value_counts()\n",
    "\n",
    "\n",
    "\n",
    "train_categorical_features = ['Pclass', 'Sex','Title','Cabin', 'Embarked']\n",
    "\n",
    "# No need to use sklearn's encoders\n",
    "# pandas has a pandas.get_dummies() function that takes in a series\n",
    "#     and returns a HOT encoded dataframe of that series\n",
    "#     use the add_prefix() method of dataframe to add the feature name in front of the category name\n",
    "#     then join the dataframe sideways (similar to pd.concat([train, dummies], axis=1))\n",
    "for feature in train_categorical_features:\n",
    "    dummies = pd.get_dummies(train[feature]).add_prefix(feature + '_')\n",
    "    train = train.join(dummies)\n",
    "    \n",
    "    \n",
    "    \n",
    "test_categorical_features = ['Pclass', 'Sex','Title', 'Cabin', 'Embarked']\n",
    "\n",
    "# No need to use sklearn's encoders\n",
    "# pandas has a pandas.get_dummies() function that takes in a series\n",
    "#     and returns a HOT encoded dataframe of that series\n",
    "#     use the add_prefix() method of dataframe to add the feature name in front of the category name\n",
    "#     then join the dataframe sideways (similar to pd.concat([train, dummies], axis=1))\n",
    "for feature in test_categorical_features:\n",
    "    dummies = pd.get_dummies(test[feature]).add_prefix(feature + '_')\n",
    "    test = test.join(dummies)\n",
    "    \n",
    "    \n",
    "    \n",
    "drop_column = ['Pclass','Name','Sex','Cabin', 'Embarked','Surname','Title','Newname', 'Ticket', 'PassengerId']\n",
    "train.drop(drop_column, axis=1, inplace = True)\n",
    "\n",
    "drop_column = ['Pclass','Name','Sex','Cabin', 'Embarked','Surname','Title','Newname', 'Ticket', 'PassengerId']\n",
    "test.drop(drop_column, axis=1, inplace = True)\n",
    "\n",
    "#1 In this tutorial, we will used the titanic data set.\n",
    "X = train.drop(\"Survived\", axis=1)\n",
    "y = train[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import preprocessing\n",
    "clf = make_pipeline(preprocessing.StandardScaler(), LogisticRegression())\n",
    "print(\"Accuracy Scores: \" , cross_val_score(logreg,X,y,cv=k_fold,scoring=\"f1\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out of Fold - The usual way by Kagglers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be very frank, I was extremely confused with the difference between `Out of Fold predictions` and the normal `K-Fold Cross Validation`, aren't they the same thing? The differences are subtle, but as [Braquino (Fellow Kaggler)](https://www.kaggle.com/braquino) put it, it simply means:**in a cross validation environment, when you split the train and validation dataset in each fold, oof is the validation piece, the part of the dataset that was Out of the model in that fold.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out of Fold Part I - Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common use for `out of fold` predictions is to estimate the **performance** of the model.\n",
    "\n",
    "That is, predictions on data that were not used to train the model can be made and evaluated using a `scoring metric` such as `rmse` or `accuracy`. This metric provides an estimate of the performance of the model when used to make predictions on new data, such as when the model will be used in practice to make predictions.\n",
    "\n",
    "Generally, predictions made on data not used to train a model provide insight into how the model will generalize to new situations. As such, scores that evaluate these predictions are referred to as the generalized performance of a machine learning model.\n",
    "\n",
    "There are two main approaches that these predictions can use to estimate the performance of the model.\n",
    "\n",
    "The first is to `score` the model on the predictions made during each **fold**, then calculate the average of those scores. For example, if we are evaluating a classification model, then classification accuracy can be calculated on each group of out-of-fold predictions, then the mean accuracy can be reported. This approach is illustrated in earlier sections like **section 1.3**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: Estimate performance as the mean score estimated on each group of out-of-fold predictions.\n",
    "\n",
    "In this example here, we used the `mean accuracy score` but in reality, there are many different metrics we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#1 Default: class sklearn.model_selection.KFold(n_splits=5, shuffle=False, random_state=None)\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "#2 normalize it by using our StandardScaler(), will mention more on this later.\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "#3 instantiate a model logistic regression and fit it to the training set.\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "\n",
    "#4 Here, cross_val_score performed 10-fold cross-validation and therefore returns 10 scores, the scoring\n",
    "#  system is based on the accuracy metric.\n",
    "\n",
    "print(\"Accuracy Scores: \" + format(cross_val_score(logreg,X,y,cv=k_fold,scoring=\"accuracy\")))\n",
    "print(\" \") \n",
    "\n",
    "#5 A common way to summarize the cross-validation accuracy is to compute the mean of the 10 folds.\n",
    "\n",
    "print(\"Mean Accuracy Score: \", cross_val_score(logreg,X,y,cv=k_fold,scoring=\"accuracy\").mean())\n",
    "print(\" \")\n",
    "\n",
    "#6 Knowing the mean score may not be enough, we can have a 95% confidence interval of the score estimate given by:\n",
    "#  A variance value of 0.0007 score seems quite reasonably small and tells us the 10 scores are closely tied together\n",
    "#  and don't fluctuate much, which is good news.\n",
    "print(\"Variance of Accuracy:\", cross_val_score(logreg,X,y,cv=k_fold,scoring=\"accuracy\").std()**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we go through how `K-Fold cross_validation_score ` works, a recap of how to use `KFold.split(X)`. This code belows illustrates fully how the `K-Fold cross_validation_score ` is generated via `KFold`. I have commented out each line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = list()\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# enumerate splits\n",
    "for train_ix, test_ix in k_fold.split(X):\n",
    "    # for each split, define the training fold to be [train_X, train_y]\n",
    "    # define the test fold to be [test_X, text_y]\n",
    "    # in each split, the training and test fold cam be obtained by passing in the \n",
    "    # indices indexes train_ix and test_ix\n",
    "    train_X, test_X = X[train_ix], X[test_ix]\n",
    "    train_y, test_y = y[train_ix], y[test_ix]\n",
    "    \n",
    "    # fit model: fit the model on our training set, for example, in the first loop, \n",
    "    # we have our first split, and we fit on that train_X and train_y and get the \n",
    "    # necessary parameters for our selected model.\n",
    "    model = LogisticRegression()\n",
    "    model.fit(train_X, train_y)\n",
    "    \n",
    "\n",
    "    # Predict class labels in test_X\n",
    "    y_pred = model.predict(test_X)\n",
    "    \n",
    "    # \n",
    "    acc = accuracy_score(test_y, y_pred)\n",
    "    \n",
    "    # store score: Store the scores in our scores variable.\n",
    "    scores.append(acc)\n",
    "    print('Accuracy Score ', acc)\n",
    "    \n",
    "# summarize model performance with mean and std. Note that this gives us exactly\n",
    "# what cross_val_score(logreg,X,y,cv=k_fold,scoring=\"accuracy\") does earlier on.\n",
    "# this implementation is just to facilitate understanding of OOF evaluation next.\n",
    "\n",
    "mean_score, std_score = np.mean(scores), np.std(scores)\n",
    "print('Mean: {}.9f, Standard Deviation: {}.3f'.format(mean_score, std_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Estimate performance using the aggregate of all out-of-fold predictions.\n",
    "\n",
    "The second approach is to consider that each example appears just once in each test set. That is, each example in the training dataset has a single prediction made during the k-fold cross-validation process. As such, we can collect all predictions and compare them to their expected outcome and calculate a score directly across the entire training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe there is a slight difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y, data_y_pred = list(), list()\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "# enumerate splits\n",
    "for train_ix, test_ix in k_fold.split(X):\n",
    "# get data\n",
    "    train_X, test_X = X[train_ix], X[test_ix]\n",
    "    train_y, test_y = y[train_ix], y[test_ix]\n",
    "    # fit model\n",
    "    model = LogisticRegression()\n",
    "    model.fit(train_X, train_y)\n",
    "    # make predictions\n",
    "    y_pred = model.predict(test_X)\n",
    "    # store\n",
    "    data_y.extend(test_y)\n",
    "    data_y_pred.extend(y_pred)\n",
    "# evaluate the model\n",
    "acc = accuracy_score(data_y, data_y_pred)\n",
    "print('Accuracy: %.9f' % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out of Fold Part II - Emsembling\n",
    "\n",
    "OOF prediction can be used as **features** in **ensembling method**. This knowledge and techniques require you to first understand how ensembling works, if not you will be as puzzled as me at first.\n",
    "\n",
    "As said previously, this is the only most basic way of using `oof` as compared to normal method. The real reason why many `Kagglers` use `oof` is because of ensembling. Basically, it is a go to technique when Kagglers use ensemble methods. Below highlights the reason why. All thanks to [Jason Brownlee (click me)](https://machinelearningmastery.com/out-of-fold-predictions-in-machine-learning/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/questions-and-answers/52606\n",
    "https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
