{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69e45cd-60ef-4676-815f-ea5b91f9cef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d6426ff-060c-485d-8278-5d584e4e1612",
   "metadata": {},
   "source": [
    "## Intuition\n",
    "\n",
    "With courtesy of Applied AI.\n",
    "\n",
    "### Algebraic Intuition\n",
    "\n",
    "Let us use one of our favourite dataset Iris and just use 2 features, Petal length (pl) and Sepal length (sl). Given a query point $x_{q}$ with only two features, we aim to predict $y_{q}$'s class, whether it is 1, 2 or 3.\n",
    "\n",
    "A decision tree, like its name, is simply asking questions. In a simplified manner, it is merely a yes-no binary question. Programmers can easily relate with a nested `if-else` statement.\n",
    "\n",
    "\n",
    "```python\n",
    "if pl < a: # yes\n",
    "    y = class 1\n",
    "else: # no\n",
    "    if sl < b:\n",
    "        y = class 2\n",
    "    else:\n",
    "        y = class 3\n",
    "```\n",
    "\n",
    "This seems simple, we first ask whether $pl < a$ as the main question, if yes, the our query point is of class 1, if not, we simply check if $sl < b$, if yes, then it is of class 2, and class 3 otherwise.\n",
    "\n",
    "This seems simple, but the keen learner may soon ask: which feature to choose as main node? We will answer this later, but in a nutshell, decision tree does the above, and as we traverse through the nodes, we end up with a final answer (on the class).\n",
    "\n",
    "Note $a, b$ are just cutoff points for the variable $pl, sl$, which denotes in (cm). \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Geometric Intuition\n",
    "\n",
    "Following the image below, we can visualize it in image below. \n",
    "\n",
    "- $\\pi_{1}$: our hyperplane here denotes our first if question, which is our first decision boundary, simply put, left side of $\\pi_{1}$ is the class 1.\n",
    "- $\\pi_{2}$: our hyperplane here denotes our decision on if $sl < b$, and see that anything below this plane is class 2.\n",
    "- $\\pi_{3}$: this follows to be class 3.\n",
    "\n",
    "All hyperplanes are axis-parallel, that is to say, each hyperplane (decision boundary) is parallel to x or y axis (assuming 2 dimensional space) and can be generalized.\n",
    "\n",
    "![title](https://storage.googleapis.com/reighns/reighns_ml_projects/docs/machine_learning_algorithms/decision_tree_geometric_intuition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ab6989-b02e-42cf-90d2-96fa173bc540",
   "metadata": {},
   "source": [
    "## Notations and Definitions\n",
    "\n",
    "- root node: First note in a tree.\n",
    "- internal node: Non root and non leaf node, this is where we make decisions.\n",
    "- leaf/terminal note (leaf): End of the node where no decisions are made but points to a class label. \n",
    "\n",
    "### Entropy (Information Theory)\n",
    "\n",
    "Read my notebook named `entropy.ipynb` for more information.\n",
    "\n",
    "### KL Divergence\n",
    "\n",
    "Read my notebook named `kl_divergence.ipynb` for more information.\n",
    "\n",
    "---\n",
    "\n",
    "### Information Gain\n",
    "\n",
    "**Information Gain**: The information gained by splitting the current (sub)-dataset using the attribute.\n",
    "\n",
    "Recall:  Information Gain is a metric that measures the expected reduction in the impurity of the collection $S$, caused by splitting the data according to any given attribute. A chosen attribute $x_i$ divides the example set S into subsets\n",
    "$S_1 , S_2 , ... , S_{C_i}$ according to the $C_i$ distinct values for $x_i$ .\n",
    "The entropy then reduces to the entropy of the subsets $S_1 , S_2 , ... , S_{C_i}$:\n",
    "\n",
    "\n",
    "$$\\text{remainder}(S, x_i) = \\sum_{j=1}^{C_i} \\frac{|S_j|}{|S|} H(S_j)$$\n",
    "\n",
    "\n",
    "The Information Gain (IG; “reduction in entropy”) from knowing the value of $x_i$ is:\n",
    "\n",
    "$$IG(S, x_i) = H(S) - \\text{remainder}(S, x_i) $$  \n",
    "\n",
    "\n",
    "Subsequently, we choose the attribute with the largest IG.\n",
    "\n",
    "---\n",
    "\n",
    "### Gini Impurity\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ee7db1-4ae4-4346-b6ee-a48e80328249",
   "metadata": {},
   "source": [
    "## Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981bb296-dab4-4ab8-a72b-ec0a10fb9bdc",
   "metadata": {},
   "source": [
    "## Hypothesis Space and Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631d8e60-7fa7-4c23-aa3b-a37ad4c50ba1",
   "metadata": {},
   "source": [
    "## Decision Tree Algorithm\n",
    "\n",
    "### ID3 (Both Classification & Regression)\n",
    "\n",
    "> Invented by Ross Quinlan, ID3 uses a top-down greedy approach to build a decision tree. In simple words, the top-down approach means that we start building the tree from the top and the greedy approach means that at each iteration we select the best feature at the present moment to create a node.\n",
    "\n",
    "#### Classification\n",
    "\n",
    "- Given a dataset $\\mathcal{D}$, with a list of tuples $(\\mathrm{x}, \\mathrm{y})$ where we assume that $\\mathrm{y}$ has unique labels `[0,1]`. Note the rest of the notations remain the same, (i.e. m = num_samples, n = num_features).\n",
    "\n",
    "- Given a pre-defined metric $\\mathcal{H}$ where in the case of classification we use Entropy, where for Regression we use MSE. In particular, from this pre-defined metric, we can find the Information Gain of each sub-dataset, we will discuss later, but for now the notation for this is $IG$.\n",
    "  \n",
    "- Calculate the total entropy of $\\mathcal{D}$, more concretely, we do the below:\n",
    "  - Calculate the class frequency for both classes, it should be in the form of $$y^{+} = \\dfrac{\\text{num positives}}{m}$$ $$y^{-} = \\dfrac{\\text{num negatives}}{m}$$     \n",
    "    - If all examples are positive, Return the single-node tree Root, with label = +.\n",
    "    - If all examples are negative, Return the single-node tree Root, with label = -.\n",
    "  - Calculate total entropy of $\\mathcal{D}$ to be $\\mathcal{H}(D)$\n",
    "  \n",
    "-  Assume further that $n = 3$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c015b1-f0ba-4274-8fe0-a0020d0f43be",
   "metadata": {},
   "source": [
    "- Take in a the whole dataset $D$ in the shape of `(num_samples, n_features)`\n",
    "- Split $D$ into $D_x, D_y$.\n",
    "- Get `m = num_sample, n = num_feature`.\n",
    "- In our first iteration, we get the best split:\n",
    "\n",
    "      ```python\n",
    "      {'bounds': 0.1,\n",
    "      'feat_idx': 3,\n",
    "      'info_gain': 0.26899559358928116,\n",
    "      'left': array([[4.9, 3.1, 1.5, 0.1, 0. ],\n",
    "            [4.3, 3. , 1.1, 0.1, 0. ]]),\n",
    "      'right': array([[5.1, 3.5, 1.4, 0.2, 2. ],\n",
    "            [5.1, 3.5, 1.4, 0.2, 1. ],\n",
    "            [4.7, 3.2, 1.3, 0.2, 1. ],\n",
    "            [4.7, 3.2, 1.3, 0.2, 2. ],\n",
    "            [4.9, 3. , 1.4, 0.2, 2. ],\n",
    "            [5. , 3.6, 1.4, 0.2, 1. ],\n",
    "            [5.1, 3.5, 1.4, 0.2, 1. ],\n",
    "            [4.6, 3.4, 1.4, 0.3, 1. ],\n",
    "            [5. , 3.6, 1.4, 0.2, 2. ],\n",
    "            [5.4, 3.7, 1.5, 0.2, 2. ],\n",
    "            [4.9, 3. , 1.4, 0.2, 1. ],\n",
    "            [4.7, 3.2, 1.3, 0.2, 0. ],\n",
    "            [4.6, 3.4, 1.4, 0.3, 0. ],\n",
    "            [5.4, 3.7, 1.5, 0.2, 1. ],\n",
    "            [5.7, 4.4, 1.5, 0.4, 2. ],\n",
    "            [4.4, 2.9, 1.4, 0.2, 1. ],\n",
    "            [5.7, 4.4, 1.5, 0.4, 1. ],\n",
    "            [4.6, 3.1, 1.5, 0.2, 2. ]])}\n",
    "      ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b401a7-5b56-4797-9839-c3a15d20e27d",
   "metadata": {},
   "source": [
    "## Pros and Cons\n",
    "\n",
    "### Cons\n",
    "\n",
    "#### Overfitting\n",
    "\n",
    "This is well known, if you just use a very naive and plain decision tree, then it is almost guaranteed to not generalize well. A prime example modified from Data Science From Scratch is as such:\n",
    "\n",
    "> For the sake of example, we are using features of a customer of a bank to predict his credit default rating (discrete ratings). And also just hypothetically say, we are also using a potentially useless feature security social number which is uniquely generated for each customer (note in real world, I do not think we will be feeding this feature in in the first place as in general, we cannot derive useful information from this number). Then further assume, our tree splits to the last node, where the last node is the security number attribute, and then our tree will have $m$ nodes, where $m$ is the number of data (assume distinct customers in dataset). This will cause a problem, because each node has 0 entropy and future predictions will always look into this node of security number (actually if the SSN is unique, then how does tree go down to that unique number for unseen predictions)?\n",
    "\n",
    "#### High Variance\n",
    "\n",
    "This term is almost a synonym to **overfitting**. Decision Trees are often termed as a **model with high variance**. Quoting a well explained answer from [here](https://datascience.stackexchange.com/questions/48166/why-can-decision-trees-have-a-high-amount-of-variance#:~:text=A%20decision%20tree%20has%20high,predictions%20to%20every%20single%20input.&text=THEN%20team%20A%20wins.,game%20in%20your%20training%20data.), it says:\n",
    "\n",
    "**Intuitive answer**\n",
    "\n",
    "It is relatively simple if you understand what variance refers to in this context. A model has high variance if it is very sensitive to (small) changes in the training data. \n",
    "\n",
    "A decision tree has high variance because, if you imagine a very large tree, it can basically adjust its predictions to every single input.\n",
    "\n",
    "Consider you wanted to predict the outcome of a soccer game. A decision tree could make decisions like:\n",
    "\n",
    "> IF \n",
    "> \n",
    ">  1. player X is on the field AND\n",
    ">  2. team A has a home game AND\n",
    ">  3. the weather is sunny AND \n",
    ">  4. the number of attending fans >= 26000 AND\n",
    ">  5. it is past 3pm  \n",
    "> \n",
    "> THEN team A wins.\n",
    "\n",
    "If the tree is very deep, it will get very specific and you may only have one such game in your training data. It probably would not be appropriate to base your predictions on just one example.\n",
    "\n",
    "Now, if you make a small change e.g. set the number of attending fans to 25999, a decision tree might give you a completely different answer (because the game now doesn't meet the 4th condition). \n",
    "\n",
    "Linear regression, for example, would not be so sensitive to a small change because it is limited (\"biased\" -> see [bias-variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)) to linear relationships and cannot represent sudden changes from 25999 to 26000 fans. \n",
    "\n",
    "That's why it is important to not make decision trees arbitrary large/deep. This limits its variance. \n",
    "\n",
    "(See e.g. [here](https://datascience.stackexchange.com/a/20355/23305) for more on how random forests can help with this further.)\n",
    "\n",
    "**Follow Up [post](https://datascience.stackexchange.com/questions/20304/why-do-we-pick-random-features-in-random-forest/20355#20355) on how RF can help mitigate**\n",
    "\n",
    "Can connect back to intuition of ensembling by Francis.\n",
    "\n",
    "The idea of random forests is basically to build many decision trees (or other weak learners) that are *decorrelated*, so that their average is less prone to overfitting (reducing the variance). One way is subsampling of the training set. The reason why subsampling features can further decorrelate trees is, that if there are few dominating features, these features will be selected in many trees even for different subsamples, making the trees in the forest similar (correlated) again **and we do not want them to be too similar!**\n",
    "\n",
    "The lower the number of sampled features, the higher the decorrelation effect. On the other hand, the bias of a random forest is the same as the bias of any of the sampled trees (see for example Elements of Statistical Learning), but the randomization of random forests restrict the model, so that the bias is usually higher than a fully-grown (unpruned) tree. You are correct in that you can expect a higher bias if you sample fewer features. So, \"feature bagging\" really gives you a classical trade-off in bias and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c1edf5-dded-45ce-a884-ce45495f07ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
