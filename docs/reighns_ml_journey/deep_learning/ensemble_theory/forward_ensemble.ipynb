{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c35dfa6-6cd6-4315-8a61-061881cefe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Callable\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1f1123-6df5-444d-8023-ee8ac3613c94",
   "metadata": {},
   "source": [
    "## Convert OOFs and Ground Truth to NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76db51a7-8de7-4e5c-a202-4d154812db7f",
   "metadata": {},
   "source": [
    "First, we define a function `return_list_of_dataframes` to return either the OOFs or the SUBs. At this point, one should be clear what are OOFs, if not, please read the post [here](https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/175614)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d1f366c6-d504-4aee-aba2-27dbb59205b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_list_of_dataframes(\n",
    "    path: str, is_oof: bool = True\n",
    ") -> Tuple[List[pd.DataFrame], int]:\n",
    "    \"\"\"Return a list of dataframes from a directory of files.\n",
    "\n",
    "    The boolean is_oof is used to determine whether the list of dataframes contains oof or subs.\n",
    "\n",
    "    Args:\n",
    "        path (str): The path to the directory containing the files.\n",
    "        is_oof (bool, optional): Determine whether the list of dataframes contains oof or subs. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        List[pd.DataFrame]: The list of dataframes for either oof or subs.\n",
    "        int: The number of files in the directory.\n",
    "    \"\"\"\n",
    "    oof_and_subs_files = os.listdir(path)\n",
    "\n",
    "    if is_oof:\n",
    "        oof_files_sorted = np.sort(\n",
    "            [f for f in oof_and_subs_files if \"oof\" in f]\n",
    "        )\n",
    "        return [\n",
    "            pd.read_csv(os.path.join(path, k)) for k in oof_files_sorted\n",
    "        ], len(oof_files_sorted)\n",
    "\n",
    "    else:\n",
    "        sub_files_sorted = np.sort(\n",
    "            [f for f in oof_and_subs_files if \"sub\" in f]\n",
    "        )\n",
    "        return [\n",
    "            pd.read_csv(os.path.join(path, k)) for k in sub_files_sorted\n",
    "        ], len(sub_files_sorted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac731436-54eb-44fd-a7b5-e0a1e75db4fe",
   "metadata": {},
   "source": [
    "This function is first applied to `is_oof=True`. For now, we just want all our OOFs converted to a pandas dataframe, and stored in a list.\n",
    "\n",
    "Note, we also conveniently returned the number of files in the director for OOFs and SUBs respectively, it should be clear in this context that the number of files for OOF is the same as the number of files for SUB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0b6ae55b-02f4-47eb-aa78-c19e7664e7e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>y_trues</th>\n",
       "      <th>class_0_oof</th>\n",
       "      <th>class_1_oof</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id  y_trues  class_0_oof  class_1_oof\n",
       "0         1        1         0.70         0.30\n",
       "1         2        0         0.70         0.30\n",
       "2         3        0         0.65         0.35"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>y_trues</th>\n",
       "      <th>class_0_oof</th>\n",
       "      <th>class_1_oof</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id  y_trues  class_0_oof  class_1_oof\n",
       "0         1        1          0.6          0.4\n",
       "1         2        0          0.8          0.2\n",
       "2         3        0          0.4          0.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>y_trues</th>\n",
       "      <th>class_0_oof</th>\n",
       "      <th>class_1_oof</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id  y_trues  class_0_oof  class_1_oof\n",
       "0         1        1          0.1          0.8\n",
       "1         2        0          0.4          0.8\n",
       "2         3        0          0.9          0.7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 3 oof files.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oof_and_subs_path = \"./oof_and_subs\"\n",
    "oof_dfs_list, num_oofs = return_list_of_dataframes(path=oof_and_subs_path, is_oof=True)\n",
    "\n",
    "display(oof_dfs_list[0])\n",
    "display(oof_dfs_list[1])\n",
    "display(oof_dfs_list[2])\n",
    "print(f\"We have {num_oofs} oof files.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1259a781-970a-4598-a6dc-02ccc6f3f9b9",
   "metadata": {},
   "source": [
    "At this junction, we need to be clear of a few things. Let us use an example to illustrate.\n",
    "\n",
    "Let us say we trained our model over 5 folds and get our OOF predictions in terms of the **Macro-AUROC** with respect to the positive **class 1**. We then need to calculate our OOF scores with our corresponding y_trues.\n",
    "\n",
    "Just by eyeballing, we can deduce the **Macro-AUROC** score for the positive class as follows:\n",
    "\n",
    "```python\n",
    "oof_1_auroc = roc_auc_score([1,0,0], [0.3, 0.3, 0.35]) -> 0.25\n",
    "oof_2_auroc = roc_auc_score([1,0,0], [0.4, 0.2, 0.6])  -> 0.5\n",
    "oof_3_auroc = roc_auc_score([1,0,0], [0.8, 0.8, 0.7])  -> 0.75\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69549869-1c76-46c9-af65-31e1151d6c25",
   "metadata": {},
   "source": [
    "To compute the above, we can manually hardcode them, but for larger data, we will have to maintain a better data structure. We will store them into matrices and vectors (numpy) to compute the OOF scores efficiently.\n",
    "\n",
    "To do so, we define two variables:\n",
    "\n",
    "```python\n",
    "ground_truth_column_name = [\"y_trues\"]\n",
    "positive_class_oof_column_name = [\"class_1_oof\"]\n",
    "```\n",
    "where the first variable is the name of the column(s) for the ground truth, while the second is the name of the column(s) that we will be using to compute the OOF scores.\n",
    "\n",
    "We will then create a function `stack_oofs` to convert the list of OOF dataframes into an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5be03766-2c6f-4835-af2f-c39d06a7b742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_oofs(\n",
    "    oof_dfs: List[pd.DataFrame], pred_column_names: List[str]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Stack all oof predictions horziontally.\n",
    "\n",
    "    Args:\n",
    "        oof_dfs (List[pd.DataFrame]): The list of oof predictions in dataframes.\n",
    "        pred_column_names (List[str]): The list of prediction column names.\n",
    "\n",
    "    Returns:\n",
    "        all_oof_preds (np.ndarray): The stacked oof predictions of shape (num_samples, num_oofs * num_pred_columns).\n",
    "\n",
    "    Example:\n",
    "        >>> oof_1 = pd.DataFrame([1,2,3], columns=['class_1_oof'])\n",
    "        >>> oof_2 = pd.DataFrame([4,5,6], columns=['class_1_oof'])\n",
    "        >>> all_oof_preds = stack_oofs([oof_1, oof_2], ['class_1_oof'])\n",
    "        >>> all_oof_preds = np.array([1, 4], [2, 5], [3, 6])\n",
    "    \"\"\"\n",
    "    num_oofs = len(oof_dfs)\n",
    "    num_samples = len(oof_dfs[0])\n",
    "    num_target_cols = len(pred_column_names)\n",
    "    all_oof_preds = np.zeros((num_samples, num_oofs * num_target_cols))\n",
    "\n",
    "    if num_target_cols == 1:\n",
    "        for index, oof_df in enumerate(oof_dfs):\n",
    "            all_oof_preds[:, index : index + 1] = oof_df[\n",
    "                pred_column_names\n",
    "            ].values\n",
    "\n",
    "    elif num_target_cols > 1:\n",
    "        # Used in RANZCR where there are 11 target columns\n",
    "        for index, oof_df in enumerate(oof_dfs):\n",
    "            all_oof_preds[\n",
    "                :, index * num_target_cols : (index + 1) * num_target_cols\n",
    "            ] = oof_df[pred_column_names].values\n",
    "\n",
    "    return all_oof_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4b4053e2-354d-4f94-ab2a-d619bba9f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_column_name = [\"y_trues\"]\n",
    "positive_class_oof_column_name = [\"class_1_oof\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1126e3-99f5-4c58-a60d-d30d903d6fdf",
   "metadata": {},
   "source": [
    "In my oof files, I also saved the corresponding `y_trues` as a column, we thus take the first `oof_df` from `oof_dfs_list` and use it to get the `y_trues`, assuming they are the same for all oof files. Note of caution, if you use different resampling methods, you will need to change the `y_trues` accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a7427d0d-5efa-4ac4-b734-2ccc0b66dc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_trues shape: (3, 1)\n",
      "This variable is global and holds all ground truth.\n",
      "\n",
      "all_oof_preds shape: (3, 3)\n",
      "This variable is global and holds all oof predictions stacked horizontally.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_trues = oof_dfs_list[0][ground_truth_column_name].values\n",
    "all_oof_preds = stack_oofs(oof_dfs_list, positive_class_oof_column_name)\n",
    "\n",
    "print(\n",
    "    f\"y_trues shape: {y_trues.shape}\\nThis variable is global and holds all ground truth.\\n\"\n",
    ")\n",
    "print(\n",
    "    f\"all_oof_preds shape: {all_oof_preds.shape}\\nThis variable is global and holds all oof predictions stacked horizontally.\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f094f64-1782-4288-86a2-30b670dec4d1",
   "metadata": {},
   "source": [
    "After converting to numpy, we should have this following representation:\n",
    "\n",
    "$$\n",
    "\\textbf{y_true} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} \\in \\mathbb{R}^3 \\quad \\textbf{all_oof_preds} = \\begin{bmatrix} 0.3 & 0.4 & 0.8 \\\\ 0.3 & 0.2 & 0.8 \\\\ 0.35 & 0.6 & 0.7 \\end{bmatrix} \\in \\mathbb{R}^{3 \\times 3}\n",
    "$$\n",
    "\n",
    "where **y_true** is a $3 \\times 1$ column vector, each row entry is the corresponding ground truth for sample $i$ (i.e `y_true[0]` is ground truth for sample 1). \n",
    "\n",
    "For **all_oof_preds**, this is a $3 \\times 3$ matrix, where each **column** $i$ represents the OOF predictions made by model $i$. In other words, the first column $\\begin{bmatrix} 0.3 \\\\ 0.35 \\\\ 0.3 \\end{bmatrix}$ is the OOF predictions made by the first model. It is important to note that we are dealing with classification (binary or multiclass), so it is usually the case that both our target and predicted columns are just $1$. In **multi-label** however, we will have multiple target columns, and will be the focus in part II."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7f02aa-46b2-4506-8a15-cf5ebee4ea84",
   "metadata": {},
   "source": [
    "## Compute Scores of OOFs and Find the Best Score\n",
    "\n",
    "This part is very crucial, when we apply our **Hill Climbing (Forward Ensembling)** technique here, we want to initialize with the best models first and **iteratively blend with the rest**.\n",
    "\n",
    "In other words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6d4ebd04-6f21-4d1a-b882-6d21426a3158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_best_oof(\n",
    "    all_oof_preds: np.ndarray,\n",
    "    y_trues: np.ndarray,\n",
    "    num_oofs: int,\n",
    "    performance_metric: Callable,\n",
    ") -> Tuple[float, int]:\n",
    "    \"\"\"Compute the oof score of all models using a performance metric and return the best model index and score.\n",
    "\n",
    "    Args:\n",
    "        all_oof_preds (np.ndarray): The stacked oof predictions of shape (num_samples, num_oofs * num_pred_columns). Taken from stack_oofs.\n",
    "        y_trues (np.ndarray): The true labels of shape (num_samples, num_target_cols).\n",
    "        num_oofs (int): The number of oof predictions.\n",
    "        performance_metric (Callable): The performance metric to use, this is a function.\n",
    "\n",
    "    Returns:\n",
    "        best_oof_metric_score (float): The best oof score.\n",
    "        best_model_index (int): The index of the best model.\n",
    "    \"\"\"\n",
    "    all_oof_scores = []\n",
    "    for k in range(num_oofs):\n",
    "        oof_k = all_oof_preds[:, k].reshape(-1,1)\n",
    "        metric_score = performance_metric(\n",
    "            y_trues,\n",
    "            oof_k,\n",
    "            num_target_cols=1,\n",
    "            multilabel=False,\n",
    "        )\n",
    "        all_oof_scores.append(metric_score)\n",
    "        print(f\"Model {k} has OOF AUC = {metric_score}\")\n",
    "\n",
    "    best_oof_metric_score, best_oof_index = np.max(all_oof_scores), np.argmax(\n",
    "        all_oof_scores\n",
    "    )\n",
    "    return best_oof_metric_score, best_oof_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "886e8b66-9b38-4d41-9fef-0f557307b343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_multilabel_auc(\n",
    "    label, pred, num_target_cols: int = 1, multilabel: bool = False\n",
    "):\n",
    "    \"\"\"Also works for binary AUC like Melanoma\"\"\"\n",
    "\n",
    "    if not multilabel:\n",
    "        return roc_auc_score(label, pred)\n",
    "    else:\n",
    "        aucs = []\n",
    "        for i in range(num_target_cols):\n",
    "            print(label[:, i])\n",
    "            print()\n",
    "            print(pred[:, i])\n",
    "            print(roc_auc_score(label[:, i], pred[:, i]))\n",
    "        aucs.append(roc_auc_score(label, pred))\n",
    "\n",
    "        return np.mean(aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "fb10b9c0-7ffd-4cb0-bf9e-a85297093581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0 has OOF AUC = 0.25\n",
      "Model 1 has OOF AUC = 0.5\n",
      "Model 2 has OOF AUC = 0.75\n",
      "\n",
      "### Computing Best OOF scores among all models ###\n",
      "The best OOF AUC score is 0.75 and the best model index is 2 corresponding to the oof file oof_3.csv\n"
     ]
    }
   ],
   "source": [
    "best_oof_metric_score, best_oof_index = compute_best_oof(\n",
    "    all_oof_preds=all_oof_preds,\n",
    "    y_trues=y_trues,\n",
    "    num_oofs=num_oofs,\n",
    "    performance_metric=macro_multilabel_auc,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\n### Computing Best OOF scores among all models ###\\nThe best OOF AUC score is {best_oof_metric_score} and the best model index is {best_oof_index} corresponding to the oof file {oof_files_sorted[best_oof_index]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e6c99e-97fe-4cc0-98a7-4d939313aab1",
   "metadata": {},
   "source": [
    "## The Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377739bc-3892-49a7-99c9-893d65268021",
   "metadata": {},
   "source": [
    "Now the logic flows as follows:\n",
    "\n",
    "- Start with the model with the best OOF score: we get that from `compute_best_oof` which yields us the key variables `best_oof_index`. \n",
    "    - `best_oof_index`: 2 indicates that our oof_2 (model 2) has the best OOF score amongst the 3 OOFs (models), as we can see from above, it is indeed the case since Model 2 has OOF AUC of $0.75$.\n",
    "- We now need to blend **Model 2** with **Model 0** and **Model 1** respectively to find out which weight combination gives a better OOF score.\n",
    "    - We need to define a variable `weight_interval`, which tells us the weights to sample from. For example, if `weight_interval` is $3$, then we will uniformly sample the weights $\\frac{0}{3}, \\frac{1}{3}, \\frac{2}{3}$. Let us see a concrete example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e102ded9-5c7a-4d01-8216-65d1c5619e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_interval = 3\n",
    "patience_counter = 0\n",
    "model_0_oof = all_oof_preds[:, 0].reshape(y_trues.shape)\n",
    "model_1_oof = all_oof_preds[:, 1].reshape(y_trues.shape)\n",
    "model_2_oof = all_oof_preds[:, best_oof_index].reshape(y_trues.shape) # best_oof_index=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839b52e6-1526-429f-a1de-609c4ab0d8a2",
   "metadata": {},
   "source": [
    "We define\n",
    "\n",
    "```python\n",
    "best_oof_index_list = [best_oof_index]\n",
    "best_weights_list = []\n",
    "```\n",
    "\n",
    "which keeps track two lists which holds the best OOFs to blend with and their corresponding weights.\n",
    "\n",
    "We use a naive example to illustrate:\n",
    "\n",
    "```python\n",
    "best_oof_index_list = [2, 0, 1]\n",
    "best_weights_list = [0.3, 0.8]\n",
    "all_oof_preds_matrix = [[0.3, 0.4, 0.8],\n",
    "                        [0.3, 0.2, 0.8],\n",
    "                        [0.35, 0.6, 0.7]]\n",
    "initial_best_vector = A_matrix[:, 2]\n",
    "```\n",
    "\n",
    "In other words, if after our Hill Climbing algorithm, we have the above variables and to calculate the **final blended OOF** using the above optimal weights, we have:\n",
    "\n",
    "Let $\\mathbf{c_i}$ be the columns of the matrix **all_oof_preds_matrix**, you can think $\\mathbf{c_i}$ as the Model $i$'s OOFs predictions, since in this naive example, we see that our initial best model is Model 2.\n",
    "\n",
    "$$\n",
    "((1-0.3) \\times \\mathbf{c_2} + 0.3 \\times \\mathbf{c_0}) \\times (1-0.8) + 0.8 \\times \\mathbf{c_1}\n",
    "$$\n",
    "\n",
    "1. To dissect clearly, we start with the first index, which is Model 2, and blend with the next index, which is Model 0, they will be blended with weights of $0.3$, which means $(1-0.3) \\times \\mathbf{c_2} + 0.3 \\times \\mathbf{c_0}$. We call it **oof_blend_1** which indicates the linear combination of the weights of the first blend. Note that this results in a new and better \"OOF predictions\".\n",
    "2. The next blend will be of **oof_blend_1** with Model 1, with a weight of $0.8$ and the assignment is $(1-0.8) \\times \\textbf{oof_blend_1} + 0.8 \\times \\mathbf{c_0}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "1a7d8a96-7a0b-4220-abbb-a0d705f32cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Tracked Model List: [2]\n",
      "Current Weights List: []\n"
     ]
    }
   ],
   "source": [
    "best_oof_index_list = [best_oof_index]\n",
    "best_weights_list = []\n",
    "print(f\"Current Tracked Model List: {best_oof_index_list}\")\n",
    "print(f\"Current Weights List: {best_weights_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f66dc94-2a16-47ba-8e34-0a7a9c0fa57e",
   "metadata": {},
   "source": [
    "### First Round - Blending Model 2 with Model 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b43081-dd47-44e3-83f0-8241e4e394f6",
   "metadata": {},
   "source": [
    "- We check the blending results of Model 2 and Model 0 over 3 weights: $0, \\frac{1}{3}, \\frac{2}{3}$. \n",
    "- We also assign 3 variables\n",
    "```python\n",
    "running_best_score, running_best_weight, running_best_oof_index = best_oof_metric_score, 0, 0\n",
    "```\n",
    "such that whenever a blend of $w_1 \\times \\textbf{model_i_oof} + (1-w_2) \\times \\textbf{model_j_oof}$ produces a better OOF score, then we will assign \n",
    "\n",
    "```python\n",
    "if temp_ensemble_oof_score >= running_best_score:\n",
    "    print(f\"The blend of weight {temp_weight} of model 2 and model 0 led to a greater or equals to OOF score = {temp_ensemble_oof_score}\\n\")\n",
    "    running_best_score = temp_ensemble_oof_score\n",
    "    running_best_weight = temp_weight\n",
    "    running_best_oof_index = 0\n",
    "```\n",
    "\n",
    "Notice that we **hardcoded** the `running_best_oof_index` to be $0$ since we know we are only looking at the interaction of Model 2 and Model 0. In proper code, this part should not be hardcoded and you can refer to my full code for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "4684163b-7fcc-44df-8fc8-fb5c9169b3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight = 0.0\n",
      "blended OOF score with model_0_oof = 0.75\n",
      "The blend of weight 0.0 of model 2 and model 0 led to a greater or equals to OOF score = 0.75\n",
      "\n",
      "weight = 0.3333333333333333\n",
      "blended OOF score with model_0_oof = 0.75\n",
      "The blend of weight 0.3333333333333333 of model 2 and model 0 led to a greater or equals to OOF score = 0.75\n",
      "\n",
      "weight = 0.6666666666666666\n",
      "blended OOF score with model_0_oof = 0.5\n"
     ]
    }
   ],
   "source": [
    "running_best_score, running_best_weight, running_best_oof_index = best_oof_metric_score, 0, 0\n",
    "\n",
    "for weight in range(weight_interval):\n",
    "    temp_weight = weight / weight_interval\n",
    "    print(f\"weight = {temp_weight}\")\n",
    "\n",
    "    temp_ensemble_oof_preds = (\n",
    "        temp_weight * model_0_oof + (1 - temp_weight) * model_2_oof\n",
    "    )\n",
    "\n",
    "    temp_ensemble_oof_score = macro_multilabel_auc(\n",
    "        y_trues,\n",
    "        temp_ensemble_oof_preds,\n",
    "        num_target_cols=1,\n",
    "        multilabel=False,\n",
    "    )\n",
    "    print(f\"blended OOF score with model_0_oof = {temp_ensemble_oof_score}\")\n",
    "    \n",
    "    if temp_ensemble_oof_score >= running_best_score:\n",
    "        running_best_score = temp_ensemble_oof_score\n",
    "        running_best_weight = temp_weight\n",
    "        running_best_oof_index = 0\n",
    "        print(f\"The blend of weight {temp_weight} of model 2 and model {running_best_oof_index} led to a greater or equals to OOF score = {temp_ensemble_oof_score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23189200-7c89-49e5-a523-2324a1727506",
   "metadata": {},
   "source": [
    "So now we finished blending Model 2 and Model 0 and we have:\n",
    "\n",
    "```python\n",
    "running_best_score = 0.75\n",
    "running_best_weight = 0.333...\n",
    "```\n",
    "\n",
    "We need to blend Model 2 and Model 1 now to see if Model 2 and Model 1 can give better OOF scores when blended! We repeat the exact same loop as above, but bear in mind that the `running_best_score` and `running_best_weight` is already updated to the ones we got in the blend of Model 2 and Model 0 because we want to check if Model 2 and Model 1's blend can give better results than the previous `running_best_score`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ebaf1d-e89f-44de-931c-352aff0f7643",
   "metadata": {},
   "source": [
    "### First Round - Blending Model 2 and Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "1885ebe5-a089-4db4-9e73-bbce6acea9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight = 0.0\n",
      "blended OOF score with model_1_oof = 0.75\n",
      "The blend of weight 0.0 of model 2 and model 1 led to a greater or equals to OOF score = 0.75\n",
      "\n",
      "weight = 0.3333333333333333\n",
      "blended OOF score with model_1_oof = 1.0\n",
      "The blend of weight 0.3333333333333333 of model 2 and model 1 led to a greater or equals to OOF score = 1.0\n",
      "\n",
      "weight = 0.6666666666666666\n",
      "blended OOF score with model_1_oof = 0.5\n"
     ]
    }
   ],
   "source": [
    "for weight in range(weight_interval):\n",
    "    temp_weight = weight / weight_interval\n",
    "    print(f\"weight = {temp_weight}\")\n",
    "    temp_ensemble_oof_preds = (\n",
    "        temp_weight * model_1_oof + (1 - temp_weight) * model_2_oof\n",
    "    )\n",
    "\n",
    "    temp_ensemble_oof_score = macro_multilabel_auc(\n",
    "        y_trues,\n",
    "        temp_ensemble_oof_preds,\n",
    "        num_target_cols=1,\n",
    "        multilabel=False,\n",
    "    )\n",
    "    print(f\"blended OOF score with model_1_oof = {temp_ensemble_oof_score}\")\n",
    "    \n",
    "    if temp_ensemble_oof_score >= running_best_score:\n",
    "        running_best_score = temp_ensemble_oof_score\n",
    "        running_best_weight = temp_weight\n",
    "        running_best_oof_index = 1\n",
    "        print(f\"The blend of weight {temp_weight} of model 2 and model {running_best_oof_index} led to a greater or equals to OOF score = {temp_ensemble_oof_score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0809c51-9778-421f-bb90-6268a09aae9a",
   "metadata": {},
   "source": [
    "So now we finished blending Model 2 and Model 1 and we have:\n",
    "\n",
    "```python\n",
    "running_best_score = 1\n",
    "running_best_weight = 0.333...\n",
    "```\n",
    "\n",
    "and we have a new winner in town! After our first round of iterating our **initial best OOF Model 2** with the rest (Model 0 and 1), we found out that if we take $w_1 = 1 - \\frac{2}{3}$ and $w_2 = \\frac{1}{3}$, Model 2 and 1 gives us a better overall score.\n",
    "\n",
    "That is to say:\n",
    "\n",
    "$$\n",
    "w_1 * \\textbf{OOF_2} + w_2 \\times \\textbf{OOF_1}\n",
    "$$\n",
    "\n",
    "leads to the greatest increase in our OOF score!\n",
    "\n",
    "Notice that we **hardcoded** the `running_best_oof_index` to be $1$ since we know we are only looking at the interaction of Model 2 and Model 1. In proper code, this part should not be hardcoded and you can refer to my full code for clarity.\n",
    "\n",
    "Technically, we can stop the algorithm now since the metric **Macro-AUROC** is capped at $1$, but for the sake of explanation, let us continue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3984b2a-a991-477d-9fc3-ce351d0df033",
   "metadata": {},
   "source": [
    "### First Round - Save Results for Loop 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95bf6c7-c7a6-4520-88db-bff4d41b4856",
   "metadata": {},
   "source": [
    "We then append the best OOF index and the corresponding weight to the `best_oof_index_list` and `best_weights_list` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "cdd69d8f-26ab-4fe9-bf1d-df2082cec86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Tracked Model List: [2, 1]\n",
      "Current Weights List: [0.3333333333333333]\n"
     ]
    }
   ],
   "source": [
    "best_oof_index_list.append(running_best_oof_index)\n",
    "best_weights_list.append(running_best_weight)\n",
    "\n",
    "print(f\"Current Tracked Model List: {best_oof_index_list}\")\n",
    "print(f\"Current Weights List: {best_weights_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af3282a-4bba-4196-a909-f24b00c8c304",
   "metadata": {},
   "source": [
    "### Second Round - Blend OOF\n",
    "\n",
    "Now we have a brand new OOF after blending Model 2 and Model 1, we call it **blended_oof_1** and note that this is our new best OOF!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "05403c8e-553a-4f39-b43e-cef7c1a70b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blended_oof_1 = all_oof_preds[:, best_oof_index_list[0]].reshape(-1, 1) * (1 - best_weights_list[0]) + all_oof_preds[:, best_oof_index_list[1]].reshape(-1, 1) * best_weights_list[0]\n",
    "blended_oof_1 = model_2_oof * (1 - 1/3) + model_1_oof * (1/3)\n",
    "assert macro_multilabel_auc(y_trues, blended_oof_1) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27980d7b-541c-4217-a51b-87113d6baa9f",
   "metadata": {},
   "source": [
    "### Second Round - Blending blended_oof_1 with Model 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45e13fc-82aa-4b2e-aa89-cdeb5179a5a0",
   "metadata": {},
   "source": [
    "We continue to try out **blended_oof_1** with the rest of the models that were not selected. This means we have to check our **Current Tracked Model List** `best_oof_index_list` and see that we already have $[2, 1]$ being used up, in our simple example here, there only left with Model 0 to try! So make sure in your code you do not try **blended_oof_1** with Model 0, 1 and 2 again since the **blended_oof_1** is already made up with Model 1 and 2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "e415256b-8993-4c3f-bc7c-37f8249bdd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "weight = 0.0\n",
      "blended OOF score with model_0_oof = 1.0\n",
      "The blend of weight 0.0 of model 2 and model 0 led to a greater or equals to OOF score = 1.0\n",
      "\n",
      "\n",
      "weight = 0.3333333333333333\n",
      "blended OOF score with model_0_oof = 0.5\n",
      "\n",
      "weight = 0.6666666666666666\n",
      "blended OOF score with model_0_oof = 0.5\n"
     ]
    }
   ],
   "source": [
    "for weight in range(weight_interval):\n",
    "    temp_weight = weight / weight_interval\n",
    "    print(f\"\\nweight = {temp_weight}\")\n",
    "\n",
    "    temp_ensemble_oof_preds = (\n",
    "        temp_weight * model_0_oof + (1 - temp_weight) * blended_oof_1\n",
    "    )\n",
    "\n",
    "    temp_ensemble_oof_score = macro_multilabel_auc(\n",
    "        y_trues,\n",
    "        temp_ensemble_oof_preds,\n",
    "        num_target_cols=1,\n",
    "        multilabel=False,\n",
    "    )\n",
    "    print(f\"blended OOF score with model_0_oof = {temp_ensemble_oof_score}\")\n",
    "    \n",
    "    if temp_ensemble_oof_score >= running_best_score:\n",
    "        running_best_score = temp_ensemble_oof_score\n",
    "        running_best_weight = temp_weight\n",
    "        running_best_oof_index = 0\n",
    "        print(f\"The blend of weight {temp_weight} of model 2 and model {running_best_oof_index} led to a greater or equals to OOF score = {temp_ensemble_oof_score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454b8763-821c-4c4e-974f-0b333985cc1b",
   "metadata": {},
   "source": [
    "Since we have done checking **blended_oof_1** with the last remaining Model 0 and found that blending Model 0 with a weight of 0 (what a surprise haha!) yields the best result, we once again update the `running` metrics and also append to our global lists below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "1b2c10bb-ab5b-400b-a3a1-a9738700faca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Tracked Model List: [2, 1, 0]\n",
      "Current Weights List: [0.3333333333333333, 0.0]\n"
     ]
    }
   ],
   "source": [
    "best_oof_index_list.append(running_best_oof_index)\n",
    "best_weights_list.append(running_best_weight)\n",
    "\n",
    "print(f\"Current Tracked Model List: {best_oof_index_list}\")\n",
    "print(f\"Current Weights List: {best_weights_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "b6c30e16-a90f-43ce-8258-38b00bb81c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blended_oof_2 = blended_oof_1 * (1 - best_weights_list[1]) + all_oof_preds[:, best_oof_index_list[2]].reshape(-1, 1) * best_weights_list[1]\n",
    "blended_oof_2 = blended_oof_1 * (1 - 0.0) + model_0_oof.reshape(-1, 1) * 0.0\n",
    "assert macro_multilabel_auc(y_trues, blended_oof_2) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed873edc-3652-490b-b690-21bca47caf32",
   "metadata": {},
   "source": [
    "## Ensembling Model Predictions with the Found Optimal Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70af1fd9-337b-476c-b245-e7782a3d6f04",
   "metadata": {},
   "source": [
    "So we end the discussion with what to do with the weights we got. We already established to the readers that our **Initial Best OOF is Model 2** with a **Macro-AUROC** score of $0.75$, and by way of Hill Climbing, we found out that we can blend the 3 Models with some weights such that their new OOF produces a **Macro-AUROC** score of $1.0$, a huge improvement. We aren't done yet! We want to apply these optimal weights to our **test set predictions** as well. Note that our **test set predictions** are **unseen** and our usual ensemble methods can be as simple as mean averaging.\n",
    "\n",
    "More concretely, let us check out the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "759a0555-ae1e-49e0-aa3e-46cfd21e65ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>class_0_preds</th>\n",
       "      <th>class_1_preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id  class_0_preds  class_1_preds\n",
       "0         4            0.2            0.8\n",
       "1         5            0.3            0.7\n",
       "2         6            0.6            0.4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>class_0_preds</th>\n",
       "      <th>class_1_preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id  class_0_preds  class_1_preds\n",
       "0         4            0.3            0.7\n",
       "1         5            0.9            0.1\n",
       "2         6            0.2            0.8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>class_0_preds</th>\n",
       "      <th>class_1_preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id  class_0_preds  class_1_preds\n",
       "0         4            0.3            0.7\n",
       "1         5            0.7            0.3\n",
       "2         6            0.8            0.2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 3 sub files.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oof_and_subs_path = \"./oof_and_subs\"\n",
    "sub_dfs_list, num_subs = return_list_of_dataframes(path=oof_and_subs_path, is_oof=False)\n",
    "\n",
    "display(sub_dfs_list[0])\n",
    "display(sub_dfs_list[1])\n",
    "display(sub_dfs_list[2])\n",
    "print(f\"We have {num_subs} sub files.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6765bc79-678d-4892-bbec-4ba0e0b2d7f6",
   "metadata": {},
   "source": [
    "The `stack_subs` function does the same thing as `stack_oofs` so they can be combined into one function for code clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "82fcb006-1846-49de-b90a-a1d90fd1b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_subs(\n",
    "    sub_dfs: List[pd.DataFrame], pred_column_names: List[str]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Stack all sub predictions horziontally.\n",
    "\n",
    "    Args:\n",
    "        sub_dfs (List[pd.DataFrame]): The list of sub predictions in dataframes.\n",
    "        pred_column_names (List[str]): The list of prediction column names.\n",
    "\n",
    "    Returns:\n",
    "        all_sub_preds (np.ndarray): The stacked oof predictions of shape (num_samples, num_subs * num_pred_columns).\n",
    "    \"\"\"\n",
    "    num_subs = len(sub_dfs)\n",
    "    num_samples = len(sub_dfs[0])\n",
    "    num_target_cols = len(pred_column_names)\n",
    "    all_sub_preds = np.zeros((num_samples, num_subs * num_target_cols))\n",
    "\n",
    "    if num_target_cols == 1:\n",
    "        for index, sub_df in enumerate(sub_dfs):\n",
    "            all_sub_preds[:, index : index + 1] = sub_df[\n",
    "                pred_column_names\n",
    "            ].values\n",
    "\n",
    "    elif num_target_cols > 1:\n",
    "        # Used in RANZCR where there are 11 target columns\n",
    "        for index, sub_df in enumerate(sub_dfs):\n",
    "            all_sub_preds[\n",
    "                :, index * num_target_cols : (index + 1) * num_target_cols\n",
    "            ] = sub_df[pred_column_names].values\n",
    "\n",
    "    return all_sub_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "26012c82-fd4c-4501-94b6-2b694a59d6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_target_column_name = [\"class_1_preds\"]\n",
    "all_subs_preds = stack_subs(sub_dfs_list, test_set_target_column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "7a047436-a148-4567-9e94-b72f4608af81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0_sub = all_subs_preds[:, 0].reshape(y_trues.shape)\n",
    "model_1_sub = all_subs_preds[:, 1].reshape(y_trues.shape)\n",
    "model_2_sub = all_subs_preds[:, 2].reshape(y_trues.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7b91d6-57fc-4bc6-87f3-f522acf2f9ee",
   "metadata": {},
   "source": [
    "Recall the optimal weights earlier:\n",
    "\n",
    "```python\n",
    "Current Tracked Model List: [2, 1, 0]\n",
    "Current Weights List: [0.3333333333333333, 0.0]\n",
    "```\n",
    "\n",
    "and we now have a way to ensemble our model subs accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "73a5058b-0867-4bf9-b205-bef04b33baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blended_oof_1 = all_oof_preds[:, best_oof_index_list[0]].reshape(-1, 1) * (1 - best_weights_list[0]) + all_oof_preds[:, best_oof_index_list[1]].reshape(-1, 1) * best_weights_list[0]\n",
    "blended_sub_1 = model_2_sub * (1 - 1/3) + model_1_sub * (1/3)\n",
    "blended_sub_2 = blended_sub_1 * (1-0.0) + model_0_sub * 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "8d916a87-a57c-4fcf-87c5-1b7fae0ecad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7       ],\n",
       "       [0.23333333],\n",
       "       [0.4       ]])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blended_sub_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297e29ee-9df4-4ed5-a103-20590fb0165f",
   "metadata": {},
   "source": [
    "Then `blended_sub_2` should be our final **test set predictions**!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
